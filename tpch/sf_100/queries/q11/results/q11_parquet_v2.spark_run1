 -- the query
insert into table q11_part_tmp_par
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
    join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
    join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par) sum_tmp
    join q11_part_tmp_par
where part_value > total_value * 0.000001
order by value desc; 
15/08/21 19:46:30 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 19:46:30 INFO metastore: Connected to metastore.
15/08/21 19:46:31 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/21 19:46:31 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:31 INFO SparkContext: Running Spark version 1.4.1
15/08/21 19:46:31 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:31 INFO SecurityManager: Changing view acls to: hive
15/08/21 19:46:31 INFO SecurityManager: Changing modify acls to: hive
15/08/21 19:46:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/21 19:46:32 INFO Slf4jLogger: Slf4jLogger started
15/08/21 19:46:32 INFO Remoting: Starting remoting
15/08/21 19:46:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:49543]
15/08/21 19:46:33 INFO Utils: Successfully started service 'sparkDriver' on port 49543.
15/08/21 19:46:33 INFO SparkEnv: Registering MapOutputTracker
15/08/21 19:46:33 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:33 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:33 INFO SparkEnv: Registering BlockManagerMaster
15/08/21 19:46:33 INFO DiskBlockManager: Created local directory at /tmp/spark-07b7866a-0531-4123-9f8f-3926e05d7110/blockmgr-417ef505-7212-4569-8bae-01a6403ef683
15/08/21 19:46:33 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/21 19:46:33 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:33 INFO HttpFileServer: HTTP File server directory is /tmp/spark-07b7866a-0531-4123-9f8f-3926e05d7110/httpd-6ac50840-8139-4318-93c2-05652b1dddef
15/08/21 19:46:33 INFO HttpServer: Starting HTTP Server
15/08/21 19:46:33 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 19:46:33 INFO AbstractConnector: Started SocketConnector@0.0.0.0:33140
15/08/21 19:46:33 INFO Utils: Successfully started service 'HTTP file server' on port 33140.
15/08/21 19:46:33 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/21 19:46:33 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 19:46:33 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/21 19:46:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/21 19:46:33 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/21 19:46:34 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:34 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:34 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:34 INFO Executor: Starting executor ID driver on host localhost
15/08/21 19:46:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54384.
15/08/21 19:46:34 INFO NettyBlockTransferService: Server created on 54384
15/08/21 19:46:34 INFO BlockManagerMaster: Trying to register BlockManager
15/08/21 19:46:34 INFO BlockManagerMasterEndpoint: Registering block manager localhost:54384 with 20.7 GB RAM, BlockManagerId(driver, localhost, 54384)
15/08/21 19:46:34 INFO BlockManagerMaster: Registered BlockManager
15/08/21 19:46:34 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 19:46:34 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/21 19:46:35 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/21 19:46:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/21 19:46:36 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 19:46:36 INFO metastore: Connected to metastore.
15/08/21 19:46:37 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/21 19:46:37 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/21 19:46:37 INFO ParseDriver: Parsing command: -- the query
insert into table q11_part_tmp_par
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
    join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
    join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey
15/08/21 19:46:38 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/21 19:46:41 INFO MemoryStore: ensureFreeSpace(326528) called with curMem=0, maxMem=22226833244
15/08/21 19:46:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 19:46:42 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=326528, maxMem=22226833244
15/08/21 19:46:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 19:46:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:54384 (size: 22.3 KB, free: 20.7 GB)
15/08/21 19:46:42 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/21 19:46:42 INFO MemoryStore: ensureFreeSpace(326584) called with curMem=349321, maxMem=22226833244
15/08/21 19:46:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 19:46:42 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=675905, maxMem=22226833244
15/08/21 19:46:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 19:46:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:54384 (size: 22.3 KB, free: 20.7 GB)
15/08/21 19:46:42 INFO SparkContext: Created broadcast 1 from processCmd at CliDriver.java:423
15/08/21 19:46:42 INFO MemoryStore: ensureFreeSpace(326584) called with curMem=698698, maxMem=22226833244
15/08/21 19:46:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 19:46:42 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1025282, maxMem=22226833244
15/08/21 19:46:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 19:46:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:54384 (size: 22.3 KB, free: 20.7 GB)
15/08/21 19:46:42 INFO SparkContext: Created broadcast 2 from processCmd at CliDriver.java:423
15/08/21 19:46:42 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/21 19:46:42 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/21 19:46:42 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 19:46:42 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 19:46:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 19:46:43 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/21 19:46:43 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 19:46:43 INFO DAGScheduler: Got job 0 (run at ThreadPoolExecutor.java:1145) with 1 output partitions (allowLocal=false)
15/08/21 19:46:43 INFO DAGScheduler: Final stage: ResultStage 0(run at ThreadPoolExecutor.java:1145)
15/08/21 19:46:43 INFO DAGScheduler: Parents of final stage: List()
15/08/21 19:46:43 INFO DAGScheduler: Missing parents: List()
15/08/21 19:46:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/21 19:46:43 INFO MemoryStore: ensureFreeSpace(6144) called with curMem=1048075, maxMem=22226833244
15/08/21 19:46:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.0 KB, free 20.7 GB)
15/08/21 19:46:43 INFO MemoryStore: ensureFreeSpace(3283) called with curMem=1054219, maxMem=22226833244
15/08/21 19:46:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.2 KB, free 20.7 GB)
15/08/21 19:46:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:54384 (size: 3.2 KB, free: 20.7 GB)
15/08/21 19:46:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/21 19:46:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at run at ThreadPoolExecutor.java:1145)
15/08/21 19:46:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/08/21 19:46:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1682 bytes)
15/08/21 19:46:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/21 19:46:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 2330 length: 2330 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1856 bytes result sent to driver
15/08/21 19:46:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 430 ms on localhost (1/1)
15/08/21 19:46:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/21 19:46:43 INFO DAGScheduler: ResultStage 0 (run at ThreadPoolExecutor.java:1145) finished in 0.453 s
15/08/21 19:46:43 INFO DAGScheduler: Job 0 finished: run at ThreadPoolExecutor.java:1145, took 0.596042 s
15/08/21 19:46:43 INFO MemoryStore: ensureFreeSpace(344) called with curMem=1057502, maxMem=22226833244
15/08/21 19:46:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 344.0 B, free 20.7 GB)
15/08/21 19:46:43 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@ec0c06f
15/08/21 19:46:43 INFO MemoryStore: ensureFreeSpace(176) called with curMem=1057846, maxMem=22226833244
15/08/21 19:46:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 176.0 B, free 20.7 GB)
15/08/21 19:46:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:54384 (size: 176.0 B, free: 20.7 GB)
15/08/21 19:46:43 INFO SparkContext: Created broadcast 4 from run at ThreadPoolExecutor.java:1145
15/08/21 19:46:43 INFO StatsReportListener: task runtime:(count: 1, mean: 430.000000, stdev: 0.000000, max: 430.000000, min: 430.000000)
15/08/21 19:46:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:46:43 INFO StatsReportListener: 	430.0 ms	430.0 ms	430.0 ms	430.0 ms	430.0 ms	430.0 ms	430.0 ms	430.0 ms	430.0 ms
15/08/21 19:46:43 INFO StatsReportListener: task result size:(count: 1, mean: 1856.000000, stdev: 0.000000, max: 1856.000000, min: 1856.000000)
15/08/21 19:46:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:46:43 INFO StatsReportListener: 	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B
15/08/21 19:46:43 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 68.604651, stdev: 0.000000, max: 68.604651, min: 68.604651)
15/08/21 19:46:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:46:43 INFO StatsReportListener: 	69 %	69 %	69 %	69 %	69 %	69 %	69 %	69 %	69 %
15/08/21 19:46:43 INFO StatsReportListener: other time pct: (count: 1, mean: 31.395349, stdev: 0.000000, max: 31.395349, min: 31.395349)
15/08/21 19:46:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:46:43 INFO StatsReportListener: 	31 %	31 %	31 %	31 %	31 %	31 %	31 %	31 %	31 %
15/08/21 19:46:43 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/21 19:46:43 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/21 19:46:43 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/21 19:46:43 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/21 19:46:43 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/21 19:46:43 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 19:46:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:46:44 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 19:46:44 INFO DAGScheduler: Registering RDD 9 (processCmd at CliDriver.java:423)
15/08/21 19:46:44 INFO DAGScheduler: Registering RDD 14 (processCmd at CliDriver.java:423)
15/08/21 19:46:44 INFO DAGScheduler: Registering RDD 20 (processCmd at CliDriver.java:423)
15/08/21 19:46:44 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 19:46:44 INFO DAGScheduler: Final stage: ResultStage 4(processCmd at CliDriver.java:423)
15/08/21 19:46:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
15/08/21 19:46:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
15/08/21 19:46:44 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 19:46:44 INFO MemoryStore: ensureFreeSpace(6536) called with curMem=1058022, maxMem=22226833244
15/08/21 19:46:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.4 KB, free 20.7 GB)
15/08/21 19:46:44 INFO MemoryStore: ensureFreeSpace(3554) called with curMem=1064558, maxMem=22226833244
15/08/21 19:46:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KB, free 20.7 GB)
15/08/21 19:46:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:54384 (size: 3.5 KB, free: 20.7 GB)
15/08/21 19:46:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:874
15/08/21 19:46:44 INFO DAGScheduler: Submitting 39 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423)
15/08/21 19:46:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 39 tasks
15/08/21 19:46:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, ANY, 1694 bytes)
15/08/21 19:46:44 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 19:46:44 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, ANY, 1699 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, ANY, 1708 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, localhost, ANY, 1694 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, localhost, ANY, 1699 bytes)
15/08/21 19:46:44 INFO MemoryStore: ensureFreeSpace(9520) called with curMem=1068112, maxMem=22226833244
15/08/21 19:46:44 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.3 KB, free 20.7 GB)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, localhost, ANY, 1707 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, localhost, ANY, 1693 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, localhost, ANY, 1699 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, localhost, ANY, 1709 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, localhost, ANY, 1693 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, localhost, ANY, 1699 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, localhost, ANY, 1708 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, localhost, ANY, 1694 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, localhost, ANY, 1699 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, localhost, ANY, 1707 bytes)
15/08/21 19:46:44 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, localhost, ANY, 1694 bytes)
15/08/21 19:46:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/08/21 19:46:44 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
15/08/21 19:46:44 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
15/08/21 19:46:44 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
15/08/21 19:46:44 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
15/08/21 19:46:44 INFO MemoryStore: ensureFreeSpace(4935) called with curMem=1077632, maxMem=22226833244
15/08/21 19:46:44 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
15/08/21 19:46:44 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.8 KB, free 20.7 GB)
15/08/21 19:46:44 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
15/08/21 19:46:44 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
15/08/21 19:46:44 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:54384 (size: 4.8 KB, free: 20.7 GB)
15/08/21 19:46:44 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
15/08/21 19:46:44 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:874
15/08/21 19:46:44 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
15/08/21 19:46:44 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
15/08/21 19:46:44 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
15/08/21 19:46:44 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
15/08/21 19:46:44 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
15/08/21 19:46:44 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:423)
15/08/21 19:46:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks
15/08/21 19:46:44 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
15/08/21 19:46:44 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 268435456 end: 334658433 length: 66222977 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 268435456 end: 336837881 length: 68402425 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 268435456 end: 335698071 length: 67262615 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 268435456 end: 336808364 length: 68372908 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 268435456 end: 334660800 length: 66225344 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
21-Aug-2015 19:46:39 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 19:46:39 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 19:46:39 INFO: parquet.hadoop.ParquetFileReader: reading another 1 footers
21-Aug-2015 19:46:39 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 19:46:40 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 19:46:40 INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
21-Aug-2015 19:46:40 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 19:46:40 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 19:46:40 INFO: parquet.hadoop.ParquetFileReader: reading another 13 footers
21-Aug-2015 19:46:40 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 19:46:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
21-Aug-2015 19:46:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 32 ms. row count = 25
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464958 records.
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465101 records.
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243981 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203347 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465498 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465186 records.
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1222637 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464663 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465498 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464967 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465546 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465634 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465108 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243920 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203579 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464813 records.
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at r15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:45 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 19:46:50 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2125 bytes result sent to driver
15/08/21 19:46:50 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, localhost, ANY, 1699 bytes)
15/08/21 19:46:50 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
15/08/21 19:46:50 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2125 bytes result sent to driver
15/08/21 19:46:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:50 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, localhost, ANY, 1707 bytes)
15/08/21 19:46:50 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
15/08/21 19:46:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 268435456 end: 336808298 length: 68372842 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:50 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 6548 ms on localhost (1/39)
15/08/21 19:46:50 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2125 bytes result sent to driver
15/08/21 19:46:50 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, localhost, ANY, 1694 bytes)
15/08/21 19:46:50 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
15/08/21 19:46:50 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 6566 ms on localhost (2/39)
15/08/21 19:46:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:50 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 6596 ms on localhost (3/39)
15/08/21 19:46:51 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2125 bytes result sent to driver
15/08/21 19:46:51 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, localhost, ANY, 1699 bytes)
15/08/21 19:46:51 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
15/08/21 19:46:51 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 6732 ms on localhost (4/39)
15/08/21 19:46:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:51 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2125 bytes result sent to driver
15/08/21 19:46:51 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, localhost, ANY, 1710 bytes)
15/08/21 19:46:51 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
15/08/21 19:46:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 268435456 end: 333041820 length: 64606364 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:51 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 7335 ms on localhost (5/39)
15/08/21 19:46:54 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2125 bytes result sent to driver
15/08/21 19:46:54 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2125 bytes result sent to driver
15/08/21 19:46:54 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, localhost, ANY, 1694 bytes)
15/08/21 19:46:54 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23, localhost, ANY, 1699 bytes)
15/08/21 19:46:54 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
15/08/21 19:46:54 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
15/08/21 19:46:54 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 10488 ms on localhost (6/39)
15/08/21 19:46:54 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 10479 ms on localhost (7/39)
15/08/21 19:46:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:55 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2125 bytes result sent to driver
15/08/21 19:46:55 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 24, localhost, ANY, 1710 bytes)
15/08/21 19:46:55 INFO Executor: Running task 23.0 in stage 1.0 (TID 24)
15/08/21 19:46:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 268435456 end: 334667494 length: 66232038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:55 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 10744 ms on localhost (8/39)
15/08/21 19:46:55 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2125 bytes result sent to driver
15/08/21 19:46:55 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 25, localhost, ANY, 1693 bytes)
15/08/21 19:46:55 INFO Executor: Running task 24.0 in stage 1.0 (TID 25)
15/08/21 19:46:55 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 10978 ms on localhost (9/39)
15/08/21 19:46:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:55 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2125 bytes result sent to driver
15/08/21 19:46:55 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 26, localhost, ANY, 1698 bytes)
15/08/21 19:46:55 INFO Executor: Running task 25.0 in stage 1.0 (TID 26)
15/08/21 19:46:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:55 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 11110 ms on localhost (10/39)
15/08/21 19:46:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2125 bytes result sent to driver
15/08/21 19:46:55 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 27, localhost, ANY, 1706 bytes)
15/08/21 19:46:55 INFO Executor: Running task 26.0 in stage 1.0 (TID 27)
15/08/21 19:46:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 268435456 end: 338400334 length: 69964878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:55 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2125 bytes result sent to driver
15/08/21 19:46:55 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 28, localhost, ANY, 1693 bytes)
15/08/21 19:46:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11216 ms on localhost (11/39)
15/08/21 19:46:55 INFO Executor: Running task 27.0 in stage 1.0 (TID 28)
ow 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 286 ms. row count = 1203347
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 309 ms. row count = 1243920
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 297 ms. row count = 1203579
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 355 ms. row count = 1243981
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 334 ms. row count = 1222637
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 537 ms. row count = 2465634
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 540 ms. row count = 2465546
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 530 ms. row count = 2465498
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 536 ms. row count = 2464813
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 550 ms. row count = 2465108
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 584 ms. row count = 2465498
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 579 ms. row count = 2465101
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 596 ms. row count = 2465186
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 589 ms. row count = 2464958
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 585 ms. row count = 2464967
21-Aug-2015 19:46:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 612 ms. row count = 2464663
21-Aug-2015 19:46:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243832 records.
21-Aug-2015 19:46:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2463721 records.
21-Aug-2015 19:46:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464953 records.
21-Aug-2015 19:46:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464761 records.
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 205 ms. row count = 1243832
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 180 ms. row count = 2464953
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 266 ms. row count = 2463721
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 219 ms. row count = 2464761
21-Aug-2015 19:46:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1174247 records.
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 104 ms. row count = 1174247
21-Aug-2015 19:46:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2466033 records.
21-Aug-2015 19:46:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464261 records.
21-Aug-2015 19:46:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203394 records.
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 273 ms. row count = 2466033
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 323 ms. row count = 2464261
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 85 ms. row count = 1203394
21-Aug-2015 19:46:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464540 records.
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465074 records.
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 154 ms. row count = 2464540
21-Aug-2015 19:46:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter 15/08/21 19:46:55 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 11223 ms on localhost (12/39)
15/08/21 19:46:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:55 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2125 bytes result sent to driver
15/08/21 19:46:55 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 29, localhost, ANY, 1699 bytes)
15/08/21 19:46:55 INFO Executor: Running task 28.0 in stage 1.0 (TID 29)
15/08/21 19:46:55 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 11275 ms on localhost (13/39)
15/08/21 19:46:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:55 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2125 bytes result sent to driver
15/08/21 19:46:55 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 30, localhost, ANY, 1707 bytes)
15/08/21 19:46:55 INFO Executor: Running task 29.0 in stage 1.0 (TID 30)
15/08/21 19:46:55 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 11335 ms on localhost (14/39)
15/08/21 19:46:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 268435456 end: 336803495 length: 68368039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:56 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:54384 in memory (size: 3.2 KB, free: 20.7 GB)
15/08/21 19:46:56 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2125 bytes result sent to driver
15/08/21 19:46:56 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2125 bytes result sent to driver
15/08/21 19:46:56 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 31, localhost, ANY, 1693 bytes)
15/08/21 19:46:56 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 32, localhost, ANY, 1699 bytes)
15/08/21 19:46:56 INFO Executor: Running task 31.0 in stage 1.0 (TID 32)
15/08/21 19:46:56 INFO Executor: Running task 30.0 in stage 1.0 (TID 31)
15/08/21 19:46:56 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 12254 ms on localhost (15/39)
15/08/21 19:46:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:56 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 12266 ms on localhost (16/39)
15/08/21 19:46:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:57 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2125 bytes result sent to driver
15/08/21 19:46:57 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 33, localhost, ANY, 1707 bytes)
15/08/21 19:46:57 INFO Executor: Running task 32.0 in stage 1.0 (TID 33)
15/08/21 19:46:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 268435456 end: 334633553 length: 66198097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:57 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 6327 ms on localhost (17/39)
15/08/21 19:46:57 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2125 bytes result sent to driver
15/08/21 19:46:57 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 34, localhost, ANY, 1694 bytes)
15/08/21 19:46:57 INFO Executor: Running task 33.0 in stage 1.0 (TID 34)
15/08/21 19:46:57 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 5651 ms on localhost (18/39)
15/08/21 19:46:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:59 INFO Executor: Finished task 23.0 in stage 1.0 (TID 24). 2125 bytes result sent to driver
15/08/21 19:46:59 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 35, localhost, ANY, 1699 bytes)
15/08/21 19:46:59 INFO Executor: Running task 34.0 in stage 1.0 (TID 35)
15/08/21 19:46:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:46:59 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 24) in 4649 ms on localhost (19/39)
15/08/21 19:47:00 INFO Executor: Finished task 26.0 in stage 1.0 (TID 27). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 36, localhost, ANY, 1710 bytes)
15/08/21 19:47:00 INFO Executor: Running task 35.0 in stage 1.0 (TID 36)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 268435456 end: 334670294 length: 66234838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:00 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 27) in 4665 ms on localhost (20/39)
15/08/21 19:47:00 INFO Executor: Finished task 29.0 in stage 1.0 (TID 30). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 37, localhost, ANY, 1693 bytes)
15/08/21 19:47:00 INFO Executor: Running task 36.0 in stage 1.0 (TID 37)
15/08/21 19:47:00 INFO Executor: Finished task 32.0 in stage 1.0 (TID 33). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 38, localhost, ANY, 1699 bytes)
15/08/21 19:47:00 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:00 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 33) in 3163 ms on localhost (21/39)
15/08/21 19:47:00 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 30) in 4699 ms on localhost (22/39)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:00 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 39, localhost, ANY, 1707 bytes)
15/08/21 19:47:00 INFO Executor: Running task 38.0 in stage 1.0 (TID 39)
15/08/21 19:47:00 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 9465 ms on localhost (23/39)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 268435456 end: 336840779 length: 68405323 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:00 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 40, localhost, ANY, 1695 bytes)
15/08/21 19:47:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 40)
15/08/21 19:47:00 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 9549 ms on localhost (24/39)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000001_0 start: 0 end: 10091464 length: 10091464 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:00 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 41, localhost, ANY, 1696 bytes)
15/08/21 19:47:00 INFO Executor: Running task 1.0 in stage 2.0 (TID 41)
15/08/21 19:47:00 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 9384 ms on localhost (25/39)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000006_0 start: 0 end: 10091200 length: 10091200 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1273547 records.
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465823 records.
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 2465074
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464354 records.
21-Aug-2015 19:46:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 748 ms. row count = 1273547
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 813 ms. row count = 2465823
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1244217 records.
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 798 ms. row count = 2464354
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 1244217
21-Aug-2015 19:46:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464440 records.
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465446 records.
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 121 ms. row count = 2465446
21-Aug-2015 19:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 143 ms. row count = 2464440
21-Aug-2015 19:46:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203324 records.
21-Aug-2015 19:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 1203324
21-Aug-2015 19:46:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465130 records.
21-Aug-2015 19:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 147 ms. row count = 2465130
21-Aug-2015 19:46:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:46:59 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465185 records.
21-Aug-2015 19:46:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:46:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 102 ms. row count = 2465185
21-Aug-2015 19:47:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203179 records.
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 56 ms. row count = 1203179
21-Aug-2015 19:47:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:47:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:47:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2466709 records.
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465174 records.
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1244220 records.
21-Aug-2015 19:47:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124899 records.
21-Aug-2015 19:47:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124880 records.
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 124899
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 19:47:00 INFO: parquet.hadoop.InternalParquetRecordRea15/08/21 19:47:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 40). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 42, localhost, ANY, 1696 bytes)
15/08/21 19:47:00 INFO Executor: Running task 2.0 in stage 2.0 (TID 42)
15/08/21 19:47:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 40) in 335 ms on localhost (1/8)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000007_0 start: 0 end: 10090201 length: 10090201 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:00 INFO Executor: Finished task 1.0 in stage 2.0 (TID 41). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 43, localhost, ANY, 1694 bytes)
15/08/21 19:47:00 INFO Executor: Running task 3.0 in stage 2.0 (TID 43)
15/08/21 19:47:00 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 41) in 450 ms on localhost (2/8)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000005_0 start: 0 end: 10091941 length: 10091941 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:00 INFO Executor: Finished task 2.0 in stage 2.0 (TID 42). 2125 bytes result sent to driver
15/08/21 19:47:00 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 44, localhost, ANY, 1694 bytes)
15/08/21 19:47:00 INFO Executor: Running task 4.0 in stage 2.0 (TID 44)
15/08/21 19:47:00 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 42) in 238 ms on localhost (3/8)
15/08/21 19:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000004_0 start: 0 end: 10091082 length: 10091082 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:01 INFO Executor: Finished task 3.0 in stage 2.0 (TID 43). 2125 bytes result sent to driver
15/08/21 19:47:01 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 45, localhost, ANY, 1694 bytes)
15/08/21 19:47:01 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 43) in 286 ms on localhost (4/8)
15/08/21 19:47:01 INFO Executor: Running task 5.0 in stage 2.0 (TID 45)
15/08/21 19:47:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 10153723 length: 10153723 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:01 INFO Executor: Finished task 4.0 in stage 2.0 (TID 44). 2125 bytes result sent to driver
15/08/21 19:47:01 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 46, localhost, ANY, 1695 bytes)
15/08/21 19:47:01 INFO Executor: Running task 6.0 in stage 2.0 (TID 46)
15/08/21 19:47:01 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 44) in 218 ms on localhost (5/8)
15/08/21 19:47:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000002_0 start: 0 end: 10086725 length: 10086725 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:01 INFO Executor: Finished task 6.0 in stage 2.0 (TID 46). 2125 bytes result sent to driver
15/08/21 19:47:01 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 47, localhost, ANY, 1695 bytes)
15/08/21 19:47:01 INFO Executor: Running task 7.0 in stage 2.0 (TID 47)
15/08/21 19:47:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000003_0 start: 0 end: 10090247 length: 10090247 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:01 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 46) in 235 ms on localhost (6/8)
15/08/21 19:47:01 INFO Executor: Finished task 5.0 in stage 2.0 (TID 45). 2125 bytes result sent to driver
15/08/21 19:47:01 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 45) in 292 ms on localhost (7/8)
15/08/21 19:47:01 INFO Executor: Finished task 7.0 in stage 2.0 (TID 47). 2125 bytes result sent to driver
15/08/21 19:47:01 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 47) in 228 ms on localhost (8/8)
15/08/21 19:47:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/21 19:47:01 INFO DAGScheduler: ShuffleMapStage 2 (processCmd at CliDriver.java:423) finished in 17.239 s
15/08/21 19:47:01 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@71e6503c
15/08/21 19:47:01 INFO DAGScheduler: looking for newly runnable stages
15/08/21 19:47:01 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
15/08/21 19:47:01 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
15/08/21 19:47:01 INFO StatsReportListener: task runtime:(count: 33, mean: 6575.121212, stdev: 4285.527023, max: 12266.000000, min: 218.000000)
15/08/21 19:47:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:01 INFO StatsReportListener: 	218.0 ms	228.0 ms	238.0 ms	3.2 s	6.6 s	10.7 s	11.3 s	12.3 s	12.3 s
15/08/21 19:47:01 INFO DAGScheduler: failed: Set()
15/08/21 19:47:01 INFO StatsReportListener: shuffle bytes written:(count: 33, mean: 32464792.939394, stdev: 21911143.073813, max: 55122523.000000, min: 39409.000000)
15/08/21 19:47:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:01 INFO StatsReportListener: 	38.5 KB	38.6 KB	38.7 KB	25.0 MB	26.5 MB	52.5 MB	52.6 MB	52.6 MB	52.6 MB
15/08/21 19:47:01 INFO StatsReportListener: task result size:(count: 33, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 19:47:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:01 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 19:47:01 INFO StatsReportListener: executor (non-fetch) time pct: (count: 33, mean: 96.414695, stdev: 4.136005, max: 99.664886, min: 87.280702)
15/08/21 19:47:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:01 INFO StatsReportListener: 	87 %	88 %	89 %	97 %	99 %	99 %	99 %	100 %	100 %
15/08/21 19:47:01 INFO StatsReportListener: other time pct: (count: 33, mean: 3.585305, stdev: 4.136005, max: 12.719298, min: 0.335114)
15/08/21 19:47:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:01 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 3 %	11 %	12 %	13 %
15/08/21 19:47:01 INFO DAGScheduler: Missing parents for ShuffleMapStage 3: List(ShuffleMapStage 1)
15/08/21 19:47:01 INFO DAGScheduler: Missing parents for ResultStage 4: List(ShuffleMapStage 3)
15/08/21 19:47:01 INFO Executor: Finished task 24.0 in stage 1.0 (TID 25). 2125 bytes result sent to driver
15/08/21 19:47:01 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 25) in 6650 ms on localhost (26/39)
15/08/21 19:47:01 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 7233 ms on localhost (27/39)
15/08/21 19:47:02 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 7469 ms on localhost (28/39)
15/08/21 19:47:02 INFO Executor: Finished task 27.0 in stage 1.0 (TID 28). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 28) in 6848 ms on localhost (29/39)
15/08/21 19:47:02 INFO Executor: Finished task 28.0 in stage 1.0 (TID 29). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 29) in 6830 ms on localhost (30/39)
15/08/21 19:47:02 INFO Executor: Finished task 31.0 in stage 1.0 (TID 32). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO Executor: Finished task 25.0 in stage 1.0 (TID 26). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO Executor: Finished task 30.0 in stage 1.0 (TID 31). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 26) in 7069 ms on localhost (31/39)
15/08/21 19:47:02 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 32) in 5865 ms on localhost (32/39)
15/08/21 19:47:02 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 31) in 5872 ms on localhost (33/39)
15/08/21 19:47:02 INFO Executor: Finished task 33.0 in stage 1.0 (TID 34). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 34) in 5389 ms on localhost (34/39)
15/08/21 19:47:02 INFO Executor: Finished task 35.0 in stage 1.0 (TID 36). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 36) in 2554 ms on localhost (35/39)
15/08/21 19:47:02 INFO Executor: Finished task 38.0 in stage 1.0 (TID 39). 2125 bytes result sent to driver
15/08/21 19:47:02 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 39) in 2515 ms on localhost (36/39)
15/08/21 19:47:04 INFO Executor: Finished task 34.0 in stage 1.0 (TID 35). 2125 bytes result sent to driver
15/08/21 19:47:04 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 35) in 4434 ms on localhost (37/39)
15/08/21 19:47:04 INFO Executor: Finished task 36.0 in stage 1.0 (TID 37). 2125 bytes result sent to driver
15/08/21 19:47:04 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2125 bytes result sent to driver
15/08/21 19:47:04 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 37) in 4265 ms on localhost (38/39)
15/08/21 19:47:04 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 38) in 4263 ms on localhost (39/39)
15/08/21 19:47:04 INFO DAGScheduler: ShuffleMapStage 1 (processCmd at CliDriver.java:423) finished in 20.259 s
15/08/21 19:47:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/21 19:47:04 INFO DAGScheduler: looking for newly runnable stages
15/08/21 19:47:04 INFO DAGScheduler: running: Set()
15/08/21 19:47:04 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
15/08/21 19:47:04 INFO DAGScheduler: failed: Set()
15/08/21 19:47:04 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@666b08e4
15/08/21 19:47:04 INFO StatsReportListener: task runtime:(count: 14, mean: 5518.285714, stdev: 1610.471423, max: 7469.000000, min: 2515.000000)
15/08/21 19:47:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:04 INFO StatsReportListener: 	2.5 s	2.5 s	2.6 s	4.3 s	5.9 s	6.8 s	7.2 s	7.5 s	7.5 s
15/08/21 19:47:04 INFO StatsReportListener: shuffle bytes written:(count: 14, mean: 50500654.214286, stdev: 9576027.721338, max: 55117260.000000, min: 26900011.000000)
15/08/21 19:47:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:04 INFO StatsReportListener: 	25.7 MB	25.7 MB	26.5 MB	49.4 MB	52.5 MB	52.6 MB	52.6 MB	52.6 MB	52.6 MB
15/08/21 19:47:04 INFO StatsReportListener: task result size:(count: 14, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 19:47:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:04 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 19:47:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 14, mean: 99.273523, stdev: 0.237527, max: 99.624893, min: 98.629601)
15/08/21 19:47:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:04 INFO StatsReportListener: 	99 %	99 %	99 %	99 %	99 %	99 %	100 %	100 %	100 %
15/08/21 19:47:04 INFO StatsReportListener: other time pct: (count: 14, mean: 0.726477, stdev: 0.237527, max: 1.370399, min: 0.375107)
15/08/21 19:47:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:04 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %	 1 %	 1 %	 1 %
15/08/21 19:47:04 INFO DAGScheduler: Missing parents for ShuffleMapStage 3: List()
15/08/21 19:47:04 INFO DAGScheduler: Missing parents for ResultStage 4: List(ShuffleMapStage 3)
15/08/21 19:47:04 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[20] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 19:47:04 INFO MemoryStore: ensureFreeSpace(14224) called with curMem=1073140, maxMem=22226833244
15/08/21 19:47:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.9 KB, free 20.7 GB)
15/08/21 19:47:04 INFO MemoryStore: ensureFreeSpace(6785) called with curMem=1087364, maxMem=22226833244
15/08/21 19:47:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.6 KB, free 20.7 GB)
15/08/21 19:47:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:54384 (size: 6.6 KB, free: 20.7 GB)
15/08/21 19:47:04 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874
15/08/21 19:47:04 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[20] at processCmd at CliDriver.java:423)
15/08/21 19:47:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 200 tasks
15/08/21 19:47:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 48, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 49, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 50, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 51, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 52, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 53, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 54, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 55, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 56, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 57, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 58, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 59, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 60, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 61, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 62, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 63, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 48)
15/08/21 19:47:04 INFO Executor: Running task 3.0 in stage 3.0 (TID 51)
15/08/21 19:47:04 INFO Executor: Running task 1.0 in stage 3.0 (TID 49)
15/08/21 19:47:04 INFO Executor: Running task 2.0 in stage 3.0 (TID 50)
15/08/21 19:47:04 INFO Executor: Running task 6.0 in stage 3.0 (TID 54)
15/08/21 19:47:04 INFO Executor: Running task 4.0 in stage 3.0 (TID 52)
15/08/21 19:47:04 INFO Executor: Running task 5.0 in stage 3.0 (TID 53)
15/08/21 19:47:04 INFO Executor: Running task 7.0 in stage 3.0 (TID 55)
15/08/21 19:47:04 INFO Executor: Running task 9.0 in stage 3.0 (TID 57)
15/08/21 19:47:04 INFO Executor: Running task 11.0 in stage 3.0 (TID 59)
15/08/21 19:47:04 INFO Executor: Running task 13.0 in stage 3.0 (TID 61)
15/08/21 19:47:04 INFO Executor: Running task 15.0 in stage 3.0 (TID 63)
15/08/21 19:47:04 INFO Executor: Running task 8.0 in stage 3.0 (TID 56)
15/08/21 19:47:04 INFO Executor: Running task 14.0 in stage 3.0 (TID 62)
15/08/21 19:47:04 INFO Executor: Running task 12.0 in stage 3.0 (TID 60)
15/08/21 19:47:04 INFO Executor: Running task 10.0 in stage 3.0 (TID 58)
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:06 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:54384 in memory (size: 4.8 KB, free: 20.7 GB)
15/08/21 19:47:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:54384 in memory (size: 3.5 KB, free: 20.7 GB)
15/08/21 19:47:08 INFO Executor: Finished task 12.0 in stage 3.0 (TID 60). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO Executor: Finished task 11.0 in stage 3.0 (TID 59). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 64, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 16.0 in stage 3.0 (TID 64)
15/08/21 19:47:08 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 65, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 17.0 in stage 3.0 (TID 65)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 60) in 3346 ms on localhost (1/200)
15/08/21 19:47:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 48). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 66, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO Executor: Running task 18.0 in stage 3.0 (TID 66)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 59) in 3366 ms on localhost (2/200)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 48) in 3389 ms on localhost (3/200)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO Executor: Finished task 8.0 in stage 3.0 (TID 56). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 67, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 56) in 3403 ms on localhost (4/200)
15/08/21 19:47:08 INFO Executor: Running task 19.0 in stage 3.0 (TID 67)
15/08/21 19:47:08 INFO Executor: Finished task 4.0 in stage 3.0 (TID 52). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 68, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 20.0 in stage 3.0 (TID 68)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 52) in 3430 ms on localhost (5/200)
15/08/21 19:47:08 INFO Executor: Finished task 7.0 in stage 3.0 (TID 55). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 69, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO Executor: Finished task 9.0 in stage 3.0 (TID 57). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO Executor: Finished task 15.0 in stage 3.0 (TID 63). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO Executor: Running task 21.0 in stage 3.0 (TID 69)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 55) in 3438 ms on localhost (6/200)
15/08/21 19:47:08 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 70, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 22.0 in stage 3.0 (TID 70)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 57) in 3442 ms on localhost (7/200)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO Executor: Finished task 5.0 in stage 3.0 (TID 53). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 71, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Finished task 6.0 in stage 3.0 (TID 54). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO Executor: Finished task 13.0 in stage 3.0 (TID 61). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 63) in 3451 ms on localhost (8/200)
15/08/21 19:47:08 INFO Executor: Running task 23.0 in stage 3.0 (TID 71)
15/08/21 19:47:08 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 72, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 73, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 25.0 in stage 3.0 (TID 73)
15/08/21 19:47:08 INFO Executor: Running task 24.0 in stage 3.0 (TID 72)
15/08/21 19:47:08 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 74, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 26.0 in stage 3.0 (TID 74)
15/08/21 19:47:08 INFO Executor: Finished task 3.0 in stage 3.0 (TID 51). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 61) in 3460 ms on localhost (9/200)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO Executor: Finished task 14.0 in stage 3.0 (TID 62). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 75, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 51) in 3485 ms on localhost (10/200)
15/08/21 19:47:08 INFO Executor: Running task 27.0 in stage 3.0 (TID 75)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 53) in 3484 ms on localhost (11/200)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 54) in 3484 ms on localhost (12/200)
15/08/21 19:47:08 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 76, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 28.0 in stage 3.0 (TID 76)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 62) in 3491 ms on localhost (13/200)
15/08/21 19:47:08 INFO Executor: Finished task 1.0 in stage 3.0 (TID 49). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 77, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 29.0 in stage 3.0 (TID 77)
15/08/21 19:47:08 INFO Executor: Finished task 2.0 in stage 3.0 (TID 50). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 78, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 30.0 in stage 3.0 (TID 78)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 49) in 3510 ms on localhost (14/200)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO Executor: Finished task 10.0 in stage 3.0 (TID 58). 1219 bytes result sent to driver
15/08/21 19:47:08 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 79, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:08 INFO Executor: Running task 31.0 in stage 3.0 (TID 79)
15/08/21 19:47:08 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 50) in 3514 ms on localhost (15/200)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 58) in 3513 ms on localhost (16/200)
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 19:47:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 19:47:09 INFO Executor: Finished task 18.0 in stage 3.0 (TID 66). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 80, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 32.0 in stage 3.0 (TID 80)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 66) in 976 ms on localhost (17/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 31.0 in stage 3.0 (TID 79). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 81, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 33.0 in stage 3.0 (TID 81)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 79) in 908 ms on localhost (18/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO Executor: Finished task 22.0 in stage 3.0 (TID 70). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:09 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 82, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 34.0 in stage 3.0 (TID 82)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 70) in 991 ms on localhost (19/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO Executor: Finished task 29.0 in stage 3.0 (TID 77). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 83, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 77) in 1006 ms on localhost (20/200)
15/08/21 19:47:09 INFO Executor: Running task 35.0 in stage 3.0 (TID 83)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 27.0 in stage 3.0 (TID 75). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 84, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 36.0 in stage 3.0 (TID 84)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 75) in 1040 ms on localhost (21/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 16.0 in stage 3.0 (TID 64). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 85, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 37.0 in stage 3.0 (TID 85)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 64) in 1279 ms on localhost (22/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 24.0 in stage 3.0 (TID 72). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO Executor: Finished task 21.0 in stage 3.0 (TID 69). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 86, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 38.0 in stage 3.0 (TID 86)
15/08/21 19:47:09 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 87, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 39.0 in stage 3.0 (TID 87)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 72) in 1273 ms on localhost (23/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 69) in 1307 ms on localhost (24/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 26.0 in stage 3.0 (TID 74). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 88, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 40.0 in stage 3.0 (TID 88)
15/08/21 19:47:09 INFO Executor: Finished task 20.0 in stage 3.0 (TID 68). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 89, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 74) in 1344 ms on localhost (25/200)
15/08/21 19:47:09 INFO Executor: Running task 41.0 in stage 3.0 (TID 89)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 68) in 1392 ms on localhost (26/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 25.0 in stage 3.0 (TID 73). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 90, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Finished task 23.0 in stage 3.0 (TID 71). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO Executor: Running task 42.0 in stage 3.0 (TID 90)
15/08/21 19:47:09 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 91, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 43.0 in stage 3.0 (TID 91)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 73) in 1378 ms on localhost (27/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 71) in 1401 ms on localhost (28/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 17.0 in stage 3.0 (TID 65). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 92, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 44.0 in stage 3.0 (TID 92)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 65) in 1546 ms on localhost (29/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 19.0 in stage 3.0 (TID 67). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 93, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Finished task 28.0 in stage 3.0 (TID 76). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 94, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 46.0 in stage 3.0 (TID 94)
15/08/21 19:47:09 INFO Executor: Running task 45.0 in stage 3.0 (TID 93)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 76) in 1428 ms on localhost (30/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 67) in 1524 ms on localhost (31/200)
15/08/21 19:47:09 INFO Executor: Finished task 30.0 in stage 3.0 (TID 78). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 95, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 47.0 in stage 3.0 (TID 95)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 78) in 1440 ms on localhost (32/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO Executor: Finished task 32.0 in stage 3.0 (TID 80). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 96, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 80) in 801 ms on localhost (33/200)
15/08/21 19:47:09 INFO Executor: Running task 48.0 in stage 3.0 (TID 96)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:09 INFO Executor: Finished task 33.0 in stage 3.0 (TID 81). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 97, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 49.0 in stage 3.0 (TID 97)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 81) in 773 ms on localhost (34/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO Executor: Finished task 34.0 in stage 3.0 (TID 82). 1219 bytes result sent to driver
15/08/21 19:47:09 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 98, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:09 INFO Executor: Running task 50.0 in stage 3.0 (TID 98)
15/08/21 19:47:09 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 82) in 864 ms on localhost (35/200)
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 36.0 in stage 3.0 (TID 84). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 99, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 51.0 in stage 3.0 (TID 99)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 84) in 809 ms on localhost (36/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 35.0 in stage 3.0 (TID 83). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 100, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 52.0 in stage 3.0 (TID 100)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 83) in 881 ms on localhost (37/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 38.0 in stage 3.0 (TID 86). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 101, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 53.0 in stage 3.0 (TID 101)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 86) in 879 ms on localhost (38/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 37.0 in stage 3.0 (TID 85). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 102, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 54.0 in stage 3.0 (TID 102)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 85) in 1065 ms on localhost (39/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 41.0 in stage 3.0 (TID 89). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 103, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 55.0 in stage 3.0 (TID 103)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 89) in 906 ms on localhost (40/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 39.0 in stage 3.0 (TID 87). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO Executor: Finished task 46.0 in stage 3.0 (TID 94). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 104, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 56.0 in stage 3.0 (TID 104)
15/08/21 19:47:10 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 105, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 57.0 in stage 3.0 (TID 105)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 87) in 1101 ms on localhost (41/200)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 94) in 931 ms on localhost (42/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 44.0 in stage 3.0 (TID 92). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 106, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 58.0 in stage 3.0 (TID 106)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 92) in 1056 ms on localhost (43/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 43.0 in stage 3.0 (TID 91). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO Executor: Finished task 42.0 in stage 3.0 (TID 90). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 107, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 59.0 in stage 3.0 (TID 107)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 108, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 60.0 in stage 3.0 (TID 108)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 91) in 1167 ms on localhost (44/200)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 90) in 1176 ms on localhost (45/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 40.0 in stage 3.0 (TID 88). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:10 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 109, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 61.0 in stage 3.0 (TID 109)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 88) in 1239 ms on localhost (46/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 45.0 in stage 3.0 (TID 93). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 110, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 62.0 in stage 3.0 (TID 110)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 93) in 1236 ms on localhost (47/200)
15/08/21 19:47:10 INFO Executor: Finished task 48.0 in stage 3.0 (TID 96). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 111, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 63.0 in stage 3.0 (TID 111)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 96) in 1020 ms on localhost (48/200)
15/08/21 19:47:10 INFO Executor: Finished task 49.0 in stage 3.0 (TID 97). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 112, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 64.0 in stage 3.0 (TID 112)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 97) in 982 ms on localhost (49/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 47.0 in stage 3.0 (TID 95). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 113, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 65.0 in stage 3.0 (TID 113)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 95) in 1252 ms on localhost (50/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO Executor: Finished task 51.0 in stage 3.0 (TID 99). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 114, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 66.0 in stage 3.0 (TID 114)
15/08/21 19:47:10 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 99) in 960 ms on localhost (51/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO Executor: Finished task 50.0 in stage 3.0 (TID 98). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 115, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 67.0 in stage 3.0 (TID 115)
15/08/21 19:47:10 INFO Executor: Finished task 52.0 in stage 3.0 (TID 100). 1219 bytes result sent to driver
15/08/21 19:47:10 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 116, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:10 INFO Executor: Running task 68.0 in stage 3.0 (TID 116)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 98) in 1008 ms on localhost (52/200)
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:11 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 100) in 931 ms on localhost (53/200)
15/08/21 19:47:11 INFO Executor: Finished task 53.0 in stage 3.0 (TID 101). 1219 bytes result sent to driver
15/08/21 19:47:11 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 117, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:11 INFO Executor: Running task 69.0 in stage 3.0 (TID 117)
15/08/21 19:47:11 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 101) in 761 ms on localhost (54/200)
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:11 INFO Executor: Finished task 55.0 in stage 3.0 (TID 103). 1219 bytes result sent to driver
15/08/21 19:47:11 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 118, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:11 INFO Executor: Running task 70.0 in stage 3.0 (TID 118)
15/08/21 19:47:11 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 103) in 692 ms on localhost (55/200)
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:11 INFO Executor: Finished task 54.0 in stage 3.0 (TID 102). 1219 bytes result sent to driver
15/08/21 19:47:11 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 119, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:11 INFO Executor: Running task 71.0 in stage 3.0 (TID 119)
15/08/21 19:47:11 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 102) in 781 ms on localhost (56/200)
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:11 INFO Executor: Finished task 56.0 in stage 3.0 (TID 104). 1219 bytes result sent to driver
15/08/21 19:47:11 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 120, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:11 INFO Executor: Running task 72.0 in stage 3.0 (TID 120)
15/08/21 19:47:11 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 104) in 1350 ms on localhost (57/200)
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 57.0 in stage 3.0 (TID 105). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 121, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 73.0 in stage 3.0 (TID 121)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 105) in 1514 ms on localhost (58/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO Executor: Finished task 58.0 in stage 3.0 (TID 106). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 122, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 106) in 1546 ms on localhost (59/200)
15/08/21 19:47:12 INFO Executor: Running task 74.0 in stage 3.0 (TID 122)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 62.0 in stage 3.0 (TID 110). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 123, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 110) in 1484 ms on localhost (60/200)
15/08/21 19:47:12 INFO Executor: Running task 75.0 in stage 3.0 (TID 123)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 61.0 in stage 3.0 (TID 109). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 124, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 76.0 in stage 3.0 (TID 124)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 109) in 1642 ms on localhost (61/200)
15/08/21 19:47:12 INFO Executor: Finished task 59.0 in stage 3.0 (TID 107). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 125, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 77.0 in stage 3.0 (TID 125)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 107) in 1666 ms on localhost (62/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO Executor: Finished task 63.0 in stage 3.0 (TID 111). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO Executor: Finished task 60.0 in stage 3.0 (TID 108). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 126, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 127, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 79.0 in stage 3.0 (TID 127)
15/08/21 19:47:12 INFO Executor: Running task 78.0 in stage 3.0 (TID 126)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 111) in 1567 ms on localhost (63/200)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 108) in 1711 ms on localhost (64/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 64.0 in stage 3.0 (TID 112). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 128, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 80.0 in stage 3.0 (TID 128)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 112) in 1603 ms on localhost (65/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 65.0 in stage 3.0 (TID 113). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 129, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 81.0 in stage 3.0 (TID 129)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 113) in 1653 ms on localhost (66/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 67.0 in stage 3.0 (TID 115). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 130, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 82.0 in stage 3.0 (TID 130)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 115) in 1747 ms on localhost (67/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO Executor: Finished task 66.0 in stage 3.0 (TID 114). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 131, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Finished task 68.0 in stage 3.0 (TID 116). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO Executor: Running task 83.0 in stage 3.0 (TID 131)
15/08/21 19:47:12 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 132, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 116) in 1778 ms on localhost (68/200)
15/08/21 19:47:12 INFO Executor: Running task 84.0 in stage 3.0 (TID 132)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 114) in 1801 ms on localhost (69/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 69.0 in stage 3.0 (TID 117). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 133, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 85.0 in stage 3.0 (TID 133)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 117) in 1754 ms on localhost (70/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO Executor: Finished task 70.0 in stage 3.0 (TID 118). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 134, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 86.0 in stage 3.0 (TID 134)
15/08/21 19:47:12 INFO Executor: Finished task 71.0 in stage 3.0 (TID 119). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 118) in 1776 ms on localhost (71/200)
15/08/21 19:47:12 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 135, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 87.0 in stage 3.0 (TID 135)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 119) in 1715 ms on localhost (72/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 72.0 in stage 3.0 (TID 120). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 136, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 120) in 1045 ms on localhost (73/200)
15/08/21 19:47:12 INFO Executor: Running task 88.0 in stage 3.0 (TID 136)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO Executor: Finished task 74.0 in stage 3.0 (TID 122). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 137, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO Executor: Running task 89.0 in stage 3.0 (TID 137)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 122) in 755 ms on localhost (74/200)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:12 INFO Executor: Finished task 73.0 in stage 3.0 (TID 121). 1219 bytes result sent to driver
15/08/21 19:47:12 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 138, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:12 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 121) in 940 ms on localhost (75/200)
15/08/21 19:47:12 INFO Executor: Running task 90.0 in stage 3.0 (TID 138)
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 76.0 in stage 3.0 (TID 124). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 139, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 124) in 702 ms on localhost (76/200)
15/08/21 19:47:13 INFO Executor: Running task 91.0 in stage 3.0 (TID 139)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 77.0 in stage 3.0 (TID 125). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 140, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 92.0 in stage 3.0 (TID 140)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 125) in 758 ms on localhost (77/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO Executor: Finished task 75.0 in stage 3.0 (TID 123). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 141, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 93.0 in stage 3.0 (TID 141)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 123) in 861 ms on localhost (78/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 81.0 in stage 3.0 (TID 129). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 142, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 94.0 in stage 3.0 (TID 142)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 129) in 688 ms on localhost (79/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO Executor: Finished task 79.0 in stage 3.0 (TID 127). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO Executor: Finished task 80.0 in stage 3.0 (TID 128). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 143, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 144, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 96.0 in stage 3.0 (TID 144)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 127) in 876 ms on localhost (80/200)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 128) in 834 ms on localhost (81/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Running task 95.0 in stage 3.0 (TID 143)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 78.0 in stage 3.0 (TID 126). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 145, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 126) in 990 ms on localhost (82/200)
15/08/21 19:47:13 INFO Executor: Running task 97.0 in stage 3.0 (TID 145)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 85.0 in stage 3.0 (TID 133). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 146, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 98.0 in stage 3.0 (TID 146)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 133) in 661 ms on localhost (83/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 83.0 in stage 3.0 (TID 131). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 147, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 99.0 in stage 3.0 (TID 147)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 131) in 743 ms on localhost (84/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 82.0 in stage 3.0 (TID 130). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 148, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 100.0 in stage 3.0 (TID 148)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 130) in 803 ms on localhost (85/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO Executor: Finished task 87.0 in stage 3.0 (TID 135). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 149, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 101.0 in stage 3.0 (TID 149)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 135) in 733 ms on localhost (86/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO Executor: Finished task 86.0 in stage 3.0 (TID 134). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 150, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 102.0 in stage 3.0 (TID 150)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 134) in 763 ms on localhost (87/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO Executor: Finished task 89.0 in stage 3.0 (TID 137). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 151, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 103.0 in stage 3.0 (TID 151)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 137) in 836 ms on localhost (88/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 88.0 in stage 3.0 (TID 136). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 152, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 104.0 in stage 3.0 (TID 152)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 136) in 886 ms on localhost (89/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 90.0 in stage 3.0 (TID 138). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 153, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 105.0 in stage 3.0 (TID 153)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 138) in 855 ms on localhost (90/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO Executor: Finished task 84.0 in stage 3.0 (TID 132). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 154, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 106.0 in stage 3.0 (TID 154)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 132) in 1066 ms on localhost (91/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 92.0 in stage 3.0 (TID 140). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 155, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 107.0 in stage 3.0 (TID 155)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 140) in 781 ms on localhost (92/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO Executor: Finished task 93.0 in stage 3.0 (TID 141). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 156, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 108.0 in stage 3.0 (TID 156)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 91.0 in stage 3.0 (TID 139). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 141) in 781 ms on localhost (93/200)
15/08/21 19:47:13 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 157, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 109.0 in stage 3.0 (TID 157)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 139) in 894 ms on localhost (94/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:13 INFO Executor: Finished task 94.0 in stage 3.0 (TID 142). 1219 bytes result sent to driver
15/08/21 19:47:13 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 158, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:13 INFO Executor: Running task 110.0 in stage 3.0 (TID 158)
15/08/21 19:47:13 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 142) in 771 ms on localhost (95/200)
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 96.0 in stage 3.0 (TID 144). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 159, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 111.0 in stage 3.0 (TID 159)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 144) in 756 ms on localhost (96/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO Executor: Finished task 95.0 in stage 3.0 (TID 143). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 160, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 143) in 772 ms on localhost (97/200)
15/08/21 19:47:14 INFO Executor: Running task 112.0 in stage 3.0 (TID 160)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 98.0 in stage 3.0 (TID 146). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 161, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 113.0 in stage 3.0 (TID 161)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 146) in 696 ms on localhost (98/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 97.0 in stage 3.0 (TID 145). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 162, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 114.0 in stage 3.0 (TID 162)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 145) in 811 ms on localhost (99/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 99.0 in stage 3.0 (TID 147). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 163, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO Executor: Running task 115.0 in stage 3.0 (TID 163)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 147) in 710 ms on localhost (100/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 102.0 in stage 3.0 (TID 150). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 164, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 116.0 in stage 3.0 (TID 164)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 150) in 701 ms on localhost (101/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO Executor: Finished task 100.0 in stage 3.0 (TID 148). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 165, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 117.0 in stage 3.0 (TID 165)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 148) in 806 ms on localhost (102/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 101.0 in stage 3.0 (TID 149). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 166, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 118.0 in stage 3.0 (TID 166)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 149) in 754 ms on localhost (103/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO Executor: Finished task 103.0 in stage 3.0 (TID 151). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO Executor: Finished task 104.0 in stage 3.0 (TID 152). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 167, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 119.0 in stage 3.0 (TID 167)
15/08/21 19:47:14 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 120.0 in stage 3.0 (TID 168)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 151) in 641 ms on localhost (104/200)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 152) in 615 ms on localhost (105/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO Executor: Finished task 106.0 in stage 3.0 (TID 154). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 121.0 in stage 3.0 (TID 169)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 154) in 623 ms on localhost (106/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO Executor: Finished task 105.0 in stage 3.0 (TID 153). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 170, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 122.0 in stage 3.0 (TID 170)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 153) in 723 ms on localhost (107/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 109.0 in stage 3.0 (TID 157). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 171, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 123.0 in stage 3.0 (TID 171)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 157) in 642 ms on localhost (108/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 107.0 in stage 3.0 (TID 155). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 172, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 155) in 714 ms on localhost (109/200)
15/08/21 19:47:14 INFO Executor: Running task 124.0 in stage 3.0 (TID 172)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO Executor: Finished task 111.0 in stage 3.0 (TID 159). 1219 bytes result sent to driver
15/08/21 19:47:14 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 173, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:14 INFO Executor: Running task 125.0 in stage 3.0 (TID 173)
15/08/21 19:47:14 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 159) in 584 ms on localhost (110/200)
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 110.0 in stage 3.0 (TID 158). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 174, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 126.0 in stage 3.0 (TID 174)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 158) in 1177 ms on localhost (111/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 108.0 in stage 3.0 (TID 156). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 175, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 127.0 in stage 3.0 (TID 175)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 156) in 1328 ms on localhost (112/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 112.0 in stage 3.0 (TID 160). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 176, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 128.0 in stage 3.0 (TID 176)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 160) in 1307 ms on localhost (113/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 115.0 in stage 3.0 (TID 163). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 177, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 163) in 1187 ms on localhost (114/200)
15/08/21 19:47:15 INFO Executor: Running task 129.0 in stage 3.0 (TID 177)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 114.0 in stage 3.0 (TID 162). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 178, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 162) in 1244 ms on localhost (115/200)
15/08/21 19:47:15 INFO Executor: Running task 130.0 in stage 3.0 (TID 178)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO Executor: Finished task 113.0 in stage 3.0 (TID 161). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 161) in 1394 ms on localhost (116/200)
15/08/21 19:47:15 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 179, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 131.0 in stage 3.0 (TID 179)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 116.0 in stage 3.0 (TID 164). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 180, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 132.0 in stage 3.0 (TID 180)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 164) in 1259 ms on localhost (117/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 117.0 in stage 3.0 (TID 165). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 181, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 133.0 in stage 3.0 (TID 181)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 165) in 1287 ms on localhost (118/200)
15/08/21 19:47:15 INFO Executor: Finished task 118.0 in stage 3.0 (TID 166). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 182, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 134.0 in stage 3.0 (TID 182)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 166) in 1280 ms on localhost (119/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 119.0 in stage 3.0 (TID 167). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO Executor: Finished task 120.0 in stage 3.0 (TID 168). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 183, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 135.0 in stage 3.0 (TID 183)
15/08/21 19:47:15 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 184, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 167) in 1259 ms on localhost (120/200)
15/08/21 19:47:15 INFO Executor: Running task 136.0 in stage 3.0 (TID 184)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 1258 ms on localhost (121/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 121.0 in stage 3.0 (TID 169). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 185, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 137.0 in stage 3.0 (TID 185)
15/08/21 19:47:15 INFO Executor: Finished task 122.0 in stage 3.0 (TID 170). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 186, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 138.0 in stage 3.0 (TID 186)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 1286 ms on localhost (122/200)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 170) in 1200 ms on localhost (123/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:15 INFO Executor: Finished task 123.0 in stage 3.0 (TID 171). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 187, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 139.0 in stage 3.0 (TID 187)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 171) in 1218 ms on localhost (124/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 126.0 in stage 3.0 (TID 174). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 188, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Finished task 124.0 in stage 3.0 (TID 172). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 189, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 141.0 in stage 3.0 (TID 189)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 174) in 724 ms on localhost (125/200)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 172) in 1288 ms on localhost (126/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Running task 140.0 in stage 3.0 (TID 188)
15/08/21 19:47:15 INFO Executor: Finished task 125.0 in stage 3.0 (TID 173). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 190, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 173) in 1280 ms on localhost (127/200)
15/08/21 19:47:15 INFO Executor: Running task 142.0 in stage 3.0 (TID 190)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:15 INFO Executor: Finished task 128.0 in stage 3.0 (TID 176). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 191, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 143.0 in stage 3.0 (TID 191)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 176) in 565 ms on localhost (128/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:15 INFO Executor: Finished task 127.0 in stage 3.0 (TID 175). 1219 bytes result sent to driver
15/08/21 19:47:15 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 192, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:15 INFO Executor: Running task 144.0 in stage 3.0 (TID 192)
15/08/21 19:47:15 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 175) in 680 ms on localhost (129/200)
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 132.0 in stage 3.0 (TID 180). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 193, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 145.0 in stage 3.0 (TID 193)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 180) in 641 ms on localhost (130/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 131.0 in stage 3.0 (TID 179). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 194, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 179) in 681 ms on localhost (131/200)
15/08/21 19:47:16 INFO Executor: Running task 146.0 in stage 3.0 (TID 194)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 129.0 in stage 3.0 (TID 177). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 195, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 147.0 in stage 3.0 (TID 195)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 177) in 898 ms on localhost (132/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO Executor: Finished task 130.0 in stage 3.0 (TID 178). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 196, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 148.0 in stage 3.0 (TID 196)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 178) in 976 ms on localhost (133/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 137.0 in stage 3.0 (TID 185). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 197, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 149.0 in stage 3.0 (TID 197)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 185) in 800 ms on localhost (134/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 134.0 in stage 3.0 (TID 182). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 198, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 150.0 in stage 3.0 (TID 198)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 182) in 935 ms on localhost (135/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 136.0 in stage 3.0 (TID 184). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 199, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 151.0 in stage 3.0 (TID 199)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 184) in 949 ms on localhost (136/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO Executor: Finished task 135.0 in stage 3.0 (TID 183). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 200, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 152.0 in stage 3.0 (TID 200)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 183) in 964 ms on localhost (137/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 133.0 in stage 3.0 (TID 181). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 201, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 153.0 in stage 3.0 (TID 201)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 181) in 1034 ms on localhost (138/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 138.0 in stage 3.0 (TID 186). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO Executor: Finished task 144.0 in stage 3.0 (TID 192). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 202, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 203, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 155.0 in stage 3.0 (TID 203)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 186) in 992 ms on localhost (139/200)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 192) in 792 ms on localhost (140/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Running task 154.0 in stage 3.0 (TID 202)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 140.0 in stage 3.0 (TID 188). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 204, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 156.0 in stage 3.0 (TID 204)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 188) in 906 ms on localhost (141/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO Executor: Finished task 141.0 in stage 3.0 (TID 189). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 205, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Finished task 139.0 in stage 3.0 (TID 187). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO Executor: Running task 157.0 in stage 3.0 (TID 205)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 189) in 955 ms on localhost (142/200)
15/08/21 19:47:16 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 206, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Finished task 143.0 in stage 3.0 (TID 191). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO Executor: Running task 158.0 in stage 3.0 (TID 206)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 187) in 1043 ms on localhost (143/200)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 191) in 926 ms on localhost (144/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 207, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 159.0 in stage 3.0 (TID 207)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 142.0 in stage 3.0 (TID 190). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 208, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 160.0 in stage 3.0 (TID 208)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 190) in 971 ms on localhost (145/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 147.0 in stage 3.0 (TID 195). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 209, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 161.0 in stage 3.0 (TID 209)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 195) in 603 ms on localhost (146/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO Executor: Finished task 146.0 in stage 3.0 (TID 194). 1219 bytes result sent to driver
15/08/21 19:47:16 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 210, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:16 INFO Executor: Running task 162.0 in stage 3.0 (TID 210)
15/08/21 19:47:16 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 194) in 728 ms on localhost (147/200)
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 148.0 in stage 3.0 (TID 196). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 211, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 163.0 in stage 3.0 (TID 211)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 196) in 671 ms on localhost (148/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 149.0 in stage 3.0 (TID 197). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 212, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 164.0 in stage 3.0 (TID 212)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 197) in 612 ms on localhost (149/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 145.0 in stage 3.0 (TID 193). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 213, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 165.0 in stage 3.0 (TID 213)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 193) in 1016 ms on localhost (150/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 153.0 in stage 3.0 (TID 201). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 214, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 166.0 in stage 3.0 (TID 214)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 201) in 659 ms on localhost (151/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 152.0 in stage 3.0 (TID 200). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 215, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 167.0 in stage 3.0 (TID 215)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 200) in 723 ms on localhost (152/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 150.0 in stage 3.0 (TID 198). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 216, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 168.0 in stage 3.0 (TID 216)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 198) in 875 ms on localhost (153/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 154.0 in stage 3.0 (TID 202). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 217, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 169.0 in stage 3.0 (TID 217)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 202) in 726 ms on localhost (154/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 155.0 in stage 3.0 (TID 203). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 218, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 170.0 in stage 3.0 (TID 218)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 203) in 751 ms on localhost (155/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 156.0 in stage 3.0 (TID 204). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 219, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 171.0 in stage 3.0 (TID 219)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 204) in 727 ms on localhost (156/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 151.0 in stage 3.0 (TID 199). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 220, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 172.0 in stage 3.0 (TID 220)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 199) in 957 ms on localhost (157/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 160.0 in stage 3.0 (TID 208). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 221, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 173.0 in stage 3.0 (TID 221)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 208) in 798 ms on localhost (158/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 158.0 in stage 3.0 (TID 206). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 222, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 174.0 in stage 3.0 (TID 222)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 206) in 841 ms on localhost (159/200)
15/08/21 19:47:17 INFO Executor: Finished task 161.0 in stage 3.0 (TID 209). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 209) in 800 ms on localhost (160/200)
15/08/21 19:47:17 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 223, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 175.0 in stage 3.0 (TID 223)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO Executor: Finished task 159.0 in stage 3.0 (TID 207). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 224, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 176.0 in stage 3.0 (TID 224)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 207) in 862 ms on localhost (161/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO Executor: Finished task 157.0 in stage 3.0 (TID 205). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 225, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 177.0 in stage 3.0 (TID 225)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 205) in 893 ms on localhost (162/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO Executor: Finished task 162.0 in stage 3.0 (TID 210). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 226, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 178.0 in stage 3.0 (TID 226)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 210) in 829 ms on localhost (163/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO Executor: Finished task 164.0 in stage 3.0 (TID 212). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 227, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 179.0 in stage 3.0 (TID 227)
15/08/21 19:47:17 INFO Executor: Finished task 163.0 in stage 3.0 (TID 211). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 228, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 212) in 678 ms on localhost (164/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 211) in 736 ms on localhost (165/200)
15/08/21 19:47:17 INFO Executor: Running task 180.0 in stage 3.0 (TID 228)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO Executor: Finished task 165.0 in stage 3.0 (TID 213). 1219 bytes result sent to driver
15/08/21 19:47:17 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 229, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:17 INFO Executor: Running task 181.0 in stage 3.0 (TID 229)
15/08/21 19:47:17 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 213) in 650 ms on localhost (166/200)
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Finished task 167.0 in stage 3.0 (TID 215). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 230, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO Executor: Running task 182.0 in stage 3.0 (TID 230)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 215) in 1187 ms on localhost (167/200)
15/08/21 19:47:18 INFO Executor: Finished task 166.0 in stage 3.0 (TID 214). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 231, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 214) in 1223 ms on localhost (168/200)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Running task 183.0 in stage 3.0 (TID 231)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Finished task 169.0 in stage 3.0 (TID 217). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 232, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 217) in 1109 ms on localhost (169/200)
15/08/21 19:47:18 INFO Executor: Running task 184.0 in stage 3.0 (TID 232)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:18 INFO Executor: Finished task 170.0 in stage 3.0 (TID 218). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO Executor: Finished task 168.0 in stage 3.0 (TID 216). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 233, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 234, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO Executor: Running task 186.0 in stage 3.0 (TID 234)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 218) in 1189 ms on localhost (170/200)
15/08/21 19:47:18 INFO Executor: Running task 185.0 in stage 3.0 (TID 233)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 216) in 1237 ms on localhost (171/200)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Finished task 171.0 in stage 3.0 (TID 219). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 235, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO Executor: Running task 187.0 in stage 3.0 (TID 235)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 219) in 1222 ms on localhost (172/200)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Finished task 172.0 in stage 3.0 (TID 220). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 236, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO Executor: Running task 188.0 in stage 3.0 (TID 236)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 220) in 1226 ms on localhost (173/200)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Finished task 176.0 in stage 3.0 (TID 224). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 237, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO Executor: Running task 189.0 in stage 3.0 (TID 237)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 224) in 1177 ms on localhost (174/200)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Finished task 173.0 in stage 3.0 (TID 221). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 238, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO Executor: Running task 190.0 in stage 3.0 (TID 238)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 221) in 1278 ms on localhost (175/200)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO Executor: Finished task 175.0 in stage 3.0 (TID 223). 1219 bytes result sent to driver
15/08/21 19:47:18 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 239, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:18 INFO Executor: Running task 191.0 in stage 3.0 (TID 239)
15/08/21 19:47:18 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 223) in 1301 ms on localhost (176/200)
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO Executor: Finished task 179.0 in stage 3.0 (TID 227). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 240, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 192.0 in stage 3.0 (TID 240)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 227) in 1228 ms on localhost (177/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:19 INFO Executor: Finished task 174.0 in stage 3.0 (TID 222). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 241, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 193.0 in stage 3.0 (TID 241)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 222) in 1391 ms on localhost (178/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO Executor: Finished task 177.0 in stage 3.0 (TID 225). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 242, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 194.0 in stage 3.0 (TID 242)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 225) in 1358 ms on localhost (179/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO Executor: Finished task 178.0 in stage 3.0 (TID 226). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 243, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 195.0 in stage 3.0 (TID 243)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 226) in 1371 ms on localhost (180/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:19 INFO Executor: Finished task 180.0 in stage 3.0 (TID 228). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 244, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 196.0 in stage 3.0 (TID 244)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 228) in 1353 ms on localhost (181/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO Executor: Finished task 182.0 in stage 3.0 (TID 230). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 245, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 197.0 in stage 3.0 (TID 245)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 230) in 757 ms on localhost (182/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO Executor: Finished task 181.0 in stage 3.0 (TID 229). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 246, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 198.0 in stage 3.0 (TID 246)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 229) in 1427 ms on localhost (183/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO Executor: Finished task 184.0 in stage 3.0 (TID 232). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 247, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 19:47:19 INFO Executor: Running task 199.0 in stage 3.0 (TID 247)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 232) in 757 ms on localhost (184/200)
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 19:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:19 INFO Executor: Finished task 183.0 in stage 3.0 (TID 231). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO Executor: Finished task 185.0 in stage 3.0 (TID 233). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 231) in 816 ms on localhost (185/200)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 233) in 684 ms on localhost (186/200)
15/08/21 19:47:19 INFO Executor: Finished task 186.0 in stage 3.0 (TID 234). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 234) in 709 ms on localhost (187/200)
15/08/21 19:47:19 INFO Executor: Finished task 188.0 in stage 3.0 (TID 236). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 236) in 672 ms on localhost (188/200)
15/08/21 19:47:19 INFO Executor: Finished task 187.0 in stage 3.0 (TID 235). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 235) in 728 ms on localhost (189/200)
15/08/21 19:47:19 INFO Executor: Finished task 189.0 in stage 3.0 (TID 237). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 237) in 635 ms on localhost (190/200)
15/08/21 19:47:19 INFO Executor: Finished task 190.0 in stage 3.0 (TID 238). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 238) in 628 ms on localhost (191/200)
15/08/21 19:47:19 INFO Executor: Finished task 191.0 in stage 3.0 (TID 239). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 239) in 660 ms on localhost (192/200)
15/08/21 19:47:19 INFO Executor: Finished task 192.0 in stage 3.0 (TID 240). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 240) in 671 ms on localhost (193/200)
15/08/21 19:47:19 INFO Executor: Finished task 193.0 in stage 3.0 (TID 241). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 241) in 647 ms on localhost (194/200)
15/08/21 19:47:19 INFO Executor: Finished task 194.0 in stage 3.0 (TID 242). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 242) in 634 ms on localhost (195/200)
15/08/21 19:47:19 INFO Executor: Finished task 195.0 in stage 3.0 (TID 243). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO Executor: Finished task 196.0 in stage 3.0 (TID 244). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 244) in 636 ms on localhost (196/200)
15/08/21 19:47:19 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 243) in 651 ms on localhost (197/200)
15/08/21 19:47:19 INFO Executor: Finished task 197.0 in stage 3.0 (TID 245). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 245) in 571 ms on localhost (198/200)
15/08/21 19:47:19 INFO Executor: Finished task 198.0 in stage 3.0 (TID 246). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 246) in 552 ms on localhost (199/200)
15/08/21 19:47:19 INFO Executor: Finished task 199.0 in stage 3.0 (TID 247). 1219 bytes result sent to driver
15/08/21 19:47:19 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 247) in 560 ms on localhost (200/200)
15/08/21 19:47:19 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/21 19:47:19 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 15.177 s
15/08/21 19:47:19 INFO DAGScheduler: looking for newly runnable stages
15/08/21 19:47:19 INFO DAGScheduler: running: Set()
15/08/21 19:47:19 INFO DAGScheduler: waiting: Set(ResultStage 4)
15/08/21 19:47:19 INFO DAGScheduler: failed: Set()
15/08/21 19:47:19 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7019f92c
15/08/21 19:47:19 INFO StatsReportListener: task runtime:(count: 200, mean: 1198.255000, stdev: 729.846970, max: 3514.000000, min: 552.000000)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	552.0 ms	634.0 ms	660.0 ms	756.0 ms	964.0 ms	1.3 s	1.8 s	3.4 s	3.5 s
15/08/21 19:47:19 INFO DAGScheduler: Missing parents for ResultStage 4: List()
15/08/21 19:47:19 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 19:47:19 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 243498.685000, stdev: 16472.291857, max: 288923.000000, min: 202930.000000)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	198.2 KB	213.0 KB	218.2 KB	225.2 KB	237.2 KB	247.6 KB	260.7 KB	264.8 KB	282.2 KB
15/08/21 19:47:19 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.480000, stdev: 0.774338, max: 5.000000, min: 0.000000)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	5.0 ms
15/08/21 19:47:19 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 19:47:19 INFO StatsReportListener: task result size:(count: 200, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/21 19:47:19 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.538873, stdev: 2.837010, max: 99.241983, min: 59.983008)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	60 %	96 %	97 %	97 %	98 %	98 %	99 %	99 %	99 %
15/08/21 19:47:19 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.049114, stdev: 0.086437, max: 0.564175, min: 0.000000)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/21 19:47:19 INFO StatsReportListener: other time pct: (count: 200, mean: 2.412013, stdev: 2.837282, max: 40.016992, min: 0.758017)
15/08/21 19:47:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:19 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 2 %	 2 %	 3 %	 3 %	 4 %	40 %
15/08/21 19:47:19 INFO MemoryStore: ensureFreeSpace(82504) called with curMem=1069604, maxMem=22226833244
15/08/21 19:47:19 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 80.6 KB, free 20.7 GB)
15/08/21 19:47:19 INFO MemoryStore: ensureFreeSpace(31758) called with curMem=1152108, maxMem=22226833244
15/08/21 19:47:19 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.0 KB, free 20.7 GB)
15/08/21 19:47:19 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:54384 (size: 31.0 KB, free: 20.7 GB)
15/08/21 19:47:19 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/21 19:47:19 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at processCmd at CliDriver.java:423)
15/08/21 19:47:19 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks
15/08/21 19:47:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:19 INFO Executor: Running task 1.0 in stage 4.0 (TID 249)
15/08/21 19:47:19 INFO Executor: Running task 7.0 in stage 4.0 (TID 255)
15/08/21 19:47:19 INFO Executor: Running task 13.0 in stage 4.0 (TID 261)
15/08/21 19:47:19 INFO Executor: Running task 12.0 in stage 4.0 (TID 260)
15/08/21 19:47:19 INFO Executor: Running task 9.0 in stage 4.0 (TID 257)
15/08/21 19:47:19 INFO Executor: Running task 6.0 in stage 4.0 (TID 254)
15/08/21 19:47:19 INFO Executor: Running task 14.0 in stage 4.0 (TID 262)
15/08/21 19:47:19 INFO Executor: Running task 0.0 in stage 4.0 (TID 248)
15/08/21 19:47:19 INFO Executor: Running task 8.0 in stage 4.0 (TID 256)
15/08/21 19:47:19 INFO Executor: Running task 5.0 in stage 4.0 (TID 253)
15/08/21 19:47:19 INFO Executor: Running task 4.0 in stage 4.0 (TID 252)
15/08/21 19:47:19 INFO Executor: Running task 3.0 in stage 4.0 (TID 251)
15/08/21 19:47:19 INFO Executor: Running task 2.0 in stage 4.0 (TID 250)
15/08/21 19:47:19 INFO Executor: Running task 15.0 in stage 4.0 (TID 263)
15/08/21 19:47:19 INFO Executor: Running task 11.0 in stage 4.0 (TID 259)
15/08/21 19:47:19 INFO Executor: Running task 10.0 in stage 4.0 (TID 258)
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,068
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,452
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,948
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,848
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,752
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,436
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,368
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,948
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,548
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,648
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,396
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,076
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,936
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,012
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,048
15/08/21 19:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,540
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,481B for [ps_partkey] INT32: 14,814 values, 59,264B raw, 49,442B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 50,483B for [ps_partkey] INT32: 15,106 values, 60,432B raw, 50,444B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,340B for [ps_partkey] INT32: 14,755 values, 59,028B raw, 49,301B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,162B for [ps_partkey] INT32: 14,740 values, 58,968B raw, 49,123B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 50,296B for [ps_partkey] INT32: 15,058 values, 60,240B raw, 50,257B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,523B for [ps_partkey] INT32: 14,825 values, 59,308B raw, 49,484B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 48,680B for [ps_partkey] INT32: 14,560 values, 58,248B raw, 48,641B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 50,251B for [ps_partkey] INT32: 15,038 values, 60,160B raw, 50,212B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,185B for [ps_partkey] INT32: 14,784 values, 59,144B raw, 49,146B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 75,891B for [part_value] DOUBLE: 14,755 values, 118,048B raw, 75,844B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 48,462B for [ps_partkey] INT32: 14,489 values, 57,964B raw, 48,423B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 51,274B for [ps_partkey] INT32: 15,383 values, 61,540B raw, 51,235B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,044B for [ps_partkey] INT32: 14,684 values, 58,744B raw, 49,005B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,596B for [ps_partkey] INT32: 14,829 values, 59,324B raw, 49,557B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 76,306B for [part_value] DOUBLE: 14,825 values, 118,608B raw, 76,259B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 48,712B for [ps_partkey] INT32: 14,564 values, 58,264B raw, 48,673B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 75,844B for [part_value] DOUBLE: 14,740 values, 117,928B raw, 75,797B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 76,082B for [part_value] DOUBLE: 14,784 values, 118,280B raw, 76,035B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 74,523B for [part_value] DOUBLE: 14,489 values, 115,920B raw, 74,476B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 77,727B for [part_value] DOUBLE: 15,106 values, 120,856B raw, 77,680B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 77,520B for [part_value] DOUBLE: 15,058 values, 120,472B raw, 77,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 76,190B for [part_value] DOUBLE: 14,814 values, 118,520B raw, 76,143B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 48,792B for [ps_partkey] INT32: 14,590 values, 58,368B raw, 48,753B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 79,068B for [part_value] DOUBLE: 15,383 values, 123,072B raw, 79,021B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 74,857B for [part_value] DOUBLE: 14,560 values, 116,488B raw, 74,810B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 49,574B for [ps_partkey] INT32: 14,919 values, 59,684B raw, 49,535B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 74,922B for [part_value] DOUBLE: 14,564 values, 116,520B raw, 74,875B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 75,037B for [part_value] DOUBLE: 14,590 values, 116,728B raw, 74,990B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 77,368B for [part_value] DOUBLE: 15,038 values, 120,312B raw, 77,321B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 76,230B for [part_value] DOUBLE: 14,829 values, 118,640B raw, 76,183B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 75,515B for [part_value] DOUBLE: 14,684 values, 117,480B raw, 75,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 76,702B for [part_value] DOUBLE: 14,919 values, 119,360B raw, 76,655B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000013
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000013_0: Committed
15/08/21 19:47:21 INFO Executor: Finished task 13.0 in stage 4.0 (TID 261). 843 bytes result sent to driver
15/08/21 19:47:21 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 16.0 in stage 4.0 (TID 264)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 261) in 1640 ms on localhost (1/200)
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000001
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000001_0: Committed
15/08/21 19:47:21 INFO Executor: Finished task 1.0 in stage 4.0 (TID 249). 843 bytes result sent to driver
15/08/21 19:47:21 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 17.0 in stage 4.0 (TID 265)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 249) in 1674 ms on localhost (2/200)
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:21 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:21 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:21 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:21 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,744,160
15/08/21 19:47:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,900
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 47,681B for [ps_partkey] INT32: 14,345 values, 57,388B raw, 47,642B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 73,847B for [part_value] DOUBLE: 14,345 values, 114,768B raw, 73,800B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 50,538B for [ps_partkey] INT32: 15,132 values, 60,536B raw, 50,499B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO ColumnChunkPageWriteStore: written 77,861B for [part_value] DOUBLE: 15,132 values, 121,064B raw, 77,814B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000017
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000017_0: Committed
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000016
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000016_0: Committed
15/08/21 19:47:21 INFO Executor: Finished task 17.0 in stage 4.0 (TID 265). 843 bytes result sent to driver
15/08/21 19:47:21 INFO Executor: Finished task 16.0 in stage 4.0 (TID 264). 843 bytes result sent to driver
15/08/21 19:47:21 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 18.0 in stage 4.0 (TID 266)
15/08/21 19:47:21 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 19.0 in stage 4.0 (TID 267)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 265) in 322 ms on localhost (3/200)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 264) in 354 ms on localhost (4/200)
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000005
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000005_0: Committed
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000015
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000014
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000015_0: Committed
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000014_0: Committed
15/08/21 19:47:21 INFO Executor: Finished task 5.0 in stage 4.0 (TID 253). 843 bytes result sent to driver
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000000
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000000_0: Committed
15/08/21 19:47:21 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 20.0 in stage 4.0 (TID 268)
15/08/21 19:47:21 INFO Executor: Finished task 14.0 in stage 4.0 (TID 262). 843 bytes result sent to driver
15/08/21 19:47:21 INFO Executor: Finished task 15.0 in stage 4.0 (TID 263). 843 bytes result sent to driver
15/08/21 19:47:21 INFO Executor: Finished task 0.0 in stage 4.0 (TID 248). 843 bytes result sent to driver
15/08/21 19:47:21 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 21.0 in stage 4.0 (TID 269)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 253) in 2002 ms on localhost (5/200)
15/08/21 19:47:21 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 22.0 in stage 4.0 (TID 270)
15/08/21 19:47:21 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 23.0 in stage 4.0 (TID 271)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 262) in 2002 ms on localhost (6/200)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 263) in 2003 ms on localhost (7/200)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 248) in 2015 ms on localhost (8/200)
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000002
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000002_0: Committed
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000004
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000011
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000004_0: Committed
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000011_0: Committed
15/08/21 19:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000008
15/08/21 19:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000008_0: Committed
15/08/21 19:47:21 INFO Executor: Finished task 4.0 in stage 4.0 (TID 252). 843 bytes result sent to driver
15/08/21 19:47:21 INFO Executor: Finished task 2.0 in stage 4.0 (TID 250). 843 bytes result sent to driver
15/08/21 19:47:21 INFO Executor: Finished task 11.0 in stage 4.0 (TID 259). 843 bytes result sent to driver
15/08/21 19:47:21 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Finished task 8.0 in stage 4.0 (TID 256). 843 bytes result sent to driver
15/08/21 19:47:21 INFO Executor: Running task 24.0 in stage 4.0 (TID 272)
15/08/21 19:47:21 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 25.0 in stage 4.0 (TID 273)
15/08/21 19:47:21 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 26.0 in stage 4.0 (TID 274)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 252) in 2075 ms on localhost (9/200)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 250) in 2076 ms on localhost (10/200)
15/08/21 19:47:21 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:21 INFO Executor: Running task 27.0 in stage 4.0 (TID 275)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 259) in 2076 ms on localhost (11/200)
15/08/21 19:47:21 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 256) in 2079 ms on localhost (12/200)
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000009
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000003
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000009_0: Committed
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000003_0: Committed
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000010
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000010_0: Committed
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000006
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000006_0: Committed
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000007
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000007_0: Committed
15/08/21 19:47:22 INFO Executor: Finished task 3.0 in stage 4.0 (TID 251). 843 bytes result sent to driver
15/08/21 19:47:22 INFO Executor: Finished task 10.0 in stage 4.0 (TID 258). 843 bytes result sent to driver
15/08/21 19:47:22 INFO Executor: Finished task 7.0 in stage 4.0 (TID 255). 843 bytes result sent to driver
15/08/21 19:47:22 INFO Executor: Finished task 6.0 in stage 4.0 (TID 254). 843 bytes result sent to driver
15/08/21 19:47:22 INFO Executor: Finished task 9.0 in stage 4.0 (TID 257). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 251) in 2113 ms on localhost (13/200)
15/08/21 19:47:22 INFO Executor: Running task 28.0 in stage 4.0 (TID 276)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000012
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000012_0: Committed
15/08/21 19:47:22 INFO Executor: Running task 29.0 in stage 4.0 (TID 277)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 258) in 2116 ms on localhost (14/200)
15/08/21 19:47:22 INFO Executor: Finished task 12.0 in stage 4.0 (TID 260). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 255) in 2119 ms on localhost (15/200)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 30.0 in stage 4.0 (TID 278)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 31.0 in stage 4.0 (TID 279)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 32.0 in stage 4.0 (TID 280)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 33.0 in stage 4.0 (TID 281)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 257) in 2123 ms on localhost (16/200)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 254) in 2124 ms on localhost (17/200)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 260) in 2125 ms on localhost (18/200)
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,836
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,928
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 49,062B for [ps_partkey] INT32: 14,678 values, 58,720B raw, 49,023B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 75,433B for [part_value] DOUBLE: 14,678 values, 117,432B raw, 75,386B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,148
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,440
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 51,270B for [ps_partkey] INT32: 15,383 values, 61,540B raw, 51,231B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 79,065B for [part_value] DOUBLE: 15,383 values, 123,072B raw, 79,018B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 49,910B for [ps_partkey] INT32: 14,944 values, 59,784B raw, 49,871B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 76,958B for [part_value] DOUBLE: 14,944 values, 119,560B raw, 76,911B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 48,495B for [ps_partkey] INT32: 14,509 values, 58,044B raw, 48,456B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 74,606B for [part_value] DOUBLE: 14,509 values, 116,080B raw, 74,559B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,156
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,840
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,676
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,423B for [ps_partkey] INT32: 15,079 values, 60,324B raw, 50,384B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 51,293B for [ps_partkey] INT32: 15,344 values, 61,384B raw, 51,254B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 77,549B for [part_value] DOUBLE: 15,079 values, 120,640B raw, 77,502B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 78,932B for [part_value] DOUBLE: 15,344 values, 122,760B raw, 78,885B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,502B for [ps_partkey] INT32: 15,120 values, 60,488B raw, 50,463B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 77,808B for [part_value] DOUBLE: 15,120 values, 120,968B raw, 77,761B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000020
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000020_0: Committed
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000019
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000019_0: Committed
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO Executor: Finished task 19.0 in stage 4.0 (TID 267). 843 bytes result sent to driver
15/08/21 19:47:22 INFO Executor: Finished task 20.0 in stage 4.0 (TID 268). 843 bytes result sent to driver
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 34.0 in stage 4.0 (TID 282)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 35.0 in stage 4.0 (TID 283)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 268) in 464 ms on localhost (19/200)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 267) in 471 ms on localhost (20/200)
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,308
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,060
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,128
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000023
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000023_0: Committed
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000018
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000021
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000018_0: Committed
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000021_0: Committed
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,840
15/08/21 19:47:22 INFO Executor: Finished task 23.0 in stage 4.0 (TID 271). 843 bytes result sent to driver
15/08/21 19:47:22 INFO Executor: Finished task 18.0 in stage 4.0 (TID 266). 843 bytes result sent to driver
15/08/21 19:47:22 INFO Executor: Finished task 21.0 in stage 4.0 (TID 269). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 36.0 in stage 4.0 (TID 284)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 285, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 37.0 in stage 4.0 (TID 285)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 286, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 38.0 in stage 4.0 (TID 286)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 266) in 517 ms on localhost (21/200)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 271) in 503 ms on localhost (22/200)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 269) in 509 ms on localhost (23/200)
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,048
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,136
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000022
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000022_0: Committed
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 48,539B for [ps_partkey] INT32: 14,590 values, 58,368B raw, 48,500B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 74,949B for [part_value] DOUBLE: 14,590 values, 116,728B raw, 74,902B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO Executor: Finished task 22.0 in stage 4.0 (TID 270). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 287, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 39.0 in stage 4.0 (TID 287)
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 51,315B for [ps_partkey] INT32: 15,393 values, 61,580B raw, 51,276B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 79,116B for [part_value] DOUBLE: 15,393 values, 123,152B raw, 79,069B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 270) in 525 ms on localhost (24/200)
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 51,154B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 51,115B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 78,754B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,707B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,088
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,178B for [ps_partkey] INT32: 15,029 values, 60,124B raw, 50,139B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,796
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,219B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,180B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 77,329B for [part_value] DOUBLE: 15,029 values, 120,240B raw, 77,282B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 77,318B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,271B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 49,591B for [ps_partkey] INT32: 14,843 values, 59,380B raw, 49,552B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 76,334B for [part_value] DOUBLE: 14,843 values, 118,752B raw, 76,287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,596
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 49,362B for [ps_partkey] INT32: 14,776 values, 59,112B raw, 49,323B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 49,175B for [ps_partkey] INT32: 14,791 values, 59,172B raw, 49,136B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 75,995B for [part_value] DOUBLE: 14,776 values, 118,216B raw, 75,948B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 75,932B for [part_value] DOUBLE: 14,791 values, 118,336B raw, 75,885B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 49,541B for [ps_partkey] INT32: 14,816 values, 59,272B raw, 49,502B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 76,158B for [part_value] DOUBLE: 14,816 values, 118,536B raw, 76,111B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000027
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000027_0: Committed
15/08/21 19:47:22 INFO Executor: Finished task 27.0 in stage 4.0 (TID 275). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 288, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 40.0 in stage 4.0 (TID 288)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 275) in 552 ms on localhost (25/200)
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000029
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000029_0: Committed
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000030
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000030_0: Committed
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO Executor: Finished task 29.0 in stage 4.0 (TID 277). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 289, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO Executor: Running task 41.0 in stage 4.0 (TID 289)
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 277) in 625 ms on localhost (26/200)
15/08/21 19:47:22 INFO Executor: Finished task 30.0 in stage 4.0 (TID 278). 843 bytes result sent to driver
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 290, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 42.0 in stage 4.0 (TID 290)
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 278) in 626 ms on localhost (27/200)
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,328
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,256
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,688
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,188
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,548
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,996
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,173B for [ps_partkey] INT32: 15,049 values, 60,204B raw, 50,134B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,774B for [ps_partkey] INT32: 15,203 values, 60,820B raw, 50,735B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 77,439B for [part_value] DOUBLE: 15,049 values, 120,400B raw, 77,392B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 78,172B for [part_value] DOUBLE: 15,203 values, 121,632B raw, 78,125B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 49,394B for [ps_partkey] INT32: 14,771 values, 59,092B raw, 49,355B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 75,957B for [part_value] DOUBLE: 14,771 values, 118,176B raw, 75,910B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,301B for [ps_partkey] INT32: 15,046 values, 60,192B raw, 50,262B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 77,375B for [part_value] DOUBLE: 15,046 values, 120,376B raw, 77,328B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,670B for [ps_partkey] INT32: 15,164 values, 60,664B raw, 50,631B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 78,004B for [part_value] DOUBLE: 15,164 values, 121,320B raw, 77,957B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 51,365B for [ps_partkey] INT32: 15,386 values, 61,552B raw, 51,326B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 79,089B for [part_value] DOUBLE: 15,386 values, 123,096B raw, 79,042B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,700
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000034
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000034_0: Committed
15/08/21 19:47:22 INFO Executor: Finished task 34.0 in stage 4.0 (TID 282). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 291, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 43.0 in stage 4.0 (TID 291)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 282) in 457 ms on localhost (28/200)
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000039
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000039_0: Committed
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 50,045B for [ps_partkey] INT32: 14,972 values, 59,896B raw, 50,006B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 76,973B for [part_value] DOUBLE: 14,972 values, 119,784B raw, 76,926B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO Executor: Finished task 39.0 in stage 4.0 (TID 287). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 292, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 44.0 in stage 4.0 (TID 292)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 287) in 418 ms on localhost (29/200)
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:22 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:22 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000035
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000035_0: Committed
15/08/21 19:47:22 INFO Executor: Finished task 35.0 in stage 4.0 (TID 283). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 293, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 45.0 in stage 4.0 (TID 293)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 283) in 509 ms on localhost (30/200)
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000037
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000037_0: Committed
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO Executor: Finished task 37.0 in stage 4.0 (TID 285). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 294, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 46.0 in stage 4.0 (TID 294)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 285) in 479 ms on localhost (31/200)
15/08/21 19:47:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000026
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000026_0: Committed
15/08/21 19:47:22 INFO Executor: Finished task 26.0 in stage 4.0 (TID 274). 843 bytes result sent to driver
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,416
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000024
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000024_0: Committed
15/08/21 19:47:22 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 295, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 47.0 in stage 4.0 (TID 295)
15/08/21 19:47:22 INFO Executor: Finished task 24.0 in stage 4.0 (TID 272). 843 bytes result sent to driver
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000025
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000025_0: Committed
15/08/21 19:47:22 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 274) in 978 ms on localhost (32/200)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 296, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 48.0 in stage 4.0 (TID 296)
15/08/21 19:47:22 INFO Executor: Finished task 25.0 in stage 4.0 (TID 273). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 297, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 49.0 in stage 4.0 (TID 297)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 272) in 985 ms on localhost (33/200)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 273) in 986 ms on localhost (34/200)
15/08/21 19:47:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,968
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000028
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000028_0: Committed
15/08/21 19:47:22 INFO Executor: Finished task 28.0 in stage 4.0 (TID 276). 843 bytes result sent to driver
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000032
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000032_0: Committed
15/08/21 19:47:22 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 298, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 50.0 in stage 4.0 (TID 298)
15/08/21 19:47:22 INFO Executor: Finished task 32.0 in stage 4.0 (TID 280). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 276) in 958 ms on localhost (35/200)
15/08/21 19:47:22 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 299, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 51.0 in stage 4.0 (TID 299)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 280) in 947 ms on localhost (36/200)
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000033
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000033_0: Committed
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 51,806B for [ps_partkey] INT32: 15,557 values, 62,236B raw, 51,767B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO ColumnChunkPageWriteStore: written 79,910B for [part_value] DOUBLE: 15,557 values, 124,464B raw, 79,863B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000031
15/08/21 19:47:22 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000031_0: Committed
15/08/21 19:47:22 INFO Executor: Finished task 33.0 in stage 4.0 (TID 281). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 300, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 52.0 in stage 4.0 (TID 300)
15/08/21 19:47:22 INFO Executor: Finished task 31.0 in stage 4.0 (TID 279). 843 bytes result sent to driver
15/08/21 19:47:22 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 301, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:22 INFO Executor: Running task 53.0 in stage 4.0 (TID 301)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 279) in 961 ms on localhost (37/200)
15/08/21 19:47:22 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 281) in 960 ms on localhost (38/200)
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 50,581B for [ps_partkey] INT32: 15,235 values, 60,948B raw, 50,542B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 78,357B for [part_value] DOUBLE: 15,235 values, 121,888B raw, 78,310B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000042
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000042_0: Committed
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO Executor: Finished task 42.0 in stage 4.0 (TID 290). 843 bytes result sent to driver
15/08/21 19:47:23 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 302, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 54.0 in stage 4.0 (TID 302)
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 290) in 393 ms on localhost (39/200)
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000041
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000041_0: Committed
15/08/21 19:47:23 INFO Executor: Finished task 41.0 in stage 4.0 (TID 289). 843 bytes result sent to driver
15/08/21 19:47:23 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 303, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 55.0 in stage 4.0 (TID 303)
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,168
15/08/21 19:47:23 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 289) in 447 ms on localhost (40/200)
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,328
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,100
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 50,600B for [ps_partkey] INT32: 15,145 values, 60,588B raw, 50,561B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 77,934B for [part_value] DOUBLE: 15,145 values, 121,168B raw, 77,887B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 52,094B for [ps_partkey] INT32: 15,603 values, 62,420B raw, 52,055B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 80,396B for [part_value] DOUBLE: 15,603 values, 124,832B raw, 80,349B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 49,580B for [ps_partkey] INT32: 14,842 values, 59,376B raw, 49,541B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 76,384B for [part_value] DOUBLE: 14,842 values, 118,744B raw, 76,337B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,920
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 51,228B for [ps_partkey] INT32: 15,333 values, 61,340B raw, 51,189B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 78,917B for [part_value] DOUBLE: 15,333 values, 122,672B raw, 78,870B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000044
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000044_0: Committed
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO Executor: Finished task 44.0 in stage 4.0 (TID 292). 843 bytes result sent to driver
15/08/21 19:47:23 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 304, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 56.0 in stage 4.0 (TID 304)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 292) in 406 ms on localhost (41/200)
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,020
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,396
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000036
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000043
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,396
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000036_0: Committed
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000043_0: Committed
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,956
15/08/21 19:47:23 INFO Executor: Finished task 43.0 in stage 4.0 (TID 291). 843 bytes result sent to driver
15/08/21 19:47:23 INFO Executor: Finished task 36.0 in stage 4.0 (TID 284). 843 bytes result sent to driver
15/08/21 19:47:23 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 305, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 57.0 in stage 4.0 (TID 305)
15/08/21 19:47:23 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 306, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 58.0 in stage 4.0 (TID 306)
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000038
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000038_0: Committed
15/08/21 19:47:23 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 284) in 877 ms on localhost (42/200)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 291) in 473 ms on localhost (43/200)
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO Executor: Finished task 38.0 in stage 4.0 (TID 286). 843 bytes result sent to driver
15/08/21 19:47:23 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 307, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 59.0 in stage 4.0 (TID 307)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 286) in 888 ms on localhost (44/200)
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 50,882B for [ps_partkey] INT32: 15,238 values, 60,960B raw, 50,843B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 49,817B for [ps_partkey] INT32: 14,906 values, 59,632B raw, 49,778B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 76,596B for [part_value] DOUBLE: 14,906 values, 119,256B raw, 76,549B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 78,305B for [part_value] DOUBLE: 15,238 values, 121,912B raw, 78,258B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,280
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,116
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,508
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 51,391B for [ps_partkey] INT32: 15,384 values, 61,544B raw, 51,352B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 49,751B for [ps_partkey] INT32: 14,956 values, 59,832B raw, 49,712B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 79,099B for [part_value] DOUBLE: 15,384 values, 123,080B raw, 79,052B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 76,847B for [part_value] DOUBLE: 14,956 values, 119,656B raw, 76,800B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 51,758B for [ps_partkey] INT32: 15,501 values, 62,012B raw, 51,719B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 49,962B for [ps_partkey] INT32: 14,992 values, 59,976B raw, 49,923B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 79,721B for [part_value] DOUBLE: 15,501 values, 124,016B raw, 79,674B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 77,067B for [part_value] DOUBLE: 14,992 values, 119,944B raw, 77,020B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000040
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000040_0: Committed
15/08/21 19:47:23 INFO Executor: Finished task 40.0 in stage 4.0 (TID 288). 843 bytes result sent to driver
15/08/21 19:47:23 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 308, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 50,331B for [ps_partkey] INT32: 15,062 values, 60,256B raw, 50,292B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,516
15/08/21 19:47:23 INFO Executor: Running task 60.0 in stage 4.0 (TID 308)
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 77,320B for [part_value] DOUBLE: 15,062 values, 120,504B raw, 77,273B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 288) in 837 ms on localhost (45/200)
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000048
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000048_0: Committed
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 48,826B for [ps_partkey] INT32: 14,612 values, 58,456B raw, 48,787B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 75,159B for [part_value] DOUBLE: 14,612 values, 116,904B raw, 75,112B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO Executor: Finished task 48.0 in stage 4.0 (TID 296). 843 bytes result sent to driver
15/08/21 19:47:23 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 309, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 61.0 in stage 4.0 (TID 309)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 296) in 453 ms on localhost (46/200)
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,504
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000050
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000050_0: Committed
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000052
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000052_0: Committed
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 50,137B for [ps_partkey] INT32: 15,013 values, 60,060B raw, 50,098B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO Executor: Finished task 50.0 in stage 4.0 (TID 298). 843 bytes result sent to driver
15/08/21 19:47:23 INFO Executor: Finished task 52.0 in stage 4.0 (TID 300). 843 bytes result sent to driver
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 77,255B for [part_value] DOUBLE: 15,013 values, 120,112B raw, 77,208B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 310, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 62.0 in stage 4.0 (TID 310)
15/08/21 19:47:23 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 311, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 63.0 in stage 4.0 (TID 311)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 298) in 535 ms on localhost (47/200)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 300) in 524 ms on localhost (48/200)
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000053
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000053_0: Committed
15/08/21 19:47:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000054
15/08/21 19:47:23 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000054_0: Committed
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO Executor: Finished task 54.0 in stage 4.0 (TID 302). 843 bytes result sent to driver
15/08/21 19:47:23 INFO Executor: Finished task 53.0 in stage 4.0 (TID 301). 843 bytes result sent to driver
15/08/21 19:47:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:23 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 312, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 64.0 in stage 4.0 (TID 312)
15/08/21 19:47:23 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 313, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:23 INFO Executor: Running task 65.0 in stage 4.0 (TID 313)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 302) in 530 ms on localhost (49/200)
15/08/21 19:47:23 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 301) in 586 ms on localhost (50/200)
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,716
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,296
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,088
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 49,109B for [ps_partkey] INT32: 14,772 values, 59,096B raw, 49,070B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 75,834B for [part_value] DOUBLE: 14,772 values, 118,184B raw, 75,787B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,356
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 50,523B for [ps_partkey] INT32: 15,101 values, 60,412B raw, 50,484B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 50,476B for [ps_partkey] INT32: 15,141 values, 60,572B raw, 50,437B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 77,680B for [part_value] DOUBLE: 15,101 values, 120,816B raw, 77,633B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 77,823B for [part_value] DOUBLE: 15,141 values, 121,136B raw, 77,776B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 49,650B for [ps_partkey] INT32: 14,854 values, 59,424B raw, 49,611B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO ColumnChunkPageWriteStore: written 76,442B for [part_value] DOUBLE: 14,854 values, 118,840B raw, 76,395B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:23 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:23 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000055
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000055_0: Committed
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO Executor: Finished task 55.0 in stage 4.0 (TID 303). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 314, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 66.0 in stage 4.0 (TID 314)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 303) in 1296 ms on localhost (51/200)
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:54384 in memory (size: 6.6 KB, free: 20.7 GB)
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000045
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000045_0: Committed
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000046
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000046_0: Committed
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000051
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000051_0: Committed
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,656
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,040
15/08/21 19:47:24 INFO Executor: Finished task 46.0 in stage 4.0 (TID 294). 843 bytes result sent to driver
15/08/21 19:47:24 INFO Executor: Finished task 51.0 in stage 4.0 (TID 299). 843 bytes result sent to driver
15/08/21 19:47:24 INFO Executor: Finished task 45.0 in stage 4.0 (TID 293). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 315, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 67.0 in stage 4.0 (TID 315)
15/08/21 19:47:24 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 316, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 317, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 68.0 in stage 4.0 (TID 316)
15/08/21 19:47:24 INFO Executor: Running task 69.0 in stage 4.0 (TID 317)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 293) in 1571 ms on localhost (52/200)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 294) in 1553 ms on localhost (53/200)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 299) in 1465 ms on localhost (54/200)
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,048
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000047
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000047_0: Committed
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000059
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000059_0: Committed
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000049
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000049_0: Committed
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,372
15/08/21 19:47:24 INFO Executor: Finished task 47.0 in stage 4.0 (TID 295). 843 bytes result sent to driver
15/08/21 19:47:24 INFO Executor: Finished task 59.0 in stage 4.0 (TID 307). 843 bytes result sent to driver
15/08/21 19:47:24 INFO Executor: Finished task 49.0 in stage 4.0 (TID 297). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 318, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 70.0 in stage 4.0 (TID 318)
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 49,530B for [ps_partkey] INT32: 14,819 values, 59,284B raw, 49,491B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 319, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 320, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 71.0 in stage 4.0 (TID 319)
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 76,342B for [part_value] DOUBLE: 14,819 values, 118,560B raw, 76,295B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 295) in 1504 ms on localhost (55/200)
15/08/21 19:47:24 INFO Executor: Running task 72.0 in stage 4.0 (TID 320)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 297) in 1498 ms on localhost (56/200)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 307) in 1172 ms on localhost (57/200)
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 50,219B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,180B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 77,415B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,368B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 51,124B for [ps_partkey] INT32: 15,306 values, 61,232B raw, 51,085B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 78,692B for [part_value] DOUBLE: 15,306 values, 122,456B raw, 78,645B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 50,691B for [ps_partkey] INT32: 15,189 values, 60,764B raw, 50,652B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 78,111B for [part_value] DOUBLE: 15,189 values, 121,520B raw, 78,064B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000056
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000056_0: Committed
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,556
15/08/21 19:47:24 INFO Executor: Finished task 56.0 in stage 4.0 (TID 304). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 321, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,196
15/08/21 19:47:24 INFO Executor: Running task 73.0 in stage 4.0 (TID 321)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 304) in 1250 ms on localhost (58/200)
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 49,851B for [ps_partkey] INT32: 14,914 values, 59,664B raw, 49,812B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 76,734B for [part_value] DOUBLE: 14,914 values, 119,320B raw, 76,687B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 49,476B for [ps_partkey] INT32: 14,896 values, 59,592B raw, 49,437B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 76,602B for [part_value] DOUBLE: 14,896 values, 119,176B raw, 76,555B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000063
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000063_0: Committed
15/08/21 19:47:24 INFO Executor: Finished task 63.0 in stage 4.0 (TID 311). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 322, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 74.0 in stage 4.0 (TID 322)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 311) in 1035 ms on localhost (59/200)
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000061
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000061_0: Committed
15/08/21 19:47:24 INFO Executor: Finished task 61.0 in stage 4.0 (TID 309). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 323, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 75.0 in stage 4.0 (TID 323)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 309) in 1149 ms on localhost (60/200)
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000062
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000062_0: Committed
15/08/21 19:47:24 INFO Executor: Finished task 62.0 in stage 4.0 (TID 310). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 310) in 1082 ms on localhost (61/200)
15/08/21 19:47:24 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 324, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000064
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000064_0: Committed
15/08/21 19:47:24 INFO Executor: Running task 76.0 in stage 4.0 (TID 324)
15/08/21 19:47:24 INFO Executor: Finished task 64.0 in stage 4.0 (TID 312). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 325, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 77.0 in stage 4.0 (TID 325)
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 312) in 1026 ms on localhost (62/200)
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,960
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 49,955B for [ps_partkey] INT32: 14,985 values, 59,948B raw, 49,916B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 77,033B for [part_value] DOUBLE: 14,985 values, 119,888B raw, 76,986B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,636
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,088
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000066
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000066_0: Committed
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,708
15/08/21 19:47:24 INFO Executor: Finished task 66.0 in stage 4.0 (TID 314). 843 bytes result sent to driver
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 50,100B for [ps_partkey] INT32: 14,991 values, 59,972B raw, 50,061B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 326, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 77,153B for [part_value] DOUBLE: 14,991 values, 119,936B raw, 77,106B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO Executor: Running task 78.0 in stage 4.0 (TID 326)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 314) in 415 ms on localhost (63/200)
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,908
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 51,296B for [ps_partkey] INT32: 15,368 values, 61,480B raw, 51,257B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 79,001B for [part_value] DOUBLE: 15,368 values, 122,952B raw, 78,954B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 51,007B for [ps_partkey] INT32: 15,272 values, 61,096B raw, 50,968B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 78,724B for [part_value] DOUBLE: 15,272 values, 122,184B raw, 78,677B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,528
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,636
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,620
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 50,034B for [ps_partkey] INT32: 14,982 values, 59,936B raw, 49,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 77,103B for [part_value] DOUBLE: 14,982 values, 119,864B raw, 77,056B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 48,889B for [ps_partkey] INT32: 14,718 values, 58,880B raw, 48,850B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 75,723B for [part_value] DOUBLE: 14,718 values, 117,752B raw, 75,676B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 48,837B for [ps_partkey] INT32: 14,613 values, 58,460B raw, 48,798B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 50,207B for [ps_partkey] INT32: 15,018 values, 60,080B raw, 50,168B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 77,277B for [part_value] DOUBLE: 15,018 values, 120,152B raw, 77,230B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 75,206B for [part_value] DOUBLE: 14,613 values, 116,912B raw, 75,159B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,892
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000058
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000058_0: Committed
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000069
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000069_0: Committed
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000071
15/08/21 19:47:24 INFO Executor: Finished task 58.0 in stage 4.0 (TID 306). 843 bytes result sent to driver
15/08/21 19:47:24 INFO Executor: Finished task 69.0 in stage 4.0 (TID 317). 843 bytes result sent to driver
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000071_0: Committed
15/08/21 19:47:24 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 327, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 328, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 80.0 in stage 4.0 (TID 328)
15/08/21 19:47:24 INFO Executor: Finished task 71.0 in stage 4.0 (TID 319). 843 bytes result sent to driver
15/08/21 19:47:24 INFO Executor: Running task 79.0 in stage 4.0 (TID 327)
15/08/21 19:47:24 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 329, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 81.0 in stage 4.0 (TID 329)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 306) in 1582 ms on localhost (64/200)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 317) in 434 ms on localhost (65/200)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 319) in 414 ms on localhost (66/200)
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000057
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000057_0: Committed
15/08/21 19:47:24 INFO Executor: Finished task 57.0 in stage 4.0 (TID 305). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 330, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,676
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,771,796
15/08/21 19:47:24 INFO Executor: Running task 82.0 in stage 4.0 (TID 330)
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 50,888B for [ps_partkey] INT32: 15,282 values, 61,136B raw, 50,849B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 78,626B for [part_value] DOUBLE: 15,282 values, 122,264B raw, 78,579B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 305) in 1615 ms on localhost (67/200)
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000067
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000067_0: Committed
15/08/21 19:47:24 INFO Executor: Finished task 67.0 in stage 4.0 (TID 315). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 331, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 83.0 in stage 4.0 (TID 331)
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 315) in 481 ms on localhost (68/200)
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,736
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000068
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000068_0: Committed
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 52,486B for [ps_partkey] INT32: 15,726 values, 62,912B raw, 52,447B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 80,849B for [part_value] DOUBLE: 15,726 values, 125,816B raw, 80,802B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 51,686B for [ps_partkey] INT32: 15,470 values, 61,888B raw, 51,647B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 79,465B for [part_value] DOUBLE: 15,470 values, 123,768B raw, 79,418B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO Executor: Finished task 68.0 in stage 4.0 (TID 316). 843 bytes result sent to driver
15/08/21 19:47:24 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 332, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 84.0 in stage 4.0 (TID 332)
15/08/21 19:47:24 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 316) in 497 ms on localhost (69/200)
15/08/21 19:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000073
15/08/21 19:47:24 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000073_0: Committed
15/08/21 19:47:24 INFO Executor: Finished task 73.0 in stage 4.0 (TID 321). 843 bytes result sent to driver
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 333, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:24 INFO Executor: Running task 85.0 in stage 4.0 (TID 333)
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 49,878B for [ps_partkey] INT32: 14,923 values, 59,700B raw, 49,839B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO ColumnChunkPageWriteStore: written 76,795B for [part_value] DOUBLE: 14,923 values, 119,392B raw, 76,748B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:24 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 321) in 454 ms on localhost (70/200)
15/08/21 19:47:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:24 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:24 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000060
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000074
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000074_0: Committed
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000060_0: Committed
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO Executor: Finished task 74.0 in stage 4.0 (TID 322). 843 bytes result sent to driver
15/08/21 19:47:25 INFO Executor: Finished task 60.0 in stage 4.0 (TID 308). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 334, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 335, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 86.0 in stage 4.0 (TID 334)
15/08/21 19:47:25 INFO Executor: Running task 87.0 in stage 4.0 (TID 335)
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000065
15/08/21 19:47:25 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 322) in 491 ms on localhost (71/200)
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000065_0: Committed
15/08/21 19:47:25 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 308) in 1660 ms on localhost (72/200)
15/08/21 19:47:25 INFO Executor: Finished task 65.0 in stage 4.0 (TID 313). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 336, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 88.0 in stage 4.0 (TID 336)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 313) in 1467 ms on localhost (73/200)
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,072
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,755B for [ps_partkey] INT32: 15,191 values, 60,772B raw, 50,716B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 78,053B for [part_value] DOUBLE: 15,191 values, 121,536B raw, 78,006B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,056
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,616
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,116
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 49,928B for [ps_partkey] INT32: 14,939 values, 59,764B raw, 49,889B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 76,865B for [part_value] DOUBLE: 14,939 values, 119,520B raw, 76,818B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,002B for [ps_partkey] INT32: 14,967 values, 59,876B raw, 49,963B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 77,120B for [part_value] DOUBLE: 14,967 values, 119,744B raw, 77,073B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,548
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,270B for [ps_partkey] INT32: 15,142 values, 60,576B raw, 50,231B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 77,924B for [part_value] DOUBLE: 15,142 values, 121,144B raw, 77,877B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000078
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000078_0: Committed
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 49,611B for [ps_partkey] INT32: 14,864 values, 59,464B raw, 49,572B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 76,384B for [part_value] DOUBLE: 14,864 values, 118,920B raw, 76,337B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,428
15/08/21 19:47:25 INFO Executor: Finished task 78.0 in stage 4.0 (TID 326). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 337, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 89.0 in stage 4.0 (TID 337)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 326) in 422 ms on localhost (74/200)
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 51,142B for [ps_partkey] INT32: 15,358 values, 61,440B raw, 51,103B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 78,917B for [part_value] DOUBLE: 15,358 values, 122,872B raw, 78,870B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,036
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,296
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000080
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000080_0: Committed
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000079
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000079_0: Committed
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO Executor: Finished task 80.0 in stage 4.0 (TID 328). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 338, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Finished task 79.0 in stage 4.0 (TID 327). 843 bytes result sent to driver
15/08/21 19:47:25 INFO Executor: Running task 90.0 in stage 4.0 (TID 338)
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,216B for [ps_partkey] INT32: 15,038 values, 60,160B raw, 50,177B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 339, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 77,257B for [part_value] DOUBLE: 15,038 values, 120,312B raw, 77,210B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO Executor: Running task 91.0 in stage 4.0 (TID 339)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 328) in 393 ms on localhost (75/200)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 327) in 395 ms on localhost (76/200)
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,082B for [ps_partkey] INT32: 15,001 values, 60,012B raw, 50,043B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 77,129B for [part_value] DOUBLE: 15,001 values, 120,016B raw, 77,082B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000081
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000083
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000081_0: Committed
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000083_0: Committed
15/08/21 19:47:25 INFO Executor: Finished task 83.0 in stage 4.0 (TID 331). 843 bytes result sent to driver
15/08/21 19:47:25 INFO Executor: Finished task 81.0 in stage 4.0 (TID 329). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 340, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 92.0 in stage 4.0 (TID 340)
15/08/21 19:47:25 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 341, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 93.0 in stage 4.0 (TID 341)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 331) in 394 ms on localhost (77/200)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 329) in 438 ms on localhost (78/200)
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000070
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000082
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000070_0: Committed
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000082_0: Committed
15/08/21 19:47:25 INFO Executor: Finished task 70.0 in stage 4.0 (TID 318). 843 bytes result sent to driver
15/08/21 19:47:25 INFO Executor: Finished task 82.0 in stage 4.0 (TID 330). 843 bytes result sent to driver
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,536
15/08/21 19:47:25 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 342, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 94.0 in stage 4.0 (TID 342)
15/08/21 19:47:25 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 343, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 95.0 in stage 4.0 (TID 343)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 330) in 437 ms on localhost (79/200)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 318) in 878 ms on localhost (80/200)
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000072
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000072_0: Committed
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,188
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000084
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000084_0: Committed
15/08/21 19:47:25 INFO Executor: Finished task 72.0 in stage 4.0 (TID 320). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 344, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 96.0 in stage 4.0 (TID 344)
15/08/21 19:47:25 INFO Executor: Finished task 84.0 in stage 4.0 (TID 332). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 345, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 332) in 411 ms on localhost (81/200)
15/08/21 19:47:25 INFO Executor: Running task 97.0 in stage 4.0 (TID 345)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 320) in 886 ms on localhost (82/200)
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,088
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 51,121B for [ps_partkey] INT32: 15,296 values, 61,192B raw, 51,082B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 51,622B for [ps_partkey] INT32: 15,463 values, 61,860B raw, 51,583B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 79,556B for [part_value] DOUBLE: 15,463 values, 123,712B raw, 79,509B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 78,667B for [part_value] DOUBLE: 15,296 values, 122,376B raw, 78,620B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 49,730B for [ps_partkey] INT32: 14,891 values, 59,572B raw, 49,691B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 76,638B for [part_value] DOUBLE: 14,891 values, 119,136B raw, 76,591B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000077
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000077_0: Committed
15/08/21 19:47:25 INFO Executor: Finished task 77.0 in stage 4.0 (TID 325). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 346, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 98.0 in stage 4.0 (TID 346)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 325) in 842 ms on localhost (83/200)
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000075
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000076
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000075_0: Committed
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000076_0: Committed
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO Executor: Finished task 76.0 in stage 4.0 (TID 324). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 347, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 99.0 in stage 4.0 (TID 347)
15/08/21 19:47:25 INFO Executor: Finished task 75.0 in stage 4.0 (TID 323). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 348, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 100.0 in stage 4.0 (TID 348)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 324) in 866 ms on localhost (84/200)
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000087
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000087_0: Committed
15/08/21 19:47:25 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 323) in 898 ms on localhost (85/200)
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000086
15/08/21 19:47:25 INFO Executor: Finished task 87.0 in stage 4.0 (TID 335). 843 bytes result sent to driver
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000086_0: Committed
15/08/21 19:47:25 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 349, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000088
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000088_0: Committed
15/08/21 19:47:25 INFO Executor: Running task 101.0 in stage 4.0 (TID 349)
15/08/21 19:47:25 INFO Executor: Finished task 86.0 in stage 4.0 (TID 334). 843 bytes result sent to driver
15/08/21 19:47:25 INFO Executor: Finished task 88.0 in stage 4.0 (TID 336). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 350, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 102.0 in stage 4.0 (TID 350)
15/08/21 19:47:25 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 351, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 103.0 in stage 4.0 (TID 351)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 335) in 445 ms on localhost (86/200)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 336) in 438 ms on localhost (87/200)
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 334) in 449 ms on localhost (88/200)
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,316
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,836B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 50,797B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 78,688B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,641B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,548
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,360
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 52,016B for [ps_partkey] INT32: 15,614 values, 62,464B raw, 51,977B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 80,268B for [part_value] DOUBLE: 15,614 values, 124,920B raw, 80,221B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000089
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000089_0: Committed
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO Executor: Finished task 89.0 in stage 4.0 (TID 337). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 352, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 337) in 431 ms on localhost (89/200)
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO Executor: Running task 104.0 in stage 4.0 (TID 352)
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,969B for [ps_partkey] INT32: 15,255 values, 61,028B raw, 50,930B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 78,402B for [part_value] DOUBLE: 15,255 values, 122,048B raw, 78,355B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,356
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,268
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000090
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000090_0: Committed
15/08/21 19:47:25 INFO Executor: Finished task 90.0 in stage 4.0 (TID 338). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 353, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 105.0 in stage 4.0 (TID 353)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 338) in 442 ms on localhost (90/200)
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 51,764B for [ps_partkey] INT32: 15,500 values, 62,008B raw, 51,725B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 79,825B for [part_value] DOUBLE: 15,500 values, 124,008B raw, 79,778B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,413B for [ps_partkey] INT32: 15,104 values, 60,424B raw, 50,374B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 77,613B for [part_value] DOUBLE: 15,104 values, 120,840B raw, 77,566B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,328
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,588
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,773,160
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 50,466B for [ps_partkey] INT32: 15,103 values, 60,420B raw, 50,427B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 77,626B for [part_value] DOUBLE: 15,103 values, 120,832B raw, 77,579B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,296
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 51,665B for [ps_partkey] INT32: 15,566 values, 62,272B raw, 51,626B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 79,967B for [part_value] DOUBLE: 15,566 values, 124,536B raw, 79,920B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 49,323B for [ps_partkey] INT32: 14,751 values, 59,012B raw, 49,284B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 52,751B for [ps_partkey] INT32: 15,795 values, 63,188B raw, 52,712B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 75,816B for [part_value] DOUBLE: 14,751 values, 118,016B raw, 75,769B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO ColumnChunkPageWriteStore: written 81,317B for [part_value] DOUBLE: 15,795 values, 126,368B raw, 81,270B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000091
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000091_0: Committed
15/08/21 19:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000085
15/08/21 19:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000085_0: Committed
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:25 INFO Executor: Finished task 91.0 in stage 4.0 (TID 339). 843 bytes result sent to driver
15/08/21 19:47:25 INFO Executor: Finished task 85.0 in stage 4.0 (TID 333). 843 bytes result sent to driver
15/08/21 19:47:25 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 354, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 106.0 in stage 4.0 (TID 354)
15/08/21 19:47:25 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 355, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:25 INFO Executor: Running task 107.0 in stage 4.0 (TID 355)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 333) in 1029 ms on localhost (91/200)
15/08/21 19:47:25 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 339) in 719 ms on localhost (92/200)
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000093
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000093_0: Committed
15/08/21 19:47:26 INFO Executor: Finished task 93.0 in stage 4.0 (TID 341). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 356, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 108.0 in stage 4.0 (TID 356)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 341) in 732 ms on localhost (93/200)
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000095
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000095_0: Committed
15/08/21 19:47:26 INFO Executor: Finished task 95.0 in stage 4.0 (TID 343). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 357, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO Executor: Running task 109.0 in stage 4.0 (TID 357)
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 343) in 725 ms on localhost (94/200)
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,648
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,076
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,348
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,836
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,748
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,188
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,096
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,436
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 51,460B for [ps_partkey] INT32: 15,404 values, 61,624B raw, 51,421B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,681B for [ps_partkey] INT32: 14,878 values, 59,520B raw, 49,642B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 48,714B for [ps_partkey] INT32: 14,569 values, 58,284B raw, 48,675B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 50,464B for [ps_partkey] INT32: 15,090 values, 60,368B raw, 50,425B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 79,159B for [part_value] DOUBLE: 15,404 values, 123,240B raw, 79,112B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,614B for [part_value] DOUBLE: 14,878 values, 119,032B raw, 76,567B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 77,675B for [part_value] DOUBLE: 15,090 values, 120,728B raw, 77,628B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 75,126B for [part_value] DOUBLE: 14,569 values, 116,560B raw, 75,079B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,711B for [ps_partkey] INT32: 14,874 values, 59,504B raw, 49,672B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,490B for [part_value] DOUBLE: 14,874 values, 119,000B raw, 76,443B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,813B for [ps_partkey] INT32: 14,991 values, 59,972B raw, 49,774B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 48,414B for [ps_partkey] INT32: 14,508 values, 58,040B raw, 48,375B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 77,080B for [part_value] DOUBLE: 14,991 values, 119,936B raw, 77,033B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 74,748B for [part_value] DOUBLE: 14,508 values, 116,072B raw, 74,701B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 50,436B for [ps_partkey] INT32: 15,096 values, 60,392B raw, 50,397B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 77,569B for [part_value] DOUBLE: 15,096 values, 120,776B raw, 77,522B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000105
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000105_0: Committed
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000100
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000100_0: Committed
15/08/21 19:47:26 INFO Executor: Finished task 100.0 in stage 4.0 (TID 348). 843 bytes result sent to driver
15/08/21 19:47:26 INFO Executor: Finished task 105.0 in stage 4.0 (TID 353). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 358, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 110.0 in stage 4.0 (TID 358)
15/08/21 19:47:26 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 359, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 111.0 in stage 4.0 (TID 359)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 348) in 743 ms on localhost (95/200)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 353) in 500 ms on localhost (96/200)
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000104
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000104_0: Committed
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000103
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000103_0: Committed
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000102
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000102_0: Committed
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO Executor: Finished task 103.0 in stage 4.0 (TID 351). 843 bytes result sent to driver
15/08/21 19:47:26 INFO Executor: Finished task 104.0 in stage 4.0 (TID 352). 843 bytes result sent to driver
15/08/21 19:47:26 INFO Executor: Finished task 102.0 in stage 4.0 (TID 350). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 360, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 112.0 in stage 4.0 (TID 360)
15/08/21 19:47:26 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 361, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 113.0 in stage 4.0 (TID 361)
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 362, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 114.0 in stage 4.0 (TID 362)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 351) in 735 ms on localhost (97/200)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 352) in 566 ms on localhost (98/200)
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000098
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000098_0: Committed
15/08/21 19:47:26 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 350) in 744 ms on localhost (99/200)
15/08/21 19:47:26 INFO Executor: Finished task 98.0 in stage 4.0 (TID 346). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 363, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 115.0 in stage 4.0 (TID 363)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 346) in 787 ms on localhost (100/200)
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,236
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,888
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,492B for [ps_partkey] INT32: 14,848 values, 59,400B raw, 49,453B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,418B for [part_value] DOUBLE: 14,848 values, 118,792B raw, 76,371B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 50,561B for [ps_partkey] INT32: 15,131 values, 60,532B raw, 50,522B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 77,778B for [part_value] DOUBLE: 15,131 values, 121,056B raw, 77,731B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,756
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,028
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,529B for [ps_partkey] INT32: 14,824 values, 59,304B raw, 49,490B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,382B for [part_value] DOUBLE: 14,824 values, 118,600B raw, 76,335B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,599B for [ps_partkey] INT32: 14,838 values, 59,360B raw, 49,560B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,341B for [part_value] DOUBLE: 14,838 values, 118,712B raw, 76,294B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000107
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000107_0: Committed
15/08/21 19:47:26 INFO Executor: Finished task 107.0 in stage 4.0 (TID 355). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 364, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 116.0 in stage 4.0 (TID 364)
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 355) in 435 ms on localhost (101/200)
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000109
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000109_0: Committed
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO Executor: Finished task 109.0 in stage 4.0 (TID 357). 843 bytes result sent to driver
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 365, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO Executor: Running task 117.0 in stage 4.0 (TID 365)
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 357) in 349 ms on localhost (102/200)
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000097
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000097_0: Committed
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000094
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000094_0: Committed
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO Executor: Finished task 97.0 in stage 4.0 (TID 345). 843 bytes result sent to driver
15/08/21 19:47:26 INFO Executor: Finished task 94.0 in stage 4.0 (TID 342). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 366, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO Executor: Running task 118.0 in stage 4.0 (TID 366)
15/08/21 19:47:26 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 367, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 119.0 in stage 4.0 (TID 367)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 345) in 1111 ms on localhost (103/200)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 342) in 1123 ms on localhost (104/200)
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000092
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000092_0: Committed
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO Executor: Finished task 92.0 in stage 4.0 (TID 340). 843 bytes result sent to driver
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000108
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000108_0: Committed
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000096
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000096_0: Committed
15/08/21 19:47:26 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 368, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 340) in 1160 ms on localhost (105/200)
15/08/21 19:47:26 INFO Executor: Running task 120.0 in stage 4.0 (TID 368)
15/08/21 19:47:26 INFO Executor: Finished task 108.0 in stage 4.0 (TID 356). 843 bytes result sent to driver
15/08/21 19:47:26 INFO Executor: Finished task 96.0 in stage 4.0 (TID 344). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 369, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 121.0 in stage 4.0 (TID 369)
15/08/21 19:47:26 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 370, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 122.0 in stage 4.0 (TID 370)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 344) in 1138 ms on localhost (106/200)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 356) in 444 ms on localhost (107/200)
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,744,856
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,268
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,956
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,116
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,200
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,860
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 47,957B for [ps_partkey] INT32: 14,379 values, 57,524B raw, 47,918B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 73,881B for [part_value] DOUBLE: 14,379 values, 115,040B raw, 73,834B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,965B for [ps_partkey] INT32: 14,942 values, 59,776B raw, 49,926B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,902B for [ps_partkey] INT32: 14,934 values, 59,744B raw, 49,863B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,305B for [ps_partkey] INT32: 14,750 values, 59,008B raw, 49,266B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,683B for [part_value] DOUBLE: 14,942 values, 119,544B raw, 76,636B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,798B for [part_value] DOUBLE: 14,934 values, 119,480B raw, 76,751B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 75,859B for [part_value] DOUBLE: 14,750 values, 118,008B raw, 75,812B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 48,805B for [ps_partkey] INT32: 14,597 values, 58,396B raw, 48,766B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 75,162B for [part_value] DOUBLE: 14,597 values, 116,784B raw, 75,115B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,133B for [ps_partkey] INT32: 14,780 values, 59,128B raw, 49,094B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,143B for [part_value] DOUBLE: 14,780 values, 118,248B raw, 76,096B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000099
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000099_0: Committed
15/08/21 19:47:26 INFO Executor: Finished task 99.0 in stage 4.0 (TID 347). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 371, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 123.0 in stage 4.0 (TID 371)
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 347) in 1163 ms on localhost (108/200)
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000101
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000101_0: Committed
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO Executor: Finished task 101.0 in stage 4.0 (TID 349). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 372, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 124.0 in stage 4.0 (TID 372)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 349) in 1190 ms on localhost (109/200)
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000110
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000110_0: Committed
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000115
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000114
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000115_0: Committed
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000114_0: Committed
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO Executor: Finished task 110.0 in stage 4.0 (TID 358). 843 bytes result sent to driver
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000113
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000113_0: Committed
15/08/21 19:47:26 INFO Executor: Finished task 115.0 in stage 4.0 (TID 363). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 373, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 125.0 in stage 4.0 (TID 373)
15/08/21 19:47:26 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 374, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Finished task 113.0 in stage 4.0 (TID 361). 843 bytes result sent to driver
15/08/21 19:47:26 INFO Executor: Finished task 114.0 in stage 4.0 (TID 362). 843 bytes result sent to driver
15/08/21 19:47:26 INFO Executor: Running task 126.0 in stage 4.0 (TID 374)
15/08/21 19:47:26 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 375, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 127.0 in stage 4.0 (TID 375)
15/08/21 19:47:26 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 376, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 358) in 498 ms on localhost (110/200)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 363) in 478 ms on localhost (111/200)
15/08/21 19:47:26 INFO Executor: Running task 128.0 in stage 4.0 (TID 376)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 361) in 490 ms on localhost (112/200)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 362) in 491 ms on localhost (113/200)
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,008
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,746,856
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000106
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000106_0: Committed
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,105B for [ps_partkey] INT32: 14,687 values, 58,756B raw, 49,066B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 75,641B for [part_value] DOUBLE: 14,687 values, 117,504B raw, 75,594B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO Executor: Finished task 106.0 in stage 4.0 (TID 354). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 377, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 129.0 in stage 4.0 (TID 377)
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 354) in 827 ms on localhost (114/200)
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 48,423B for [ps_partkey] INT32: 14,479 values, 57,924B raw, 48,384B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 74,557B for [part_value] DOUBLE: 14,479 values, 115,840B raw, 74,510B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,276
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,746,052
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,288
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000116
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000116_0: Committed
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 48,681B for [ps_partkey] INT32: 14,550 values, 58,208B raw, 48,642B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 74,857B for [part_value] DOUBLE: 14,550 values, 116,408B raw, 74,810B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO Executor: Finished task 116.0 in stage 4.0 (TID 364). 843 bytes result sent to driver
15/08/21 19:47:26 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 378, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO Executor: Running task 130.0 in stage 4.0 (TID 378)
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 48,143B for [ps_partkey] INT32: 14,440 values, 57,768B raw, 48,104B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 74,116B for [part_value] DOUBLE: 14,440 values, 115,528B raw, 74,069B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 364) in 491 ms on localhost (115/200)
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,971B for [ps_partkey] INT32: 14,951 values, 59,812B raw, 49,932B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,964B for [part_value] DOUBLE: 14,951 values, 119,616B raw, 76,917B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000117
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000117_0: Committed
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,332
15/08/21 19:47:26 INFO Executor: Finished task 117.0 in stage 4.0 (TID 365). 843 bytes result sent to driver
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 379, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO Executor: Running task 131.0 in stage 4.0 (TID 379)
15/08/21 19:47:26 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 365) in 532 ms on localhost (116/200)
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 49,980B for [ps_partkey] INT32: 14,954 values, 59,824B raw, 49,941B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO ColumnChunkPageWriteStore: written 76,859B for [part_value] DOUBLE: 14,954 values, 119,640B raw, 76,812B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000118
15/08/21 19:47:26 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000118_0: Committed
15/08/21 19:47:26 INFO Executor: Finished task 118.0 in stage 4.0 (TID 366). 843 bytes result sent to driver
15/08/21 19:47:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:26 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,300
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:26 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 380, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:26 INFO Executor: Running task 132.0 in stage 4.0 (TID 380)
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:26 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:26 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 366) in 546 ms on localhost (117/200)
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000122
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000122_0: Committed
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000120
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000120_0: Committed
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO Executor: Finished task 120.0 in stage 4.0 (TID 368). 843 bytes result sent to driver
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 381, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 133.0 in stage 4.0 (TID 381)
15/08/21 19:47:27 INFO Executor: Finished task 122.0 in stage 4.0 (TID 370). 843 bytes result sent to driver
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 382, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 134.0 in stage 4.0 (TID 382)
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 368) in 575 ms on localhost (118/200)
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000119
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000119_0: Committed
15/08/21 19:47:27 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 370) in 563 ms on localhost (119/200)
15/08/21 19:47:27 INFO Executor: Finished task 119.0 in stage 4.0 (TID 367). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 383, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 135.0 in stage 4.0 (TID 383)
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000112
15/08/21 19:47:27 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 367) in 596 ms on localhost (120/200)
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,235B for [ps_partkey] INT32: 14,802 values, 59,216B raw, 49,196B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000112_0: Committed
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,170B for [part_value] DOUBLE: 14,802 values, 118,424B raw, 76,123B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000111
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000111_0: Committed
15/08/21 19:47:27 INFO Executor: Finished task 112.0 in stage 4.0 (TID 360). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 384, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Finished task 111.0 in stage 4.0 (TID 359). 843 bytes result sent to driver
15/08/21 19:47:27 INFO Executor: Running task 136.0 in stage 4.0 (TID 384)
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,528
15/08/21 19:47:27 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 385, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 137.0 in stage 4.0 (TID 385)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 360) in 861 ms on localhost (121/200)
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 359) in 875 ms on localhost (122/200)
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,476
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,896
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,801B for [ps_partkey] INT32: 14,913 values, 59,660B raw, 49,762B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,631B for [part_value] DOUBLE: 14,913 values, 119,312B raw, 76,584B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,574B for [ps_partkey] INT32: 14,831 values, 59,332B raw, 49,535B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,286B for [part_value] DOUBLE: 14,831 values, 118,656B raw, 76,239B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,525B for [ps_partkey] INT32: 14,810 values, 59,248B raw, 49,486B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,193B for [part_value] DOUBLE: 14,810 values, 118,488B raw, 76,146B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,416
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,368
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 50,147B for [ps_partkey] INT32: 15,007 values, 60,036B raw, 50,108B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 77,232B for [part_value] DOUBLE: 15,007 values, 120,064B raw, 77,185B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 50,319B for [ps_partkey] INT32: 15,055 values, 60,228B raw, 50,280B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 77,437B for [part_value] DOUBLE: 15,055 values, 120,448B raw, 77,390B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000121
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000121_0: Committed
15/08/21 19:47:27 INFO Executor: Finished task 121.0 in stage 4.0 (TID 369). 843 bytes result sent to driver
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 386, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,676
15/08/21 19:47:27 INFO Executor: Running task 138.0 in stage 4.0 (TID 386)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 369) in 769 ms on localhost (123/200)
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000126
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000126_0: Committed
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 52,003B for [ps_partkey] INT32: 15,570 values, 62,288B raw, 51,964B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 80,039B for [part_value] DOUBLE: 15,570 values, 124,568B raw, 79,992B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO Executor: Finished task 126.0 in stage 4.0 (TID 374). 843 bytes result sent to driver
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 387, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 139.0 in stage 4.0 (TID 387)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 374) in 607 ms on localhost (124/200)
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000125
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000125_0: Committed
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,848
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000124
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000124_0: Committed
15/08/21 19:47:27 INFO Executor: Finished task 124.0 in stage 4.0 (TID 372). 843 bytes result sent to driver
15/08/21 19:47:27 INFO Executor: Finished task 125.0 in stage 4.0 (TID 373). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 388, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 140.0 in stage 4.0 (TID 388)
15/08/21 19:47:27 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 389, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 141.0 in stage 4.0 (TID 389)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 372) in 681 ms on localhost (125/200)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 373) in 647 ms on localhost (126/200)
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,300
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,502B for [ps_partkey] INT32: 14,879 values, 59,524B raw, 49,463B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,540B for [part_value] DOUBLE: 14,879 values, 119,040B raw, 76,493B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 48,404B for [ps_partkey] INT32: 14,502 values, 58,016B raw, 48,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,088
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 74,467B for [part_value] DOUBLE: 14,502 values, 116,024B raw, 74,420B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,270B for [ps_partkey] INT32: 14,741 values, 58,972B raw, 49,231B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 75,768B for [part_value] DOUBLE: 14,741 values, 117,936B raw, 75,721B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,216
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,556
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000129
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000129_0: Committed
15/08/21 19:47:27 INFO Executor: Finished task 129.0 in stage 4.0 (TID 377). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 390, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 142.0 in stage 4.0 (TID 390)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 377) in 701 ms on localhost (127/200)
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000130
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,468
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000130_0: Committed
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000131
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000131_0: Committed
15/08/21 19:47:27 INFO Executor: Finished task 130.0 in stage 4.0 (TID 378). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 391, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 143.0 in stage 4.0 (TID 391)
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 50,222B for [ps_partkey] INT32: 15,114 values, 60,464B raw, 50,183B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO Executor: Finished task 131.0 in stage 4.0 (TID 379). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 392, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 77,686B for [part_value] DOUBLE: 15,114 values, 120,920B raw, 77,639B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 51,267B for [ps_partkey] INT32: 15,347 values, 61,396B raw, 51,228B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 379) in 596 ms on localhost (128/200)
15/08/21 19:47:27 INFO Executor: Running task 144.0 in stage 4.0 (TID 392)
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 78,878B for [part_value] DOUBLE: 15,347 values, 122,784B raw, 78,831B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 378) in 663 ms on localhost (129/200)
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 50,105B for [ps_partkey] INT32: 15,010 values, 60,048B raw, 50,066B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 77,136B for [part_value] DOUBLE: 15,010 values, 120,088B raw, 77,089B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,856
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,136
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,136
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,594B for [ps_partkey] INT32: 14,829 values, 59,324B raw, 49,555B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,110B for [part_value] DOUBLE: 14,829 values, 118,640B raw, 76,063B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,112B for [ps_partkey] INT32: 14,693 values, 58,780B raw, 49,073B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 75,560B for [part_value] DOUBLE: 14,693 values, 117,552B raw, 75,513B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 50,257B for [ps_partkey] INT32: 15,043 values, 60,180B raw, 50,218B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 77,384B for [part_value] DOUBLE: 15,043 values, 120,352B raw, 77,337B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000135
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000135_0: Committed
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000123
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000123_0: Committed
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO Executor: Finished task 135.0 in stage 4.0 (TID 383). 843 bytes result sent to driver
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000136
15/08/21 19:47:27 INFO Executor: Finished task 123.0 in stage 4.0 (TID 371). 843 bytes result sent to driver
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000136_0: Committed
15/08/21 19:47:27 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 393, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 145.0 in stage 4.0 (TID 393)
15/08/21 19:47:27 INFO Executor: Finished task 136.0 in stage 4.0 (TID 384). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 394, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 146.0 in stage 4.0 (TID 394)
15/08/21 19:47:27 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 395, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 147.0 in stage 4.0 (TID 395)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 383) in 663 ms on localhost (130/200)
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 384) in 668 ms on localhost (131/200)
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 371) in 1112 ms on localhost (132/200)
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000127
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000127_0: Committed
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO Executor: Finished task 127.0 in stage 4.0 (TID 375). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 396, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 148.0 in stage 4.0 (TID 396)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 375) in 1054 ms on localhost (133/200)
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,480
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 51,320B for [ps_partkey] INT32: 15,361 values, 61,452B raw, 51,281B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 79,054B for [part_value] DOUBLE: 15,361 values, 122,896B raw, 79,007B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,770,848
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,176
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 52,405B for [ps_partkey] INT32: 15,679 values, 62,724B raw, 52,366B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 80,646B for [part_value] DOUBLE: 15,679 values, 125,440B raw, 80,599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,912
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,514B for [ps_partkey] INT32: 14,845 values, 59,388B raw, 49,475B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,816
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,410B for [part_value] DOUBLE: 14,845 values, 118,768B raw, 76,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,724B for [ps_partkey] INT32: 14,883 values, 59,540B raw, 49,685B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 76,569B for [part_value] DOUBLE: 14,883 values, 119,072B raw, 76,522B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 49,094B for [ps_partkey] INT32: 14,677 values, 58,716B raw, 49,055B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO ColumnChunkPageWriteStore: written 75,416B for [part_value] DOUBLE: 14,677 values, 117,424B raw, 75,369B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000128
15/08/21 19:47:27 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000128_0: Committed
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:27 INFO Executor: Finished task 128.0 in stage 4.0 (TID 376). 843 bytes result sent to driver
15/08/21 19:47:27 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 397, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:27 INFO Executor: Running task 149.0 in stage 4.0 (TID 397)
15/08/21 19:47:27 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 376) in 1291 ms on localhost (134/200)
15/08/21 19:47:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:27 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:27 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000137
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000137_0: Committed
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000134
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000134_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 134.0 in stage 4.0 (TID 382). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 398, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 150.0 in stage 4.0 (TID 398)
15/08/21 19:47:28 INFO Executor: Finished task 137.0 in stage 4.0 (TID 385). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 399, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 151.0 in stage 4.0 (TID 399)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 382) in 975 ms on localhost (135/200)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 385) in 973 ms on localhost (136/200)
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,908
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000139
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000139_0: Committed
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,925B for [ps_partkey] INT32: 15,532 values, 62,136B raw, 51,886B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 80,023B for [part_value] DOUBLE: 15,532 values, 124,264B raw, 79,976B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO Executor: Finished task 139.0 in stage 4.0 (TID 387). 843 bytes result sent to driver
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,028
15/08/21 19:47:28 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 400, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 152.0 in stage 4.0 (TID 400)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 387) in 788 ms on localhost (137/200)
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000140
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000140_0: Committed
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO Executor: Finished task 140.0 in stage 4.0 (TID 388). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 401, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 153.0 in stage 4.0 (TID 401)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 388) in 789 ms on localhost (138/200)
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000138
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000138_0: Committed
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,548
15/08/21 19:47:28 INFO Executor: Finished task 138.0 in stage 4.0 (TID 386). 843 bytes result sent to driver
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 402, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 49,437B for [ps_partkey] INT32: 14,788 values, 59,160B raw, 49,398B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO Executor: Running task 154.0 in stage 4.0 (TID 402)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 76,001B for [part_value] DOUBLE: 14,788 values, 118,312B raw, 75,954B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 386) in 899 ms on localhost (139/200)
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000132
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000132_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 132.0 in stage 4.0 (TID 380). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 403, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 155.0 in stage 4.0 (TID 403)
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,308
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 380) in 1187 ms on localhost (140/200)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 49,700B for [ps_partkey] INT32: 14,914 values, 59,664B raw, 49,661B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 76,716B for [part_value] DOUBLE: 14,914 values, 119,320B raw, 76,669B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,096B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 51,057B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 78,697B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,650B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000133
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000133_0: Committed
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,448
15/08/21 19:47:28 INFO Executor: Finished task 133.0 in stage 4.0 (TID 381). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 404, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 156.0 in stage 4.0 (TID 404)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 381) in 1228 ms on localhost (141/200)
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,168
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,781B for [ps_partkey] INT32: 15,509 values, 62,044B raw, 51,742B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 79,704B for [part_value] DOUBLE: 15,509 values, 124,080B raw, 79,657B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000142
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000142_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 142.0 in stage 4.0 (TID 390). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 405, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 157.0 in stage 4.0 (TID 405)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 390) in 824 ms on localhost (142/200)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,652B for [ps_partkey] INT32: 15,545 values, 62,188B raw, 51,613B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 79,880B for [part_value] DOUBLE: 15,545 values, 124,368B raw, 79,833B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000145
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000145_0: Committed
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000148
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000148_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 145.0 in stage 4.0 (TID 393). 843 bytes result sent to driver
15/08/21 19:47:28 INFO Executor: Finished task 148.0 in stage 4.0 (TID 396). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 406, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 158.0 in stage 4.0 (TID 406)
15/08/21 19:47:28 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 407, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 159.0 in stage 4.0 (TID 407)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 396) in 662 ms on localhost (143/200)
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 393) in 709 ms on localhost (144/200)
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000141
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000141_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 141.0 in stage 4.0 (TID 389). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 408, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 160.0 in stage 4.0 (TID 408)
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,456
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 389) in 1132 ms on localhost (145/200)
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,520
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 49,504B for [ps_partkey] INT32: 14,809 values, 59,244B raw, 49,465B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 76,281B for [part_value] DOUBLE: 14,809 values, 118,480B raw, 76,234B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,736
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000143
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000143_0: Committed
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO Executor: Finished task 143.0 in stage 4.0 (TID 391). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 409, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 391) in 1011 ms on localhost (146/200)
15/08/21 19:47:28 INFO Executor: Running task 161.0 in stage 4.0 (TID 409)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 50,311B for [ps_partkey] INT32: 15,063 values, 60,260B raw, 50,272B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 77,535B for [part_value] DOUBLE: 15,063 values, 120,512B raw, 77,488B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 50,196B for [ps_partkey] INT32: 15,023 values, 60,100B raw, 50,157B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 77,252B for [part_value] DOUBLE: 15,023 values, 120,192B raw, 77,205B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,756
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000152
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000152_0: Committed
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000144
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000144_0: Committed
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,936
15/08/21 19:47:28 INFO Executor: Finished task 144.0 in stage 4.0 (TID 392). 843 bytes result sent to driver
15/08/21 19:47:28 INFO Executor: Finished task 152.0 in stage 4.0 (TID 400). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 410, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 162.0 in stage 4.0 (TID 410)
15/08/21 19:47:28 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 411, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 163.0 in stage 4.0 (TID 411)
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 400) in 515 ms on localhost (147/200)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 392) in 1068 ms on localhost (148/200)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,005B for [ps_partkey] INT32: 15,274 values, 61,104B raw, 50,966B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 78,604B for [part_value] DOUBLE: 15,274 values, 122,200B raw, 78,557B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,996
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 50,652B for [ps_partkey] INT32: 15,183 values, 60,740B raw, 50,613B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 78,131B for [part_value] DOUBLE: 15,183 values, 121,472B raw, 78,084B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 52,190B for [ps_partkey] INT32: 15,636 values, 62,552B raw, 52,151B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 80,404B for [part_value] DOUBLE: 15,636 values, 125,096B raw, 80,357B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,448
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,684
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,023B for [ps_partkey] INT32: 15,359 values, 61,444B raw, 50,984B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 78,933B for [part_value] DOUBLE: 15,359 values, 122,880B raw, 78,886B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 50,997B for [ps_partkey] INT32: 15,272 values, 61,096B raw, 50,958B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 78,449B for [part_value] DOUBLE: 15,272 values, 122,184B raw, 78,402B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000149
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000155
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000155_0: Committed
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000150
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000149_0: Committed
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000150_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 149.0 in stage 4.0 (TID 397). 843 bytes result sent to driver
15/08/21 19:47:28 INFO Executor: Finished task 155.0 in stage 4.0 (TID 403). 843 bytes result sent to driver
15/08/21 19:47:28 INFO Executor: Finished task 150.0 in stage 4.0 (TID 398). 843 bytes result sent to driver
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 412, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 164.0 in stage 4.0 (TID 412)
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 413, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 414, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 166.0 in stage 4.0 (TID 414)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 397) in 757 ms on localhost (149/200)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 403) in 565 ms on localhost (150/200)
15/08/21 19:47:28 INFO Executor: Running task 165.0 in stage 4.0 (TID 413)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 398) in 717 ms on localhost (151/200)
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000154
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000154_0: Committed
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000146
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000146_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 154.0 in stage 4.0 (TID 402). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 415, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 167.0 in stage 4.0 (TID 415)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 402) in 621 ms on localhost (152/200)
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,988
15/08/21 19:47:28 INFO Executor: Finished task 146.0 in stage 4.0 (TID 394). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 416, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 168.0 in stage 4.0 (TID 416)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 394) in 1042 ms on localhost (153/200)
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000147
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000147_0: Committed
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000156
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000156_0: Committed
15/08/21 19:47:28 INFO Executor: Finished task 156.0 in stage 4.0 (TID 404). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 417, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 169.0 in stage 4.0 (TID 417)
15/08/21 19:47:28 INFO Executor: Finished task 147.0 in stage 4.0 (TID 395). 843 bytes result sent to driver
15/08/21 19:47:28 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 418, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 170.0 in stage 4.0 (TID 418)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 404) in 501 ms on localhost (154/200)
15/08/21 19:47:28 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 395) in 1058 ms on localhost (155/200)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 49,271B for [ps_partkey] INT32: 14,736 values, 58,952B raw, 49,232B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 75,793B for [part_value] DOUBLE: 14,736 values, 117,896B raw, 75,746B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,770,436
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000159
15/08/21 19:47:28 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000159_0: Committed
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,256
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO Executor: Finished task 159.0 in stage 4.0 (TID 407). 843 bytes result sent to driver
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 419, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:28 INFO Executor: Running task 171.0 in stage 4.0 (TID 419)
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:28 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 407) in 436 ms on localhost (156/200)
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 52,299B for [ps_partkey] INT32: 15,658 values, 62,640B raw, 52,260B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 80,491B for [part_value] DOUBLE: 15,658 values, 125,272B raw, 80,444B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:28 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:28 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,756
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,914B for [ps_partkey] INT32: 15,549 values, 62,204B raw, 51,875B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 79,983B for [part_value] DOUBLE: 15,549 values, 124,400B raw, 79,936B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,240
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 51,705B for [ps_partkey] INT32: 15,474 values, 61,904B raw, 51,666B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 79,616B for [part_value] DOUBLE: 15,474 values, 123,800B raw, 79,569B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 50,149B for [ps_partkey] INT32: 15,099 values, 60,404B raw, 50,110B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:28 INFO ColumnChunkPageWriteStore: written 77,711B for [part_value] DOUBLE: 15,099 values, 120,800B raw, 77,664B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,748
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,244
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 50,579B for [ps_partkey] INT32: 15,150 values, 60,608B raw, 50,540B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 51,721B for [ps_partkey] INT32: 15,524 values, 62,104B raw, 51,682B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 77,838B for [part_value] DOUBLE: 15,150 values, 121,208B raw, 77,791B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 79,875B for [part_value] DOUBLE: 15,524 values, 124,200B raw, 79,828B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000158
15/08/21 19:47:29 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000158_0: Committed
15/08/21 19:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000160
15/08/21 19:47:29 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000160_0: Committed
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO Executor: Finished task 160.0 in stage 4.0 (TID 408). 843 bytes result sent to driver
15/08/21 19:47:29 INFO Executor: Finished task 158.0 in stage 4.0 (TID 406). 843 bytes result sent to driver
15/08/21 19:47:29 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 420, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 421, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:29 INFO Executor: Running task 173.0 in stage 4.0 (TID 421)
15/08/21 19:47:29 INFO Executor: Running task 172.0 in stage 4.0 (TID 420)
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,048
15/08/21 19:47:29 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 408) in 695 ms on localhost (157/200)
15/08/21 19:47:29 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 406) in 735 ms on localhost (158/200)
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,788
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 50,076B for [ps_partkey] INT32: 14,989 values, 59,964B raw, 50,037B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 77,123B for [part_value] DOUBLE: 14,989 values, 119,920B raw, 77,076B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 49,558B for [ps_partkey] INT32: 14,826 values, 59,312B raw, 49,519B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 76,257B for [part_value] DOUBLE: 14,826 values, 118,616B raw, 76,210B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000153
15/08/21 19:47:29 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000153_0: Committed
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO Executor: Finished task 153.0 in stage 4.0 (TID 401). 843 bytes result sent to driver
15/08/21 19:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000151
15/08/21 19:47:29 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000151_0: Committed
15/08/21 19:47:29 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 422, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:29 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 401) in 1115 ms on localhost (159/200)
15/08/21 19:47:29 INFO Executor: Running task 174.0 in stage 4.0 (TID 422)
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO Executor: Finished task 151.0 in stage 4.0 (TID 399). 843 bytes result sent to driver
15/08/21 19:47:29 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 423, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:29 INFO Executor: Running task 175.0 in stage 4.0 (TID 423)
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:29 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 399) in 1215 ms on localhost (160/200)
15/08/21 19:47:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000162
15/08/21 19:47:29 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000162_0: Committed
15/08/21 19:47:29 INFO Executor: Finished task 162.0 in stage 4.0 (TID 410). 843 bytes result sent to driver
15/08/21 19:47:29 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 424, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:29 INFO Executor: Running task 176.0 in stage 4.0 (TID 424)
15/08/21 19:47:29 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 410) in 666 ms on localhost (161/200)
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,200
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,236
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,888
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 50,594B for [ps_partkey] INT32: 15,147 values, 60,596B raw, 50,555B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 77,989B for [part_value] DOUBLE: 15,147 values, 121,184B raw, 77,942B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,036
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 50,438B for [ps_partkey] INT32: 15,098 values, 60,400B raw, 50,399B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 77,641B for [part_value] DOUBLE: 15,098 values, 120,792B raw, 77,594B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,608
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,956
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 49,221B for [ps_partkey] INT32: 14,731 values, 58,932B raw, 49,182B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 75,709B for [part_value] DOUBLE: 14,731 values, 117,856B raw, 75,662B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 49,662B for [ps_partkey] INT32: 14,938 values, 59,760B raw, 49,623B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 76,717B for [part_value] DOUBLE: 14,938 values, 119,512B raw, 76,670B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 49,777B for [ps_partkey] INT32: 14,884 values, 59,544B raw, 49,738B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 50,882B for [ps_partkey] INT32: 15,267 values, 61,076B raw, 50,843B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 76,597B for [part_value] DOUBLE: 14,884 values, 119,080B raw, 76,550B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 78,518B for [part_value] DOUBLE: 15,267 values, 122,144B raw, 78,471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,056
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,432
15/08/21 19:47:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 50,231B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,192B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 77,408B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,361B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 52,093B for [ps_partkey] INT32: 15,609 values, 62,444B raw, 52,054B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO ColumnChunkPageWriteStore: written 80,224B for [part_value] DOUBLE: 15,609 values, 124,880B raw, 80,177B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:29 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:29 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,556
15/08/21 19:47:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,856
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 50,524B for [ps_partkey] INT32: 15,129 values, 60,524B raw, 50,485B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 77,969B for [part_value] DOUBLE: 15,129 values, 121,040B raw, 77,922B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 51,348B for [ps_partkey] INT32: 15,364 values, 61,464B raw, 51,309B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 79,172B for [part_value] DOUBLE: 15,364 values, 122,920B raw, 79,125B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,788
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 49,687B for [ps_partkey] INT32: 14,876 values, 59,512B raw, 49,648B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 76,407B for [part_value] DOUBLE: 14,876 values, 119,016B raw, 76,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000161
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000157
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000163
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000157_0: Committed
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000163_0: Committed
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000161_0: Committed
15/08/21 19:47:30 INFO Executor: Finished task 157.0 in stage 4.0 (TID 405). 843 bytes result sent to driver
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000167
15/08/21 19:47:30 INFO Executor: Finished task 163.0 in stage 4.0 (TID 411). 843 bytes result sent to driver
15/08/21 19:47:30 INFO Executor: Finished task 161.0 in stage 4.0 (TID 409). 843 bytes result sent to driver
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000167_0: Committed
15/08/21 19:47:30 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 425, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 177.0 in stage 4.0 (TID 425)
15/08/21 19:47:30 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 426, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Finished task 167.0 in stage 4.0 (TID 415). 843 bytes result sent to driver
15/08/21 19:47:30 INFO Executor: Running task 178.0 in stage 4.0 (TID 426)
15/08/21 19:47:30 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 427, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 179.0 in stage 4.0 (TID 427)
15/08/21 19:47:30 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 428, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 180.0 in stage 4.0 (TID 428)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 411) in 1901 ms on localhost (162/200)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 405) in 2188 ms on localhost (163/200)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 409) in 1976 ms on localhost (164/200)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 415) in 1746 ms on localhost (165/200)
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000164
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000164_0: Committed
15/08/21 19:47:30 INFO Executor: Finished task 164.0 in stage 4.0 (TID 412). 843 bytes result sent to driver
15/08/21 19:47:30 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 429, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 181.0 in stage 4.0 (TID 429)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 412) in 1784 ms on localhost (166/200)
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000165
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000165_0: Committed
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000173
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000169
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000173_0: Committed
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000169_0: Committed
15/08/21 19:47:30 INFO Executor: Finished task 165.0 in stage 4.0 (TID 413). 843 bytes result sent to driver
15/08/21 19:47:30 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 430, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Finished task 173.0 in stage 4.0 (TID 421). 843 bytes result sent to driver
15/08/21 19:47:30 INFO Executor: Finished task 169.0 in stage 4.0 (TID 417). 843 bytes result sent to driver
15/08/21 19:47:30 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 431, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 183.0 in stage 4.0 (TID 431)
15/08/21 19:47:30 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 432, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 182.0 in stage 4.0 (TID 430)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 413) in 1793 ms on localhost (167/200)
15/08/21 19:47:30 INFO Executor: Running task 184.0 in stage 4.0 (TID 432)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 417) in 1758 ms on localhost (168/200)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 421) in 1396 ms on localhost (169/200)
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000176
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000176_0: Committed
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO Executor: Finished task 176.0 in stage 4.0 (TID 424). 843 bytes result sent to driver
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:30 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 433, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 185.0 in stage 4.0 (TID 433)
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000175
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000175_0: Committed
15/08/21 19:47:30 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 424) in 1320 ms on localhost (170/200)
15/08/21 19:47:30 INFO Executor: Finished task 175.0 in stage 4.0 (TID 423). 843 bytes result sent to driver
15/08/21 19:47:30 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 434, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 186.0 in stage 4.0 (TID 434)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 423) in 1342 ms on localhost (171/200)
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,788
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:30 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:30 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 49,299B for [ps_partkey] INT32: 14,826 values, 59,312B raw, 49,260B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 76,279B for [part_value] DOUBLE: 14,826 values, 118,616B raw, 76,232B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,028
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,048
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,948
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,536
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,996
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,008
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,308
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,488
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 49,423B for [ps_partkey] INT32: 14,788 values, 59,160B raw, 49,384B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 51,029B for [ps_partkey] INT32: 15,313 values, 61,260B raw, 50,990B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 49,428B for [ps_partkey] INT32: 14,784 values, 59,144B raw, 49,389B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 51,700B for [ps_partkey] INT32: 15,489 values, 61,964B raw, 51,661B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 76,170B for [part_value] DOUBLE: 14,788 values, 118,312B raw, 76,123B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 76,037B for [part_value] DOUBLE: 14,784 values, 118,280B raw, 75,990B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 79,563B for [part_value] DOUBLE: 15,489 values, 123,920B raw, 79,516B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 78,700B for [part_value] DOUBLE: 15,313 values, 122,512B raw, 78,653B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 52,053B for [ps_partkey] INT32: 15,586 values, 62,352B raw, 52,014B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 50,452B for [ps_partkey] INT32: 15,137 values, 60,556B raw, 50,413B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 80,219B for [part_value] DOUBLE: 15,586 values, 124,696B raw, 80,172B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 77,897B for [part_value] DOUBLE: 15,137 values, 121,104B raw, 77,850B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,600
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 50,836B for [ps_partkey] INT32: 15,211 values, 60,852B raw, 50,797B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 78,247B for [part_value] DOUBLE: 15,211 values, 121,696B raw, 78,200B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 49,666B for [ps_partkey] INT32: 14,952 values, 59,816B raw, 49,627B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO ColumnChunkPageWriteStore: written 76,926B for [part_value] DOUBLE: 14,952 values, 119,624B raw, 76,879B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000166
15/08/21 19:47:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000171
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000166_0: Committed
15/08/21 19:47:30 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000171_0: Committed
15/08/21 19:47:30 INFO Executor: Finished task 166.0 in stage 4.0 (TID 414). 843 bytes result sent to driver
15/08/21 19:47:30 INFO Executor: Finished task 171.0 in stage 4.0 (TID 419). 843 bytes result sent to driver
15/08/21 19:47:30 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 435, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 187.0 in stage 4.0 (TID 435)
15/08/21 19:47:30 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 436, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:30 INFO Executor: Running task 188.0 in stage 4.0 (TID 436)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 419) in 2170 ms on localhost (172/200)
15/08/21 19:47:30 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 414) in 2276 ms on localhost (173/200)
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 50,512B for [ps_partkey] INT32: 15,117 values, 60,476B raw, 50,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 77,798B for [part_value] DOUBLE: 15,117 values, 120,944B raw, 77,751B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000170
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000170_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 170.0 in stage 4.0 (TID 418). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 437, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Running task 189.0 in stage 4.0 (TID 437)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 418) in 2285 ms on localhost (174/200)
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000177
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000174
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000174_0: Committed
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000172
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000172_0: Committed
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000168
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000168_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 172.0 in stage 4.0 (TID 420). 843 bytes result sent to driver
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000177_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 174.0 in stage 4.0 (TID 422). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 438, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Finished task 177.0 in stage 4.0 (TID 425). 843 bytes result sent to driver
15/08/21 19:47:31 INFO Executor: Finished task 168.0 in stage 4.0 (TID 416). 843 bytes result sent to driver
15/08/21 19:47:31 INFO Executor: Running task 190.0 in stage 4.0 (TID 438)
15/08/21 19:47:31 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 439, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Running task 191.0 in stage 4.0 (TID 439)
15/08/21 19:47:31 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 440, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Running task 192.0 in stage 4.0 (TID 440)
15/08/21 19:47:31 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 441, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Running task 193.0 in stage 4.0 (TID 441)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 422) in 1898 ms on localhost (175/200)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 420) in 1993 ms on localhost (176/200)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 425) in 633 ms on localhost (177/200)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 416) in 2367 ms on localhost (178/200)
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000183
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000183_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 183.0 in stage 4.0 (TID 431). 843 bytes result sent to driver
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 442, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO Executor: Running task 194.0 in stage 4.0 (TID 442)
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 431) in 659 ms on localhost (179/200)
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000186
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000186_0: Committed
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000182
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000182_0: Committed
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO Executor: Finished task 186.0 in stage 4.0 (TID 434). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 443, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Finished task 182.0 in stage 4.0 (TID 430). 843 bytes result sent to driver
15/08/21 19:47:31 INFO Executor: Running task 195.0 in stage 4.0 (TID 443)
15/08/21 19:47:31 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 444, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Running task 196.0 in stage 4.0 (TID 444)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 434) in 656 ms on localhost (180/200)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 430) in 710 ms on localhost (181/200)
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,448
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,416
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 50,186B for [ps_partkey] INT32: 15,009 values, 60,044B raw, 50,147B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 77,274B for [part_value] DOUBLE: 15,009 values, 120,080B raw, 77,227B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 50,785B for [ps_partkey] INT32: 15,207 values, 60,836B raw, 50,746B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 78,386B for [part_value] DOUBLE: 15,207 values, 121,664B raw, 78,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,168
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 49,470B for [ps_partkey] INT32: 14,795 values, 59,188B raw, 49,431B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 75,985B for [part_value] DOUBLE: 14,795 values, 118,368B raw, 75,938B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000188
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000188_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 188.0 in stage 4.0 (TID 436). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 445, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Running task 197.0 in stage 4.0 (TID 445)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 436) in 518 ms on localhost (182/200)
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,068
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000178
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000178_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 178.0 in stage 4.0 (TID 426). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 446, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,768
15/08/21 19:47:31 INFO Executor: Running task 198.0 in stage 4.0 (TID 446)
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000184
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000184_0: Committed
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000180
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000180_0: Committed
15/08/21 19:47:31 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 426) in 1038 ms on localhost (183/200)
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,908
15/08/21 19:47:31 INFO Executor: Finished task 184.0 in stage 4.0 (TID 432). 843 bytes result sent to driver
15/08/21 19:47:31 INFO Executor: Finished task 180.0 in stage 4.0 (TID 428). 843 bytes result sent to driver
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000179
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000179_0: Committed
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,868
15/08/21 19:47:31 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 447, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:31 INFO Executor: Running task 199.0 in stage 4.0 (TID 447)
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000181
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000181_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 179.0 in stage 4.0 (TID 427). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 428) in 1041 ms on localhost (184/200)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 432) in 1013 ms on localhost (185/200)
15/08/21 19:47:31 INFO Executor: Finished task 181.0 in stage 4.0 (TID 429). 843 bytes result sent to driver
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,416
15/08/21 19:47:31 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 427) in 1046 ms on localhost (186/200)
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,120
15/08/21 19:47:31 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 429) in 1032 ms on localhost (187/200)
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,688
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 52,194B for [ps_partkey] INT32: 15,625 values, 62,508B raw, 52,155B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 80,315B for [part_value] DOUBLE: 15,625 values, 125,008B raw, 80,268B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 51,188B for [ps_partkey] INT32: 15,330 values, 61,328B raw, 51,149B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 51,297B for [ps_partkey] INT32: 15,340 values, 61,368B raw, 51,258B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 78,891B for [part_value] DOUBLE: 15,330 values, 122,648B raw, 78,844B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 78,833B for [part_value] DOUBLE: 15,340 values, 122,728B raw, 78,786B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 51,427B for [ps_partkey] INT32: 15,432 values, 61,736B raw, 51,388B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 79,261B for [part_value] DOUBLE: 15,432 values, 123,464B raw, 79,214B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 50,514B for [ps_partkey] INT32: 15,107 values, 60,436B raw, 50,475B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 77,758B for [part_value] DOUBLE: 15,107 values, 120,864B raw, 77,711B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 51,976B for [ps_partkey] INT32: 15,571 values, 62,292B raw, 51,937B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 79,945B for [part_value] DOUBLE: 15,571 values, 124,576B raw, 79,898B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 49,166B for [ps_partkey] INT32: 14,793 values, 59,180B raw, 49,127B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 75,993B for [part_value] DOUBLE: 14,793 values, 118,352B raw, 75,946B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000185
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000185_0: Committed
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO Executor: Finished task 185.0 in stage 4.0 (TID 433). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 433) in 1149 ms on localhost (188/200)
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:31 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:31 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,088
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,196
15/08/21 19:47:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,996
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 51,393B for [ps_partkey] INT32: 15,396 values, 61,592B raw, 51,354B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 79,186B for [part_value] DOUBLE: 15,396 values, 123,176B raw, 79,139B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 49,411B for [ps_partkey] INT32: 14,791 values, 59,172B raw, 49,372B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 76,124B for [part_value] DOUBLE: 14,791 values, 118,336B raw, 76,077B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 49,578B for [ps_partkey] INT32: 14,836 values, 59,352B raw, 49,539B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO ColumnChunkPageWriteStore: written 76,367B for [part_value] DOUBLE: 14,836 values, 118,696B raw, 76,320B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000187
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000187_0: Committed
15/08/21 19:47:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000189
15/08/21 19:47:31 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000189_0: Committed
15/08/21 19:47:31 INFO Executor: Finished task 189.0 in stage 4.0 (TID 437). 843 bytes result sent to driver
15/08/21 19:47:31 INFO Executor: Finished task 187.0 in stage 4.0 (TID 435). 843 bytes result sent to driver
15/08/21 19:47:31 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 437) in 951 ms on localhost (189/200)
15/08/21 19:47:31 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 435) in 984 ms on localhost (190/200)
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000198
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000198_0: Committed
15/08/21 19:47:32 INFO Executor: Finished task 198.0 in stage 4.0 (TID 446). 843 bytes result sent to driver
15/08/21 19:47:32 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 446) in 625 ms on localhost (191/200)
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000199
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000199_0: Committed
15/08/21 19:47:32 INFO Executor: Finished task 199.0 in stage 4.0 (TID 447). 843 bytes result sent to driver
15/08/21 19:47:32 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 447) in 698 ms on localhost (192/200)
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000195
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000195_0: Committed
15/08/21 19:47:32 INFO Executor: Finished task 195.0 in stage 4.0 (TID 443). 843 bytes result sent to driver
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000196
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000196_0: Committed
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000192
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000192_0: Committed
15/08/21 19:47:32 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 443) in 1060 ms on localhost (193/200)
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000191
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000191_0: Committed
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000193
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000190
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000194
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000193_0: Committed
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000190_0: Committed
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000194_0: Committed
15/08/21 19:47:32 INFO Executor: Finished task 192.0 in stage 4.0 (TID 440). 843 bytes result sent to driver
15/08/21 19:47:32 INFO Executor: Finished task 191.0 in stage 4.0 (TID 439). 843 bytes result sent to driver
15/08/21 19:47:32 INFO Executor: Finished task 193.0 in stage 4.0 (TID 441). 843 bytes result sent to driver
15/08/21 19:47:32 INFO Executor: Finished task 194.0 in stage 4.0 (TID 442). 843 bytes result sent to driver
15/08/21 19:47:32 INFO Executor: Finished task 190.0 in stage 4.0 (TID 438). 843 bytes result sent to driver
15/08/21 19:47:32 INFO Executor: Finished task 196.0 in stage 4.0 (TID 444). 843 bytes result sent to driver
15/08/21 19:47:32 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 440) in 1172 ms on localhost (194/200)
15/08/21 19:47:32 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 439) in 1172 ms on localhost (195/200)
15/08/21 19:47:32 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 441) in 1171 ms on localhost (196/200)
15/08/21 19:47:32 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 442) in 1115 ms on localhost (197/200)
15/08/21 19:47:32 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 444) in 1067 ms on localhost (198/200)
15/08/21 19:47:32 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 438) in 1177 ms on localhost (199/200)
15/08/21 19:47:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0004_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508211947_0004_m_000197
15/08/21 19:47:32 INFO SparkHadoopMapRedUtil: attempt_201508211947_0004_m_000197_0: Committed
15/08/21 19:47:32 INFO Executor: Finished task 197.0 in stage 4.0 (TID 445). 843 bytes result sent to driver
15/08/21 19:47:32 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 445) in 986 ms on localhost (200/200)
15/08/21 19:47:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/21 19:47:32 INFO DAGScheduler: ResultStage 4 (processCmd at CliDriver.java:423) finished in 12.585 s
15/08/21 19:47:32 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 48.388357 s
15/08/21 19:47:32 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1037dc4a
15/08/21 19:47:32 INFO StatsReportListener: task runtime:(count: 200, mean: 977.050000, stdev: 526.747660, max: 2367.000000, min: 322.000000)
15/08/21 19:47:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:32 INFO StatsReportListener: 	322.0 ms	415.0 ms	442.0 ms	530.0 ms	866.0 ms	1.2 s	2.0 s	2.1 s	2.4 s
15/08/21 19:47:32 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.265000, stdev: 0.561048, max: 4.000000, min: 0.000000)
15/08/21 19:47:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:32 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	4.0 ms
15/08/21 19:47:32 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 19:47:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:32 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 19:47:32 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 19:47:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:32 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 19:47:32 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 89.146942, stdev: 4.988472, max: 97.050691, min: 71.650212)
15/08/21 19:47:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:32 INFO StatsReportListener: 	72 %	79 %	82 %	86 %	90 %	93 %	95 %	96 %	97 %
15/08/21 19:47:32 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.032766, stdev: 0.075113, max: 0.447427, min: 0.000000)
15/08/21 19:47:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:32 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 19:47:32 INFO StatsReportListener: other time pct: (count: 200, mean: 10.820292, stdev: 4.978539, max: 28.349788, min: 2.949309)
15/08/21 19:47:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:32 INFO StatsReportListener: 	 3 %	 4 %	 5 %	 7 %	10 %	14 %	18 %	21 %	28 %
15/08/21 19:47:34 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 19:47:35 INFO DefaultWriterContainer: Job job_201508211946_0000 committed.
15/08/21 19:47:35 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 19:47:35 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_common_metadata
15/08/21 19:47:35 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 19:47:35 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 19:47:35 INFO DAGScheduler: Final stage: ResultStage 5(processCmd at CliDriver.java:423)
15/08/21 19:47:35 INFO DAGScheduler: Parents of final stage: List()
15/08/21 19:47:35 INFO DAGScheduler: Missing parents: List()
15/08/21 19:47:35 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 19:47:35 INFO MemoryStore: ensureFreeSpace(2968) called with curMem=1162857, maxMem=22226833244
15/08/21 19:47:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 19:47:35 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=1165825, maxMem=22226833244
15/08/21 19:47:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1776.0 B, free 20.7 GB)
15/08/21 19:47:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:54384 (size: 1776.0 B, free: 20.7 GB)
15/08/21 19:47:35 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874
15/08/21 19:47:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/21 19:47:35 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/08/21 19:47:35 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 448, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 19:47:35 INFO Executor: Running task 0.0 in stage 5.0 (TID 448)
15/08/21 19:47:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 448). 606 bytes result sent to driver
15/08/21 19:47:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 448) in 53 ms on localhost (1/1)
15/08/21 19:47:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/21 19:47:36 INFO DAGScheduler: ResultStage 5 (processCmd at CliDriver.java:423) finished in 0.053 s
15/08/21 19:47:36 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@86a50ab
15/08/21 19:47:36 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 0.071950 s
15/08/21 19:47:36 INFO StatsReportListener: task runtime:(count: 1, mean: 53.000000, stdev: 0.000000, max: 53.000000, min: 53.000000)
15/08/21 19:47:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:36 INFO StatsReportListener: 	53.0 ms	53.0 ms	53.0 ms	53.0 ms	53.0 ms	53.0 ms	53.0 ms	53.0 ms	53.0 ms
15/08/21 19:47:36 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 19:47:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:36 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 19:47:36 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 26.415094, stdev: 0.000000, max: 26.415094, min: 26.415094)
15/08/21 19:47:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:36 INFO StatsReportListener: 	26 %	26 %	26 %	26 %	26 %	26 %	26 %	26 %	26 %
15/08/21 19:47:36 INFO StatsReportListener: other time pct: (count: 1, mean: 73.584906, stdev: 0.000000, max: 73.584906, min: 73.584906)
15/08/21 19:47:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:36 INFO StatsReportListener: 	74 %	74 %	74 %	74 %	74 %	74 %	74 %	74 %	74 %
Time taken: 58.778 seconds
15/08/21 19:47:36 INFO CliDriver: Time taken: 58.778 seconds
15/08/21 19:47:36 INFO ParseDriver: Parsing command: insert into table q11_important_stock_par
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par) sum_tmp
    join q11_part_tmp_par
where part_value > total_value * 0.000001
order by value desc
15/08/21 19:47:36 INFO ParseDriver: Parse Completed
15/08/21 19:47:36 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 19:47:36 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1167601, maxMem=22226833244
15/08/21 19:47:36 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 19:47:36 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1494209, maxMem=22226833244
15/08/21 19:47:36 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 19:47:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:54384 (size: 22.3 KB, free: 20.7 GB)
15/08/21 19:47:36 INFO SparkContext: Created broadcast 10 from processCmd at CliDriver.java:423
15/08/21 19:47:36 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1517002, maxMem=22226833244
15/08/21 19:47:36 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 19:47:36 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1843610, maxMem=22226833244
15/08/21 19:47:36 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 19:47:36 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:54384 (size: 22.3 KB, free: 20.7 GB)
15/08/21 19:47:36 INFO SparkContext: Created broadcast 11 from processCmd at CliDriver.java:423
15/08/21 19:47:36 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 19:47:36 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 19:47:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 19:47:36 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 19:47:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 19:47:37 INFO DAGScheduler: Registering RDD 31 (processCmd at CliDriver.java:423)
15/08/21 19:47:37 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 19:47:37 INFO DAGScheduler: Final stage: ResultStage 7(processCmd at CliDriver.java:423)
15/08/21 19:47:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
15/08/21 19:47:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)
15/08/21 19:47:37 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 19:47:37 INFO MemoryStore: ensureFreeSpace(7688) called with curMem=1866403, maxMem=22226833244
15/08/21 19:47:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.5 KB, free 20.7 GB)
15/08/21 19:47:37 INFO MemoryStore: ensureFreeSpace(3917) called with curMem=1874091, maxMem=22226833244
15/08/21 19:47:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.8 KB, free 20.7 GB)
15/08/21 19:47:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:54384 (size: 3.8 KB, free: 20.7 GB)
15/08/21 19:47:37 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/21 19:47:37 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at processCmd at CliDriver.java:423)
15/08/21 19:47:37 INFO TaskSchedulerImpl: Adding task set 6.0 with 200 tasks
15/08/21 19:47:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 449, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 450, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 451, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 452, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 453, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 454, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 455, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 456, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 457, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 458, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 459, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 460, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 461, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 462, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 463, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 464, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 0.0 in stage 6.0 (TID 449)
15/08/21 19:47:37 INFO Executor: Running task 1.0 in stage 6.0 (TID 450)
15/08/21 19:47:37 INFO Executor: Running task 5.0 in stage 6.0 (TID 454)
15/08/21 19:47:37 INFO Executor: Running task 4.0 in stage 6.0 (TID 453)
15/08/21 19:47:37 INFO Executor: Running task 3.0 in stage 6.0 (TID 452)
15/08/21 19:47:37 INFO Executor: Running task 2.0 in stage 6.0 (TID 451)
15/08/21 19:47:37 INFO Executor: Running task 6.0 in stage 6.0 (TID 455)
15/08/21 19:47:37 INFO Executor: Running task 7.0 in stage 6.0 (TID 456)
15/08/21 19:47:37 INFO Executor: Running task 8.0 in stage 6.0 (TID 457)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 9.0 in stage 6.0 (TID 458)
15/08/21 19:47:37 INFO Executor: Running task 11.0 in stage 6.0 (TID 460)
15/08/21 19:47:37 INFO Executor: Running task 13.0 in stage 6.0 (TID 462)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 14.0 in stage 6.0 (TID 463)
15/08/21 19:47:37 INFO Executor: Running task 12.0 in stage 6.0 (TID 461)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 10.0 in stage 6.0 (TID 459)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 15.0 in stage 6.0 (TID 464)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14967
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 15235
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 14838
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15679
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 14829
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15029
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15393
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15586
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15384
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 14878
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 15001
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14731
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14718
15/08/21 19:47:37 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15079
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14824
15/08/21 19:47:37 INFO Executor: Finished task 1.0 in stage 6.0 (TID 450). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 449). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 3.0 in stage 6.0 (TID 452). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 14.0 in stage 6.0 (TID 463). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 465, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 6.0 in stage 6.0 (TID 455). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 466, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 16.0 in stage 6.0 (TID 465)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 467, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO Executor: Running task 17.0 in stage 6.0 (TID 466)
15/08/21 19:47:37 INFO Executor: Running task 18.0 in stage 6.0 (TID 467)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 468, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO Executor: Running task 19.0 in stage 6.0 (TID 468)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 469, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 20.0 in stage 6.0 (TID 469)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 450) in 110 ms on localhost (1/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 449) in 112 ms on localhost (2/200)
15/08/21 19:47:37 INFO Executor: Finished task 7.0 in stage 6.0 (TID 456). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 452) in 109 ms on localhost (3/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 470, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 463) in 104 ms on localhost (4/200)
15/08/21 19:47:37 INFO Executor: Running task 21.0 in stage 6.0 (TID 470)
15/08/21 19:47:37 INFO Executor: Finished task 13.0 in stage 6.0 (TID 462). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 471, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 22.0 in stage 6.0 (TID 471)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 456) in 116 ms on localhost (5/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 455) in 117 ms on localhost (6/200)
15/08/21 19:47:37 INFO Executor: Finished task 5.0 in stage 6.0 (TID 454). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 4.0 in stage 6.0 (TID 453). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 15.0 in stage 6.0 (TID 464). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 472, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO Executor: Running task 23.0 in stage 6.0 (TID 472)
15/08/21 19:47:37 INFO Executor: Finished task 12.0 in stage 6.0 (TID 461). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 462) in 115 ms on localhost (7/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 10.0 in stage 6.0 (TID 459). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 2.0 in stage 6.0 (TID 451). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 9.0 in stage 6.0 (TID 458). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 454) in 123 ms on localhost (8/200)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 473, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15010
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 8.0 in stage 6.0 (TID 457). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 24.0 in stage 6.0 (TID 473)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14954
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15282
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15023
15/08/21 19:47:37 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 474, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15302
15/08/21 19:47:37 INFO Executor: Running task 25.0 in stage 6.0 (TID 474)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 453) in 136 ms on localhost (9/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 464) in 133 ms on localhost (10/200)
15/08/21 19:47:37 INFO Executor: Finished task 11.0 in stage 6.0 (TID 460). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15570
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 475, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 461) in 144 ms on localhost (11/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 26.0 in stage 6.0 (TID 475)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14913
15/08/21 19:47:37 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 476, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO Executor: Running task 27.0 in stage 6.0 (TID 476)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 477, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 28.0 in stage 6.0 (TID 477)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 478, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 29.0 in stage 6.0 (TID 478)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 479, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 19.0 in stage 6.0 (TID 468). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 30.0 in stage 6.0 (TID 479)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 480, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 459) in 159 ms on localhost (12/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 31.0 in stage 6.0 (TID 480)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 451) in 164 ms on localhost (13/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 16.0 in stage 6.0 (TID 465). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14992
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14831
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 17.0 in stage 6.0 (TID 466). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 481, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 20.0 in stage 6.0 (TID 469). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 32.0 in stage 6.0 (TID 481)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 458) in 170 ms on localhost (14/200)
15/08/21 19:47:37 INFO Executor: Finished task 21.0 in stage 6.0 (TID 470). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 460) in 173 ms on localhost (15/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 457) in 176 ms on localhost (16/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14776
15/08/21 19:47:37 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 482, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 19:47:37 INFO Executor: Running task 33.0 in stage 6.0 (TID 482)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15131
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15055
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15106
15/08/21 19:47:37 INFO Executor: Finished task 23.0 in stage 6.0 (TID 472). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 18.0 in stage 6.0 (TID 467). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 483, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 24.0 in stage 6.0 (TID 473). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 468) in 87 ms on localhost (17/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 25.0 in stage 6.0 (TID 474). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 34.0 in stage 6.0 (TID 483)
15/08/21 19:47:37 INFO Executor: Finished task 22.0 in stage 6.0 (TID 471). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 484, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 35.0 in stage 6.0 (TID 484)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 465) in 94 ms on localhost (18/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15432
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14939
15/08/21 19:47:37 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 466) in 98 ms on localhost (19/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 485, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 36.0 in stage 6.0 (TID 485)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 470) in 94 ms on localhost (20/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14502
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 469) in 99 ms on localhost (21/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15272
15/08/21 19:47:37 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 486, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 37.0 in stage 6.0 (TID 486)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 487, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14597
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14942
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 472) in 90 ms on localhost (22/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 467) in 110 ms on localhost (23/200)
15/08/21 19:47:37 INFO Executor: Running task 38.0 in stage 6.0 (TID 487)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 488, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO Executor: Running task 39.0 in stage 6.0 (TID 488)
15/08/21 19:47:37 INFO Executor: Finished task 27.0 in stage 6.0 (TID 476). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 489, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 40.0 in stage 6.0 (TID 489)
15/08/21 19:47:37 INFO Executor: Finished task 28.0 in stage 6.0 (TID 477). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 31.0 in stage 6.0 (TID 480). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 490, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 41.0 in stage 6.0 (TID 490)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 473) in 99 ms on localhost (24/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 491, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 474) in 91 ms on localhost (25/200)
15/08/21 19:47:37 INFO Executor: Running task 42.0 in stage 6.0 (TID 491)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 492, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 43.0 in stage 6.0 (TID 492)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 26.0 in stage 6.0 (TID 475). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 33.0 in stage 6.0 (TID 482). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15107
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 29.0 in stage 6.0 (TID 478). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 493, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 476) in 76 ms on localhost (26/200)
15/08/21 19:47:37 INFO Executor: Finished task 32.0 in stage 6.0 (TID 481). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 30.0 in stage 6.0 (TID 479). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 34.0 in stage 6.0 (TID 483). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14972
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 471) in 115 ms on localhost (27/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 19:47:37 INFO Executor: Finished task 35.0 in stage 6.0 (TID 484). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 44.0 in stage 6.0 (TID 493)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 480) in 76 ms on localhost (28/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15302
15/08/21 19:47:37 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 494, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 477) in 81 ms on localhost (29/200)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 495, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 46.0 in stage 6.0 (TID 495)
15/08/21 19:47:37 INFO Executor: Running task 45.0 in stage 6.0 (TID 494)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15099
15/08/21 19:47:37 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 496, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 47.0 in stage 6.0 (TID 496)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 497, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15557
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 498, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 48.0 in stage 6.0 (TID 497)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 49.0 in stage 6.0 (TID 498)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 50.0 in stage 6.0 (TID 499, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 50.0 in stage 6.0 (TID 499)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 51.0 in stage 6.0 (TID 500, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 51.0 in stage 6.0 (TID 500)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14590
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14678
15/08/21 19:47:37 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 482) in 64 ms on localhost (30/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 475) in 107 ms on localhost (31/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 478) in 100 ms on localhost (32/200)
15/08/21 19:47:37 INFO Executor: Finished task 37.0 in stage 6.0 (TID 486). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15358
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 39.0 in stage 6.0 (TID 488). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 52.0 in stage 6.0 (TID 501, localhost, ANY, 1684 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15532
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14489
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14379
15/08/21 19:47:37 INFO Executor: Finished task 38.0 in stage 6.0 (TID 487). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 52.0 in stage 6.0 (TID 501)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 43.0 in stage 6.0 (TID 492). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 40.0 in stage 6.0 (TID 489). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 53.0 in stage 6.0 (TID 502, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 53.0 in stage 6.0 (TID 502)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 483) in 92 ms on localhost (33/200)
15/08/21 19:47:37 INFO Executor: Finished task 36.0 in stage 6.0 (TID 485). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 479) in 118 ms on localhost (34/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 41.0 in stage 6.0 (TID 490). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 54.0 in stage 6.0 (TID 503, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 55.0 in stage 6.0 (TID 504, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 54.0 in stage 6.0 (TID 503)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 55.0 in stage 6.0 (TID 504)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15614
15/08/21 19:47:37 INFO Executor: Finished task 46.0 in stage 6.0 (TID 495). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 56.0 in stage 6.0 (TID 505, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 56.0 in stage 6.0 (TID 505)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 57.0 in stage 6.0 (TID 506, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 481) in 119 ms on localhost (35/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 484) in 93 ms on localhost (36/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 57.0 in stage 6.0 (TID 506)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 58.0 in stage 6.0 (TID 507, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 58.0 in stage 6.0 (TID 507)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14906
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15104
15/08/21 19:47:37 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 492) in 64 ms on localhost (37/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14919
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 59.0 in stage 6.0 (TID 508, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 59.0 in stage 6.0 (TID 508)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 487) in 92 ms on localhost (38/200)
15/08/21 19:47:37 INFO Executor: Finished task 49.0 in stage 6.0 (TID 498). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 42.0 in stage 6.0 (TID 491). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 19:47:37 INFO Executor: Finished task 45.0 in stage 6.0 (TID 494). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14891
15/08/21 19:47:37 INFO Executor: Finished task 51.0 in stage 6.0 (TID 500). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 60.0 in stage 6.0 (TID 509, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15038
15/08/21 19:47:37 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 485) in 110 ms on localhost (39/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 486) in 106 ms on localhost (40/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 19:47:37 INFO Executor: Running task 60.0 in stage 6.0 (TID 509)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 488) in 94 ms on localhost (41/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 489) in 94 ms on localhost (42/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 48.0 in stage 6.0 (TID 497). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 44.0 in stage 6.0 (TID 493). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 47.0 in stage 6.0 (TID 496). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 50.0 in stage 6.0 (TID 499). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 19:47:37 INFO TaskSetManager: Starting task 61.0 in stage 6.0 (TID 510, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15383
15/08/21 19:47:37 INFO TaskSetManager: Starting task 62.0 in stage 6.0 (TID 511, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 62.0 in stage 6.0 (TID 511)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 63.0 in stage 6.0 (TID 512, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 63.0 in stage 6.0 (TID 512)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 64.0 in stage 6.0 (TID 513, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 52.0 in stage 6.0 (TID 501). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 64.0 in stage 6.0 (TID 513)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 19:47:37 INFO TaskSetManager: Starting task 65.0 in stage 6.0 (TID 514, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 65.0 in stage 6.0 (TID 514)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 66.0 in stage 6.0 (TID 515, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15145
15/08/21 19:47:37 INFO Executor: Finished task 53.0 in stage 6.0 (TID 502). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 61.0 in stage 6.0 (TID 510)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 495) in 87 ms on localhost (43/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 66.0 in stage 6.0 (TID 515)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15603
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 67.0 in stage 6.0 (TID 516, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 67.0 in stage 6.0 (TID 516)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 498) in 86 ms on localhost (44/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 68.0 in stage 6.0 (TID 517, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 491) in 111 ms on localhost (45/200)
15/08/21 19:47:37 INFO Executor: Running task 68.0 in stage 6.0 (TID 517)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 494) in 101 ms on localhost (46/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 490) in 116 ms on localhost (47/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 69.0 in stage 6.0 (TID 518, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 497) in 103 ms on localhost (48/200)
15/08/21 19:47:37 INFO Executor: Running task 69.0 in stage 6.0 (TID 518)
15/08/21 19:47:37 INFO Executor: Finished task 57.0 in stage 6.0 (TID 506). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 493) in 121 ms on localhost (49/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 56.0 in stage 6.0 (TID 505). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 50.0 in stage 6.0 (TID 499) in 103 ms on localhost (50/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 51.0 in stage 6.0 (TID 500) in 103 ms on localhost (51/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 496) in 110 ms on localhost (52/200)
15/08/21 19:47:37 INFO Executor: Finished task 59.0 in stage 6.0 (TID 508). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 55.0 in stage 6.0 (TID 504). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 70.0 in stage 6.0 (TID 519, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO Executor: Running task 70.0 in stage 6.0 (TID 519)
15/08/21 19:47:37 INFO Executor: Finished task 60.0 in stage 6.0 (TID 509). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15191
15/08/21 19:47:37 INFO TaskSetManager: Starting task 71.0 in stage 6.0 (TID 520, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 19:47:37 INFO Executor: Finished task 54.0 in stage 6.0 (TID 503). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 52.0 in stage 6.0 (TID 501) in 93 ms on localhost (53/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15046
15/08/21 19:47:37 INFO TaskSetManager: Starting task 72.0 in stage 6.0 (TID 521, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 71.0 in stage 6.0 (TID 520)
15/08/21 19:47:37 INFO Executor: Running task 72.0 in stage 6.0 (TID 521)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 73.0 in stage 6.0 (TID 522, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 73.0 in stage 6.0 (TID 522)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14772
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 74.0 in stage 6.0 (TID 523, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 57.0 in stage 6.0 (TID 506) in 80 ms on localhost (54/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15043
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 74.0 in stage 6.0 (TID 523)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 53.0 in stage 6.0 (TID 502) in 89 ms on localhost (55/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 56.0 in stage 6.0 (TID 505) in 83 ms on localhost (56/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 75.0 in stage 6.0 (TID 524, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 75.0 in stage 6.0 (TID 524)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 55.0 in stage 6.0 (TID 504) in 89 ms on localhost (57/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 60.0 in stage 6.0 (TID 509) in 61 ms on localhost (58/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 54.0 in stage 6.0 (TID 503) in 91 ms on localhost (59/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 58.0 in stage 6.0 (TID 507). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 59.0 in stage 6.0 (TID 508) in 78 ms on localhost (60/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 76.0 in stage 6.0 (TID 525, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15364
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15063
15/08/21 19:47:37 INFO TaskSetManager: Finished task 58.0 in stage 6.0 (TID 507) in 90 ms on localhost (61/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14845
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 76.0 in stage 6.0 (TID 525)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15009
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15333
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15609
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15132
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14612
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15058
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15211
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14740
15/08/21 19:47:37 INFO Executor: Finished task 64.0 in stage 6.0 (TID 513). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 65.0 in stage 6.0 (TID 514). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 63.0 in stage 6.0 (TID 512). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 61.0 in stage 6.0 (TID 510). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 77.0 in stage 6.0 (TID 526, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 68.0 in stage 6.0 (TID 517). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 75.0 in stage 6.0 (TID 524). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 62.0 in stage 6.0 (TID 511). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 78.0 in stage 6.0 (TID 527, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 77.0 in stage 6.0 (TID 526)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 79.0 in stage 6.0 (TID 528, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 79.0 in stage 6.0 (TID 528)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 80.0 in stage 6.0 (TID 529, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO Executor: Running task 80.0 in stage 6.0 (TID 529)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 81.0 in stage 6.0 (TID 530, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 81.0 in stage 6.0 (TID 530)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 65.0 in stage 6.0 (TID 514) in 84 ms on localhost (62/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 64.0 in stage 6.0 (TID 513) in 86 ms on localhost (63/200)
15/08/21 19:47:37 INFO Executor: Finished task 69.0 in stage 6.0 (TID 518). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 78.0 in stage 6.0 (TID 527)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 82.0 in stage 6.0 (TID 531, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 83.0 in stage 6.0 (TID 532, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 83.0 in stage 6.0 (TID 532)
15/08/21 19:47:37 INFO Executor: Finished task 67.0 in stage 6.0 (TID 516). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 66.0 in stage 6.0 (TID 515). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 82.0 in stage 6.0 (TID 531)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 74.0 in stage 6.0 (TID 523). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 70.0 in stage 6.0 (TID 519). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14989
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 84.0 in stage 6.0 (TID 533, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO Executor: Running task 84.0 in stage 6.0 (TID 533)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 72.0 in stage 6.0 (TID 521). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 85.0 in stage 6.0 (TID 534, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 85.0 in stage 6.0 (TID 534)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 86.0 in stage 6.0 (TID 535, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 86.0 in stage 6.0 (TID 535)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 87.0 in stage 6.0 (TID 536, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 75.0 in stage 6.0 (TID 524) in 54 ms on localhost (64/200)
15/08/21 19:47:37 INFO Executor: Running task 87.0 in stage 6.0 (TID 536)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 63.0 in stage 6.0 (TID 512) in 103 ms on localhost (65/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 61.0 in stage 6.0 (TID 510) in 105 ms on localhost (66/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 68.0 in stage 6.0 (TID 517) in 92 ms on localhost (67/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 88.0 in stage 6.0 (TID 537, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 88.0 in stage 6.0 (TID 537)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 89.0 in stage 6.0 (TID 538, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15114
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14440
15/08/21 19:47:37 INFO Executor: Running task 89.0 in stage 6.0 (TID 538)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 62.0 in stage 6.0 (TID 511) in 110 ms on localhost (68/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14934
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 71.0 in stage 6.0 (TID 520). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 74.0 in stage 6.0 (TID 523) in 67 ms on localhost (69/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 67.0 in stage 6.0 (TID 516) in 104 ms on localhost (70/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 70.0 in stage 6.0 (TID 519) in 80 ms on localhost (71/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 69.0 in stage 6.0 (TID 518) in 93 ms on localhost (72/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 90.0 in stage 6.0 (TID 539, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 73.0 in stage 6.0 (TID 522). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 90.0 in stage 6.0 (TID 539)
15/08/21 19:47:37 INFO Executor: Finished task 76.0 in stage 6.0 (TID 525). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15203
15/08/21 19:47:37 INFO TaskSetManager: Finished task 72.0 in stage 6.0 (TID 521) in 82 ms on localhost (73/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 66.0 in stage 6.0 (TID 515) in 113 ms on localhost (74/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 71.0 in stage 6.0 (TID 520) in 88 ms on localhost (75/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 91.0 in stage 6.0 (TID 540, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 73.0 in stage 6.0 (TID 522) in 83 ms on localhost (76/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15500
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 92.0 in stage 6.0 (TID 541, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15120
15/08/21 19:47:37 INFO Executor: Running task 91.0 in stage 6.0 (TID 540)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14550
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 92.0 in stage 6.0 (TID 541)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14509
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15103
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14876
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15545
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14825
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 77.0 in stage 6.0 (TID 526). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 80.0 in stage 6.0 (TID 529). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 83.0 in stage 6.0 (TID 532). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 14985
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 93.0 in stage 6.0 (TID 542, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15404
15/08/21 19:47:37 INFO Executor: Running task 93.0 in stage 6.0 (TID 542)
15/08/21 19:47:37 INFO Executor: Finished task 79.0 in stage 6.0 (TID 528). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 94.0 in stage 6.0 (TID 543, localhost, ANY, 1689 bytes)
15/08/21 19:47:37 INFO Executor: Running task 94.0 in stage 6.0 (TID 543)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 95.0 in stage 6.0 (TID 544, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 95.0 in stage 6.0 (TID 544)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 96.0 in stage 6.0 (TID 545, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14874
15/08/21 19:47:37 INFO TaskSetManager: Finished task 76.0 in stage 6.0 (TID 525) in 103 ms on localhost (77/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 89.0 in stage 6.0 (TID 538). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 79.0 in stage 6.0 (TID 528) in 75 ms on localhost (78/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 82.0 in stage 6.0 (TID 531). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 19:47:37 INFO TaskSetManager: Starting task 97.0 in stage 6.0 (TID 546, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 78.0 in stage 6.0 (TID 527). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15137
15/08/21 19:47:37 INFO Executor: Running task 97.0 in stage 6.0 (TID 546)
15/08/21 19:47:37 INFO Executor: Running task 96.0 in stage 6.0 (TID 545)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 98.0 in stage 6.0 (TID 547, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 85.0 in stage 6.0 (TID 534). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 87.0 in stage 6.0 (TID 536). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 77.0 in stage 6.0 (TID 526) in 79 ms on localhost (79/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 80.0 in stage 6.0 (TID 529) in 76 ms on localhost (80/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 83.0 in stage 6.0 (TID 532) in 69 ms on localhost (81/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 98.0 in stage 6.0 (TID 547)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 89.0 in stage 6.0 (TID 538) in 57 ms on localhost (82/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15238
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 19:47:37 INFO Executor: Finished task 88.0 in stage 6.0 (TID 537). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 82.0 in stage 6.0 (TID 531) in 78 ms on localhost (83/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 78.0 in stage 6.0 (TID 527) in 89 ms on localhost (84/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 19:47:37 INFO TaskSetManager: Starting task 99.0 in stage 6.0 (TID 548, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 99.0 in stage 6.0 (TID 548)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 81.0 in stage 6.0 (TID 530). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 85.0 in stage 6.0 (TID 534) in 79 ms on localhost (85/200)
15/08/21 19:47:37 INFO Executor: Finished task 86.0 in stage 6.0 (TID 535). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 92.0 in stage 6.0 (TID 541). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 100.0 in stage 6.0 (TID 549, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 100.0 in stage 6.0 (TID 549)
15/08/21 19:47:37 INFO Executor: Finished task 84.0 in stage 6.0 (TID 533). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 90.0 in stage 6.0 (TID 539). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 87.0 in stage 6.0 (TID 536) in 78 ms on localhost (86/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 101.0 in stage 6.0 (TID 550, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 101.0 in stage 6.0 (TID 550)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14982
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15272
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15189
15/08/21 19:47:37 INFO TaskSetManager: Finished task 88.0 in stage 6.0 (TID 537) in 80 ms on localhost (87/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 102.0 in stage 6.0 (TID 551, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 102.0 in stage 6.0 (TID 551)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15096
15/08/21 19:47:37 INFO Executor: Finished task 95.0 in stage 6.0 (TID 544). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 19:47:37 INFO Executor: Finished task 91.0 in stage 6.0 (TID 540). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 103.0 in stage 6.0 (TID 552, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 103.0 in stage 6.0 (TID 552)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15018
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 94.0 in stage 6.0 (TID 543). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 81.0 in stage 6.0 (TID 530) in 108 ms on localhost (88/200)
15/08/21 19:47:37 INFO Executor: Finished task 93.0 in stage 6.0 (TID 542). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 104.0 in stage 6.0 (TID 553, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 104.0 in stage 6.0 (TID 553)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 86.0 in stage 6.0 (TID 535) in 99 ms on localhost (89/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 92.0 in stage 6.0 (TID 541) in 78 ms on localhost (90/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 105.0 in stage 6.0 (TID 554, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO Executor: Running task 105.0 in stage 6.0 (TID 554)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14784
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 84.0 in stage 6.0 (TID 533) in 112 ms on localhost (91/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 106.0 in stage 6.0 (TID 555, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 98.0 in stage 6.0 (TID 547). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 106.0 in stage 6.0 (TID 555)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 96.0 in stage 6.0 (TID 545). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 19:47:37 INFO Executor: Finished task 97.0 in stage 6.0 (TID 546). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 99.0 in stage 6.0 (TID 548). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 107.0 in stage 6.0 (TID 556, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 107.0 in stage 6.0 (TID 556)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14819
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 108.0 in stage 6.0 (TID 557, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 100.0 in stage 6.0 (TID 549). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15463
15/08/21 19:47:37 INFO Executor: Running task 108.0 in stage 6.0 (TID 557)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 109.0 in stage 6.0 (TID 558, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 90.0 in stage 6.0 (TID 539) in 98 ms on localhost (92/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 109.0 in stage 6.0 (TID 558)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 110.0 in stage 6.0 (TID 559, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 95.0 in stage 6.0 (TID 544) in 63 ms on localhost (93/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 91.0 in stage 6.0 (TID 540) in 94 ms on localhost (94/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15267
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 110.0 in stage 6.0 (TID 559)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 111.0 in stage 6.0 (TID 560, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14816
15/08/21 19:47:37 INFO TaskSetManager: Finished task 94.0 in stage 6.0 (TID 543) in 70 ms on localhost (95/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 111.0 in stage 6.0 (TID 560)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15183
15/08/21 19:47:37 INFO TaskSetManager: Starting task 112.0 in stage 6.0 (TID 561, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 93.0 in stage 6.0 (TID 542) in 81 ms on localhost (96/200)
15/08/21 19:47:37 INFO Executor: Running task 112.0 in stage 6.0 (TID 561)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 98.0 in stage 6.0 (TID 547) in 63 ms on localhost (97/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15509
15/08/21 19:47:37 INFO TaskSetManager: Starting task 113.0 in stage 6.0 (TID 562, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15566
15/08/21 19:47:37 INFO Executor: Running task 113.0 in stage 6.0 (TID 562)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 102.0 in stage 6.0 (TID 551). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15274
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Finished task 101.0 in stage 6.0 (TID 550). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 114.0 in stage 6.0 (TID 563, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15313
15/08/21 19:47:37 INFO Executor: Finished task 103.0 in stage 6.0 (TID 552). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 114.0 in stage 6.0 (TID 563)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 96.0 in stage 6.0 (TID 545) in 85 ms on localhost (98/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 115.0 in stage 6.0 (TID 564, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 99.0 in stage 6.0 (TID 548) in 69 ms on localhost (99/200)
15/08/21 19:47:37 INFO Executor: Finished task 106.0 in stage 6.0 (TID 555). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 19:47:37 INFO TaskSetManager: Finished task 97.0 in stage 6.0 (TID 546) in 82 ms on localhost (100/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 115.0 in stage 6.0 (TID 564)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Starting task 116.0 in stage 6.0 (TID 565, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 14883
15/08/21 19:47:37 INFO Executor: Running task 116.0 in stage 6.0 (TID 565)
15/08/21 19:47:37 INFO Executor: Finished task 105.0 in stage 6.0 (TID 554). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 117.0 in stage 6.0 (TID 566, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 118.0 in stage 6.0 (TID 567, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 118.0 in stage 6.0 (TID 567)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 100.0 in stage 6.0 (TID 549) in 73 ms on localhost (101/200)
15/08/21 19:47:37 INFO Executor: Finished task 107.0 in stage 6.0 (TID 556). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 117.0 in stage 6.0 (TID 566)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15147
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 101.0 in stage 6.0 (TID 550) in 72 ms on localhost (102/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 19:47:37 INFO Executor: Finished task 104.0 in stage 6.0 (TID 553). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Finished task 102.0 in stage 6.0 (TID 551) in 68 ms on localhost (103/200)
15/08/21 19:47:37 INFO Executor: Finished task 108.0 in stage 6.0 (TID 557). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 119.0 in stage 6.0 (TID 568, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 111.0 in stage 6.0 (TID 560). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Finished task 109.0 in stage 6.0 (TID 558). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO Executor: Running task 119.0 in stage 6.0 (TID 568)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 120.0 in stage 6.0 (TID 569, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO Executor: Running task 120.0 in stage 6.0 (TID 569)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 103.0 in stage 6.0 (TID 552) in 77 ms on localhost (104/200)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 106.0 in stage 6.0 (TID 555) in 64 ms on localhost (105/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 110.0 in stage 6.0 (TID 559). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO TaskSetManager: Starting task 121.0 in stage 6.0 (TID 570, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO Executor: Running task 121.0 in stage 6.0 (TID 570)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14564
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 122.0 in stage 6.0 (TID 571, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 122.0 in stage 6.0 (TID 571)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 123.0 in stage 6.0 (TID 572, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Finished task 112.0 in stage 6.0 (TID 561). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 123.0 in stage 6.0 (TID 572)
15/08/21 19:47:37 INFO TaskSetManager: Starting task 124.0 in stage 6.0 (TID 573, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 125.0 in stage 6.0 (TID 574, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 125.0 in stage 6.0 (TID 574)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15625
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 107.0 in stage 6.0 (TID 556) in 66 ms on localhost (106/200)
15/08/21 19:47:37 INFO Executor: Running task 124.0 in stage 6.0 (TID 573)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 105.0 in stage 6.0 (TID 554) in 75 ms on localhost (107/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14693
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Starting task 126.0 in stage 6.0 (TID 575, localhost, ANY, 1685 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO Executor: Running task 126.0 in stage 6.0 (TID 575)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 104.0 in stage 6.0 (TID 553) in 84 ms on localhost (108/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14938
15/08/21 19:47:37 INFO TaskSetManager: Finished task 108.0 in stage 6.0 (TID 557) in 73 ms on localhost (109/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 109.0 in stage 6.0 (TID 558) in 72 ms on localhost (110/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 111.0 in stage 6.0 (TID 560) in 66 ms on localhost (111/200)
15/08/21 19:47:37 INFO Executor: Finished task 113.0 in stage 6.0 (TID 562). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 127.0 in stage 6.0 (TID 576, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO Executor: Running task 127.0 in stage 6.0 (TID 576)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 110.0 in stage 6.0 (TID 559) in 75 ms on localhost (112/200)
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15117
15/08/21 19:47:37 INFO TaskSetManager: Starting task 128.0 in stage 6.0 (TID 577, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 19:47:37 INFO Executor: Running task 128.0 in stage 6.0 (TID 577)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 19:47:37 INFO Executor: Finished task 114.0 in stage 6.0 (TID 563). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15795
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 14751
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15142
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 19:47:37 INFO TaskSetManager: Starting task 129.0 in stage 6.0 (TID 578, localhost, ANY, 1688 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO TaskSetManager: Finished task 112.0 in stage 6.0 (TID 561) in 78 ms on localhost (113/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14864
15/08/21 19:47:37 INFO Executor: Finished task 118.0 in stage 6.0 (TID 567). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Running task 129.0 in stage 6.0 (TID 578)
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 19:47:37 INFO TaskSetManager: Starting task 130.0 in stage 6.0 (TID 579, localhost, ANY, 1687 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14687
15/08/21 19:47:37 INFO Executor: Running task 130.0 in stage 6.0 (TID 579)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15571
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Starting task 131.0 in stage 6.0 (TID 580, localhost, ANY, 1686 bytes)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15129
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Running task 131.0 in stage 6.0 (TID 580)
15/08/21 19:47:37 INFO Executor: Finished task 117.0 in stage 6.0 (TID 566). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15726
15/08/21 19:47:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 19:47:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:37 INFO Executor: Finished task 115.0 in stage 6.0 (TID 564). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 INFO TaskSetManager: Finished task 113.0 in stage 6.0 (TID 562) in 94 ms on localhost (114/200)
15/08/21 19:47:37 INFO Executor: Finished task 116.0 in stage 6.0 (TID 565). 1925 bytes result sent to driver
15/08/21 19:47:37 INFO Executor: Finished task 120.0 in stage 6.0 (TID 569). 1925 bytes result sent to driver
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:37 INFO TaskSetManager: Finished task 114.0 in stage 6.0 (TID 563) in 93 ms on localhost (115/200)
15/08/21 19:47:37 INFO TaskSetManager: Finished task 118.0 in stage 6.0 (TID 567) in 72 ms on localhost (116/200)
15/08/21 19:47:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14793
15/08/21 19:47:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 132.0 in stage 6.0 (TID 581, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 133.0 in stage 6.0 (TID 582, localhost, ANY, 1685 bytes)
15/08/21 19:47:38 INFO Executor: Running task 132.0 in stage 6.0 (TID 581)
15/08/21 19:47:38 INFO Executor: Running task 133.0 in stage 6.0 (TID 582)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 134.0 in stage 6.0 (TID 583, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 135.0 in stage 6.0 (TID 584, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 119.0 in stage 6.0 (TID 568). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 134.0 in stage 6.0 (TID 583)
15/08/21 19:47:38 INFO Executor: Finished task 121.0 in stage 6.0 (TID 570). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 115.0 in stage 6.0 (TID 564) in 94 ms on localhost (117/200)
15/08/21 19:47:38 INFO Executor: Finished task 124.0 in stage 6.0 (TID 573). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 19:47:38 INFO Executor: Finished task 122.0 in stage 6.0 (TID 571). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15361
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 136.0 in stage 6.0 (TID 585, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 135.0 in stage 6.0 (TID 584)
15/08/21 19:47:38 INFO Executor: Finished task 125.0 in stage 6.0 (TID 574). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14826
15/08/21 19:47:38 INFO Executor: Running task 136.0 in stage 6.0 (TID 585)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 120.0 in stage 6.0 (TID 569) in 78 ms on localhost (118/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 123.0 in stage 6.0 (TID 572). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 126.0 in stage 6.0 (TID 575). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 116.0 in stage 6.0 (TID 565) in 95 ms on localhost (119/200)
15/08/21 19:47:38 INFO Executor: Finished task 127.0 in stage 6.0 (TID 576). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15049
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 137.0 in stage 6.0 (TID 586, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Running task 137.0 in stage 6.0 (TID 586)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 117.0 in stage 6.0 (TID 566) in 95 ms on localhost (120/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 119.0 in stage 6.0 (TID 568) in 89 ms on localhost (121/200)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 138.0 in stage 6.0 (TID 587, localhost, ANY, 1684 bytes)
15/08/21 19:47:38 INFO Executor: Running task 138.0 in stage 6.0 (TID 587)
15/08/21 19:47:38 INFO Executor: Finished task 128.0 in stage 6.0 (TID 577). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 139.0 in stage 6.0 (TID 588, localhost, ANY, 1685 bytes)
15/08/21 19:47:38 INFO Executor: Running task 139.0 in stage 6.0 (TID 588)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Starting task 140.0 in stage 6.0 (TID 589, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15038
15/08/21 19:47:38 INFO Executor: Running task 140.0 in stage 6.0 (TID 589)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 141.0 in stage 6.0 (TID 590, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO Executor: Running task 141.0 in stage 6.0 (TID 590)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15344
15/08/21 19:47:38 INFO TaskSetManager: Starting task 142.0 in stage 6.0 (TID 591, localhost, ANY, 1689 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 121.0 in stage 6.0 (TID 570) in 83 ms on localhost (122/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14741
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Running task 142.0 in stage 6.0 (TID 591)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 143.0 in stage 6.0 (TID 592, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO Executor: Running task 143.0 in stage 6.0 (TID 592)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 124.0 in stage 6.0 (TID 573) in 84 ms on localhost (123/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14809
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 125.0 in stage 6.0 (TID 574) in 85 ms on localhost (124/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Finished task 130.0 in stage 6.0 (TID 579). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14814
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 144.0 in stage 6.0 (TID 593, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 134.0 in stage 6.0 (TID 583). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 123.0 in stage 6.0 (TID 572) in 93 ms on localhost (125/200)
15/08/21 19:47:38 INFO Executor: Running task 144.0 in stage 6.0 (TID 593)
15/08/21 19:47:38 INFO Executor: Finished task 131.0 in stage 6.0 (TID 580). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 122.0 in stage 6.0 (TID 571) in 95 ms on localhost (126/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 126.0 in stage 6.0 (TID 575) in 88 ms on localhost (127/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 127.0 in stage 6.0 (TID 576) in 84 ms on localhost (128/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 128.0 in stage 6.0 (TID 577) in 79 ms on localhost (129/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 129.0 in stage 6.0 (TID 578). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 133.0 in stage 6.0 (TID 582). 1925 bytes result sent to driver
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14951
15/08/21 19:47:38 INFO Executor: Finished task 135.0 in stage 6.0 (TID 584). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 145.0 in stage 6.0 (TID 594, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Running task 145.0 in stage 6.0 (TID 594)
15/08/21 19:47:38 INFO Executor: Finished task 132.0 in stage 6.0 (TID 581). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 146.0 in stage 6.0 (TID 595, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Running task 146.0 in stage 6.0 (TID 595)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 19:47:38 INFO TaskSetManager: Starting task 147.0 in stage 6.0 (TID 596, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15658
15/08/21 19:47:38 INFO TaskSetManager: Starting task 148.0 in stage 6.0 (TID 597, localhost, ANY, 1683 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 19:47:38 INFO Executor: Running task 147.0 in stage 6.0 (TID 596)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Running task 148.0 in stage 6.0 (TID 597)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15501
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 149.0 in stage 6.0 (TID 598, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14991
15/08/21 19:47:38 INFO TaskSetManager: Finished task 130.0 in stage 6.0 (TID 579) in 78 ms on localhost (130/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14836
15/08/21 19:47:38 INFO TaskSetManager: Finished task 134.0 in stage 6.0 (TID 583) in 56 ms on localhost (131/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 131.0 in stage 6.0 (TID 580) in 73 ms on localhost (132/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 129.0 in stage 6.0 (TID 578) in 98 ms on localhost (133/200)
15/08/21 19:47:38 INFO Executor: Running task 149.0 in stage 6.0 (TID 598)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 19:47:38 INFO TaskSetManager: Finished task 133.0 in stage 6.0 (TID 582) in 61 ms on localhost (134/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 135.0 in stage 6.0 (TID 584) in 60 ms on localhost (135/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14613
15/08/21 19:47:38 INFO TaskSetManager: Starting task 150.0 in stage 6.0 (TID 599, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 150.0 in stage 6.0 (TID 599)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14508
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Starting task 151.0 in stage 6.0 (TID 600, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Running task 151.0 in stage 6.0 (TID 600)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15141
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 132.0 in stage 6.0 (TID 581) in 70 ms on localhost (136/200)
15/08/21 19:47:38 INFO Executor: Finished task 138.0 in stage 6.0 (TID 587). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15489
15/08/21 19:47:38 INFO TaskSetManager: Starting task 152.0 in stage 6.0 (TID 601, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Running task 152.0 in stage 6.0 (TID 601)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14923
15/08/21 19:47:38 INFO TaskSetManager: Finished task 138.0 in stage 6.0 (TID 587) in 64 ms on localhost (137/200)
15/08/21 19:47:38 INFO Executor: Finished task 137.0 in stage 6.0 (TID 586). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 139.0 in stage 6.0 (TID 588). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 153.0 in stage 6.0 (TID 602, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Running task 153.0 in stage 6.0 (TID 602)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 154.0 in stage 6.0 (TID 603, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 136.0 in stage 6.0 (TID 585). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14952
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14755
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15549
15/08/21 19:47:38 INFO Executor: Finished task 140.0 in stage 6.0 (TID 589). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 139.0 in stage 6.0 (TID 588) in 69 ms on localhost (138/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 137.0 in stage 6.0 (TID 586) in 73 ms on localhost (139/200)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 155.0 in stage 6.0 (TID 604, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 136.0 in stage 6.0 (TID 585) in 81 ms on localhost (140/200)
15/08/21 19:47:38 INFO Executor: Running task 155.0 in stage 6.0 (TID 604)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Running task 154.0 in stage 6.0 (TID 603)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 156.0 in stage 6.0 (TID 605, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 144.0 in stage 6.0 (TID 593). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 156.0 in stage 6.0 (TID 605)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 145.0 in stage 6.0 (TID 594). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 142.0 in stage 6.0 (TID 591). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 140.0 in stage 6.0 (TID 589) in 72 ms on localhost (141/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 157.0 in stage 6.0 (TID 606, localhost, ANY, 1689 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 19:47:38 INFO Executor: Finished task 141.0 in stage 6.0 (TID 590). 1925 bytes result sent to driver
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 143.0 in stage 6.0 (TID 592). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 157.0 in stage 6.0 (TID 606)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 158.0 in stage 6.0 (TID 607, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO Executor: Running task 158.0 in stage 6.0 (TID 607)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 144.0 in stage 6.0 (TID 593) in 69 ms on localhost (142/200)
15/08/21 19:47:38 INFO Executor: Finished task 146.0 in stage 6.0 (TID 595). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 148.0 in stage 6.0 (TID 597). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14944
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 159.0 in stage 6.0 (TID 608, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 151.0 in stage 6.0 (TID 600). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 149.0 in stage 6.0 (TID 598). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 159.0 in stage 6.0 (TID 608)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 160.0 in stage 6.0 (TID 609, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO Executor: Running task 160.0 in stage 6.0 (TID 609)
15/08/21 19:47:38 INFO Executor: Finished task 147.0 in stage 6.0 (TID 596). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 19:47:38 INFO TaskSetManager: Starting task 161.0 in stage 6.0 (TID 610, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 145.0 in stage 6.0 (TID 594) in 62 ms on localhost (143/200)
15/08/21 19:47:38 INFO Executor: Finished task 150.0 in stage 6.0 (TID 599). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 142.0 in stage 6.0 (TID 591) in 83 ms on localhost (144/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 19:47:38 INFO Executor: Running task 161.0 in stage 6.0 (TID 610)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 141.0 in stage 6.0 (TID 590) in 86 ms on localhost (145/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15359
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Starting task 162.0 in stage 6.0 (TID 611, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15164
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 163.0 in stage 6.0 (TID 612, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Running task 162.0 in stage 6.0 (TID 611)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15636
15/08/21 19:47:38 INFO TaskSetManager: Finished task 143.0 in stage 6.0 (TID 592) in 88 ms on localhost (146/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 146.0 in stage 6.0 (TID 595) in 67 ms on localhost (147/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14896
15/08/21 19:47:38 INFO Executor: Running task 163.0 in stage 6.0 (TID 612)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 164.0 in stage 6.0 (TID 613, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 148.0 in stage 6.0 (TID 597) in 64 ms on localhost (148/200)
15/08/21 19:47:38 INFO Executor: Running task 164.0 in stage 6.0 (TID 613)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 165.0 in stage 6.0 (TID 614, localhost, ANY, 1685 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 166.0 in stage 6.0 (TID 615, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 165.0 in stage 6.0 (TID 614)
15/08/21 19:47:38 INFO Executor: Running task 166.0 in stage 6.0 (TID 615)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14842
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14560
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14884
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Finished task 152.0 in stage 6.0 (TID 601). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Starting task 167.0 in stage 6.0 (TID 616, localhost, ANY, 1689 bytes)
15/08/21 19:47:38 INFO Executor: Running task 167.0 in stage 6.0 (TID 616)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15347
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15098
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 168.0 in stage 6.0 (TID 617, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 151.0 in stage 6.0 (TID 600) in 64 ms on localhost (149/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 19:47:38 INFO TaskSetManager: Finished task 147.0 in stage 6.0 (TID 596) in 79 ms on localhost (150/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 168.0 in stage 6.0 (TID 617)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 152.0 in stage 6.0 (TID 601) in 58 ms on localhost (151/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 149.0 in stage 6.0 (TID 598) in 78 ms on localhost (152/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 150.0 in stage 6.0 (TID 599) in 69 ms on localhost (153/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 154.0 in stage 6.0 (TID 603). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 161.0 in stage 6.0 (TID 610). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 169.0 in stage 6.0 (TID 618, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 156.0 in stage 6.0 (TID 605). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14843
15/08/21 19:47:38 INFO TaskSetManager: Starting task 170.0 in stage 6.0 (TID 619, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 155.0 in stage 6.0 (TID 604). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 170.0 in stage 6.0 (TID 619)
15/08/21 19:47:38 INFO Executor: Finished task 163.0 in stage 6.0 (TID 612). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 157.0 in stage 6.0 (TID 606). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 171.0 in stage 6.0 (TID 620, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 154.0 in stage 6.0 (TID 603) in 65 ms on localhost (154/200)
15/08/21 19:47:38 INFO Executor: Running task 171.0 in stage 6.0 (TID 620)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15470
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 153.0 in stage 6.0 (TID 602). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 169.0 in stage 6.0 (TID 618)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15296
15/08/21 19:47:38 INFO Executor: Finished task 160.0 in stage 6.0 (TID 609). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 156.0 in stage 6.0 (TID 605) in 60 ms on localhost (155/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14810
15/08/21 19:47:38 INFO Executor: Finished task 159.0 in stage 6.0 (TID 608). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15039
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15013
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Finished task 158.0 in stage 6.0 (TID 607). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 161.0 in stage 6.0 (TID 610) in 47 ms on localhost (156/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 172.0 in stage 6.0 (TID 621, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 155.0 in stage 6.0 (TID 604) in 73 ms on localhost (157/200)
15/08/21 19:47:38 INFO Executor: Running task 172.0 in stage 6.0 (TID 621)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 173.0 in stage 6.0 (TID 622, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 173.0 in stage 6.0 (TID 622)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 174.0 in stage 6.0 (TID 623, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO Executor: Running task 174.0 in stage 6.0 (TID 623)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 175.0 in stage 6.0 (TID 624, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO Executor: Running task 175.0 in stage 6.0 (TID 624)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 176.0 in stage 6.0 (TID 625, localhost, ANY, 1689 bytes)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 157.0 in stage 6.0 (TID 606) in 71 ms on localhost (158/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 153.0 in stage 6.0 (TID 602) in 83 ms on localhost (159/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 163.0 in stage 6.0 (TID 612) in 51 ms on localhost (160/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 19:47:38 INFO Executor: Finished task 164.0 in stage 6.0 (TID 613). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 160.0 in stage 6.0 (TID 609) in 61 ms on localhost (161/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 176.0 in stage 6.0 (TID 625)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Finished task 165.0 in stage 6.0 (TID 614). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 177.0 in stage 6.0 (TID 626, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO Executor: Running task 177.0 in stage 6.0 (TID 626)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 167.0 in stage 6.0 (TID 616). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 178.0 in stage 6.0 (TID 627, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 159.0 in stage 6.0 (TID 608) in 66 ms on localhost (162/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Finished task 168.0 in stage 6.0 (TID 617). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 179.0 in stage 6.0 (TID 628, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Running task 178.0 in stage 6.0 (TID 627)
15/08/21 19:47:38 INFO Executor: Running task 179.0 in stage 6.0 (TID 628)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 180.0 in stage 6.0 (TID 629, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 158.0 in stage 6.0 (TID 607) in 69 ms on localhost (163/200)
15/08/21 19:47:38 INFO Executor: Running task 180.0 in stage 6.0 (TID 629)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14848
15/08/21 19:47:38 INFO TaskSetManager: Finished task 164.0 in stage 6.0 (TID 613) in 59 ms on localhost (164/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 181.0 in stage 6.0 (TID 630, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Running task 181.0 in stage 6.0 (TID 630)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 165.0 in stage 6.0 (TID 614) in 60 ms on localhost (165/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14736
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15474
15/08/21 19:47:38 INFO Executor: Finished task 166.0 in stage 6.0 (TID 615). 1925 bytes result sent to driver
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15062
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15386
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 19:47:38 INFO TaskSetManager: Starting task 182.0 in stage 6.0 (TID 631, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14788
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15368
15/08/21 19:47:38 INFO TaskSetManager: Finished task 167.0 in stage 6.0 (TID 616) in 62 ms on localhost (166/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 168.0 in stage 6.0 (TID 617) in 60 ms on localhost (167/200)
15/08/21 19:47:38 INFO Executor: Finished task 162.0 in stage 6.0 (TID 611). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14750
15/08/21 19:47:38 INFO Executor: Finished task 169.0 in stage 6.0 (TID 618). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 166.0 in stage 6.0 (TID 615) in 70 ms on localhost (168/200)
15/08/21 19:47:38 INFO Executor: Finished task 171.0 in stage 6.0 (TID 620). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 182.0 in stage 6.0 (TID 631)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Starting task 183.0 in stage 6.0 (TID 632, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15101
15/08/21 19:47:38 INFO Executor: Running task 183.0 in stage 6.0 (TID 632)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15340
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 184.0 in stage 6.0 (TID 633, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14879
15/08/21 19:47:38 INFO Executor: Finished task 170.0 in stage 6.0 (TID 619). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 173.0 in stage 6.0 (TID 622). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 175.0 in stage 6.0 (TID 624). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 184.0 in stage 6.0 (TID 633)
15/08/21 19:47:38 INFO Executor: Finished task 174.0 in stage 6.0 (TID 623). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 172.0 in stage 6.0 (TID 621). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 162.0 in stage 6.0 (TID 611) in 89 ms on localhost (169/200)
15/08/21 19:47:38 INFO Executor: Finished task 176.0 in stage 6.0 (TID 625). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 185.0 in stage 6.0 (TID 634, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO Executor: Running task 185.0 in stage 6.0 (TID 634)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 186.0 in stage 6.0 (TID 635, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 179.0 in stage 6.0 (TID 628). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 187.0 in stage 6.0 (TID 636, localhost, ANY, 1689 bytes)
15/08/21 19:47:38 INFO Executor: Running task 186.0 in stage 6.0 (TID 635)
15/08/21 19:47:38 INFO Executor: Running task 187.0 in stage 6.0 (TID 636)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15330
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 169.0 in stage 6.0 (TID 618) in 63 ms on localhost (170/200)
15/08/21 19:47:38 INFO Executor: Finished task 177.0 in stage 6.0 (TID 626). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 188.0 in stage 6.0 (TID 637, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 171.0 in stage 6.0 (TID 620) in 63 ms on localhost (171/200)
15/08/21 19:47:38 INFO Executor: Running task 188.0 in stage 6.0 (TID 637)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 170.0 in stage 6.0 (TID 619) in 64 ms on localhost (172/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15207
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 189.0 in stage 6.0 (TID 638, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Running task 189.0 in stage 6.0 (TID 638)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 181.0 in stage 6.0 (TID 630). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Starting task 190.0 in stage 6.0 (TID 639, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO Executor: Running task 190.0 in stage 6.0 (TID 639)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14569
15/08/21 19:47:38 INFO TaskSetManager: Starting task 191.0 in stage 6.0 (TID 640, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 180.0 in stage 6.0 (TID 629). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 192.0 in stage 6.0 (TID 641, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Running task 191.0 in stage 6.0 (TID 640)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 192.0 in stage 6.0 (TID 641)
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Starting task 193.0 in stage 6.0 (TID 642, localhost, ANY, 1687 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14771
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14854
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 173.0 in stage 6.0 (TID 622) in 60 ms on localhost (173/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Running task 193.0 in stage 6.0 (TID 642)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 175.0 in stage 6.0 (TID 624) in 63 ms on localhost (174/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 172.0 in stage 6.0 (TID 621) in 67 ms on localhost (175/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 174.0 in stage 6.0 (TID 623) in 65 ms on localhost (176/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 176.0 in stage 6.0 (TID 625) in 73 ms on localhost (177/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 19:47:38 INFO Executor: Finished task 178.0 in stage 6.0 (TID 627). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 14795
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 183.0 in stage 6.0 (TID 632). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 194.0 in stage 6.0 (TID 643, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14802
15/08/21 19:47:38 INFO Executor: Running task 194.0 in stage 6.0 (TID 643)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 195.0 in stage 6.0 (TID 644, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 196.0 in stage 6.0 (TID 645, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO Executor: Running task 196.0 in stage 6.0 (TID 645)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 177.0 in stage 6.0 (TID 626) in 72 ms on localhost (178/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 181.0 in stage 6.0 (TID 630) in 67 ms on localhost (179/200)
15/08/21 19:47:38 INFO Executor: Running task 195.0 in stage 6.0 (TID 644)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 179.0 in stage 6.0 (TID 628) in 73 ms on localhost (180/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15524
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 178.0 in stage 6.0 (TID 627) in 79 ms on localhost (181/200)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 197.0 in stage 6.0 (TID 646, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 197.0 in stage 6.0 (TID 646)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 198.0 in stage 6.0 (TID 647, localhost, ANY, 1686 bytes)
15/08/21 19:47:38 INFO Executor: Running task 198.0 in stage 6.0 (TID 647)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 180.0 in stage 6.0 (TID 629) in 80 ms on localhost (182/200)
15/08/21 19:47:38 INFO Executor: Finished task 184.0 in stage 6.0 (TID 633). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 182.0 in stage 6.0 (TID 631). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 19:47:38 INFO Executor: Finished task 187.0 in stage 6.0 (TID 636). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 199.0 in stage 6.0 (TID 648, localhost, ANY, 1688 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15396
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 199.0 in stage 6.0 (TID 648)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 183.0 in stage 6.0 (TID 632) in 60 ms on localhost (183/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 182.0 in stage 6.0 (TID 631) in 76 ms on localhost (184/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15090
15/08/21 19:47:38 INFO TaskSetManager: Finished task 187.0 in stage 6.0 (TID 636) in 47 ms on localhost (185/200)
15/08/21 19:47:38 INFO Executor: Finished task 186.0 in stage 6.0 (TID 635). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 189.0 in stage 6.0 (TID 638). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 184.0 in stage 6.0 (TID 633) in 64 ms on localhost (186/200)
15/08/21 19:47:38 INFO Executor: Finished task 185.0 in stage 6.0 (TID 634). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14914
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14479
15/08/21 19:47:38 INFO TaskSetManager: Finished task 186.0 in stage 6.0 (TID 635) in 60 ms on localhost (187/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15007
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15150
15/08/21 19:47:38 INFO Executor: Finished task 188.0 in stage 6.0 (TID 637). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 185.0 in stage 6.0 (TID 634) in 63 ms on localhost (188/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 193.0 in stage 6.0 (TID 642). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14780
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 190.0 in stage 6.0 (TID 639). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14677
15/08/21 19:47:38 INFO TaskSetManager: Finished task 189.0 in stage 6.0 (TID 638) in 61 ms on localhost (189/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 188.0 in stage 6.0 (TID 637) in 70 ms on localhost (190/200)
15/08/21 19:47:38 INFO Executor: Finished task 191.0 in stage 6.0 (TID 640). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 193.0 in stage 6.0 (TID 642) in 64 ms on localhost (191/200)
15/08/21 19:47:38 INFO Executor: Finished task 196.0 in stage 6.0 (TID 645). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 198.0 in stage 6.0 (TID 647). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 190.0 in stage 6.0 (TID 639) in 71 ms on localhost (192/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 191.0 in stage 6.0 (TID 640) in 71 ms on localhost (193/200)
15/08/21 19:47:38 INFO Executor: Finished task 194.0 in stage 6.0 (TID 643). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 196.0 in stage 6.0 (TID 645) in 48 ms on localhost (194/200)
15/08/21 19:47:38 INFO Executor: Finished task 192.0 in stage 6.0 (TID 641). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 198.0 in stage 6.0 (TID 647) in 38 ms on localhost (195/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 194.0 in stage 6.0 (TID 643) in 53 ms on localhost (196/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 192.0 in stage 6.0 (TID 641) in 74 ms on localhost (197/200)
15/08/21 19:47:38 INFO Executor: Finished task 195.0 in stage 6.0 (TID 644). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 197.0 in stage 6.0 (TID 646). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 199.0 in stage 6.0 (TID 648). 1925 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 195.0 in stage 6.0 (TID 644) in 56 ms on localhost (198/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 197.0 in stage 6.0 (TID 646) in 47 ms on localhost (199/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 199.0 in stage 6.0 (TID 648) in 45 ms on localhost (200/200)
15/08/21 19:47:38 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/21 19:47:38 INFO DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:423) finished in 0.946 s
15/08/21 19:47:38 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3245f9e6
15/08/21 19:47:38 INFO DAGScheduler: looking for newly runnable stages
15/08/21 19:47:38 INFO DAGScheduler: running: Set()
15/08/21 19:47:38 INFO DAGScheduler: waiting: Set(ResultStage 7)
15/08/21 19:47:38 INFO DAGScheduler: failed: Set()
15/08/21 19:47:38 INFO StatsReportListener: task runtime:(count: 200, mean: 83.985000, stdev: 23.226166, max: 176.000000, min: 38.000000)
15/08/21 19:47:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:38 INFO StatsReportListener: 	38.0 ms	56.0 ms	60.0 ms	67.0 ms	80.0 ms	94.0 ms	112.0 ms	121.0 ms	176.0 ms
15/08/21 19:47:38 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 31.000000, stdev: 0.000000, max: 31.000000, min: 31.000000)
15/08/21 19:47:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:38 INFO StatsReportListener: 	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B
15/08/21 19:47:38 INFO StatsReportListener: task result size:(count: 200, mean: 1925.000000, stdev: 0.000000, max: 1925.000000, min: 1925.000000)
15/08/21 19:47:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:38 INFO StatsReportListener: 	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B
15/08/21 19:47:38 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 59.896234, stdev: 12.056837, max: 86.666667, min: 26.050420)
15/08/21 19:47:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:38 INFO StatsReportListener: 	26 %	40 %	45 %	51 %	59 %	70 %	75 %	81 %	87 %
15/08/21 19:47:38 INFO StatsReportListener: other time pct: (count: 200, mean: 40.103766, stdev: 12.056837, max: 73.949580, min: 13.333333)
15/08/21 19:47:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:38 INFO StatsReportListener: 	13 %	20 %	25 %	30 %	41 %	50 %	55 %	60 %	74 %
15/08/21 19:47:38 INFO DAGScheduler: Missing parents for ResultStage 7: List()
15/08/21 19:47:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[43] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 19:47:38 INFO MemoryStore: ensureFreeSpace(12056) called with curMem=1878008, maxMem=22226833244
15/08/21 19:47:38 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 11.8 KB, free 20.7 GB)
15/08/21 19:47:38 INFO MemoryStore: ensureFreeSpace(5718) called with curMem=1890064, maxMem=22226833244
15/08/21 19:47:38 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.6 KB, free 20.7 GB)
15/08/21 19:47:38 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:54384 (size: 5.6 KB, free: 20.7 GB)
15/08/21 19:47:38 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/21 19:47:38 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 7 (MapPartitionsRDD[43] at processCmd at CliDriver.java:423)
15/08/21 19:47:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/21 19:47:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 649, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 650, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 651, localhost, ANY, 1984 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 652, localhost, ANY, 1984 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 653, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 654, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 655, localhost, ANY, 1979 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 656, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 657, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 658, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 659, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 660, localhost, ANY, 1980 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 661, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 662, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 663, localhost, ANY, 1980 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 664, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 1.0 in stage 7.0 (TID 650)
15/08/21 19:47:38 INFO Executor: Running task 5.0 in stage 7.0 (TID 654)
15/08/21 19:47:38 INFO Executor: Running task 2.0 in stage 7.0 (TID 651)
15/08/21 19:47:38 INFO Executor: Running task 9.0 in stage 7.0 (TID 658)
15/08/21 19:47:38 INFO Executor: Running task 3.0 in stage 7.0 (TID 652)
15/08/21 19:47:38 INFO Executor: Running task 4.0 in stage 7.0 (TID 653)
15/08/21 19:47:38 INFO Executor: Running task 11.0 in stage 7.0 (TID 660)
15/08/21 19:47:38 INFO Executor: Running task 12.0 in stage 7.0 (TID 661)
15/08/21 19:47:38 INFO Executor: Running task 6.0 in stage 7.0 (TID 655)
15/08/21 19:47:38 INFO Executor: Running task 10.0 in stage 7.0 (TID 659)
15/08/21 19:47:38 INFO Executor: Running task 8.0 in stage 7.0 (TID 657)
15/08/21 19:47:38 INFO Executor: Running task 15.0 in stage 7.0 (TID 664)
15/08/21 19:47:38 INFO Executor: Running task 13.0 in stage 7.0 (TID 662)
15/08/21 19:47:38 INFO Executor: Running task 7.0 in stage 7.0 (TID 656)
15/08/21 19:47:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 649)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO Executor: Running task 14.0 in stage 7.0 (TID 663)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15383
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14878
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15235
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 14838
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15393
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 14731
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15029
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15001
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14829
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15586
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14718
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14967
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 15079
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15384
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15679
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14824
15/08/21 19:47:38 INFO Executor: Finished task 12.0 in stage 7.0 (TID 661). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 665, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 16.0 in stage 7.0 (TID 665)
15/08/21 19:47:38 INFO Executor: Finished task 5.0 in stage 7.0 (TID 654). 3229 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 666, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 6.0 in stage 7.0 (TID 655). 3229 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 15.0 in stage 7.0 (TID 664). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 661) in 271 ms on localhost (1/200)
15/08/21 19:47:38 INFO Executor: Running task 17.0 in stage 7.0 (TID 666)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 667, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 7.0 in stage 7.0 (TID 656). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 668, localhost, ANY, 1984 bytes)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO Executor: Finished task 8.0 in stage 7.0 (TID 657). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 19.0 in stage 7.0 (TID 668)
15/08/21 19:47:38 INFO Executor: Running task 18.0 in stage 7.0 (TID 667)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 669, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 13.0 in stage 7.0 (TID 662). 3228 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 20.0 in stage 7.0 (TID 669)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 670, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 3.0 in stage 7.0 (TID 652). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 21.0 in stage 7.0 (TID 670)
15/08/21 19:47:38 INFO Executor: Finished task 14.0 in stage 7.0 (TID 663). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 671, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 654) in 282 ms on localhost (2/200)
15/08/21 19:47:38 INFO Executor: Finished task 1.0 in stage 7.0 (TID 650). 3229 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 664) in 277 ms on localhost (3/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 655) in 286 ms on localhost (4/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 656) in 289 ms on localhost (5/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 662) in 286 ms on localhost (6/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 657) in 290 ms on localhost (7/200)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 672, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Running task 22.0 in stage 7.0 (TID 671)
15/08/21 19:47:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 649). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 673, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 11.0 in stage 7.0 (TID 660). 3234 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 674, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO Executor: Running task 23.0 in stage 7.0 (TID 672)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 675, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 26.0 in stage 7.0 (TID 675)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 676, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 652) in 298 ms on localhost (8/200)
15/08/21 19:47:38 INFO Executor: Running task 27.0 in stage 7.0 (TID 676)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 663) in 292 ms on localhost (9/200)
15/08/21 19:47:38 INFO Executor: Running task 25.0 in stage 7.0 (TID 674)
15/08/21 19:47:38 INFO Executor: Finished task 9.0 in stage 7.0 (TID 658). 3233 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 2.0 in stage 7.0 (TID 651). 3228 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 677, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO Executor: Running task 28.0 in stage 7.0 (TID 677)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO Executor: Running task 24.0 in stage 7.0 (TID 673)
15/08/21 19:47:38 INFO Executor: Finished task 4.0 in stage 7.0 (TID 653). 3225 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 678, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 10.0 in stage 7.0 (TID 659). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 29.0 in stage 7.0 (TID 678)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 679, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 680, localhost, ANY, 1980 bytes)
15/08/21 19:47:38 INFO Executor: Running task 31.0 in stage 7.0 (TID 680)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 650) in 313 ms on localhost (10/200)
15/08/21 19:47:38 INFO Executor: Running task 30.0 in stage 7.0 (TID 679)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 651) in 317 ms on localhost (11/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 658) in 317 ms on localhost (12/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 649) in 326 ms on localhost (13/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 653) in 322 ms on localhost (14/200)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 659) in 320 ms on localhost (15/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 660) in 321 ms on localhost (16/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15023
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15302
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14913
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15282
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15570
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14776
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14939
15/08/21 19:47:38 INFO Executor: Finished task 16.0 in stage 7.0 (TID 665). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14954
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15010
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15432
15/08/21 19:47:38 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 681, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 15106
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 665) in 121 ms on localhost (17/200)
15/08/21 19:47:38 INFO Executor: Running task 32.0 in stage 7.0 (TID 681)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 20.0 in stage 7.0 (TID 669). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 17.0 in stage 7.0 (TID 666). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15055
15/08/21 19:47:38 INFO Executor: Finished task 22.0 in stage 7.0 (TID 671). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 682, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 33.0 in stage 7.0 (TID 682)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 683, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 684, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Running task 35.0 in stage 7.0 (TID 684)
15/08/21 19:47:38 INFO Executor: Finished task 28.0 in stage 7.0 (TID 677). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 669) in 115 ms on localhost (18/200)
15/08/21 19:47:38 INFO Executor: Running task 34.0 in stage 7.0 (TID 683)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 685, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Running task 36.0 in stage 7.0 (TID 685)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 666) in 122 ms on localhost (19/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 671) in 115 ms on localhost (20/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15131
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 19.0 in stage 7.0 (TID 668). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 31.0 in stage 7.0 (TID 680). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14831
15/08/21 19:47:38 INFO Executor: Finished task 23.0 in stage 7.0 (TID 672). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 677) in 103 ms on localhost (21/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 21.0 in stage 7.0 (TID 670). 3233 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 27.0 in stage 7.0 (TID 676). 3228 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14992
15/08/21 19:47:38 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 686, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 37.0 in stage 7.0 (TID 686)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 687, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 688, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 689, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 40.0 in stage 7.0 (TID 689)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 690, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO Executor: Running task 41.0 in stage 7.0 (TID 690)
15/08/21 19:47:38 INFO Executor: Finished task 30.0 in stage 7.0 (TID 679). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 668) in 140 ms on localhost (22/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO Executor: Running task 38.0 in stage 7.0 (TID 687)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 691, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Running task 42.0 in stage 7.0 (TID 691)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 670) in 140 ms on localhost (23/200)
15/08/21 19:47:38 INFO Executor: Finished task 18.0 in stage 7.0 (TID 667). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 672) in 130 ms on localhost (24/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO Executor: Finished task 29.0 in stage 7.0 (TID 678). 3226 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 692, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO Executor: Running task 43.0 in stage 7.0 (TID 692)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO Executor: Finished task 26.0 in stage 7.0 (TID 675). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 693, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 694, localhost, ANY, 1980 bytes)
15/08/21 19:47:38 INFO Executor: Running task 45.0 in stage 7.0 (TID 694)
15/08/21 19:47:38 INFO Executor: Running task 39.0 in stage 7.0 (TID 688)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 679) in 125 ms on localhost (25/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 680) in 125 ms on localhost (26/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO Executor: Finished task 24.0 in stage 7.0 (TID 673). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 44.0 in stage 7.0 (TID 693)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 695, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Running task 46.0 in stage 7.0 (TID 695)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 678) in 140 ms on localhost (27/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 675) in 146 ms on localhost (28/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 676) in 154 ms on localhost (29/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 667) in 172 ms on localhost (30/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 673) in 169 ms on localhost (31/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Finished task 25.0 in stage 7.0 (TID 674). 3229 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 696, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 47.0 in stage 7.0 (TID 696)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 674) in 181 ms on localhost (32/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14597
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14942
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15272
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15557
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15107
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14678
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14502
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15532
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14972
15/08/21 19:47:38 INFO Executor: Finished task 35.0 in stage 7.0 (TID 684). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 34.0 in stage 7.0 (TID 683). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 32.0 in stage 7.0 (TID 681). 3233 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 697, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 48.0 in stage 7.0 (TID 697)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 698, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO Executor: Running task 49.0 in stage 7.0 (TID 698)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 699, localhost, ANY, 1980 bytes)
15/08/21 19:47:38 INFO Executor: Running task 50.0 in stage 7.0 (TID 699)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 43.0 in stage 7.0 (TID 692). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 684) in 124 ms on localhost (33/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 19:47:38 INFO Executor: Finished task 41.0 in stage 7.0 (TID 690). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 681) in 141 ms on localhost (34/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 683) in 129 ms on localhost (35/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15358
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Finished task 42.0 in stage 7.0 (TID 691). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15099
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 692) in 101 ms on localhost (36/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15302
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14489
15/08/21 19:47:38 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 700, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 701, localhost, ANY, 1979 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 702, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Running task 52.0 in stage 7.0 (TID 701)
15/08/21 19:47:38 INFO Executor: Running task 53.0 in stage 7.0 (TID 702)
15/08/21 19:47:38 INFO Executor: Running task 51.0 in stage 7.0 (TID 700)
15/08/21 19:47:38 INFO Executor: Finished task 36.0 in stage 7.0 (TID 685). 3233 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 703, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 690) in 115 ms on localhost (37/200)
15/08/21 19:47:38 INFO Executor: Running task 54.0 in stage 7.0 (TID 703)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 691) in 114 ms on localhost (38/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 685) in 140 ms on localhost (39/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO Executor: Finished task 45.0 in stage 7.0 (TID 694). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:38 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 704, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 694) in 117 ms on localhost (40/200)
15/08/21 19:47:38 INFO Executor: Finished task 37.0 in stage 7.0 (TID 686). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 55.0 in stage 7.0 (TID 704)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 705, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Finished task 40.0 in stage 7.0 (TID 689). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Finished task 38.0 in stage 7.0 (TID 687). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO Executor: Running task 56.0 in stage 7.0 (TID 705)
15/08/21 19:47:38 INFO Executor: Finished task 46.0 in stage 7.0 (TID 695). 3232 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 706, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Running task 57.0 in stage 7.0 (TID 706)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO Executor: Finished task 39.0 in stage 7.0 (TID 688). 3230 bytes result sent to driver
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:38 INFO Executor: Finished task 33.0 in stage 7.0 (TID 682). 3231 bytes result sent to driver
15/08/21 19:47:38 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 686) in 135 ms on localhost (41/200)
15/08/21 19:47:38 INFO Executor: Finished task 44.0 in stage 7.0 (TID 693). 3234 bytes result sent to driver
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 707, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO Executor: Running task 58.0 in stage 7.0 (TID 707)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 708, localhost, ANY, 1983 bytes)
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO Executor: Running task 59.0 in stage 7.0 (TID 708)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 709, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO Executor: Running task 60.0 in stage 7.0 (TID 709)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 689) in 148 ms on localhost (42/200)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:38 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 688) in 155 ms on localhost (43/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 695) in 131 ms on localhost (44/200)
15/08/21 19:47:38 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 687) in 157 ms on localhost (45/200)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 710, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14906
15/08/21 19:47:38 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 682) in 182 ms on localhost (46/200)
15/08/21 19:47:38 INFO Executor: Running task 61.0 in stage 7.0 (TID 710)
15/08/21 19:47:38 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 711, localhost, ANY, 1982 bytes)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO Executor: Running task 62.0 in stage 7.0 (TID 711)
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15614
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15104
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 693) in 155 ms on localhost (47/200)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14891
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 19:47:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14919
15/08/21 19:47:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14379
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO Executor: Finished task 47.0 in stage 7.0 (TID 696). 3230 bytes result sent to driver
15/08/21 19:47:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:38 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 712, localhost, ANY, 1981 bytes)
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:38 INFO Executor: Running task 63.0 in stage 7.0 (TID 712)
15/08/21 19:47:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Finished task 50.0 in stage 7.0 (TID 699). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 713, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Running task 64.0 in stage 7.0 (TID 713)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO Executor: Finished task 53.0 in stage 7.0 (TID 702). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 699) in 94 ms on localhost (48/200)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 714, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO Executor: Running task 65.0 in stage 7.0 (TID 714)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Finished task 52.0 in stage 7.0 (TID 701). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 49.0 in stage 7.0 (TID 698). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 696) in 141 ms on localhost (49/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 702) in 87 ms on localhost (50/200)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 715, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15038
15/08/21 19:47:39 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 716, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 19:47:39 INFO Executor: Finished task 48.0 in stage 7.0 (TID 697). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 66.0 in stage 7.0 (TID 715)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 19:47:39 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 701) in 94 ms on localhost (51/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 698) in 109 ms on localhost (52/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO Executor: Running task 67.0 in stage 7.0 (TID 716)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 717, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO Executor: Running task 68.0 in stage 7.0 (TID 717)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 697) in 116 ms on localhost (53/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15046
15/08/21 19:47:39 INFO Executor: Finished task 51.0 in stage 7.0 (TID 700). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14914
15/08/21 19:47:39 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 718, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 55.0 in stage 7.0 (TID 704). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 69.0 in stage 7.0 (TID 718)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 704) in 106 ms on localhost (54/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15145
15/08/21 19:47:39 INFO Executor: Finished task 57.0 in stage 7.0 (TID 706). 3234 bytes result sent to driver
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 719, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15063
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15603
15/08/21 19:47:39 INFO Executor: Finished task 56.0 in stage 7.0 (TID 705). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 720, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO Executor: Running task 70.0 in stage 7.0 (TID 719)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 721, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 72.0 in stage 7.0 (TID 721)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Running task 71.0 in stage 7.0 (TID 720)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14772
15/08/21 19:47:39 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 700) in 142 ms on localhost (55/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 705) in 128 ms on localhost (56/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15609
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Finished task 54.0 in stage 7.0 (TID 703). 3229 bytes result sent to driver
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 706) in 136 ms on localhost (57/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO Executor: Finished task 61.0 in stage 7.0 (TID 710). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Finished task 59.0 in stage 7.0 (TID 708). 3229 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 722, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 73.0 in stage 7.0 (TID 722)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 703) in 153 ms on localhost (58/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 723, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 724, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 75.0 in stage 7.0 (TID 724)
15/08/21 19:47:39 INFO Executor: Running task 74.0 in stage 7.0 (TID 723)
15/08/21 19:47:39 INFO Executor: Finished task 60.0 in stage 7.0 (TID 709). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO Executor: Finished task 58.0 in stage 7.0 (TID 707). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15132
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 710) in 117 ms on localhost (59/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14845
15/08/21 19:47:39 INFO Executor: Finished task 63.0 in stage 7.0 (TID 712). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 708) in 148 ms on localhost (60/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15043
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 725, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 62.0 in stage 7.0 (TID 711). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO Executor: Running task 76.0 in stage 7.0 (TID 725)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15191
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 726, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:39 INFO Executor: Running task 77.0 in stage 7.0 (TID 726)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 727, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 78.0 in stage 7.0 (TID 727)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 728, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 707) in 163 ms on localhost (61/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 709) in 159 ms on localhost (62/200)
15/08/21 19:47:39 INFO Executor: Running task 79.0 in stage 7.0 (TID 728)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 712) in 119 ms on localhost (63/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 711) in 145 ms on localhost (64/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Finished task 67.0 in stage 7.0 (TID 716). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 729, localhost, ANY, 1980 bytes)
15/08/21 19:47:39 INFO Executor: Running task 80.0 in stage 7.0 (TID 729)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Finished task 68.0 in stage 7.0 (TID 717). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 716) in 109 ms on localhost (65/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15364
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Finished task 64.0 in stage 7.0 (TID 713). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 65.0 in stage 7.0 (TID 714). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 730, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 81.0 in stage 7.0 (TID 730)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 731, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 66.0 in stage 7.0 (TID 715). 3234 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 732, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 83.0 in stage 7.0 (TID 732)
15/08/21 19:47:39 INFO Executor: Running task 82.0 in stage 7.0 (TID 731)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 713) in 128 ms on localhost (66/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 714) in 125 ms on localhost (67/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15333
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 733, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 84.0 in stage 7.0 (TID 733)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 717) in 118 ms on localhost (68/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 715) in 128 ms on localhost (69/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15009
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14740
15/08/21 19:47:39 INFO Executor: Finished task 69.0 in stage 7.0 (TID 718). 3232 bytes result sent to driver
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 734, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 85.0 in stage 7.0 (TID 734)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 718) in 129 ms on localhost (70/200)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15114
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15120
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15211
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15058
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14989
15/08/21 19:47:39 INFO Executor: Finished task 75.0 in stage 7.0 (TID 724). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 735, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 86.0 in stage 7.0 (TID 735)
15/08/21 19:47:39 INFO Executor: Finished task 72.0 in stage 7.0 (TID 721). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 736, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Running task 87.0 in stage 7.0 (TID 736)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 724) in 101 ms on localhost (71/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14550
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO Executor: Finished task 77.0 in stage 7.0 (TID 726). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 721) in 137 ms on localhost (72/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14612
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Finished task 71.0 in stage 7.0 (TID 720). 3234 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Finished task 70.0 in stage 7.0 (TID 719). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 74.0 in stage 7.0 (TID 723). 3235 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 737, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 738, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 88.0 in stage 7.0 (TID 737)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 739, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 740, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 90.0 in stage 7.0 (TID 739)
15/08/21 19:47:39 INFO Executor: Finished task 79.0 in stage 7.0 (TID 728). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 720) in 162 ms on localhost (73/200)
15/08/21 19:47:39 INFO Executor: Running task 89.0 in stage 7.0 (TID 738)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 741, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 91.0 in stage 7.0 (TID 740)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 726) in 115 ms on localhost (74/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 719) in 170 ms on localhost (75/200)
15/08/21 19:47:39 INFO Executor: Running task 92.0 in stage 7.0 (TID 741)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Finished task 78.0 in stage 7.0 (TID 727). 3234 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 742, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 723) in 136 ms on localhost (76/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 728) in 111 ms on localhost (77/200)
15/08/21 19:47:39 INFO Executor: Finished task 73.0 in stage 7.0 (TID 722). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 76.0 in stage 7.0 (TID 725). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 93.0 in stage 7.0 (TID 742)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 743, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 94.0 in stage 7.0 (TID 743)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 744, localhost, ANY, 1980 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 727) in 113 ms on localhost (78/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 722) in 145 ms on localhost (79/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 725) in 128 ms on localhost (80/200)
15/08/21 19:47:39 INFO Executor: Running task 95.0 in stage 7.0 (TID 744)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15203
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14934
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14440
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15500
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 14985
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15137
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15103
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14825
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15238
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14876
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15404
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 14509
15/08/21 19:47:39 INFO Executor: Finished task 83.0 in stage 7.0 (TID 732). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 81.0 in stage 7.0 (TID 730). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15545
15/08/21 19:47:39 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 745, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14784
15/08/21 19:47:39 INFO Executor: Running task 96.0 in stage 7.0 (TID 745)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 746, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Running task 97.0 in stage 7.0 (TID 746)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 732) in 683 ms on localhost (81/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 730) in 685 ms on localhost (82/200)
15/08/21 19:47:39 INFO Executor: Finished task 91.0 in stage 7.0 (TID 740). 3235 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 747, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 98.0 in stage 7.0 (TID 747)
15/08/21 19:47:39 INFO Executor: Finished task 86.0 in stage 7.0 (TID 735). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 94.0 in stage 7.0 (TID 743). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO Executor: Finished task 92.0 in stage 7.0 (TID 741). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 740) in 619 ms on localhost (83/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 748, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 95.0 in stage 7.0 (TID 744). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 87.0 in stage 7.0 (TID 736). 3234 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 85.0 in stage 7.0 (TID 734). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 80.0 in stage 7.0 (TID 729). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 749, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 99.0 in stage 7.0 (TID 748)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/21 19:47:39 INFO Executor: Running task 100.0 in stage 7.0 (TID 749)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 735) in 657 ms on localhost (84/200)
15/08/21 19:47:39 INFO Executor: Finished task 82.0 in stage 7.0 (TID 731). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 89.0 in stage 7.0 (TID 738). 3234 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 93.0 in stage 7.0 (TID 742). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 743) in 622 ms on localhost (85/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 750, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 741) in 633 ms on localhost (86/200)
15/08/21 19:47:39 INFO Executor: Finished task 84.0 in stage 7.0 (TID 733). 3232 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 88.0 in stage 7.0 (TID 737). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 744) in 628 ms on localhost (87/200)
15/08/21 19:47:39 INFO Executor: Running task 101.0 in stage 7.0 (TID 750)
15/08/21 19:47:39 INFO Executor: Finished task 90.0 in stage 7.0 (TID 739). 3234 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 751, localhost, ANY, 1979 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Running task 102.0 in stage 7.0 (TID 751)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 752, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO Executor: Running task 103.0 in stage 7.0 (TID 752)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 753, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 104.0 in stage 7.0 (TID 753)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 736) in 674 ms on localhost (88/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 734) in 707 ms on localhost (89/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 754, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO Executor: Running task 105.0 in stage 7.0 (TID 754)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 729) in 740 ms on localhost (90/200)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 731) in 733 ms on localhost (91/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 755, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 756, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 107.0 in stage 7.0 (TID 756)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 757, localhost, ANY, 1979 bytes)
15/08/21 19:47:39 INFO Executor: Running task 108.0 in stage 7.0 (TID 757)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 758, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 738) in 671 ms on localhost (92/200)
15/08/21 19:47:39 INFO Executor: Running task 109.0 in stage 7.0 (TID 758)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 742) in 655 ms on localhost (93/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 733) in 738 ms on localhost (94/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 737) in 674 ms on localhost (95/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Running task 106.0 in stage 7.0 (TID 755)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 759, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Running task 110.0 in stage 7.0 (TID 759)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 760, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO Executor: Running task 111.0 in stage 7.0 (TID 760)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15272
15/08/21 19:47:39 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 739) in 677 ms on localhost (96/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15189
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14982
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15018
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO Executor: Finished task 96.0 in stage 7.0 (TID 745). 3228 bytes result sent to driver
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15183
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 761, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Running task 112.0 in stage 7.0 (TID 761)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15463
15/08/21 19:47:39 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 745) in 99 ms on localhost (97/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14819
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO Executor: Finished task 97.0 in stage 7.0 (TID 746). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15267
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 762, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Running task 113.0 in stage 7.0 (TID 762)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15096
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 746) in 109 ms on localhost (98/200)
15/08/21 19:47:39 INFO Executor: Finished task 98.0 in stage 7.0 (TID 747). 3235 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 763, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 104.0 in stage 7.0 (TID 753). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 114.0 in stage 7.0 (TID 763)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 764, localhost, ANY, 1981 bytes)
15/08/21 19:47:39 INFO Executor: Running task 115.0 in stage 7.0 (TID 764)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 747) in 113 ms on localhost (99/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 753) in 85 ms on localhost (100/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15566
15/08/21 19:47:39 INFO Executor: Finished task 100.0 in stage 7.0 (TID 749). 3233 bytes result sent to driver
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 765, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 19:47:39 INFO Executor: Running task 116.0 in stage 7.0 (TID 765)
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 15509
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14784
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 749) in 120 ms on localhost (101/200)
15/08/21 19:47:39 INFO Executor: Finished task 103.0 in stage 7.0 (TID 752). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15313
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 766, localhost, ANY, 1980 bytes)
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15274
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14816
15/08/21 19:47:39 INFO Executor: Running task 117.0 in stage 7.0 (TID 766)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 752) in 99 ms on localhost (102/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14883
15/08/21 19:47:39 INFO Executor: Finished task 105.0 in stage 7.0 (TID 754). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 767, localhost, ANY, 1983 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 99.0 in stage 7.0 (TID 748). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Finished task 101.0 in stage 7.0 (TID 750). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 768, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 754) in 108 ms on localhost (103/200)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Finished task 109.0 in stage 7.0 (TID 758). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 119.0 in stage 7.0 (TID 768)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 769, localhost, ANY, 1980 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 110.0 in stage 7.0 (TID 759). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 120.0 in stage 7.0 (TID 769)
15/08/21 19:47:39 INFO Executor: Finished task 107.0 in stage 7.0 (TID 756). 3232 bytes result sent to driver
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO Executor: Running task 118.0 in stage 7.0 (TID 767)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 770, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 771, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO Executor: Running task 122.0 in stage 7.0 (TID 771)
15/08/21 19:47:39 INFO Executor: Finished task 112.0 in stage 7.0 (TID 761). 3228 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 772, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 102.0 in stage 7.0 (TID 751). 3231 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 123.0 in stage 7.0 (TID 772)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 750) in 131 ms on localhost (104/200)
15/08/21 19:47:39 INFO Executor: Running task 121.0 in stage 7.0 (TID 770)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 758) in 109 ms on localhost (105/200)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Finished task 106.0 in stage 7.0 (TID 755). 3233 bytes result sent to driver
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 759) in 105 ms on localhost (106/200)
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 773, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO Executor: Finished task 111.0 in stage 7.0 (TID 760). 3229 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 124.0 in stage 7.0 (TID 773)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 774, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO Executor: Finished task 108.0 in stage 7.0 (TID 757). 3230 bytes result sent to driver
15/08/21 19:47:39 INFO Executor: Running task 125.0 in stage 7.0 (TID 774)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 775, localhost, ANY, 1980 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 776, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO Executor: Running task 127.0 in stage 7.0 (TID 776)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14791
15/08/21 19:47:39 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 777, localhost, ANY, 1982 bytes)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 748) in 168 ms on localhost (107/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 756) in 127 ms on localhost (108/200)
15/08/21 19:47:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:39 INFO Executor: Running task 126.0 in stage 7.0 (TID 775)
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 19:47:39 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 761) in 94 ms on localhost (109/200)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 751) in 147 ms on localhost (110/200)
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14693
15/08/21 19:47:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:39 INFO Executor: Running task 128.0 in stage 7.0 (TID 777)
15/08/21 19:47:39 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 755) in 134 ms on localhost (111/200)
15/08/21 19:47:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14938
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 757) in 140 ms on localhost (112/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 760) in 136 ms on localhost (113/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15625
15/08/21 19:47:40 INFO Executor: Finished task 114.0 in stage 7.0 (TID 763). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 116.0 in stage 7.0 (TID 765). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14864
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14751
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15147
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 778, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 779, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO Executor: Running task 129.0 in stage 7.0 (TID 778)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Running task 130.0 in stage 7.0 (TID 779)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 14564
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15142
15/08/21 19:47:40 INFO Executor: Finished task 115.0 in stage 7.0 (TID 764). 3235 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 763) in 116 ms on localhost (114/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 765) in 104 ms on localhost (115/200)
15/08/21 19:47:40 INFO Executor: Finished task 117.0 in stage 7.0 (TID 766). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 780, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 781, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO Executor: Running task 131.0 in stage 7.0 (TID 780)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 764) in 130 ms on localhost (116/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Finished task 122.0 in stage 7.0 (TID 771). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 123.0 in stage 7.0 (TID 772). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 782, localhost, ANY, 1980 bytes)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Running task 133.0 in stage 7.0 (TID 782)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 783, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Running task 132.0 in stage 7.0 (TID 781)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 771) in 91 ms on localhost (117/200)
15/08/21 19:47:40 INFO Executor: Running task 134.0 in stage 7.0 (TID 783)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 772) in 90 ms on localhost (118/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 766) in 118 ms on localhost (119/200)
15/08/21 19:47:40 INFO Executor: Finished task 113.0 in stage 7.0 (TID 762). 3230 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15117
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 784, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Running task 135.0 in stage 7.0 (TID 784)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15795
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14687
15/08/21 19:47:40 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 762) in 154 ms on localhost (120/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 118.0 in stage 7.0 (TID 767). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 785, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 119.0 in stage 7.0 (TID 768). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 136.0 in stage 7.0 (TID 785)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14793
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15726
15/08/21 19:47:40 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 768) in 107 ms on localhost (121/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 786, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15571
15/08/21 19:47:40 INFO Executor: Running task 137.0 in stage 7.0 (TID 786)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 767) in 117 ms on localhost (122/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO Executor: Finished task 121.0 in stage 7.0 (TID 770). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15129
15/08/21 19:47:40 INFO Executor: Finished task 120.0 in stage 7.0 (TID 769). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 127.0 in stage 7.0 (TID 776). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 787, localhost, ANY, 1980 bytes)
15/08/21 19:47:40 INFO Executor: Running task 138.0 in stage 7.0 (TID 787)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 788, localhost, ANY, 1979 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 770) in 123 ms on localhost (123/200)
15/08/21 19:47:40 INFO Executor: Finished task 128.0 in stage 7.0 (TID 777). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 769) in 126 ms on localhost (124/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Finished task 126.0 in stage 7.0 (TID 775). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 139.0 in stage 7.0 (TID 788)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 776) in 109 ms on localhost (125/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 789, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO Executor: Running task 140.0 in stage 7.0 (TID 789)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Finished task 125.0 in stage 7.0 (TID 774). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 790, localhost, ANY, 1980 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 777) in 114 ms on localhost (126/200)
15/08/21 19:47:40 INFO Executor: Running task 141.0 in stage 7.0 (TID 790)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 791, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Running task 142.0 in stage 7.0 (TID 791)
15/08/21 19:47:40 INFO Executor: Finished task 124.0 in stage 7.0 (TID 773). 3224 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 792, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO Executor: Running task 143.0 in stage 7.0 (TID 792)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 793, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 129.0 in stage 7.0 (TID 778). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 775) in 126 ms on localhost (127/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 774) in 132 ms on localhost (128/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 773) in 135 ms on localhost (129/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14741
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15038
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15049
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15344
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14809
15/08/21 19:47:40 INFO Executor: Running task 144.0 in stage 7.0 (TID 793)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14826
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14951
15/08/21 19:47:40 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 794, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Running task 145.0 in stage 7.0 (TID 794)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 778) in 122 ms on localhost (130/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15361
15/08/21 19:47:40 INFO Executor: Finished task 132.0 in stage 7.0 (TID 781). 3232 bytes result sent to driver
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 795, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 134.0 in stage 7.0 (TID 783). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 146.0 in stage 7.0 (TID 795)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 796, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 781) in 102 ms on localhost (131/200)
15/08/21 19:47:40 INFO Executor: Running task 147.0 in stage 7.0 (TID 796)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 783) in 101 ms on localhost (132/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14814
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15658
15/08/21 19:47:40 INFO Executor: Finished task 137.0 in stage 7.0 (TID 786). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 130.0 in stage 7.0 (TID 779). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 133.0 in stage 7.0 (TID 782). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 797, localhost, ANY, 1979 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 131.0 in stage 7.0 (TID 780). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14590
15/08/21 19:47:40 INFO Executor: Running task 148.0 in stage 7.0 (TID 797)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 798, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 136.0 in stage 7.0 (TID 785). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 149.0 in stage 7.0 (TID 798)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 799, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO Executor: Running task 150.0 in stage 7.0 (TID 799)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 779) in 148 ms on localhost (133/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 800, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 801, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 786) in 103 ms on localhost (134/200)
15/08/21 19:47:40 INFO Executor: Running task 152.0 in stage 7.0 (TID 801)
15/08/21 19:47:40 INFO Executor: Running task 151.0 in stage 7.0 (TID 800)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 782) in 121 ms on localhost (135/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 785) in 110 ms on localhost (136/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 780) in 133 ms on localhost (137/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO Executor: Finished task 143.0 in stage 7.0 (TID 792). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 140.0 in stage 7.0 (TID 789). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 135.0 in stage 7.0 (TID 784). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO Executor: Finished task 138.0 in stage 7.0 (TID 787). 3229 bytes result sent to driver
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 139.0 in stage 7.0 (TID 788). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 802, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14508
15/08/21 19:47:40 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 803, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Running task 154.0 in stage 7.0 (TID 803)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14836
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15501
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 804, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO Executor: Running task 153.0 in stage 7.0 (TID 802)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 792) in 93 ms on localhost (138/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15141
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 805, localhost, ANY, 1980 bytes)
15/08/21 19:47:40 INFO Executor: Running task 156.0 in stage 7.0 (TID 805)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 789) in 106 ms on localhost (139/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 784) in 138 ms on localhost (140/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 787) in 119 ms on localhost (141/200)
15/08/21 19:47:40 INFO Executor: Running task 155.0 in stage 7.0 (TID 804)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 788) in 118 ms on localhost (142/200)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 806, localhost, ANY, 1984 bytes)
15/08/21 19:47:40 INFO Executor: Running task 157.0 in stage 7.0 (TID 806)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO Executor: Finished task 145.0 in stage 7.0 (TID 794). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 807, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO Executor: Running task 158.0 in stage 7.0 (TID 807)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 146.0 in stage 7.0 (TID 795). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 808, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 794) in 100 ms on localhost (143/200)
15/08/21 19:47:40 INFO Executor: Running task 159.0 in stage 7.0 (TID 808)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14923
15/08/21 19:47:40 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 795) in 89 ms on localhost (144/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Finished task 144.0 in stage 7.0 (TID 793). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 809, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Running task 160.0 in stage 7.0 (TID 809)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15549
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 793) in 137 ms on localhost (145/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14613
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15489
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14952
15/08/21 19:47:40 INFO Executor: Finished task 141.0 in stage 7.0 (TID 790). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14755
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 147.0 in stage 7.0 (TID 796). 3230 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14896
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15359
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 790) in 160 ms on localhost (146/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 810, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO Executor: Running task 161.0 in stage 7.0 (TID 810)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 150.0 in stage 7.0 (TID 799). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14842
15/08/21 19:47:40 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 796) in 123 ms on localhost (147/200)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 811, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15164
15/08/21 19:47:40 INFO Executor: Running task 162.0 in stage 7.0 (TID 811)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14944
15/08/21 19:47:40 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 799) in 110 ms on localhost (148/200)
15/08/21 19:47:40 INFO Executor: Finished task 142.0 in stage 7.0 (TID 791). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 812, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 148.0 in stage 7.0 (TID 797). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 163.0 in stage 7.0 (TID 812)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15098
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 149.0 in stage 7.0 (TID 798). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 813, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 791) in 182 ms on localhost (149/200)
15/08/21 19:47:40 INFO Executor: Running task 164.0 in stage 7.0 (TID 813)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14884
15/08/21 19:47:40 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 814, localhost, ANY, 1980 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 154.0 in stage 7.0 (TID 803). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 153.0 in stage 7.0 (TID 802). 3235 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 165.0 in stage 7.0 (TID 814)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 815, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Running task 166.0 in stage 7.0 (TID 815)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 797) in 130 ms on localhost (150/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15636
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO Executor: Finished task 157.0 in stage 7.0 (TID 806). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14560
15/08/21 19:47:40 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 816, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 798) in 132 ms on localhost (151/200)
15/08/21 19:47:40 INFO Executor: Finished task 156.0 in stage 7.0 (TID 805). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO Executor: Finished task 152.0 in stage 7.0 (TID 801). 3228 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 167.0 in stage 7.0 (TID 816)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 817, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 803) in 110 ms on localhost (152/200)
15/08/21 19:47:40 INFO Executor: Running task 168.0 in stage 7.0 (TID 817)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 818, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 159.0 in stage 7.0 (TID 808). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 169.0 in stage 7.0 (TID 818)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 802) in 121 ms on localhost (153/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 819, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO Executor: Running task 170.0 in stage 7.0 (TID 819)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 806) in 105 ms on localhost (154/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 805) in 117 ms on localhost (155/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 801) in 146 ms on localhost (156/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO Executor: Finished task 158.0 in stage 7.0 (TID 807). 3229 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO Executor: Finished task 155.0 in stage 7.0 (TID 804). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 820, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Running task 171.0 in stage 7.0 (TID 820)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 821, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Running task 172.0 in stage 7.0 (TID 821)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 160.0 in stage 7.0 (TID 809). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 822, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 807) in 109 ms on localhost (157/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Running task 173.0 in stage 7.0 (TID 822)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 823, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO Executor: Running task 174.0 in stage 7.0 (TID 823)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15013
15/08/21 19:47:40 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 824, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 151.0 in stage 7.0 (TID 800). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 804) in 149 ms on localhost (158/200)
15/08/21 19:47:40 INFO Executor: Running task 175.0 in stage 7.0 (TID 824)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 808) in 123 ms on localhost (159/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 19:47:40 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 825, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 809) in 111 ms on localhost (160/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15296
15/08/21 19:47:40 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 800) in 180 ms on localhost (161/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15347
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14810
15/08/21 19:47:40 INFO Executor: Running task 176.0 in stage 7.0 (TID 825)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 14843
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15470
15/08/21 19:47:40 INFO Executor: Finished task 162.0 in stage 7.0 (TID 811). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 19:47:40 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 826, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15062
15/08/21 19:47:40 INFO Executor: Finished task 166.0 in stage 7.0 (TID 815). 3229 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 177.0 in stage 7.0 (TID 826)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14788
15/08/21 19:47:40 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 815) in 91 ms on localhost (162/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 167.0 in stage 7.0 (TID 816). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14848
15/08/21 19:47:40 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 827, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 161.0 in stage 7.0 (TID 810). 3233 bytes result sent to driver
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 828, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO Executor: Running task 178.0 in stage 7.0 (TID 827)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 811) in 112 ms on localhost (163/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Running task 179.0 in stage 7.0 (TID 828)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 829, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 14791
15/08/21 19:47:40 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 810) in 133 ms on localhost (164/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 816) in 101 ms on localhost (165/200)
15/08/21 19:47:40 INFO Executor: Finished task 165.0 in stage 7.0 (TID 814). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:40 INFO Executor: Finished task 164.0 in stage 7.0 (TID 813). 3229 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 168.0 in stage 7.0 (TID 817). 3235 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 180.0 in stage 7.0 (TID 829)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15386
15/08/21 19:47:40 INFO Executor: Finished task 170.0 in stage 7.0 (TID 819). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 169.0 in stage 7.0 (TID 818). 3235 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 830, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Running task 181.0 in stage 7.0 (TID 830)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 831, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15474
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Running task 182.0 in stage 7.0 (TID 831)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 832, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 814) in 119 ms on localhost (166/200)
15/08/21 19:47:40 INFO Executor: Running task 183.0 in stage 7.0 (TID 832)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 19:47:40 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 813) in 124 ms on localhost (167/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 833, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14736
15/08/21 19:47:40 INFO Executor: Running task 184.0 in stage 7.0 (TID 833)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 817) in 113 ms on localhost (168/200)
15/08/21 19:47:40 INFO Executor: Finished task 171.0 in stage 7.0 (TID 820). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 819) in 104 ms on localhost (169/200)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 834, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 818) in 118 ms on localhost (170/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Running task 185.0 in stage 7.0 (TID 834)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 19:47:40 INFO Executor: Finished task 172.0 in stage 7.0 (TID 821). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 14788
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 820) in 121 ms on localhost (171/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14879
15/08/21 19:47:40 INFO Executor: Finished task 175.0 in stage 7.0 (TID 824). 3229 bytes result sent to driver
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 174.0 in stage 7.0 (TID 823). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 835, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 836, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Running task 186.0 in stage 7.0 (TID 835)
15/08/21 19:47:40 INFO Executor: Running task 187.0 in stage 7.0 (TID 836)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 837, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 821) in 131 ms on localhost (172/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 824) in 114 ms on localhost (173/200)
15/08/21 19:47:40 INFO Executor: Running task 188.0 in stage 7.0 (TID 837)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15368
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 173.0 in stage 7.0 (TID 822). 3234 bytes result sent to driver
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15340
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15101
15/08/21 19:47:40 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 823) in 125 ms on localhost (174/200)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 838, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 176.0 in stage 7.0 (TID 825). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 189.0 in stage 7.0 (TID 838)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 19:47:40 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 839, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 822) in 153 ms on localhost (175/200)
15/08/21 19:47:40 INFO Executor: Running task 190.0 in stage 7.0 (TID 839)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 840, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO Executor: Running task 191.0 in stage 7.0 (TID 840)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15207
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 825) in 134 ms on localhost (176/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 14750
15/08/21 19:47:40 INFO Executor: Finished task 180.0 in stage 7.0 (TID 829). 3230 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 177.0 in stage 7.0 (TID 826). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14569
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14771
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Finished task 178.0 in stage 7.0 (TID 827). 3228 bytes result sent to driver
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Finished task 181.0 in stage 7.0 (TID 830). 3233 bytes result sent to driver
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 841, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO Executor: Running task 192.0 in stage 7.0 (TID 841)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 842, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 182.0 in stage 7.0 (TID 831). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 193.0 in stage 7.0 (TID 842)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 843, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 826) in 143 ms on localhost (177/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 829) in 125 ms on localhost (178/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 184.0 in stage 7.0 (TID 833). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Running task 194.0 in stage 7.0 (TID 843)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Finished task 185.0 in stage 7.0 (TID 834). 3233 bytes result sent to driver
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 844, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 827) in 138 ms on localhost (179/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 179.0 in stage 7.0 (TID 828). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 830) in 125 ms on localhost (180/200)
15/08/21 19:47:40 INFO Executor: Running task 195.0 in stage 7.0 (TID 844)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15524
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 163.0 in stage 7.0 (TID 812). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 845, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 831) in 123 ms on localhost (181/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14854
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15330
15/08/21 19:47:40 INFO Executor: Running task 196.0 in stage 7.0 (TID 845)
15/08/21 19:47:40 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 846, localhost, ANY, 1982 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 19:47:40 INFO Executor: Running task 197.0 in stage 7.0 (TID 846)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 14802
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14795
15/08/21 19:47:40 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 847, localhost, ANY, 1981 bytes)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Running task 198.0 in stage 7.0 (TID 847)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 833) in 141 ms on localhost (182/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 834) in 131 ms on localhost (183/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15090
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 828) in 174 ms on localhost (184/200)
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO Executor: Finished task 187.0 in stage 7.0 (TID 836). 3229 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 848, localhost, ANY, 1983 bytes)
15/08/21 19:47:40 INFO Executor: Finished task 188.0 in stage 7.0 (TID 837). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO Executor: Running task 199.0 in stage 7.0 (TID 848)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 812) in 286 ms on localhost (185/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:40 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 836) in 126 ms on localhost (186/200)
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 190.0 in stage 7.0 (TID 839). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 837) in 127 ms on localhost (187/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 183.0 in stage 7.0 (TID 832). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 189.0 in stage 7.0 (TID 838). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15007
15/08/21 19:47:40 INFO Executor: Finished task 191.0 in stage 7.0 (TID 840). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 839) in 102 ms on localhost (188/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 832) in 173 ms on localhost (189/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 838) in 116 ms on localhost (190/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 840) in 104 ms on localhost (191/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15396
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15150
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO Executor: Finished task 186.0 in stage 7.0 (TID 835). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14479
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 835) in 152 ms on localhost (192/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14780
15/08/21 19:47:40 INFO Executor: Finished task 192.0 in stage 7.0 (TID 841). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 841) in 112 ms on localhost (193/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 194.0 in stage 7.0 (TID 843). 3228 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 843) in 101 ms on localhost (194/200)
15/08/21 19:47:40 INFO Executor: Finished task 193.0 in stage 7.0 (TID 842). 3230 bytes result sent to driver
15/08/21 19:47:40 INFO Executor: Finished task 198.0 in stage 7.0 (TID 847). 3234 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 19:47:40 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 842) in 107 ms on localhost (195/200)
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO Executor: Finished task 195.0 in stage 7.0 (TID 844). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14677
15/08/21 19:47:40 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 847) in 76 ms on localhost (196/200)
15/08/21 19:47:40 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 844) in 107 ms on localhost (197/200)
15/08/21 19:47:40 INFO Executor: Finished task 197.0 in stage 7.0 (TID 846). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 846) in 99 ms on localhost (198/200)
15/08/21 19:47:40 INFO Executor: Finished task 199.0 in stage 7.0 (TID 848). 3231 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 848) in 83 ms on localhost (199/200)
15/08/21 19:47:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 19:47:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14914
15/08/21 19:47:40 INFO Executor: Finished task 196.0 in stage 7.0 (TID 845). 3232 bytes result sent to driver
15/08/21 19:47:40 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 845) in 242 ms on localhost (200/200)
15/08/21 19:47:40 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/21 19:47:40 INFO DAGScheduler: ResultStage 7 (processCmd at CliDriver.java:423) finished in 2.372 s
15/08/21 19:47:40 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@774996b8
15/08/21 19:47:40 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 3.821852 s
15/08/21 19:47:40 INFO StatsReportListener: task runtime:(count: 200, mean: 184.635000, stdev: 154.332342, max: 740.000000, min: 76.000000)
15/08/21 19:47:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:40 INFO StatsReportListener: 	76.0 ms	94.0 ms	101.0 ms	113.0 ms	128.0 ms	155.0 ms	320.0 ms	671.0 ms	740.0 ms
15/08/21 19:47:40 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.370000, stdev: 1.050286, max: 10.000000, min: 0.000000)
15/08/21 19:47:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:40 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	10.0 ms
15/08/21 19:47:40 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 19:47:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:40 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 19:47:40 INFO StatsReportListener: task result size:(count: 200, mean: 3231.525000, stdev: 1.830130, max: 3235.000000, min: 3224.000000)
15/08/21 19:47:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:40 INFO StatsReportListener: 	3.1 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB
15/08/21 19:47:40 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 78.247175, stdev: 8.896757, max: 97.080292, min: 52.554745)
15/08/21 19:47:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:40 INFO StatsReportListener: 	53 %	65 %	67 %	72 %	78 %	85 %	90 %	94 %	97 %
15/08/21 19:47:40 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.271545, stdev: 0.801629, max: 6.711409, min: 0.000000)
15/08/21 19:47:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:40 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 7 %
15/08/21 19:47:40 INFO StatsReportListener: other time pct: (count: 200, mean: 21.481280, stdev: 8.794459, max: 46.715328, min: 2.919708)
15/08/21 19:47:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:40 INFO StatsReportListener: 	 3 %	 6 %	10 %	15 %	22 %	28 %	33 %	34 %	47 %
15/08/21 19:47:40 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 19:47:40 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:41 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 19:47:41 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 198 bytes
15/08/21 19:47:41 INFO DAGScheduler: Registering RDD 44 (processCmd at CliDriver.java:423)
15/08/21 19:47:41 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 19:47:41 INFO DAGScheduler: Final stage: ResultStage 10(processCmd at CliDriver.java:423)
15/08/21 19:47:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
15/08/21 19:47:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
15/08/21 19:47:41 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 19:47:41 INFO MemoryStore: ensureFreeSpace(16104) called with curMem=1895782, maxMem=22226833244
15/08/21 19:47:41 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.7 KB, free 20.7 GB)
15/08/21 19:47:41 INFO MemoryStore: ensureFreeSpace(8457) called with curMem=1911886, maxMem=22226833244
15/08/21 19:47:41 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.3 KB, free 20.7 GB)
15/08/21 19:47:41 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:54384 (size: 8.3 KB, free: 20.7 GB)
15/08/21 19:47:41 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/21 19:47:41 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423)
15/08/21 19:47:41 INFO TaskSchedulerImpl: Adding task set 9.0 with 200 tasks
15/08/21 19:47:41 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 849, localhost, ANY, 1971 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 850, localhost, ANY, 1970 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 851, localhost, ANY, 1973 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 852, localhost, ANY, 1973 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 853, localhost, ANY, 1971 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 854, localhost, ANY, 1972 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 855, localhost, ANY, 1968 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 856, localhost, ANY, 1972 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 857, localhost, ANY, 1970 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 858, localhost, ANY, 1972 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 859, localhost, ANY, 1970 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 860, localhost, ANY, 1969 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 861, localhost, ANY, 1972 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 862, localhost, ANY, 1971 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 863, localhost, ANY, 1969 bytes)
15/08/21 19:47:41 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 864, localhost, ANY, 1972 bytes)
15/08/21 19:47:41 INFO Executor: Running task 3.0 in stage 9.0 (TID 852)
15/08/21 19:47:41 INFO Executor: Running task 1.0 in stage 9.0 (TID 850)
15/08/21 19:47:41 INFO Executor: Running task 2.0 in stage 9.0 (TID 851)
15/08/21 19:47:41 INFO Executor: Running task 0.0 in stage 9.0 (TID 849)
15/08/21 19:47:41 INFO Executor: Running task 4.0 in stage 9.0 (TID 853)
15/08/21 19:47:41 INFO Executor: Running task 6.0 in stage 9.0 (TID 855)
15/08/21 19:47:41 INFO Executor: Running task 5.0 in stage 9.0 (TID 854)
15/08/21 19:47:41 INFO Executor: Running task 8.0 in stage 9.0 (TID 857)
15/08/21 19:47:41 INFO Executor: Running task 7.0 in stage 9.0 (TID 856)
15/08/21 19:47:41 INFO Executor: Running task 9.0 in stage 9.0 (TID 858)
15/08/21 19:47:41 INFO Executor: Running task 11.0 in stage 9.0 (TID 860)
15/08/21 19:47:41 INFO Executor: Running task 10.0 in stage 9.0 (TID 859)
15/08/21 19:47:41 INFO Executor: Running task 12.0 in stage 9.0 (TID 861)
15/08/21 19:47:41 INFO Executor: Running task 14.0 in stage 9.0 (TID 863)
15/08/21 19:47:41 INFO Executor: Running task 13.0 in stage 9.0 (TID 862)
15/08/21 19:47:41 INFO Executor: Running task 15.0 in stage 9.0 (TID 864)
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15235
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15029
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14718
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15001
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14838
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15586
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14731
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14824
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15384
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15679
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15393
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15079
15/08/21 19:47:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14829
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14878
15/08/21 19:47:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 19:47:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14967
15/08/21 19:47:42 INFO Executor: Finished task 3.0 in stage 9.0 (TID 852). 2341 bytes result sent to driver
15/08/21 19:47:42 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 865, localhost, ANY, 1972 bytes)
15/08/21 19:47:42 INFO Executor: Running task 16.0 in stage 9.0 (TID 865)
15/08/21 19:47:42 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 852) in 952 ms on localhost (1/200)
15/08/21 19:47:42 INFO Executor: Finished task 7.0 in stage 9.0 (TID 856). 2341 bytes result sent to driver
15/08/21 19:47:42 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 866, localhost, ANY, 1972 bytes)
15/08/21 19:47:42 INFO Executor: Running task 17.0 in stage 9.0 (TID 866)
15/08/21 19:47:42 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 856) in 974 ms on localhost (2/200)
15/08/21 19:47:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 19:47:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 19:47:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15023
15/08/21 19:47:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 19:47:42 INFO Executor: Finished task 14.0 in stage 9.0 (TID 863). 2341 bytes result sent to driver
15/08/21 19:47:42 INFO Executor: Finished task 8.0 in stage 9.0 (TID 857). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 10.0 in stage 9.0 (TID 859). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 12.0 in stage 9.0 (TID 861). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 1.0 in stage 9.0 (TID 850). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 5.0 in stage 9.0 (TID 854). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 15.0 in stage 9.0 (TID 864). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 2.0 in stage 9.0 (TID 851). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 13.0 in stage 9.0 (TID 862). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 867, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO Executor: Running task 18.0 in stage 9.0 (TID 867)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 868, localhost, ANY, 1973 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 863) in 1746 ms on localhost (3/200)
15/08/21 19:47:43 INFO Executor: Running task 19.0 in stage 9.0 (TID 868)
15/08/21 19:47:43 INFO Executor: Finished task 4.0 in stage 9.0 (TID 853). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 869, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 857) in 1761 ms on localhost (4/200)
15/08/21 19:47:43 INFO Executor: Running task 20.0 in stage 9.0 (TID 869)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 870, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 859) in 1762 ms on localhost (5/200)
15/08/21 19:47:43 INFO Executor: Running task 21.0 in stage 9.0 (TID 870)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 871, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO Executor: Running task 22.0 in stage 9.0 (TID 871)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 861) in 1764 ms on localhost (6/200)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 872, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO Executor: Finished task 0.0 in stage 9.0 (TID 849). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 11.0 in stage 9.0 (TID 860). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 850) in 1788 ms on localhost (7/200)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 854) in 1786 ms on localhost (8/200)
15/08/21 19:47:43 INFO Executor: Running task 23.0 in stage 9.0 (TID 872)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 873, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO Executor: Running task 24.0 in stage 9.0 (TID 873)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 874, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO Executor: Finished task 9.0 in stage 9.0 (TID 858). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Running task 25.0 in stage 9.0 (TID 874)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 875, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 864) in 1783 ms on localhost (9/200)
15/08/21 19:47:43 INFO Executor: Running task 26.0 in stage 9.0 (TID 875)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 876, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO Executor: Running task 27.0 in stage 9.0 (TID 876)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 851) in 1808 ms on localhost (10/200)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:43 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 877, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 853) in 1811 ms on localhost (11/200)
15/08/21 19:47:43 INFO Executor: Running task 28.0 in stage 9.0 (TID 877)
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:43 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 849) in 1816 ms on localhost (12/200)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 862) in 1796 ms on localhost (13/200)
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 878, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO Executor: Running task 29.0 in stage 9.0 (TID 878)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO Executor: Finished task 17.0 in stage 9.0 (TID 866). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO Executor: Finished task 6.0 in stage 9.0 (TID 855). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 879, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 860) in 1810 ms on localhost (14/200)
15/08/21 19:47:43 INFO Executor: Running task 30.0 in stage 9.0 (TID 879)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 858) in 1814 ms on localhost (15/200)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 880, localhost, ANY, 1969 bytes)
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15570
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO Executor: Running task 31.0 in stage 9.0 (TID 880)
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 881, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 866) in 883 ms on localhost (16/200)
15/08/21 19:47:43 INFO Executor: Running task 32.0 in stage 9.0 (TID 881)
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 19:47:43 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 855) in 1853 ms on localhost (17/200)
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14992
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14913
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15010
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14954
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15055
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14831
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14776
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO Executor: Finished task 16.0 in stage 9.0 (TID 865). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 19:47:43 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 882, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO Executor: Running task 33.0 in stage 9.0 (TID 882)
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15131
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15282
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 865) in 961 ms on localhost (18/200)
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15106
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14939
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15272
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15432
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14502
15/08/21 19:47:43 INFO Executor: Finished task 21.0 in stage 9.0 (TID 870). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 883, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO Executor: Running task 34.0 in stage 9.0 (TID 883)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 870) in 490 ms on localhost (19/200)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14942
15/08/21 19:47:43 INFO Executor: Finished task 24.0 in stage 9.0 (TID 873). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 884, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO Executor: Running task 35.0 in stage 9.0 (TID 884)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 873) in 626 ms on localhost (20/200)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:43 INFO Executor: Finished task 18.0 in stage 9.0 (TID 867). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14597
15/08/21 19:47:43 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 885, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO Executor: Running task 36.0 in stage 9.0 (TID 885)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 867) in 776 ms on localhost (21/200)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:43 INFO Executor: Finished task 22.0 in stage 9.0 (TID 871). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 25.0 in stage 9.0 (TID 874). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 886, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO Executor: Running task 37.0 in stage 9.0 (TID 886)
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO Executor: Finished task 26.0 in stage 9.0 (TID 875). 2341 bytes result sent to driver
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 887, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 871) in 782 ms on localhost (22/200)
15/08/21 19:47:43 INFO Executor: Running task 38.0 in stage 9.0 (TID 887)
15/08/21 19:47:43 INFO Executor: Finished task 28.0 in stage 9.0 (TID 877). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14972
15/08/21 19:47:43 INFO Executor: Finished task 20.0 in stage 9.0 (TID 869). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 888, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 874) in 775 ms on localhost (23/200)
15/08/21 19:47:43 INFO Executor: Running task 39.0 in stage 9.0 (TID 888)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 889, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO Executor: Running task 40.0 in stage 9.0 (TID 889)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 890, localhost, ANY, 1970 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 877) in 775 ms on localhost (24/200)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 875) in 785 ms on localhost (25/200)
15/08/21 19:47:43 INFO Executor: Running task 41.0 in stage 9.0 (TID 890)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 869) in 822 ms on localhost (26/200)
15/08/21 19:47:43 INFO Executor: Finished task 23.0 in stage 9.0 (TID 872). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 891, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO Executor: Finished task 32.0 in stage 9.0 (TID 881). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 872) in 829 ms on localhost (27/200)
15/08/21 19:47:43 INFO Executor: Running task 42.0 in stage 9.0 (TID 891)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 881) in 766 ms on localhost (28/200)
15/08/21 19:47:43 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 892, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO Executor: Running task 43.0 in stage 9.0 (TID 892)
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 19:47:43 INFO Executor: Finished task 30.0 in stage 9.0 (TID 879). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 893, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO Executor: Running task 44.0 in stage 9.0 (TID 893)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 15107
15/08/21 19:47:43 INFO Executor: Finished task 19.0 in stage 9.0 (TID 868). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO Executor: Finished task 27.0 in stage 9.0 (TID 876). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15099
15/08/21 19:47:43 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 879) in 819 ms on localhost (29/200)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO Executor: Finished task 31.0 in stage 9.0 (TID 880). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 894, localhost, ANY, 1969 bytes)
15/08/21 19:47:43 INFO Executor: Finished task 33.0 in stage 9.0 (TID 882). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Running task 45.0 in stage 9.0 (TID 894)
15/08/21 19:47:43 INFO Executor: Finished task 34.0 in stage 9.0 (TID 883). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO Executor: Finished task 29.0 in stage 9.0 (TID 878). 2341 bytes result sent to driver
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 895, localhost, ANY, 1971 bytes)
15/08/21 19:47:43 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 868) in 917 ms on localhost (30/200)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:43 INFO Executor: Running task 46.0 in stage 9.0 (TID 895)
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14678
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15302
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 896, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO Executor: Running task 47.0 in stage 9.0 (TID 896)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15557
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 897, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO Executor: Running task 48.0 in stage 9.0 (TID 897)
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 19:47:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14489
15/08/21 19:47:43 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 898, localhost, ANY, 1972 bytes)
15/08/21 19:47:43 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15302
15/08/21 19:47:43 INFO Executor: Running task 49.0 in stage 9.0 (TID 898)
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO TaskSetManager: Starting task 50.0 in stage 9.0 (TID 899, localhost, ANY, 1969 bytes)
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO Executor: Running task 50.0 in stage 9.0 (TID 899)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 880) in 880 ms on localhost (31/200)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 876) in 920 ms on localhost (32/200)
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15532
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 19:47:44 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 878) in 915 ms on localhost (33/200)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 883) in 485 ms on localhost (34/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 882) in 831 ms on localhost (35/200)
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15358
15/08/21 19:47:44 INFO Executor: Finished task 35.0 in stage 9.0 (TID 884). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Starting task 51.0 in stage 9.0 (TID 900, localhost, ANY, 1972 bytes)
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO Executor: Running task 51.0 in stage 9.0 (TID 900)
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 884) in 387 ms on localhost (36/200)
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14906
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15104
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15614
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO Executor: Finished task 36.0 in stage 9.0 (TID 885). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14379
15/08/21 19:47:44 INFO TaskSetManager: Starting task 52.0 in stage 9.0 (TID 901, localhost, ANY, 1968 bytes)
15/08/21 19:47:44 INFO Executor: Running task 52.0 in stage 9.0 (TID 901)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 885) in 358 ms on localhost (37/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO Executor: Finished task 38.0 in stage 9.0 (TID 887). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 19:47:44 INFO TaskSetManager: Starting task 53.0 in stage 9.0 (TID 902, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 53.0 in stage 9.0 (TID 902)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 887) in 310 ms on localhost (38/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14919
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14891
15/08/21 19:47:44 INFO Executor: Finished task 42.0 in stage 9.0 (TID 891). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO Executor: Finished task 40.0 in stage 9.0 (TID 889). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Starting task 54.0 in stage 9.0 (TID 903, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 54.0 in stage 9.0 (TID 903)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO TaskSetManager: Starting task 55.0 in stage 9.0 (TID 904, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 55.0 in stage 9.0 (TID 904)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 891) in 602 ms on localhost (39/200)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 889) in 633 ms on localhost (40/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15038
15/08/21 19:47:44 INFO Executor: Finished task 43.0 in stage 9.0 (TID 892). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Starting task 56.0 in stage 9.0 (TID 905, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 56.0 in stage 9.0 (TID 905)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 892) in 813 ms on localhost (41/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO Executor: Finished task 37.0 in stage 9.0 (TID 886). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO Executor: Finished task 44.0 in stage 9.0 (TID 893). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO Executor: Finished task 39.0 in stage 9.0 (TID 888). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Starting task 57.0 in stage 9.0 (TID 906, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 57.0 in stage 9.0 (TID 906)
15/08/21 19:47:44 INFO Executor: Finished task 41.0 in stage 9.0 (TID 890). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Starting task 58.0 in stage 9.0 (TID 907, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Finished task 45.0 in stage 9.0 (TID 894). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 886) in 936 ms on localhost (42/200)
15/08/21 19:47:44 INFO Executor: Finished task 46.0 in stage 9.0 (TID 895). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO Executor: Running task 58.0 in stage 9.0 (TID 907)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO TaskSetManager: Starting task 59.0 in stage 9.0 (TID 908, localhost, ANY, 1972 bytes)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO Executor: Running task 59.0 in stage 9.0 (TID 908)
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO TaskSetManager: Starting task 60.0 in stage 9.0 (TID 909, localhost, ANY, 1970 bytes)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 888) in 906 ms on localhost (43/200)
15/08/21 19:47:44 INFO Executor: Running task 60.0 in stage 9.0 (TID 909)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 893) in 843 ms on localhost (44/200)
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO TaskSetManager: Starting task 61.0 in stage 9.0 (TID 910, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 890) in 889 ms on localhost (45/200)
15/08/21 19:47:44 INFO Executor: Running task 61.0 in stage 9.0 (TID 910)
15/08/21 19:47:44 INFO Executor: Finished task 48.0 in stage 9.0 (TID 897). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Starting task 62.0 in stage 9.0 (TID 911, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 62.0 in stage 9.0 (TID 911)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 894) in 825 ms on localhost (46/200)
15/08/21 19:47:44 INFO TaskSetManager: Starting task 63.0 in stage 9.0 (TID 912, localhost, ANY, 1970 bytes)
15/08/21 19:47:44 INFO Executor: Running task 63.0 in stage 9.0 (TID 912)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 895) in 823 ms on localhost (47/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 897) in 793 ms on localhost (48/200)
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:44 INFO Executor: Finished task 50.0 in stage 9.0 (TID 899). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO Executor: Finished task 49.0 in stage 9.0 (TID 898). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO Executor: Finished task 52.0 in stage 9.0 (TID 901). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO Executor: Finished task 47.0 in stage 9.0 (TID 896). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO TaskSetManager: Starting task 64.0 in stage 9.0 (TID 913, localhost, ANY, 1972 bytes)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 19:47:44 INFO Executor: Running task 64.0 in stage 9.0 (TID 913)
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO TaskSetManager: Starting task 65.0 in stage 9.0 (TID 914, localhost, ANY, 1972 bytes)
15/08/21 19:47:44 INFO Executor: Finished task 53.0 in stage 9.0 (TID 902). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO Executor: Running task 65.0 in stage 9.0 (TID 914)
15/08/21 19:47:44 INFO TaskSetManager: Starting task 66.0 in stage 9.0 (TID 915, localhost, ANY, 1972 bytes)
15/08/21 19:47:44 INFO Executor: Running task 66.0 in stage 9.0 (TID 915)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 50.0 in stage 9.0 (TID 899) in 792 ms on localhost (49/200)
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO TaskSetManager: Starting task 67.0 in stage 9.0 (TID 916, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 67.0 in stage 9.0 (TID 916)
15/08/21 19:47:44 INFO TaskSetManager: Starting task 68.0 in stage 9.0 (TID 917, localhost, ANY, 1970 bytes)
15/08/21 19:47:44 INFO Executor: Running task 68.0 in stage 9.0 (TID 917)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 896) in 836 ms on localhost (50/200)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 898) in 808 ms on localhost (51/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:44 INFO TaskSetManager: Finished task 52.0 in stage 9.0 (TID 901) in 700 ms on localhost (52/200)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 53.0 in stage 9.0 (TID 902) in 670 ms on localhost (53/200)
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO Executor: Finished task 51.0 in stage 9.0 (TID 900). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15039
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO TaskSetManager: Starting task 69.0 in stage 9.0 (TID 918, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 69.0 in stage 9.0 (TID 918)
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO TaskSetManager: Finished task 51.0 in stage 9.0 (TID 900) in 751 ms on localhost (54/200)
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15046
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15063
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO Executor: Finished task 54.0 in stage 9.0 (TID 903). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 19:47:44 INFO TaskSetManager: Starting task 70.0 in stage 9.0 (TID 919, localhost, ANY, 1970 bytes)
15/08/21 19:47:44 INFO Executor: Running task 70.0 in stage 9.0 (TID 919)
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 19:47:44 INFO TaskSetManager: Finished task 54.0 in stage 9.0 (TID 903) in 386 ms on localhost (55/200)
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15609
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14772
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO Executor: Finished task 55.0 in stage 9.0 (TID 904). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15603
15/08/21 19:47:44 INFO TaskSetManager: Starting task 71.0 in stage 9.0 (TID 920, localhost, ANY, 1970 bytes)
15/08/21 19:47:44 INFO Executor: Running task 71.0 in stage 9.0 (TID 920)
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14845
15/08/21 19:47:44 INFO TaskSetManager: Finished task 55.0 in stage 9.0 (TID 904) in 400 ms on localhost (56/200)
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15145
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15364
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO Executor: Finished task 56.0 in stage 9.0 (TID 905). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 15043
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO TaskSetManager: Starting task 72.0 in stage 9.0 (TID 921, localhost, ANY, 1971 bytes)
15/08/21 19:47:44 INFO Executor: Running task 72.0 in stage 9.0 (TID 921)
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO TaskSetManager: Finished task 56.0 in stage 9.0 (TID 905) in 199 ms on localhost (57/200)
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15132
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15333
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15191
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15211
15/08/21 19:47:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 19:47:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14740
15/08/21 19:47:44 INFO Executor: Finished task 57.0 in stage 9.0 (TID 906). 2341 bytes result sent to driver
15/08/21 19:47:44 INFO TaskSetManager: Starting task 73.0 in stage 9.0 (TID 922, localhost, ANY, 1972 bytes)
15/08/21 19:47:44 INFO Executor: Running task 73.0 in stage 9.0 (TID 922)
15/08/21 19:47:44 INFO TaskSetManager: Finished task 57.0 in stage 9.0 (TID 906) in 263 ms on localhost (58/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO Executor: Finished task 62.0 in stage 9.0 (TID 911). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 74.0 in stage 9.0 (TID 923, localhost, ANY, 1970 bytes)
15/08/21 19:47:45 INFO Executor: Running task 74.0 in stage 9.0 (TID 923)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 62.0 in stage 9.0 (TID 911) in 276 ms on localhost (59/200)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14612
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15058
15/08/21 19:47:45 INFO Executor: Finished task 61.0 in stage 9.0 (TID 910). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 75.0 in stage 9.0 (TID 924, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO Executor: Running task 75.0 in stage 9.0 (TID 924)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 61.0 in stage 9.0 (TID 910) in 429 ms on localhost (60/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15009
15/08/21 19:47:45 INFO Executor: Finished task 63.0 in stage 9.0 (TID 912). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 76.0 in stage 9.0 (TID 925, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO Executor: Running task 76.0 in stage 9.0 (TID 925)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 63.0 in stage 9.0 (TID 912) in 520 ms on localhost (61/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO Executor: Finished task 60.0 in stage 9.0 (TID 909). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14989
15/08/21 19:47:45 INFO TaskSetManager: Starting task 77.0 in stage 9.0 (TID 926, localhost, ANY, 1970 bytes)
15/08/21 19:47:45 INFO Executor: Running task 77.0 in stage 9.0 (TID 926)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 60.0 in stage 9.0 (TID 909) in 588 ms on localhost (62/200)
15/08/21 19:47:45 INFO Executor: Finished task 58.0 in stage 9.0 (TID 907). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO Executor: Finished task 64.0 in stage 9.0 (TID 913). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 78.0 in stage 9.0 (TID 927, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO Executor: Running task 78.0 in stage 9.0 (TID 927)
15/08/21 19:47:45 INFO Executor: Finished task 59.0 in stage 9.0 (TID 908). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO Executor: Finished task 66.0 in stage 9.0 (TID 915). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 79.0 in stage 9.0 (TID 928, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO Executor: Running task 79.0 in stage 9.0 (TID 928)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 58.0 in stage 9.0 (TID 907) in 641 ms on localhost (63/200)
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15114
15/08/21 19:47:45 INFO Executor: Finished task 68.0 in stage 9.0 (TID 917). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO TaskSetManager: Starting task 80.0 in stage 9.0 (TID 929, localhost, ANY, 1969 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 64.0 in stage 9.0 (TID 913) in 609 ms on localhost (64/200)
15/08/21 19:47:45 INFO Executor: Running task 80.0 in stage 9.0 (TID 929)
15/08/21 19:47:45 INFO Executor: Finished task 69.0 in stage 9.0 (TID 918). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 81.0 in stage 9.0 (TID 930, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 59.0 in stage 9.0 (TID 908) in 652 ms on localhost (65/200)
15/08/21 19:47:45 INFO Executor: Running task 81.0 in stage 9.0 (TID 930)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:45 INFO Executor: Finished task 70.0 in stage 9.0 (TID 919). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO Executor: Finished task 67.0 in stage 9.0 (TID 916). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14550
15/08/21 19:47:45 INFO Executor: Finished task 65.0 in stage 9.0 (TID 914). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 82.0 in stage 9.0 (TID 931, localhost, ANY, 1970 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 66.0 in stage 9.0 (TID 915) in 632 ms on localhost (66/200)
15/08/21 19:47:45 INFO Executor: Running task 82.0 in stage 9.0 (TID 931)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO TaskSetManager: Starting task 83.0 in stage 9.0 (TID 932, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 68.0 in stage 9.0 (TID 917) in 638 ms on localhost (67/200)
15/08/21 19:47:45 INFO Executor: Running task 83.0 in stage 9.0 (TID 932)
15/08/21 19:47:45 INFO TaskSetManager: Starting task 84.0 in stage 9.0 (TID 933, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 69.0 in stage 9.0 (TID 918) in 627 ms on localhost (68/200)
15/08/21 19:47:45 INFO Executor: Running task 84.0 in stage 9.0 (TID 933)
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15120
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO TaskSetManager: Starting task 85.0 in stage 9.0 (TID 934, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 70.0 in stage 9.0 (TID 919) in 608 ms on localhost (69/200)
15/08/21 19:47:45 INFO Executor: Running task 85.0 in stage 9.0 (TID 934)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO Executor: Finished task 71.0 in stage 9.0 (TID 920). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14440
15/08/21 19:47:45 INFO TaskSetManager: Starting task 86.0 in stage 9.0 (TID 935, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 67.0 in stage 9.0 (TID 916) in 664 ms on localhost (70/200)
15/08/21 19:47:45 INFO Executor: Running task 86.0 in stage 9.0 (TID 935)
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15203
15/08/21 19:47:45 INFO TaskSetManager: Finished task 65.0 in stage 9.0 (TID 914) in 672 ms on localhost (71/200)
15/08/21 19:47:45 INFO Executor: Finished task 72.0 in stage 9.0 (TID 921). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO Executor: Finished task 73.0 in stage 9.0 (TID 922). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO TaskSetManager: Starting task 87.0 in stage 9.0 (TID 936, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 71.0 in stage 9.0 (TID 920) in 614 ms on localhost (72/200)
15/08/21 19:47:45 INFO Executor: Running task 87.0 in stage 9.0 (TID 936)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO Executor: Finished task 74.0 in stage 9.0 (TID 923). 2341 bytes result sent to driver
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO TaskSetManager: Starting task 88.0 in stage 9.0 (TID 937, localhost, ANY, 1970 bytes)
15/08/21 19:47:45 INFO Executor: Running task 88.0 in stage 9.0 (TID 937)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 14934
15/08/21 19:47:45 INFO TaskSetManager: Starting task 89.0 in stage 9.0 (TID 938, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO TaskSetManager: Finished task 72.0 in stage 9.0 (TID 921) in 608 ms on localhost (73/200)
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO Executor: Running task 89.0 in stage 9.0 (TID 938)
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14985
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15500
15/08/21 19:47:45 INFO TaskSetManager: Finished task 73.0 in stage 9.0 (TID 922) in 536 ms on localhost (74/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO TaskSetManager: Starting task 90.0 in stage 9.0 (TID 939, localhost, ANY, 1970 bytes)
15/08/21 19:47:45 INFO Executor: Running task 90.0 in stage 9.0 (TID 939)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO Executor: Finished task 75.0 in stage 9.0 (TID 924). 2341 bytes result sent to driver
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO TaskSetManager: Finished task 74.0 in stage 9.0 (TID 923) in 500 ms on localhost (75/200)
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14825
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15103
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO TaskSetManager: Starting task 91.0 in stage 9.0 (TID 940, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO Executor: Running task 91.0 in stage 9.0 (TID 940)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 75.0 in stage 9.0 (TID 924) in 371 ms on localhost (76/200)
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14509
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14876
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15545
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15404
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15137
15/08/21 19:47:45 INFO Executor: Finished task 76.0 in stage 9.0 (TID 925). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 92.0 in stage 9.0 (TID 941, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO Executor: Running task 92.0 in stage 9.0 (TID 941)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 76.0 in stage 9.0 (TID 925) in 366 ms on localhost (77/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 19:47:45 INFO Executor: Finished task 77.0 in stage 9.0 (TID 926). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 93.0 in stage 9.0 (TID 942, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO Executor: Running task 93.0 in stage 9.0 (TID 942)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 77.0 in stage 9.0 (TID 926) in 451 ms on localhost (78/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 19:47:45 INFO Executor: Finished task 78.0 in stage 9.0 (TID 927). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 94.0 in stage 9.0 (TID 943, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO Executor: Running task 94.0 in stage 9.0 (TID 943)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 78.0 in stage 9.0 (TID 927) in 495 ms on localhost (79/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO Executor: Finished task 80.0 in stage 9.0 (TID 929). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO Executor: Finished task 81.0 in stage 9.0 (TID 930). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO Executor: Finished task 79.0 in stage 9.0 (TID 928). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 95.0 in stage 9.0 (TID 944, localhost, ANY, 1969 bytes)
15/08/21 19:47:45 INFO Executor: Running task 95.0 in stage 9.0 (TID 944)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:45 INFO TaskSetManager: Starting task 96.0 in stage 9.0 (TID 945, localhost, ANY, 1970 bytes)
15/08/21 19:47:45 INFO Executor: Running task 96.0 in stage 9.0 (TID 945)
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO TaskSetManager: Starting task 97.0 in stage 9.0 (TID 946, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 81.0 in stage 9.0 (TID 930) in 506 ms on localhost (80/200)
15/08/21 19:47:45 INFO Executor: Running task 97.0 in stage 9.0 (TID 946)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 80.0 in stage 9.0 (TID 929) in 514 ms on localhost (81/200)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 79.0 in stage 9.0 (TID 928) in 538 ms on localhost (82/200)
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO Executor: Finished task 82.0 in stage 9.0 (TID 931). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO Executor: Finished task 83.0 in stage 9.0 (TID 932). 2341 bytes result sent to driver
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15238
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15272
15/08/21 19:47:45 INFO TaskSetManager: Starting task 98.0 in stage 9.0 (TID 947, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO Executor: Running task 98.0 in stage 9.0 (TID 947)
15/08/21 19:47:45 INFO TaskSetManager: Starting task 99.0 in stage 9.0 (TID 948, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 82.0 in stage 9.0 (TID 931) in 541 ms on localhost (83/200)
15/08/21 19:47:45 INFO Executor: Running task 99.0 in stage 9.0 (TID 948)
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO TaskSetManager: Finished task 83.0 in stage 9.0 (TID 932) in 524 ms on localhost (84/200)
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 19:47:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14982
15/08/21 19:47:45 INFO Executor: Finished task 86.0 in stage 9.0 (TID 935). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO Executor: Finished task 84.0 in stage 9.0 (TID 933). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO Executor: Finished task 85.0 in stage 9.0 (TID 934). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 100.0 in stage 9.0 (TID 949, localhost, ANY, 1972 bytes)
15/08/21 19:47:45 INFO Executor: Running task 100.0 in stage 9.0 (TID 949)
15/08/21 19:47:45 INFO Executor: Finished task 87.0 in stage 9.0 (TID 936). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:45 INFO Executor: Finished task 88.0 in stage 9.0 (TID 937). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:45 INFO Executor: Finished task 89.0 in stage 9.0 (TID 938). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:45 INFO TaskSetManager: Starting task 101.0 in stage 9.0 (TID 950, localhost, ANY, 1971 bytes)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 86.0 in stage 9.0 (TID 935) in 559 ms on localhost (85/200)
15/08/21 19:47:45 INFO Executor: Running task 101.0 in stage 9.0 (TID 950)
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:45 INFO Executor: Finished task 91.0 in stage 9.0 (TID 940). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO TaskSetManager: Starting task 102.0 in stage 9.0 (TID 951, localhost, ANY, 1968 bytes)
15/08/21 19:47:45 INFO Executor: Finished task 90.0 in stage 9.0 (TID 939). 2341 bytes result sent to driver
15/08/21 19:47:45 INFO Executor: Running task 102.0 in stage 9.0 (TID 951)
15/08/21 19:47:45 INFO TaskSetManager: Finished task 84.0 in stage 9.0 (TID 933) in 570 ms on localhost (86/200)
15/08/21 19:47:46 INFO TaskSetManager: Starting task 103.0 in stage 9.0 (TID 952, localhost, ANY, 1970 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 85.0 in stage 9.0 (TID 934) in 574 ms on localhost (87/200)
15/08/21 19:47:46 INFO Executor: Running task 103.0 in stage 9.0 (TID 952)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 87.0 in stage 9.0 (TID 936) in 546 ms on localhost (88/200)
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15189
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15096
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 104.0 in stage 9.0 (TID 953, localhost, ANY, 1972 bytes)
15/08/21 19:47:46 INFO Executor: Running task 104.0 in stage 9.0 (TID 953)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 105.0 in stage 9.0 (TID 954, localhost, ANY, 1970 bytes)
15/08/21 19:47:46 INFO Executor: Running task 105.0 in stage 9.0 (TID 954)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 88.0 in stage 9.0 (TID 937) in 546 ms on localhost (89/200)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 89.0 in stage 9.0 (TID 938) in 524 ms on localhost (90/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 106.0 in stage 9.0 (TID 955, localhost, ANY, 1972 bytes)
15/08/21 19:47:46 INFO Executor: Running task 106.0 in stage 9.0 (TID 955)
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO TaskSetManager: Starting task 107.0 in stage 9.0 (TID 956, localhost, ANY, 1972 bytes)
15/08/21 19:47:46 INFO Executor: Running task 107.0 in stage 9.0 (TID 956)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 91.0 in stage 9.0 (TID 940) in 516 ms on localhost (91/200)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 90.0 in stage 9.0 (TID 939) in 530 ms on localhost (92/200)
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO Executor: Finished task 92.0 in stage 9.0 (TID 941). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15018
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14784
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15463
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14819
15/08/21 19:47:46 INFO TaskSetManager: Starting task 108.0 in stage 9.0 (TID 957, localhost, ANY, 1968 bytes)
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO Executor: Running task 108.0 in stage 9.0 (TID 957)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 92.0 in stage 9.0 (TID 941) in 453 ms on localhost (93/200)
15/08/21 19:47:46 INFO Executor: Finished task 93.0 in stage 9.0 (TID 942). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15267
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO TaskSetManager: Starting task 109.0 in stage 9.0 (TID 958, localhost, ANY, 1970 bytes)
15/08/21 19:47:46 INFO Executor: Running task 109.0 in stage 9.0 (TID 958)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 93.0 in stage 9.0 (TID 942) in 337 ms on localhost (94/200)
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15183
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14816
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO Executor: Finished task 94.0 in stage 9.0 (TID 943). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15566
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15509
15/08/21 19:47:46 INFO TaskSetManager: Starting task 110.0 in stage 9.0 (TID 959, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 110.0 in stage 9.0 (TID 959)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 94.0 in stage 9.0 (TID 943) in 363 ms on localhost (95/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15313
15/08/21 19:47:46 INFO Executor: Finished task 96.0 in stage 9.0 (TID 945). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO Executor: Finished task 95.0 in stage 9.0 (TID 944). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 111.0 in stage 9.0 (TID 960, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 111.0 in stage 9.0 (TID 960)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 112.0 in stage 9.0 (TID 961, localhost, ANY, 1972 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 96.0 in stage 9.0 (TID 945) in 429 ms on localhost (96/200)
15/08/21 19:47:46 INFO Executor: Running task 112.0 in stage 9.0 (TID 961)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 95.0 in stage 9.0 (TID 944) in 440 ms on localhost (97/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15274
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14883
15/08/21 19:47:46 INFO Executor: Finished task 97.0 in stage 9.0 (TID 946). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 113.0 in stage 9.0 (TID 962, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 113.0 in stage 9.0 (TID 962)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 97.0 in stage 9.0 (TID 946) in 503 ms on localhost (98/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO Executor: Finished task 99.0 in stage 9.0 (TID 948). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO Executor: Finished task 98.0 in stage 9.0 (TID 947). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 114.0 in stage 9.0 (TID 963, localhost, ANY, 1970 bytes)
15/08/21 19:47:46 INFO Executor: Running task 114.0 in stage 9.0 (TID 963)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 115.0 in stage 9.0 (TID 964, localhost, ANY, 1970 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 99.0 in stage 9.0 (TID 948) in 490 ms on localhost (99/200)
15/08/21 19:47:46 INFO Executor: Running task 115.0 in stage 9.0 (TID 964)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 98.0 in stage 9.0 (TID 947) in 506 ms on localhost (100/200)
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15147
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14938
15/08/21 19:47:46 INFO Executor: Finished task 102.0 in stage 9.0 (TID 951). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO Executor: Finished task 101.0 in stage 9.0 (TID 950). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 116.0 in stage 9.0 (TID 965, localhost, ANY, 1972 bytes)
15/08/21 19:47:46 INFO Executor: Running task 116.0 in stage 9.0 (TID 965)
15/08/21 19:47:46 INFO Executor: Finished task 103.0 in stage 9.0 (TID 952). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO Executor: Finished task 100.0 in stage 9.0 (TID 949). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 117.0 in stage 9.0 (TID 966, localhost, ANY, 1969 bytes)
15/08/21 19:47:46 INFO Executor: Running task 117.0 in stage 9.0 (TID 966)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 102.0 in stage 9.0 (TID 951) in 527 ms on localhost (101/200)
15/08/21 19:47:46 INFO TaskSetManager: Starting task 118.0 in stage 9.0 (TID 967, localhost, ANY, 1972 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 101.0 in stage 9.0 (TID 950) in 563 ms on localhost (102/200)
15/08/21 19:47:46 INFO Executor: Running task 118.0 in stage 9.0 (TID 967)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 119.0 in stage 9.0 (TID 968, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 103.0 in stage 9.0 (TID 952) in 541 ms on localhost (103/200)
15/08/21 19:47:46 INFO Executor: Running task 119.0 in stage 9.0 (TID 968)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 100.0 in stage 9.0 (TID 949) in 579 ms on localhost (104/200)
15/08/21 19:47:46 INFO Executor: Finished task 105.0 in stage 9.0 (TID 954). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14693
15/08/21 19:47:46 INFO TaskSetManager: Starting task 120.0 in stage 9.0 (TID 969, localhost, ANY, 1969 bytes)
15/08/21 19:47:46 INFO Executor: Running task 120.0 in stage 9.0 (TID 969)
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO Executor: Finished task 107.0 in stage 9.0 (TID 956). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Finished task 105.0 in stage 9.0 (TID 954) in 552 ms on localhost (105/200)
15/08/21 19:47:46 INFO Executor: Finished task 106.0 in stage 9.0 (TID 955). 2341 bytes result sent to driver
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14564
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15625
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 121.0 in stage 9.0 (TID 970, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 121.0 in stage 9.0 (TID 970)
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15142
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO Executor: Finished task 104.0 in stage 9.0 (TID 953). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 122.0 in stage 9.0 (TID 971, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 107.0 in stage 9.0 (TID 956) in 577 ms on localhost (106/200)
15/08/21 19:47:46 INFO Executor: Running task 122.0 in stage 9.0 (TID 971)
15/08/21 19:47:46 INFO TaskSetManager: Starting task 123.0 in stage 9.0 (TID 972, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 104.0 in stage 9.0 (TID 953) in 613 ms on localhost (107/200)
15/08/21 19:47:46 INFO Executor: Running task 123.0 in stage 9.0 (TID 972)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 106.0 in stage 9.0 (TID 955) in 599 ms on localhost (108/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15795
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15117
15/08/21 19:47:46 INFO Executor: Finished task 109.0 in stage 9.0 (TID 958). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO Executor: Finished task 108.0 in stage 9.0 (TID 957). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 124.0 in stage 9.0 (TID 973, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 124.0 in stage 9.0 (TID 973)
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14864
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO TaskSetManager: Starting task 125.0 in stage 9.0 (TID 974, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14751
15/08/21 19:47:46 INFO TaskSetManager: Finished task 109.0 in stage 9.0 (TID 958) in 605 ms on localhost (109/200)
15/08/21 19:47:46 INFO Executor: Running task 125.0 in stage 9.0 (TID 974)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 108.0 in stage 9.0 (TID 957) in 628 ms on localhost (110/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO Executor: Finished task 110.0 in stage 9.0 (TID 959). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 126.0 in stage 9.0 (TID 975, localhost, ANY, 1969 bytes)
15/08/21 19:47:46 INFO Executor: Running task 126.0 in stage 9.0 (TID 975)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 110.0 in stage 9.0 (TID 959) in 543 ms on localhost (111/200)
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15571
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15129
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15726
15/08/21 19:47:46 INFO Executor: Finished task 111.0 in stage 9.0 (TID 960). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO Executor: Finished task 112.0 in stage 9.0 (TID 961). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 127.0 in stage 9.0 (TID 976, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 127.0 in stage 9.0 (TID 976)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO TaskSetManager: Starting task 128.0 in stage 9.0 (TID 977, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 128.0 in stage 9.0 (TID 977)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 112.0 in stage 9.0 (TID 961) in 535 ms on localhost (112/200)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 111.0 in stage 9.0 (TID 960) in 550 ms on localhost (113/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14687
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO Executor: Finished task 113.0 in stage 9.0 (TID 962). 2341 bytes result sent to driver
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14793
15/08/21 19:47:46 INFO TaskSetManager: Starting task 129.0 in stage 9.0 (TID 978, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 129.0 in stage 9.0 (TID 978)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 113.0 in stage 9.0 (TID 962) in 518 ms on localhost (114/200)
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO Executor: Finished task 114.0 in stage 9.0 (TID 963). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO Executor: Finished task 115.0 in stage 9.0 (TID 964). 2341 bytes result sent to driver
15/08/21 19:47:46 INFO TaskSetManager: Starting task 130.0 in stage 9.0 (TID 979, localhost, ANY, 1971 bytes)
15/08/21 19:47:46 INFO Executor: Running task 130.0 in stage 9.0 (TID 979)
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO TaskSetManager: Starting task 131.0 in stage 9.0 (TID 980, localhost, ANY, 1970 bytes)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 114.0 in stage 9.0 (TID 963) in 533 ms on localhost (115/200)
15/08/21 19:47:46 INFO Executor: Running task 131.0 in stage 9.0 (TID 980)
15/08/21 19:47:46 INFO TaskSetManager: Finished task 115.0 in stage 9.0 (TID 964) in 525 ms on localhost (116/200)
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 19:47:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 19:47:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15361
15/08/21 19:47:47 INFO Executor: Finished task 116.0 in stage 9.0 (TID 965). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 132.0 in stage 9.0 (TID 981, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO Executor: Running task 132.0 in stage 9.0 (TID 981)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 116.0 in stage 9.0 (TID 965) in 544 ms on localhost (117/200)
15/08/21 19:47:47 INFO Executor: Finished task 118.0 in stage 9.0 (TID 967). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:47 INFO TaskSetManager: Starting task 133.0 in stage 9.0 (TID 982, localhost, ANY, 1969 bytes)
15/08/21 19:47:47 INFO Executor: Running task 133.0 in stage 9.0 (TID 982)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 118.0 in stage 9.0 (TID 967) in 515 ms on localhost (118/200)
15/08/21 19:47:47 INFO Executor: Finished task 117.0 in stage 9.0 (TID 966). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Starting task 134.0 in stage 9.0 (TID 983, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO Executor: Running task 134.0 in stage 9.0 (TID 983)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 117.0 in stage 9.0 (TID 966) in 541 ms on localhost (119/200)
15/08/21 19:47:47 INFO Executor: Finished task 119.0 in stage 9.0 (TID 968). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Starting task 135.0 in stage 9.0 (TID 984, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO Executor: Running task 135.0 in stage 9.0 (TID 984)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 119.0 in stage 9.0 (TID 968) in 531 ms on localhost (120/200)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14741
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15344
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO Executor: Finished task 120.0 in stage 9.0 (TID 969). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15049
15/08/21 19:47:47 INFO Executor: Finished task 121.0 in stage 9.0 (TID 970). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 136.0 in stage 9.0 (TID 985, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO Executor: Running task 136.0 in stage 9.0 (TID 985)
15/08/21 19:47:47 INFO TaskSetManager: Starting task 137.0 in stage 9.0 (TID 986, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 120.0 in stage 9.0 (TID 969) in 576 ms on localhost (121/200)
15/08/21 19:47:47 INFO Executor: Running task 137.0 in stage 9.0 (TID 986)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO TaskSetManager: Finished task 121.0 in stage 9.0 (TID 970) in 559 ms on localhost (122/200)
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15038
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO Executor: Finished task 122.0 in stage 9.0 (TID 971). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 138.0 in stage 9.0 (TID 987, localhost, ANY, 1969 bytes)
15/08/21 19:47:47 INFO Executor: Running task 138.0 in stage 9.0 (TID 987)
15/08/21 19:47:47 INFO Executor: Finished task 123.0 in stage 9.0 (TID 972). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Finished task 122.0 in stage 9.0 (TID 971) in 567 ms on localhost (123/200)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO TaskSetManager: Starting task 139.0 in stage 9.0 (TID 988, localhost, ANY, 1968 bytes)
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO Executor: Running task 139.0 in stage 9.0 (TID 988)
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO TaskSetManager: Finished task 123.0 in stage 9.0 (TID 972) in 559 ms on localhost (124/200)
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14809
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14951
15/08/21 19:47:47 INFO Executor: Finished task 125.0 in stage 9.0 (TID 974). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO Executor: Finished task 124.0 in stage 9.0 (TID 973). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 140.0 in stage 9.0 (TID 989, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO Executor: Running task 140.0 in stage 9.0 (TID 989)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO TaskSetManager: Starting task 141.0 in stage 9.0 (TID 990, localhost, ANY, 1969 bytes)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 125.0 in stage 9.0 (TID 974) in 539 ms on localhost (125/200)
15/08/21 19:47:47 INFO Executor: Running task 141.0 in stage 9.0 (TID 990)
15/08/21 19:47:47 INFO Executor: Finished task 126.0 in stage 9.0 (TID 975). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Finished task 124.0 in stage 9.0 (TID 973) in 549 ms on localhost (126/200)
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14814
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Starting task 142.0 in stage 9.0 (TID 991, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO Executor: Running task 142.0 in stage 9.0 (TID 991)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 126.0 in stage 9.0 (TID 975) in 520 ms on localhost (127/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15658
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO Executor: Finished task 127.0 in stage 9.0 (TID 976). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14991
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO TaskSetManager: Starting task 143.0 in stage 9.0 (TID 992, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO Executor: Running task 143.0 in stage 9.0 (TID 992)
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14613
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO TaskSetManager: Finished task 127.0 in stage 9.0 (TID 976) in 484 ms on localhost (128/200)
15/08/21 19:47:47 INFO Executor: Finished task 128.0 in stage 9.0 (TID 977). 2341 bytes result sent to driver
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15501
15/08/21 19:47:47 INFO TaskSetManager: Starting task 144.0 in stage 9.0 (TID 993, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO Executor: Running task 144.0 in stage 9.0 (TID 993)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 128.0 in stage 9.0 (TID 977) in 494 ms on localhost (129/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14590
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO Executor: Finished task 129.0 in stage 9.0 (TID 978). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14836
15/08/21 19:47:47 INFO TaskSetManager: Starting task 145.0 in stage 9.0 (TID 994, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO Executor: Running task 145.0 in stage 9.0 (TID 994)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 129.0 in stage 9.0 (TID 978) in 501 ms on localhost (130/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO Executor: Finished task 130.0 in stage 9.0 (TID 979). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO Executor: Finished task 131.0 in stage 9.0 (TID 980). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 146.0 in stage 9.0 (TID 995, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO Executor: Running task 146.0 in stage 9.0 (TID 995)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Starting task 147.0 in stage 9.0 (TID 996, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 19:47:47 INFO TaskSetManager: Finished task 130.0 in stage 9.0 (TID 979) in 510 ms on localhost (131/200)
15/08/21 19:47:47 INFO Executor: Running task 147.0 in stage 9.0 (TID 996)
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14508
15/08/21 19:47:47 INFO TaskSetManager: Finished task 131.0 in stage 9.0 (TID 980) in 503 ms on localhost (132/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15141
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14923
15/08/21 19:47:47 INFO Executor: Finished task 132.0 in stage 9.0 (TID 981). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 148.0 in stage 9.0 (TID 997, localhost, ANY, 1968 bytes)
15/08/21 19:47:47 INFO Executor: Running task 148.0 in stage 9.0 (TID 997)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 132.0 in stage 9.0 (TID 981) in 500 ms on localhost (133/200)
15/08/21 19:47:47 INFO Executor: Finished task 133.0 in stage 9.0 (TID 982). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO Executor: Finished task 134.0 in stage 9.0 (TID 983). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Starting task 149.0 in stage 9.0 (TID 998, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO Executor: Running task 149.0 in stage 9.0 (TID 998)
15/08/21 19:47:47 INFO TaskSetManager: Starting task 150.0 in stage 9.0 (TID 999, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Finished task 133.0 in stage 9.0 (TID 982) in 507 ms on localhost (134/200)
15/08/21 19:47:47 INFO Executor: Running task 150.0 in stage 9.0 (TID 999)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 134.0 in stage 9.0 (TID 983) in 498 ms on localhost (135/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO Executor: Finished task 135.0 in stage 9.0 (TID 984). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO TaskSetManager: Starting task 151.0 in stage 9.0 (TID 1000, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15489
15/08/21 19:47:47 INFO Executor: Running task 151.0 in stage 9.0 (TID 1000)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 135.0 in stage 9.0 (TID 984) in 514 ms on localhost (136/200)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14755
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15549
15/08/21 19:47:47 INFO Executor: Finished task 136.0 in stage 9.0 (TID 985). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 152.0 in stage 9.0 (TID 1001, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO Executor: Running task 152.0 in stage 9.0 (TID 1001)
15/08/21 19:47:47 INFO Executor: Finished task 137.0 in stage 9.0 (TID 986). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 153.0 in stage 9.0 (TID 1002, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 136.0 in stage 9.0 (TID 985) in 515 ms on localhost (137/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:47 INFO Executor: Running task 153.0 in stage 9.0 (TID 1002)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 137.0 in stage 9.0 (TID 986) in 509 ms on localhost (138/200)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO Executor: Finished task 138.0 in stage 9.0 (TID 987). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14952
15/08/21 19:47:47 INFO TaskSetManager: Starting task 154.0 in stage 9.0 (TID 1003, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO Executor: Running task 154.0 in stage 9.0 (TID 1003)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 138.0 in stage 9.0 (TID 987) in 510 ms on localhost (139/200)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14944
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO Executor: Finished task 139.0 in stage 9.0 (TID 988). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15359
15/08/21 19:47:47 INFO TaskSetManager: Starting task 155.0 in stage 9.0 (TID 1004, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO Executor: Running task 155.0 in stage 9.0 (TID 1004)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO TaskSetManager: Finished task 139.0 in stage 9.0 (TID 988) in 547 ms on localhost (140/200)
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:47 INFO Executor: Finished task 140.0 in stage 9.0 (TID 989). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14896
15/08/21 19:47:47 INFO Executor: Finished task 142.0 in stage 9.0 (TID 991). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 156.0 in stage 9.0 (TID 1005, localhost, ANY, 1969 bytes)
15/08/21 19:47:47 INFO Executor: Running task 156.0 in stage 9.0 (TID 1005)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO TaskSetManager: Starting task 157.0 in stage 9.0 (TID 1006, localhost, ANY, 1973 bytes)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Finished task 140.0 in stage 9.0 (TID 989) in 568 ms on localhost (141/200)
15/08/21 19:47:47 INFO Executor: Running task 157.0 in stage 9.0 (TID 1006)
15/08/21 19:47:47 INFO Executor: Finished task 141.0 in stage 9.0 (TID 990). 2341 bytes result sent to driver
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15636
15/08/21 19:47:47 INFO TaskSetManager: Starting task 158.0 in stage 9.0 (TID 1007, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 142.0 in stage 9.0 (TID 991) in 564 ms on localhost (142/200)
15/08/21 19:47:47 INFO Executor: Running task 158.0 in stage 9.0 (TID 1007)
15/08/21 19:47:47 INFO Executor: Finished task 143.0 in stage 9.0 (TID 992). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 159.0 in stage 9.0 (TID 1008, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Finished task 141.0 in stage 9.0 (TID 990) in 584 ms on localhost (143/200)
15/08/21 19:47:47 INFO Executor: Running task 159.0 in stage 9.0 (TID 1008)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 143.0 in stage 9.0 (TID 992) in 518 ms on localhost (144/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO Executor: Finished task 144.0 in stage 9.0 (TID 993). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15164
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14842
15/08/21 19:47:47 INFO TaskSetManager: Starting task 160.0 in stage 9.0 (TID 1009, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO Executor: Running task 160.0 in stage 9.0 (TID 1009)
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO TaskSetManager: Finished task 144.0 in stage 9.0 (TID 993) in 538 ms on localhost (145/200)
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14884
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15098
15/08/21 19:47:47 INFO Executor: Finished task 145.0 in stage 9.0 (TID 994). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO TaskSetManager: Starting task 161.0 in stage 9.0 (TID 1010, localhost, ANY, 1970 bytes)
15/08/21 19:47:47 INFO Executor: Running task 161.0 in stage 9.0 (TID 1010)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 145.0 in stage 9.0 (TID 994) in 527 ms on localhost (146/200)
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14560
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO Executor: Finished task 146.0 in stage 9.0 (TID 995). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO Executor: Finished task 147.0 in stage 9.0 (TID 996). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 162.0 in stage 9.0 (TID 1011, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO Executor: Running task 162.0 in stage 9.0 (TID 1011)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO TaskSetManager: Starting task 163.0 in stage 9.0 (TID 1012, localhost, ANY, 1972 bytes)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 146.0 in stage 9.0 (TID 995) in 527 ms on localhost (147/200)
15/08/21 19:47:47 INFO Executor: Running task 163.0 in stage 9.0 (TID 1012)
15/08/21 19:47:47 INFO TaskSetManager: Finished task 147.0 in stage 9.0 (TID 996) in 514 ms on localhost (148/200)
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15347
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15013
15/08/21 19:47:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 19:47:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:47 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14826
15/08/21 19:47:47 INFO Executor: Finished task 148.0 in stage 9.0 (TID 997). 2341 bytes result sent to driver
15/08/21 19:47:47 INFO TaskSetManager: Starting task 164.0 in stage 9.0 (TID 1013, localhost, ANY, 1971 bytes)
15/08/21 19:47:47 INFO Executor: Running task 164.0 in stage 9.0 (TID 1013)
15/08/21 19:47:47 INFO Executor: Finished task 149.0 in stage 9.0 (TID 998). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 165.0 in stage 9.0 (TID 1014, localhost, ANY, 1969 bytes)
15/08/21 19:47:48 INFO Executor: Running task 165.0 in stage 9.0 (TID 1014)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 148.0 in stage 9.0 (TID 997) in 525 ms on localhost (149/200)
15/08/21 19:47:48 INFO Executor: Finished task 150.0 in stage 9.0 (TID 999). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO TaskSetManager: Starting task 166.0 in stage 9.0 (TID 1015, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO Executor: Running task 166.0 in stage 9.0 (TID 1015)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 150.0 in stage 9.0 (TID 999) in 502 ms on localhost (150/200)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 149.0 in stage 9.0 (TID 998) in 516 ms on localhost (151/200)
15/08/21 19:47:48 INFO Executor: Finished task 151.0 in stage 9.0 (TID 1000). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO TaskSetManager: Starting task 167.0 in stage 9.0 (TID 1016, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO Executor: Running task 167.0 in stage 9.0 (TID 1016)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 151.0 in stage 9.0 (TID 1000) in 502 ms on localhost (152/200)
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14843
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15470
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO Executor: Finished task 152.0 in stage 9.0 (TID 1001). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 168.0 in stage 9.0 (TID 1017, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 168.0 in stage 9.0 (TID 1017)
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO TaskSetManager: Finished task 152.0 in stage 9.0 (TID 1001) in 486 ms on localhost (153/200)
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO Executor: Finished task 153.0 in stage 9.0 (TID 1002). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15296
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO TaskSetManager: Starting task 169.0 in stage 9.0 (TID 1018, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 169.0 in stage 9.0 (TID 1018)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 153.0 in stage 9.0 (TID 1002) in 499 ms on localhost (154/200)
15/08/21 19:47:48 INFO Executor: Finished task 154.0 in stage 9.0 (TID 1003). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14810
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO TaskSetManager: Starting task 170.0 in stage 9.0 (TID 1019, localhost, ANY, 1970 bytes)
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO Executor: Running task 170.0 in stage 9.0 (TID 1019)
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO TaskSetManager: Finished task 154.0 in stage 9.0 (TID 1003) in 488 ms on localhost (155/200)
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO Executor: Finished task 155.0 in stage 9.0 (TID 1004). 2341 bytes result sent to driver
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15062
15/08/21 19:47:48 INFO TaskSetManager: Starting task 171.0 in stage 9.0 (TID 1020, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO Executor: Running task 171.0 in stage 9.0 (TID 1020)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 155.0 in stage 9.0 (TID 1004) in 512 ms on localhost (156/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO Executor: Finished task 157.0 in stage 9.0 (TID 1006). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO Executor: Finished task 156.0 in stage 9.0 (TID 1005). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14848
15/08/21 19:47:48 INFO TaskSetManager: Starting task 172.0 in stage 9.0 (TID 1021, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO Executor: Running task 172.0 in stage 9.0 (TID 1021)
15/08/21 19:47:48 INFO Executor: Finished task 159.0 in stage 9.0 (TID 1008). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 173.0 in stage 9.0 (TID 1022, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO TaskSetManager: Finished task 157.0 in stage 9.0 (TID 1006) in 535 ms on localhost (157/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO Executor: Running task 173.0 in stage 9.0 (TID 1022)
15/08/21 19:47:48 INFO Executor: Finished task 158.0 in stage 9.0 (TID 1007). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 174.0 in stage 9.0 (TID 1023, localhost, ANY, 1970 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 159.0 in stage 9.0 (TID 1008) in 509 ms on localhost (158/200)
15/08/21 19:47:48 INFO Executor: Running task 174.0 in stage 9.0 (TID 1023)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 156.0 in stage 9.0 (TID 1005) in 570 ms on localhost (159/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO TaskSetManager: Starting task 175.0 in stage 9.0 (TID 1024, localhost, ANY, 1970 bytes)
15/08/21 19:47:48 INFO Executor: Running task 175.0 in stage 9.0 (TID 1024)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 158.0 in stage 9.0 (TID 1007) in 543 ms on localhost (160/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO Executor: Finished task 160.0 in stage 9.0 (TID 1009). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14791
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO TaskSetManager: Starting task 176.0 in stage 9.0 (TID 1025, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO Executor: Running task 176.0 in stage 9.0 (TID 1025)
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14736
15/08/21 19:47:48 INFO TaskSetManager: Finished task 160.0 in stage 9.0 (TID 1009) in 523 ms on localhost (161/200)
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15474
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15386
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 19:47:48 INFO Executor: Finished task 161.0 in stage 9.0 (TID 1010). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO Executor: Finished task 163.0 in stage 9.0 (TID 1012). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 177.0 in stage 9.0 (TID 1026, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 177.0 in stage 9.0 (TID 1026)
15/08/21 19:47:48 INFO Executor: Finished task 162.0 in stage 9.0 (TID 1011). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 178.0 in stage 9.0 (TID 1027, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 161.0 in stage 9.0 (TID 1010) in 537 ms on localhost (162/200)
15/08/21 19:47:48 INFO Executor: Running task 178.0 in stage 9.0 (TID 1027)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO TaskSetManager: Starting task 179.0 in stage 9.0 (TID 1028, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 179.0 in stage 9.0 (TID 1028)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 163.0 in stage 9.0 (TID 1012) in 509 ms on localhost (163/200)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 162.0 in stage 9.0 (TID 1011) in 528 ms on localhost (164/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO Executor: Finished task 165.0 in stage 9.0 (TID 1014). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15368
15/08/21 19:47:48 INFO Executor: Finished task 164.0 in stage 9.0 (TID 1013). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 180.0 in stage 9.0 (TID 1029, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO Executor: Running task 180.0 in stage 9.0 (TID 1029)
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14750
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO TaskSetManager: Starting task 181.0 in stage 9.0 (TID 1030, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 165.0 in stage 9.0 (TID 1014) in 482 ms on localhost (165/200)
15/08/21 19:47:48 INFO Executor: Running task 181.0 in stage 9.0 (TID 1030)
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14879
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO TaskSetManager: Finished task 164.0 in stage 9.0 (TID 1013) in 506 ms on localhost (166/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO Executor: Finished task 166.0 in stage 9.0 (TID 1015). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO Executor: Finished task 168.0 in stage 9.0 (TID 1017). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO Executor: Finished task 167.0 in stage 9.0 (TID 1016). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 182.0 in stage 9.0 (TID 1031, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 182.0 in stage 9.0 (TID 1031)
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO TaskSetManager: Starting task 183.0 in stage 9.0 (TID 1032, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 166.0 in stage 9.0 (TID 1015) in 540 ms on localhost (167/200)
15/08/21 19:47:48 INFO Executor: Running task 183.0 in stage 9.0 (TID 1032)
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15340
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15101
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO TaskSetManager: Starting task 184.0 in stage 9.0 (TID 1033, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 168.0 in stage 9.0 (TID 1017) in 496 ms on localhost (168/200)
15/08/21 19:47:48 INFO Executor: Running task 184.0 in stage 9.0 (TID 1033)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 167.0 in stage 9.0 (TID 1016) in 533 ms on localhost (169/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO Executor: Finished task 169.0 in stage 9.0 (TID 1018). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 185.0 in stage 9.0 (TID 1034, localhost, ANY, 1970 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 169.0 in stage 9.0 (TID 1018) in 511 ms on localhost (170/200)
15/08/21 19:47:48 INFO Executor: Running task 185.0 in stage 9.0 (TID 1034)
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15330
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15207
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO Executor: Finished task 170.0 in stage 9.0 (TID 1019). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14569
15/08/21 19:47:48 INFO TaskSetManager: Starting task 186.0 in stage 9.0 (TID 1035, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO Executor: Running task 186.0 in stage 9.0 (TID 1035)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 170.0 in stage 9.0 (TID 1019) in 528 ms on localhost (171/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14771
15/08/21 19:47:48 INFO Executor: Finished task 171.0 in stage 9.0 (TID 1020). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO TaskSetManager: Starting task 187.0 in stage 9.0 (TID 1036, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 187.0 in stage 9.0 (TID 1036)
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO TaskSetManager: Finished task 171.0 in stage 9.0 (TID 1020) in 497 ms on localhost (172/200)
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14795
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14854
15/08/21 19:47:48 INFO Executor: Finished task 172.0 in stage 9.0 (TID 1021). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO Executor: Finished task 173.0 in stage 9.0 (TID 1022). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 188.0 in stage 9.0 (TID 1037, localhost, ANY, 1970 bytes)
15/08/21 19:47:48 INFO Executor: Running task 188.0 in stage 9.0 (TID 1037)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO TaskSetManager: Starting task 189.0 in stage 9.0 (TID 1038, localhost, ANY, 1972 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 172.0 in stage 9.0 (TID 1021) in 526 ms on localhost (173/200)
15/08/21 19:47:48 INFO Executor: Running task 189.0 in stage 9.0 (TID 1038)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 173.0 in stage 9.0 (TID 1022) in 515 ms on localhost (174/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO Executor: Finished task 175.0 in stage 9.0 (TID 1024). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 190.0 in stage 9.0 (TID 1039, localhost, ANY, 1970 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 175.0 in stage 9.0 (TID 1024) in 524 ms on localhost (175/200)
15/08/21 19:47:48 INFO Executor: Running task 190.0 in stage 9.0 (TID 1039)
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO Executor: Finished task 174.0 in stage 9.0 (TID 1023). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO Executor: Finished task 176.0 in stage 9.0 (TID 1025). 2341 bytes result sent to driver
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15524
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14802
15/08/21 19:47:48 INFO TaskSetManager: Starting task 191.0 in stage 9.0 (TID 1040, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 191.0 in stage 9.0 (TID 1040)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO TaskSetManager: Starting task 192.0 in stage 9.0 (TID 1041, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 174.0 in stage 9.0 (TID 1023) in 568 ms on localhost (176/200)
15/08/21 19:47:48 INFO Executor: Running task 192.0 in stage 9.0 (TID 1041)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 176.0 in stage 9.0 (TID 1025) in 537 ms on localhost (177/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15090
15/08/21 19:47:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 19:47:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:48 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15007
15/08/21 19:47:48 INFO Executor: Finished task 177.0 in stage 9.0 (TID 1026). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 193.0 in stage 9.0 (TID 1042, localhost, ANY, 1971 bytes)
15/08/21 19:47:48 INFO Executor: Running task 193.0 in stage 9.0 (TID 1042)
15/08/21 19:47:48 INFO TaskSetManager: Finished task 177.0 in stage 9.0 (TID 1026) in 565 ms on localhost (178/200)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:48 INFO Executor: Finished task 178.0 in stage 9.0 (TID 1027). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO Executor: Finished task 179.0 in stage 9.0 (TID 1028). 2341 bytes result sent to driver
15/08/21 19:47:48 INFO TaskSetManager: Starting task 194.0 in stage 9.0 (TID 1043, localhost, ANY, 1970 bytes)
15/08/21 19:47:48 INFO Executor: Running task 194.0 in stage 9.0 (TID 1043)
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO TaskSetManager: Starting task 195.0 in stage 9.0 (TID 1044, localhost, ANY, 1972 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Finished task 178.0 in stage 9.0 (TID 1027) in 605 ms on localhost (179/200)
15/08/21 19:47:49 INFO Executor: Running task 195.0 in stage 9.0 (TID 1044)
15/08/21 19:47:49 INFO TaskSetManager: Finished task 179.0 in stage 9.0 (TID 1028) in 599 ms on localhost (180/200)
15/08/21 19:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 19:47:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15396
15/08/21 19:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 19:47:49 INFO Executor: Finished task 181.0 in stage 9.0 (TID 1030). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:49 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15150
15/08/21 19:47:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 19:47:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:49 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14780
15/08/21 19:47:49 INFO TaskSetManager: Starting task 196.0 in stage 9.0 (TID 1045, localhost, ANY, 1971 bytes)
15/08/21 19:47:49 INFO Executor: Running task 196.0 in stage 9.0 (TID 1045)
15/08/21 19:47:49 INFO TaskSetManager: Finished task 181.0 in stage 9.0 (TID 1030) in 593 ms on localhost (181/200)
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:49 INFO Executor: Finished task 180.0 in stage 9.0 (TID 1029). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Starting task 197.0 in stage 9.0 (TID 1046, localhost, ANY, 1971 bytes)
15/08/21 19:47:49 INFO Executor: Running task 197.0 in stage 9.0 (TID 1046)
15/08/21 19:47:49 INFO TaskSetManager: Finished task 180.0 in stage 9.0 (TID 1029) in 630 ms on localhost (182/200)
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:49 INFO Executor: Finished task 182.0 in stage 9.0 (TID 1031). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO Executor: Finished task 183.0 in stage 9.0 (TID 1032). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 19:47:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:49 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 19:47:49 INFO TaskSetManager: Starting task 198.0 in stage 9.0 (TID 1047, localhost, ANY, 1970 bytes)
15/08/21 19:47:49 INFO Executor: Running task 198.0 in stage 9.0 (TID 1047)
15/08/21 19:47:49 INFO Executor: Finished task 184.0 in stage 9.0 (TID 1033). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO TaskSetManager: Starting task 199.0 in stage 9.0 (TID 1048, localhost, ANY, 1972 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Finished task 182.0 in stage 9.0 (TID 1031) in 625 ms on localhost (183/200)
15/08/21 19:47:49 INFO Executor: Running task 199.0 in stage 9.0 (TID 1048)
15/08/21 19:47:49 INFO Executor: Finished task 185.0 in stage 9.0 (TID 1034). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 183.0 in stage 9.0 (TID 1032) in 616 ms on localhost (184/200)
15/08/21 19:47:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 19:47:49 INFO TaskSetManager: Finished task 184.0 in stage 9.0 (TID 1033) in 606 ms on localhost (185/200)
15/08/21 19:47:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14829
15/08/21 19:47:49 INFO TaskSetManager: Finished task 185.0 in stage 9.0 (TID 1034) in 580 ms on localhost (186/200)
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:49 INFO Executor: Finished task 186.0 in stage 9.0 (TID 1035). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 186.0 in stage 9.0 (TID 1035) in 593 ms on localhost (187/200)
15/08/21 19:47:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 19:47:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:49 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14479
15/08/21 19:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-3d17e5de-d82b-498a-ab7c-c236bf280e19.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 19:47:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 19:47:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 19:47:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 19:47:49 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14677
15/08/21 19:47:49 INFO Executor: Finished task 187.0 in stage 9.0 (TID 1036). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 187.0 in stage 9.0 (TID 1036) in 585 ms on localhost (188/200)
15/08/21 19:47:49 INFO Executor: Finished task 189.0 in stage 9.0 (TID 1038). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO Executor: Finished task 188.0 in stage 9.0 (TID 1037). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 189.0 in stage 9.0 (TID 1038) in 550 ms on localhost (189/200)
15/08/21 19:47:49 INFO TaskSetManager: Finished task 188.0 in stage 9.0 (TID 1037) in 574 ms on localhost (190/200)
15/08/21 19:47:49 INFO Executor: Finished task 191.0 in stage 9.0 (TID 1040). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 191.0 in stage 9.0 (TID 1040) in 517 ms on localhost (191/200)
15/08/21 19:47:49 INFO Executor: Finished task 190.0 in stage 9.0 (TID 1039). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 190.0 in stage 9.0 (TID 1039) in 541 ms on localhost (192/200)
15/08/21 19:47:49 INFO Executor: Finished task 192.0 in stage 9.0 (TID 1041). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 192.0 in stage 9.0 (TID 1041) in 513 ms on localhost (193/200)
15/08/21 19:47:49 INFO Executor: Finished task 193.0 in stage 9.0 (TID 1042). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 193.0 in stage 9.0 (TID 1042) in 432 ms on localhost (194/200)
15/08/21 19:47:49 INFO Executor: Finished task 194.0 in stage 9.0 (TID 1043). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 194.0 in stage 9.0 (TID 1043) in 404 ms on localhost (195/200)
15/08/21 19:47:49 INFO Executor: Finished task 195.0 in stage 9.0 (TID 1044). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 195.0 in stage 9.0 (TID 1044) in 392 ms on localhost (196/200)
15/08/21 19:47:49 INFO Executor: Finished task 196.0 in stage 9.0 (TID 1045). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 196.0 in stage 9.0 (TID 1045) in 350 ms on localhost (197/200)
15/08/21 19:47:49 INFO Executor: Finished task 197.0 in stage 9.0 (TID 1046). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO Executor: Finished task 199.0 in stage 9.0 (TID 1048). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 197.0 in stage 9.0 (TID 1046) in 333 ms on localhost (198/200)
15/08/21 19:47:49 INFO Executor: Finished task 198.0 in stage 9.0 (TID 1047). 2341 bytes result sent to driver
15/08/21 19:47:49 INFO TaskSetManager: Finished task 199.0 in stage 9.0 (TID 1048) in 270 ms on localhost (199/200)
15/08/21 19:47:49 INFO TaskSetManager: Finished task 198.0 in stage 9.0 (TID 1047) in 290 ms on localhost (200/200)
15/08/21 19:47:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/21 19:47:49 INFO DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:423) finished in 8.121 s
15/08/21 19:47:49 INFO DAGScheduler: looking for newly runnable stages
15/08/21 19:47:49 INFO DAGScheduler: running: Set()
15/08/21 19:47:49 INFO DAGScheduler: waiting: Set(ResultStage 10)
15/08/21 19:47:49 INFO DAGScheduler: failed: Set()
15/08/21 19:47:49 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@14570622
15/08/21 19:47:49 INFO StatsReportListener: task runtime:(count: 200, mean: 658.600000, stdev: 341.085649, max: 1853.000000, min: 199.000000)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO DAGScheduler: Missing parents for ResultStage 10: List()
15/08/21 19:47:49 INFO StatsReportListener: 	199.0 ms	363.0 ms	432.0 ms	509.0 ms	544.0 ms	641.0 ms	917.0 ms	1.8 s	1.9 s
15/08/21 19:47:49 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 19:47:49 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 10841.075000, stdev: 377.448804, max: 11719.000000, min: 9844.000000)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO StatsReportListener: 	9.6 KB	10.0 KB	10.1 KB	10.3 KB	10.6 KB	10.8 KB	11.1 KB	11.3 KB	11.4 KB
15/08/21 19:47:49 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.265000, stdev: 0.738089, max: 6.000000, min: 0.000000)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	6.0 ms
15/08/21 19:47:49 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 19:47:49 INFO StatsReportListener: task result size:(count: 200, mean: 2341.000000, stdev: 0.000000, max: 2341.000000, min: 2341.000000)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO StatsReportListener: 	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB
15/08/21 19:47:49 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 90.787102, stdev: 2.875876, max: 96.804261, min: 77.938144)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO StatsReportListener: 	78 %	86 %	87 %	89 %	91 %	93 %	94 %	95 %	97 %
15/08/21 19:47:49 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.042337, stdev: 0.106285, max: 0.760456, min: 0.000000)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/21 19:47:49 INFO StatsReportListener: other time pct: (count: 200, mean: 9.170561, stdev: 2.859635, max: 21.855670, min: 3.062583)
15/08/21 19:47:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:49 INFO StatsReportListener: 	 3 %	 5 %	 6 %	 7 %	 9 %	11 %	13 %	14 %	22 %
15/08/21 19:47:49 INFO MemoryStore: ensureFreeSpace(84240) called with curMem=1920343, maxMem=22226833244
15/08/21 19:47:49 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 82.3 KB, free 20.7 GB)
15/08/21 19:47:49 INFO MemoryStore: ensureFreeSpace(33432) called with curMem=2004583, maxMem=22226833244
15/08/21 19:47:49 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 32.6 KB, free 20.7 GB)
15/08/21 19:47:49 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:54384 (size: 32.6 KB, free: 20.7 GB)
15/08/21 19:47:49 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874
15/08/21 19:47:49 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/21 19:47:49 INFO TaskSchedulerImpl: Adding task set 10.0 with 200 tasks
15/08/21 19:47:49 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1049, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 1050, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 1051, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 1052, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 1053, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 1054, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 1055, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 1056, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 1057, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 1058, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 1059, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 1060, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 1061, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 1062, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 1063, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 1064, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:49 INFO Executor: Running task 1.0 in stage 10.0 (TID 1050)
15/08/21 19:47:49 INFO Executor: Running task 11.0 in stage 10.0 (TID 1060)
15/08/21 19:47:49 INFO Executor: Running task 14.0 in stage 10.0 (TID 1063)
15/08/21 19:47:49 INFO Executor: Running task 10.0 in stage 10.0 (TID 1059)
15/08/21 19:47:49 INFO Executor: Running task 12.0 in stage 10.0 (TID 1061)
15/08/21 19:47:49 INFO Executor: Running task 9.0 in stage 10.0 (TID 1058)
15/08/21 19:47:49 INFO Executor: Running task 2.0 in stage 10.0 (TID 1051)
15/08/21 19:47:49 INFO Executor: Running task 7.0 in stage 10.0 (TID 1056)
15/08/21 19:47:49 INFO Executor: Running task 5.0 in stage 10.0 (TID 1054)
15/08/21 19:47:49 INFO Executor: Running task 8.0 in stage 10.0 (TID 1057)
15/08/21 19:47:49 INFO Executor: Running task 6.0 in stage 10.0 (TID 1055)
15/08/21 19:47:49 INFO Executor: Running task 0.0 in stage 10.0 (TID 1049)
15/08/21 19:47:49 INFO Executor: Running task 4.0 in stage 10.0 (TID 1053)
15/08/21 19:47:49 INFO Executor: Running task 3.0 in stage 10.0 (TID 1052)
15/08/21 19:47:49 INFO Executor: Running task 15.0 in stage 10.0 (TID 1064)
15/08/21 19:47:49 INFO Executor: Running task 13.0 in stage 10.0 (TID 1062)
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:49 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,616
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,398B for [ps_partkey] INT32: 367 values, 1,475B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,881B for [value] DOUBLE: 367 values, 2,943B raw, 1,837B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,136
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,463,696
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,356
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,482B for [ps_partkey] INT32: 393 values, 1,579B raw, 1,446B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,234B for [ps_partkey] INT32: 321 values, 1,291B raw, 1,198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,021B for [value] DOUBLE: 393 values, 3,151B raw, 1,977B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,728B for [value] DOUBLE: 321 values, 2,575B raw, 1,684B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,687B for [ps_partkey] INT32: 454 values, 1,823B raw, 1,651B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,316B for [value] DOUBLE: 454 values, 3,639B raw, 2,272B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,096
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,796
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,016
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,116
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,396
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,556
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,636
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,016
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,815B for [ps_partkey] INT32: 491 values, 1,971B raw, 1,779B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,517B for [value] DOUBLE: 491 values, 3,935B raw, 2,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,876
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,631B for [ps_partkey] INT32: 437 values, 1,755B raw, 1,595B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,652B for [ps_partkey] INT32: 442 values, 1,775B raw, 1,616B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,227B for [value] DOUBLE: 437 values, 3,503B raw, 2,183B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,222B for [value] DOUBLE: 442 values, 3,543B raw, 2,178B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,867B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,831B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,513B for [value] DOUBLE: 506 values, 4,055B raw, 2,469B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,554B for [ps_partkey] INT32: 414 values, 1,663B raw, 1,518B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,092B for [value] DOUBLE: 414 values, 3,319B raw, 2,048B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,396
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,804B for [ps_partkey] INT32: 487 values, 1,955B raw, 1,768B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,578B for [ps_partkey] INT32: 418 values, 1,679B raw, 1,542B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,580B for [value] DOUBLE: 487 values, 3,903B raw, 2,536B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,940B for [ps_partkey] INT32: 526 values, 2,111B raw, 1,904B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,425B for [ps_partkey] INT32: 380 values, 1,527B raw, 1,389B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,175B for [value] DOUBLE: 418 values, 3,351B raw, 2,131B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,709B for [value] DOUBLE: 526 values, 4,215B raw, 2,665B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,967B for [value] DOUBLE: 380 values, 3,047B raw, 1,923B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,636
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,863B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,827B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,526B for [value] DOUBLE: 506 values, 4,055B raw, 2,482B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,416
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,401B for [ps_partkey] INT32: 368 values, 1,479B raw, 1,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,940B for [value] DOUBLE: 368 values, 2,951B raw, 1,896B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 1,874B for [ps_partkey] INT32: 507 values, 2,035B raw, 1,838B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:49 INFO ColumnChunkPageWriteStore: written 2,793B for [value] DOUBLE: 507 values, 4,063B raw, 2,749B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000007
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000007_0: Committed
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000012
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000012_0: Committed
15/08/21 19:47:50 INFO Executor: Finished task 7.0 in stage 10.0 (TID 1056). 843 bytes result sent to driver
15/08/21 19:47:50 INFO Executor: Finished task 12.0 in stage 10.0 (TID 1061). 843 bytes result sent to driver
15/08/21 19:47:50 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 1065, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 16.0 in stage 10.0 (TID 1065)
15/08/21 19:47:50 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 1066, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 17.0 in stage 10.0 (TID 1066)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 1061) in 570 ms on localhost (1/200)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 1056) in 572 ms on localhost (2/200)
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000010
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000005
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000010_0: Committed
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000005_0: Committed
15/08/21 19:47:50 INFO Executor: Finished task 5.0 in stage 10.0 (TID 1054). 843 bytes result sent to driver
15/08/21 19:47:50 INFO Executor: Finished task 10.0 in stage 10.0 (TID 1059). 843 bytes result sent to driver
15/08/21 19:47:50 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 1067, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 18.0 in stage 10.0 (TID 1067)
15/08/21 19:47:50 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 1068, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 19.0 in stage 10.0 (TID 1068)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 1054) in 580 ms on localhost (3/200)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 1059) in 579 ms on localhost (4/200)
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000001
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000014
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000004
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000001_0: Committed
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000015
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000004_0: Committed
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000014_0: Committed
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000015_0: Committed
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000008
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000008_0: Committed
15/08/21 19:47:50 INFO Executor: Finished task 14.0 in stage 10.0 (TID 1063). 843 bytes result sent to driver
15/08/21 19:47:50 INFO Executor: Finished task 4.0 in stage 10.0 (TID 1053). 843 bytes result sent to driver
15/08/21 19:47:50 INFO Executor: Finished task 15.0 in stage 10.0 (TID 1064). 843 bytes result sent to driver
15/08/21 19:47:50 INFO Executor: Finished task 1.0 in stage 10.0 (TID 1050). 843 bytes result sent to driver
15/08/21 19:47:50 INFO Executor: Finished task 8.0 in stage 10.0 (TID 1057). 843 bytes result sent to driver
15/08/21 19:47:50 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 1069, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 1070, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 20.0 in stage 10.0 (TID 1069)
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000000
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000000_0: Committed
15/08/21 19:47:50 INFO Executor: Running task 21.0 in stage 10.0 (TID 1070)
15/08/21 19:47:50 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1049). 843 bytes result sent to driver
15/08/21 19:47:50 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 1063) in 598 ms on localhost (5/200)
15/08/21 19:47:50 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 1071, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 22.0 in stage 10.0 (TID 1071)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 1053) in 609 ms on localhost (6/200)
15/08/21 19:47:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000009
15/08/21 19:47:50 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000009_0: Committed
15/08/21 19:47:50 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 1064) in 607 ms on localhost (7/200)
15/08/21 19:47:50 INFO Executor: Finished task 9.0 in stage 10.0 (TID 1058). 843 bytes result sent to driver
15/08/21 19:47:50 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 1072, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 23.0 in stage 10.0 (TID 1072)
15/08/21 19:47:50 INFO TaskSetManager: Starting task 24.0 in stage 10.0 (TID 1073, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 24.0 in stage 10.0 (TID 1073)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 1050) in 616 ms on localhost (8/200)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 1057) in 615 ms on localhost (9/200)
15/08/21 19:47:50 INFO TaskSetManager: Starting task 25.0 in stage 10.0 (TID 1074, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 25.0 in stage 10.0 (TID 1074)
15/08/21 19:47:50 INFO TaskSetManager: Starting task 26.0 in stage 10.0 (TID 1075, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:50 INFO Executor: Running task 26.0 in stage 10.0 (TID 1075)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1049) in 620 ms on localhost (10/200)
15/08/21 19:47:50 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 1058) in 619 ms on localhost (11/200)
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:50 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:50 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:50 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:50 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:50 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:50 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:50 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:50 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:50 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:50 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:50 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:50 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000003
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000003_0: Committed
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000013
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000013_0: Committed
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000006
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000006_0: Committed
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000011
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000011_0: Committed
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000002
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000002_0: Committed
15/08/21 19:47:51 INFO Executor: Finished task 3.0 in stage 10.0 (TID 1052). 843 bytes result sent to driver
15/08/21 19:47:51 INFO Executor: Finished task 13.0 in stage 10.0 (TID 1062). 843 bytes result sent to driver
15/08/21 19:47:51 INFO Executor: Finished task 11.0 in stage 10.0 (TID 1060). 843 bytes result sent to driver
15/08/21 19:47:51 INFO Executor: Finished task 2.0 in stage 10.0 (TID 1051). 843 bytes result sent to driver
15/08/21 19:47:51 INFO Executor: Finished task 6.0 in stage 10.0 (TID 1055). 843 bytes result sent to driver
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 1076, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO TaskSetManager: Starting task 28.0 in stage 10.0 (TID 1077, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 27.0 in stage 10.0 (TID 1076)
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO TaskSetManager: Starting task 29.0 in stage 10.0 (TID 1078, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO Executor: Running task 28.0 in stage 10.0 (TID 1077)
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO Executor: Running task 29.0 in stage 10.0 (TID 1078)
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO TaskSetManager: Starting task 30.0 in stage 10.0 (TID 1079, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 30.0 in stage 10.0 (TID 1079)
15/08/21 19:47:51 INFO TaskSetManager: Starting task 31.0 in stage 10.0 (TID 1080, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 31.0 in stage 10.0 (TID 1080)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 1052) in 2189 ms on localhost (12/200)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 1062) in 2186 ms on localhost (13/200)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 1060) in 2188 ms on localhost (14/200)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 1051) in 2191 ms on localhost (15/200)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 1055) in 2193 ms on localhost (16/200)
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,436
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,876B for [ps_partkey] INT32: 508 values, 2,039B raw, 1,840B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,519B for [value] DOUBLE: 508 values, 4,071B raw, 2,475B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,336
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,681B for [ps_partkey] INT32: 453 values, 1,819B raw, 1,645B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,262B for [value] DOUBLE: 453 values, 3,631B raw, 2,218B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:54384 in memory (size: 8.3 KB, free: 20.7 GB)
15/08/21 19:47:51 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:54384 in memory (size: 5.6 KB, free: 20.7 GB)
15/08/21 19:47:51 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:54384 in memory (size: 3.8 KB, free: 20.7 GB)
15/08/21 19:47:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:54384 in memory (size: 1776.0 B, free: 20.7 GB)
15/08/21 19:47:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:54384 in memory (size: 31.0 KB, free: 20.7 GB)
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,396
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000019
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000019_0: Committed
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,316
15/08/21 19:47:51 INFO Executor: Finished task 19.0 in stage 10.0 (TID 1068). 843 bytes result sent to driver
15/08/21 19:47:51 INFO TaskSetManager: Starting task 32.0 in stage 10.0 (TID 1081, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 32.0 in stage 10.0 (TID 1081)
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,526B for [ps_partkey] INT32: 406 values, 1,631B raw, 1,490B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,039B for [value] DOUBLE: 406 values, 3,255B raw, 1,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:51 INFO TaskSetManager: Finished task 19.0 in stage 10.0 (TID 1068) in 1728 ms on localhost (17/200)
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,681B for [ps_partkey] INT32: 452 values, 1,815B raw, 1,645B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,255B for [value] DOUBLE: 452 values, 3,623B raw, 2,211B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,036
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,806B for [ps_partkey] INT32: 488 values, 1,959B raw, 1,770B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,386B for [value] DOUBLE: 488 values, 3,911B raw, 2,342B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,376
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,695B for [ps_partkey] INT32: 455 values, 1,827B raw, 1,659B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,250B for [value] DOUBLE: 455 values, 3,647B raw, 2,206B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,196
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,663B for [ps_partkey] INT32: 446 values, 1,791B raw, 1,627B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,213B for [value] DOUBLE: 446 values, 3,575B raw, 2,169B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000017
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000017_0: Committed
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000016
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000016_0: Committed
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,636
15/08/21 19:47:51 INFO Executor: Finished task 16.0 in stage 10.0 (TID 1065). 843 bytes result sent to driver
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,376
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000026
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000026_0: Committed
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO Executor: Finished task 17.0 in stage 10.0 (TID 1066). 843 bytes result sent to driver
15/08/21 19:47:51 INFO TaskSetManager: Starting task 33.0 in stage 10.0 (TID 1082, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 33.0 in stage 10.0 (TID 1082)
15/08/21 19:47:51 INFO TaskSetManager: Starting task 34.0 in stage 10.0 (TID 1083, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 34.0 in stage 10.0 (TID 1083)
15/08/21 19:47:51 INFO Executor: Finished task 26.0 in stage 10.0 (TID 1075). 843 bytes result sent to driver
15/08/21 19:47:51 INFO TaskSetManager: Starting task 35.0 in stage 10.0 (TID 1084, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 1065) in 1839 ms on localhost (18/200)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 17.0 in stage 10.0 (TID 1066) in 1838 ms on localhost (19/200)
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,252B for [ps_partkey] INT32: 618 values, 2,479B raw, 2,216B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,980B for [value] DOUBLE: 618 values, 4,951B raw, 2,936B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 1075) in 1794 ms on localhost (20/200)
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,869B for [ps_partkey] INT32: 505 values, 2,027B raw, 1,833B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,481B for [value] DOUBLE: 505 values, 4,047B raw, 2,437B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,476
15/08/21 19:47:51 INFO Executor: Running task 35.0 in stage 10.0 (TID 1084)
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,396
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,713B for [ps_partkey] INT32: 460 values, 1,847B raw, 1,677B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,522B for [ps_partkey] INT32: 406 values, 1,631B raw, 1,486B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,286B for [value] DOUBLE: 460 values, 3,687B raw, 2,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000021
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000021_0: Committed
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,014B for [value] DOUBLE: 406 values, 3,255B raw, 1,970B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO Executor: Finished task 21.0 in stage 10.0 (TID 1070). 843 bytes result sent to driver
15/08/21 19:47:51 INFO TaskSetManager: Starting task 36.0 in stage 10.0 (TID 1085, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 36.0 in stage 10.0 (TID 1085)
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 1070) in 1848 ms on localhost (21/200)
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000023
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000023_0: Committed
15/08/21 19:47:51 INFO Executor: Finished task 23.0 in stage 10.0 (TID 1072). 843 bytes result sent to driver
15/08/21 19:47:51 INFO TaskSetManager: Starting task 37.0 in stage 10.0 (TID 1086, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 37.0 in stage 10.0 (TID 1086)
15/08/21 19:47:51 INFO TaskSetManager: Finished task 23.0 in stage 10.0 (TID 1072) in 1870 ms on localhost (22/200)
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,396
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,870B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,834B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,423B for [value] DOUBLE: 506 values, 4,055B raw, 2,379B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000022
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000022_0: Committed
15/08/21 19:47:51 INFO Executor: Finished task 22.0 in stage 10.0 (TID 1071). 843 bytes result sent to driver
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,856
15/08/21 19:47:51 INFO TaskSetManager: Starting task 38.0 in stage 10.0 (TID 1087, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,576
15/08/21 19:47:51 INFO Executor: Running task 38.0 in stage 10.0 (TID 1087)
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,556
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000024
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000024_0: Committed
15/08/21 19:47:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:51 INFO TaskSetManager: Finished task 22.0 in stage 10.0 (TID 1071) in 1902 ms on localhost (23/200)
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,951B for [ps_partkey] INT32: 529 values, 2,123B raw, 1,915B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,072B for [ps_partkey] INT32: 565 values, 2,267B raw, 2,036B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,568B for [value] DOUBLE: 529 values, 4,239B raw, 2,524B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,721B for [value] DOUBLE: 565 values, 4,527B raw, 2,677B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO Executor: Finished task 24.0 in stage 10.0 (TID 1073). 843 bytes result sent to driver
15/08/21 19:47:51 INFO TaskSetManager: Starting task 39.0 in stage 10.0 (TID 1088, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 39.0 in stage 10.0 (TID 1088)
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,550B for [ps_partkey] INT32: 414 values, 1,663B raw, 1,514B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 2,055B for [value] DOUBLE: 414 values, 3,319B raw, 2,011B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,356
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 1073) in 1901 ms on localhost (24/200)
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,519B for [ps_partkey] INT32: 404 values, 1,623B raw, 1,483B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ColumnChunkPageWriteStore: written 1,982B for [value] DOUBLE: 404 values, 3,239B raw, 1,938B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:51 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:51 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000025
15/08/21 19:47:51 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000025_0: Committed
15/08/21 19:47:51 INFO Executor: Finished task 25.0 in stage 10.0 (TID 1074). 843 bytes result sent to driver
15/08/21 19:47:51 INFO TaskSetManager: Starting task 40.0 in stage 10.0 (TID 1089, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:51 INFO Executor: Running task 40.0 in stage 10.0 (TID 1089)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 1074) in 1930 ms on localhost (25/200)
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,416
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,044B for [ps_partkey] INT32: 557 values, 2,235B raw, 2,008B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,607B for [value] DOUBLE: 557 values, 4,463B raw, 2,563B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000030
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000030_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 30.0 in stage 10.0 (TID 1079). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 41.0 in stage 10.0 (TID 1090, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 41.0 in stage 10.0 (TID 1090)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 30.0 in stage 10.0 (TID 1079) in 414 ms on localhost (26/200)
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000031
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000029
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000031_0: Committed
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000029_0: Committed
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO Executor: Finished task 29.0 in stage 10.0 (TID 1078). 843 bytes result sent to driver
15/08/21 19:47:52 INFO Executor: Finished task 31.0 in stage 10.0 (TID 1080). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 42.0 in stage 10.0 (TID 1091, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO TaskSetManager: Starting task 43.0 in stage 10.0 (TID 1092, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO Executor: Running task 43.0 in stage 10.0 (TID 1092)
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO Executor: Running task 42.0 in stage 10.0 (TID 1091)
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO TaskSetManager: Finished task 31.0 in stage 10.0 (TID 1080) in 430 ms on localhost (27/200)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 29.0 in stage 10.0 (TID 1078) in 435 ms on localhost (28/200)
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000032
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000032_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 32.0 in stage 10.0 (TID 1081). 843 bytes result sent to driver
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO TaskSetManager: Starting task 44.0 in stage 10.0 (TID 1093, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 44.0 in stage 10.0 (TID 1093)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 32.0 in stage 10.0 (TID 1081) in 333 ms on localhost (29/200)
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,596
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,996
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,803B for [ps_partkey] INT32: 486 values, 1,951B raw, 1,767B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,085B for [ps_partkey] INT32: 566 values, 2,271B raw, 2,049B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,280B for [value] DOUBLE: 486 values, 3,895B raw, 2,236B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,614B for [value] DOUBLE: 566 values, 4,535B raw, 2,570B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,136
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,396
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,650B for [ps_partkey] INT32: 443 values, 1,779B raw, 1,614B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,096B for [value] DOUBLE: 443 values, 3,551B raw, 2,052B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,690B for [ps_partkey] INT32: 456 values, 1,831B raw, 1,654B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,135B for [value] DOUBLE: 456 values, 3,655B raw, 2,091B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,716
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,587B for [ps_partkey] INT32: 422 values, 1,695B raw, 1,551B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,963B for [value] DOUBLE: 422 values, 3,383B raw, 1,919B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,396
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,529B for [ps_partkey] INT32: 406 values, 1,631B raw, 1,493B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,897B for [value] DOUBLE: 406 values, 3,255B raw, 1,853B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000018
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,356
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000018_0: Committed
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO Executor: Finished task 18.0 in stage 10.0 (TID 1067). 843 bytes result sent to driver
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,031B for [ps_partkey] INT32: 554 values, 2,223B raw, 1,995B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,496B for [value] DOUBLE: 554 values, 4,439B raw, 2,452B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO TaskSetManager: Starting task 45.0 in stage 10.0 (TID 1094, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 45.0 in stage 10.0 (TID 1094)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 18.0 in stage 10.0 (TID 1067) in 2156 ms on localhost (30/200)
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000036
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000036_0: Committed
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO Executor: Finished task 36.0 in stage 10.0 (TID 1085). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 46.0 in stage 10.0 (TID 1095, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 46.0 in stage 10.0 (TID 1095)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 36.0 in stage 10.0 (TID 1085) in 343 ms on localhost (31/200)
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000038
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000038_0: Committed
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000037
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000037_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 38.0 in stage 10.0 (TID 1087). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 47.0 in stage 10.0 (TID 1096, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 47.0 in stage 10.0 (TID 1096)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 38.0 in stage 10.0 (TID 1087) in 282 ms on localhost (32/200)
15/08/21 19:47:52 INFO Executor: Finished task 37.0 in stage 10.0 (TID 1086). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 48.0 in stage 10.0 (TID 1097, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 48.0 in stage 10.0 (TID 1097)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 37.0 in stage 10.0 (TID 1086) in 310 ms on localhost (33/200)
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,508
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,716B for [ps_partkey] INT32: 462 values, 1,855B raw, 1,680B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,111B for [value] DOUBLE: 462 values, 3,703B raw, 2,067B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000039
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000039_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 39.0 in stage 10.0 (TID 1088). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 49.0 in stage 10.0 (TID 1098, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 49.0 in stage 10.0 (TID 1098)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 39.0 in stage 10.0 (TID 1088) in 308 ms on localhost (34/200)
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,328
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,860B for [ps_partkey] INT32: 503 values, 2,019B raw, 1,824B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,273B for [value] DOUBLE: 503 values, 4,031B raw, 2,229B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,308
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,088
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,684B for [ps_partkey] INT32: 452 values, 1,815B raw, 1,648B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,043B for [value] DOUBLE: 452 values, 3,623B raw, 1,999B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,990B for [ps_partkey] INT32: 541 values, 2,171B raw, 1,954B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,429B for [value] DOUBLE: 541 values, 4,335B raw, 2,385B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,948
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,789B for [ps_partkey] INT32: 484 values, 1,943B raw, 1,753B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,177B for [value] DOUBLE: 484 values, 3,879B raw, 2,133B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000020
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000020_0: Committed
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO Executor: Finished task 20.0 in stage 10.0 (TID 1069). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 50.0 in stage 10.0 (TID 1099, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO Executor: Running task 50.0 in stage 10.0 (TID 1099)
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO TaskSetManager: Finished task 20.0 in stage 10.0 (TID 1069) in 2324 ms on localhost (35/200)
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000027
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000027_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 27.0 in stage 10.0 (TID 1076). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 51.0 in stage 10.0 (TID 1100, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 51.0 in stage 10.0 (TID 1100)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 1076) in 799 ms on localhost (36/200)
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,476
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000028
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000028_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 28.0 in stage 10.0 (TID 1077). 843 bytes result sent to driver
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,535B for [ps_partkey] INT32: 410 values, 1,647B raw, 1,499B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO TaskSetManager: Starting task 52.0 in stage 10.0 (TID 1101, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,873B for [value] DOUBLE: 410 values, 3,287B raw, 1,829B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO Executor: Running task 52.0 in stage 10.0 (TID 1101)
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO TaskSetManager: Finished task 28.0 in stage 10.0 (TID 1077) in 808 ms on localhost (37/200)
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,988
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,800B for [ps_partkey] INT32: 486 values, 1,951B raw, 1,764B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,916
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,178B for [value] DOUBLE: 486 values, 3,895B raw, 2,134B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,951B for [ps_partkey] INT32: 532 values, 2,135B raw, 1,915B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,377B for [value] DOUBLE: 532 values, 4,263B raw, 2,333B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,296
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000045
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000045_0: Committed
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,021B for [ps_partkey] INT32: 551 values, 2,211B raw, 1,985B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,442B for [value] DOUBLE: 551 values, 4,415B raw, 2,398B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO Executor: Finished task 45.0 in stage 10.0 (TID 1094). 843 bytes result sent to driver
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,060
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO TaskSetManager: Starting task 53.0 in stage 10.0 (TID 1102, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 53.0 in stage 10.0 (TID 1102)
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,810B for [ps_partkey] INT32: 490 values, 1,967B raw, 1,774B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,194B for [value] DOUBLE: 490 values, 3,927B raw, 2,150B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO TaskSetManager: Finished task 45.0 in stage 10.0 (TID 1094) in 319 ms on localhost (38/200)
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000033
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000033_0: Committed
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000034
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000034_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 33.0 in stage 10.0 (TID 1082). 843 bytes result sent to driver
15/08/21 19:47:52 INFO Executor: Finished task 34.0 in stage 10.0 (TID 1083). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 54.0 in stage 10.0 (TID 1103, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 54.0 in stage 10.0 (TID 1103)
15/08/21 19:47:52 INFO TaskSetManager: Starting task 55.0 in stage 10.0 (TID 1104, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 55.0 in stage 10.0 (TID 1104)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 33.0 in stage 10.0 (TID 1082) in 713 ms on localhost (39/200)
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,328
15/08/21 19:47:52 INFO TaskSetManager: Finished task 34.0 in stage 10.0 (TID 1083) in 711 ms on localhost (40/200)
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,680B for [ps_partkey] INT32: 453 values, 1,819B raw, 1,644B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,045B for [value] DOUBLE: 453 values, 3,631B raw, 2,001B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000035
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000035_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 35.0 in stage 10.0 (TID 1084). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 56.0 in stage 10.0 (TID 1105, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 56.0 in stage 10.0 (TID 1105)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 35.0 in stage 10.0 (TID 1084) in 759 ms on localhost (41/200)
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,396
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,688
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,578B for [ps_partkey] INT32: 421 values, 1,691B raw, 1,542B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,891B for [value] DOUBLE: 421 values, 3,375B raw, 1,847B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,216B for [ps_partkey] INT32: 606 values, 2,431B raw, 2,180B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,659B for [value] DOUBLE: 606 values, 4,855B raw, 2,615B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,868
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,123B for [ps_partkey] INT32: 580 values, 2,327B raw, 2,087B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,549B for [value] DOUBLE: 580 values, 4,647B raw, 2,505B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000040
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000040_0: Committed
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO Executor: Finished task 40.0 in stage 10.0 (TID 1089). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 57.0 in stage 10.0 (TID 1106, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 57.0 in stage 10.0 (TID 1106)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 40.0 in stage 10.0 (TID 1089) in 718 ms on localhost (42/200)
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000051
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000051_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 51.0 in stage 10.0 (TID 1100). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 58.0 in stage 10.0 (TID 1107, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 58.0 in stage 10.0 (TID 1107)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 51.0 in stage 10.0 (TID 1100) in 317 ms on localhost (43/200)
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,096
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,196
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,998B for [ps_partkey] INT32: 541 values, 2,171B raw, 1,962B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,384B for [value] DOUBLE: 541 values, 4,335B raw, 2,340B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,011B for [ps_partkey] INT32: 546 values, 2,191B raw, 1,975B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,400B for [value] DOUBLE: 546 values, 4,375B raw, 2,356B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,556
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,548B for [ps_partkey] INT32: 414 values, 1,663B raw, 1,512B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,860B for [value] DOUBLE: 414 values, 3,319B raw, 1,816B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000041
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000041_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 41.0 in stage 10.0 (TID 1090). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 59.0 in stage 10.0 (TID 1108, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 59.0 in stage 10.0 (TID 1108)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 41.0 in stage 10.0 (TID 1090) in 740 ms on localhost (44/200)
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000043
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000043_0: Committed
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO Executor: Finished task 43.0 in stage 10.0 (TID 1092). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 60.0 in stage 10.0 (TID 1109, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 60.0 in stage 10.0 (TID 1109)
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO TaskSetManager: Finished task 43.0 in stage 10.0 (TID 1092) in 727 ms on localhost (45/200)
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000042
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000042_0: Committed
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000044
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000044_0: Committed
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO Executor: Finished task 42.0 in stage 10.0 (TID 1091). 843 bytes result sent to driver
15/08/21 19:47:52 INFO Executor: Finished task 44.0 in stage 10.0 (TID 1093). 843 bytes result sent to driver
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000056
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000056_0: Committed
15/08/21 19:47:52 INFO TaskSetManager: Starting task 61.0 in stage 10.0 (TID 1110, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 61.0 in stage 10.0 (TID 1110)
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO TaskSetManager: Starting task 62.0 in stage 10.0 (TID 1111, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO Executor: Finished task 56.0 in stage 10.0 (TID 1105). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Finished task 42.0 in stage 10.0 (TID 1091) in 756 ms on localhost (46/200)
15/08/21 19:47:52 INFO Executor: Running task 62.0 in stage 10.0 (TID 1111)
15/08/21 19:47:52 INFO TaskSetManager: Starting task 63.0 in stage 10.0 (TID 1112, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 63.0 in stage 10.0 (TID 1112)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 44.0 in stage 10.0 (TID 1093) in 739 ms on localhost (47/200)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 56.0 in stage 10.0 (TID 1105) in 211 ms on localhost (48/200)
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,936
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,936
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,963B for [ps_partkey] INT32: 533 values, 2,139B raw, 1,927B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,326B for [value] DOUBLE: 533 values, 4,271B raw, 2,282B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,970B for [ps_partkey] INT32: 533 values, 2,139B raw, 1,934B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,328B for [value] DOUBLE: 533 values, 4,271B raw, 2,284B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000046
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000046_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 46.0 in stage 10.0 (TID 1095). 843 bytes result sent to driver
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000047
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000047_0: Committed
15/08/21 19:47:52 INFO TaskSetManager: Starting task 64.0 in stage 10.0 (TID 1113, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 64.0 in stage 10.0 (TID 1113)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 46.0 in stage 10.0 (TID 1095) in 701 ms on localhost (49/200)
15/08/21 19:47:52 INFO Executor: Finished task 47.0 in stage 10.0 (TID 1096). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 65.0 in stage 10.0 (TID 1114, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 65.0 in stage 10.0 (TID 1114)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 47.0 in stage 10.0 (TID 1096) in 694 ms on localhost (50/200)
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,976
15/08/21 19:47:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,536
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,623B for [ps_partkey] INT32: 435 values, 1,747B raw, 1,587B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,920B for [value] DOUBLE: 435 values, 3,487B raw, 1,876B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 1,721B for [ps_partkey] INT32: 463 values, 1,859B raw, 1,685B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO ColumnChunkPageWriteStore: written 2,048B for [value] DOUBLE: 463 values, 3,711B raw, 2,004B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000049
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000049_0: Committed
15/08/21 19:47:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000048
15/08/21 19:47:52 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000048_0: Committed
15/08/21 19:47:52 INFO Executor: Finished task 49.0 in stage 10.0 (TID 1098). 843 bytes result sent to driver
15/08/21 19:47:52 INFO Executor: Finished task 48.0 in stage 10.0 (TID 1097). 843 bytes result sent to driver
15/08/21 19:47:52 INFO TaskSetManager: Starting task 66.0 in stage 10.0 (TID 1115, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 66.0 in stage 10.0 (TID 1115)
15/08/21 19:47:52 INFO TaskSetManager: Starting task 67.0 in stage 10.0 (TID 1116, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:52 INFO Executor: Running task 67.0 in stage 10.0 (TID 1116)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 49.0 in stage 10.0 (TID 1098) in 701 ms on localhost (51/200)
15/08/21 19:47:52 INFO TaskSetManager: Finished task 48.0 in stage 10.0 (TID 1097) in 735 ms on localhost (52/200)
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,036
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,068
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,463,376
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,643B for [ps_partkey] INT32: 440 values, 1,767B raw, 1,607B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,948B for [value] DOUBLE: 440 values, 3,527B raw, 1,904B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,810B for [ps_partkey] INT32: 488 values, 1,959B raw, 1,774B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,169B for [ps_partkey] INT32: 305 values, 1,227B raw, 1,133B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,148B for [value] DOUBLE: 488 values, 3,911B raw, 2,104B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,410B for [value] DOUBLE: 305 values, 2,447B raw, 1,366B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000050
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000050_0: Committed
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000061
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000061_0: Committed
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO Executor: Finished task 61.0 in stage 10.0 (TID 1110). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Finished task 50.0 in stage 10.0 (TID 1099). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 68.0 in stage 10.0 (TID 1117, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 69.0 in stage 10.0 (TID 1118, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 69.0 in stage 10.0 (TID 1118)
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000052
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000052_0: Committed
15/08/21 19:47:53 INFO TaskSetManager: Finished task 61.0 in stage 10.0 (TID 1110) in 295 ms on localhost (53/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 50.0 in stage 10.0 (TID 1099) in 750 ms on localhost (54/200)
15/08/21 19:47:53 INFO Executor: Running task 68.0 in stage 10.0 (TID 1117)
15/08/21 19:47:53 INFO Executor: Finished task 52.0 in stage 10.0 (TID 1101). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 70.0 in stage 10.0 (TID 1119, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 70.0 in stage 10.0 (TID 1119)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 52.0 in stage 10.0 (TID 1101) in 682 ms on localhost (55/200)
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,576
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,176
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,903B for [ps_partkey] INT32: 515 values, 2,067B raw, 1,867B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,251B for [value] DOUBLE: 515 values, 4,127B raw, 2,207B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,999B for [ps_partkey] INT32: 545 values, 2,187B raw, 1,963B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,382B for [value] DOUBLE: 545 values, 4,367B raw, 2,338B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,208
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,007B for [ps_partkey] INT32: 547 values, 2,195B raw, 1,971B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,385B for [value] DOUBLE: 547 values, 4,383B raw, 2,341B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,188
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,663B for [ps_partkey] INT32: 446 values, 1,791B raw, 1,627B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,955B for [value] DOUBLE: 446 values, 3,575B raw, 1,911B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000053
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000053_0: Committed
15/08/21 19:47:53 INFO Executor: Finished task 53.0 in stage 10.0 (TID 1102). 843 bytes result sent to driver
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO TaskSetManager: Starting task 71.0 in stage 10.0 (TID 1120, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO Executor: Running task 71.0 in stage 10.0 (TID 1120)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 53.0 in stage 10.0 (TID 1102) in 698 ms on localhost (56/200)
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000055
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000055_0: Committed
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000066
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000066_0: Committed
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000064
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000064_0: Committed
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000054
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000054_0: Committed
15/08/21 19:47:53 INFO Executor: Finished task 66.0 in stage 10.0 (TID 1115). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Finished task 55.0 in stage 10.0 (TID 1104). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Finished task 64.0 in stage 10.0 (TID 1113). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Finished task 54.0 in stage 10.0 (TID 1103). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 72.0 in stage 10.0 (TID 1121, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 72.0 in stage 10.0 (TID 1121)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 73.0 in stage 10.0 (TID 1122, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 74.0 in stage 10.0 (TID 1123, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 74.0 in stage 10.0 (TID 1123)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 75.0 in stage 10.0 (TID 1124, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 73.0 in stage 10.0 (TID 1122)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 66.0 in stage 10.0 (TID 1115) in 250 ms on localhost (57/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 55.0 in stage 10.0 (TID 1104) in 650 ms on localhost (58/200)
15/08/21 19:47:53 INFO Executor: Running task 75.0 in stage 10.0 (TID 1124)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 64.0 in stage 10.0 (TID 1113) in 299 ms on localhost (59/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 54.0 in stage 10.0 (TID 1103) in 652 ms on localhost (60/200)
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000067
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000067_0: Committed
15/08/21 19:47:53 INFO Executor: Finished task 67.0 in stage 10.0 (TID 1116). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 76.0 in stage 10.0 (TID 1125, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 76.0 in stage 10.0 (TID 1125)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 67.0 in stage 10.0 (TID 1116) in 254 ms on localhost (61/200)
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,976
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,336
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,315B for [ps_partkey] INT32: 635 values, 2,547B raw, 2,279B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,701B for [value] DOUBLE: 635 values, 5,087B raw, 2,657B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,856B for [ps_partkey] INT32: 503 values, 2,019B raw, 1,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,175B for [value] DOUBLE: 503 values, 4,031B raw, 2,131B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,128
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,481B for [ps_partkey] INT32: 393 values, 1,579B raw, 1,445B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,743B for [value] DOUBLE: 393 values, 3,151B raw, 1,699B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000058
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000057
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000058_0: Committed
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000057_0: Committed
15/08/21 19:47:53 INFO Executor: Finished task 58.0 in stage 10.0 (TID 1107). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Finished task 57.0 in stage 10.0 (TID 1106). 843 bytes result sent to driver
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO TaskSetManager: Starting task 77.0 in stage 10.0 (TID 1126, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 77.0 in stage 10.0 (TID 1126)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 78.0 in stage 10.0 (TID 1127, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 78.0 in stage 10.0 (TID 1127)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 57.0 in stage 10.0 (TID 1106) in 666 ms on localhost (62/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 58.0 in stage 10.0 (TID 1107) in 650 ms on localhost (63/200)
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000069
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000069_0: Committed
15/08/21 19:47:53 INFO Executor: Finished task 69.0 in stage 10.0 (TID 1118). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 79.0 in stage 10.0 (TID 1128, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 79.0 in stage 10.0 (TID 1128)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 69.0 in stage 10.0 (TID 1118) in 303 ms on localhost (64/200)
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000060
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000060_0: Committed
15/08/21 19:47:53 INFO Executor: Finished task 60.0 in stage 10.0 (TID 1109). 843 bytes result sent to driver
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000059
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000059_0: Committed
15/08/21 19:47:53 INFO TaskSetManager: Starting task 80.0 in stage 10.0 (TID 1129, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 80.0 in stage 10.0 (TID 1129)
15/08/21 19:47:53 INFO Executor: Finished task 59.0 in stage 10.0 (TID 1108). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 81.0 in stage 10.0 (TID 1130, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 81.0 in stage 10.0 (TID 1130)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 60.0 in stage 10.0 (TID 1109) in 633 ms on localhost (65/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 59.0 in stage 10.0 (TID 1108) in 642 ms on localhost (66/200)
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,636
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,940
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,567B for [ps_partkey] INT32: 418 values, 1,679B raw, 1,531B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,831B for [value] DOUBLE: 418 values, 3,351B raw, 1,787B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,617B for [ps_partkey] INT32: 434 values, 1,743B raw, 1,581B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,909B for [value] DOUBLE: 434 values, 3,479B raw, 1,865B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,068
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,470,196
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,736
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,642B for [ps_partkey] INT32: 440 values, 1,767B raw, 1,606B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,938B for [value] DOUBLE: 440 values, 3,527B raw, 1,894B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,354B for [ps_partkey] INT32: 646 values, 2,591B raw, 2,318B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,764B for [value] DOUBLE: 646 values, 5,175B raw, 2,720B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,752B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,716B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,092B for [value] DOUBLE: 473 values, 3,791B raw, 2,048B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,868
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,124B for [ps_partkey] INT32: 580 values, 2,327B raw, 2,088B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,481B for [value] DOUBLE: 580 values, 4,647B raw, 2,437B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000071
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000071_0: Committed
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO Executor: Finished task 71.0 in stage 10.0 (TID 1120). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 82.0 in stage 10.0 (TID 1131, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 82.0 in stage 10.0 (TID 1131)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 71.0 in stage 10.0 (TID 1120) in 319 ms on localhost (67/200)
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000063
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000062
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000063_0: Committed
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000062_0: Committed
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO Executor: Finished task 62.0 in stage 10.0 (TID 1111). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Finished task 63.0 in stage 10.0 (TID 1112). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 83.0 in stage 10.0 (TID 1132, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 83.0 in stage 10.0 (TID 1132)
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000072
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000072_0: Committed
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO TaskSetManager: Starting task 84.0 in stage 10.0 (TID 1133, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO Executor: Running task 84.0 in stage 10.0 (TID 1133)
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO Executor: Finished task 72.0 in stage 10.0 (TID 1121). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Finished task 63.0 in stage 10.0 (TID 1112) in 732 ms on localhost (68/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 62.0 in stage 10.0 (TID 1111) in 735 ms on localhost (69/200)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 85.0 in stage 10.0 (TID 1134, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 85.0 in stage 10.0 (TID 1134)
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000076
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000076_0: Committed
15/08/21 19:47:53 INFO TaskSetManager: Finished task 72.0 in stage 10.0 (TID 1121) in 352 ms on localhost (70/200)
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO Executor: Finished task 76.0 in stage 10.0 (TID 1125). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 86.0 in stage 10.0 (TID 1135, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 86.0 in stage 10.0 (TID 1135)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 76.0 in stage 10.0 (TID 1125) in 353 ms on localhost (71/200)
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,888
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,949B for [ps_partkey] INT32: 531 values, 2,131B raw, 1,913B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,300B for [value] DOUBLE: 531 values, 4,255B raw, 2,256B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,576
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,436
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,568
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,382B for [ps_partkey] INT32: 365 values, 1,467B raw, 1,346B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,648B for [value] DOUBLE: 365 values, 2,927B raw, 1,604B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,156
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,707B for [ps_partkey] INT32: 458 values, 1,839B raw, 1,671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,005B for [value] DOUBLE: 458 values, 3,671B raw, 1,961B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,557B for [ps_partkey] INT32: 415 values, 1,667B raw, 1,521B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,824B for [value] DOUBLE: 415 values, 3,327B raw, 1,780B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,485B for [ps_partkey] INT32: 394 values, 1,583B raw, 1,449B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,736B for [value] DOUBLE: 394 values, 3,159B raw, 1,692B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000065
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000065_0: Committed
15/08/21 19:47:53 INFO Executor: Finished task 65.0 in stage 10.0 (TID 1114). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 87.0 in stage 10.0 (TID 1136, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 87.0 in stage 10.0 (TID 1136)
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO TaskSetManager: Finished task 65.0 in stage 10.0 (TID 1114) in 724 ms on localhost (72/200)
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000077
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000077_0: Committed
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO Executor: Finished task 77.0 in stage 10.0 (TID 1126). 843 bytes result sent to driver
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO TaskSetManager: Starting task 88.0 in stage 10.0 (TID 1137, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO Executor: Running task 88.0 in stage 10.0 (TID 1137)
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000079
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000079_0: Committed
15/08/21 19:47:53 INFO TaskSetManager: Finished task 77.0 in stage 10.0 (TID 1126) in 315 ms on localhost (73/200)
15/08/21 19:47:53 INFO Executor: Finished task 79.0 in stage 10.0 (TID 1128). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 89.0 in stage 10.0 (TID 1138, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 89.0 in stage 10.0 (TID 1138)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 79.0 in stage 10.0 (TID 1128) in 282 ms on localhost (74/200)
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,588
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,557B for [ps_partkey] INT32: 416 values, 1,671B raw, 1,521B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,804B for [value] DOUBLE: 416 values, 3,335B raw, 1,760B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,748
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,236
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,676
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,591B for [ps_partkey] INT32: 424 values, 1,703B raw, 1,555B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,867B for [value] DOUBLE: 424 values, 3,399B raw, 1,823B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,911B for [ps_partkey] INT32: 520 values, 2,087B raw, 1,875B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,670B for [ps_partkey] INT32: 448 values, 1,799B raw, 1,634B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,208B for [value] DOUBLE: 520 values, 4,167B raw, 2,164B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,945B for [value] DOUBLE: 448 values, 3,591B raw, 1,901B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,236
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000070
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000070_0: Committed
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,841B for [ps_partkey] INT32: 498 values, 1,999B raw, 1,805B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,138B for [value] DOUBLE: 498 values, 3,991B raw, 2,094B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO Executor: Finished task 70.0 in stage 10.0 (TID 1119). 843 bytes result sent to driver
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000068
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000068_0: Committed
15/08/21 19:47:53 INFO TaskSetManager: Starting task 90.0 in stage 10.0 (TID 1139, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 90.0 in stage 10.0 (TID 1139)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 70.0 in stage 10.0 (TID 1119) in 674 ms on localhost (75/200)
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO Executor: Finished task 68.0 in stage 10.0 (TID 1117). 843 bytes result sent to driver
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO TaskSetManager: Starting task 91.0 in stage 10.0 (TID 1140, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,016
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO Executor: Running task 91.0 in stage 10.0 (TID 1140)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 68.0 in stage 10.0 (TID 1117) in 701 ms on localhost (76/200)
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,630B for [ps_partkey] INT32: 437 values, 1,755B raw, 1,594B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,904B for [value] DOUBLE: 437 values, 3,503B raw, 1,860B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,568
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000086
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000086_0: Committed
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,729B for [ps_partkey] INT32: 465 values, 1,867B raw, 1,693B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,013B for [value] DOUBLE: 465 values, 3,727B raw, 1,969B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO Executor: Finished task 86.0 in stage 10.0 (TID 1135). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 92.0 in stage 10.0 (TID 1141, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 92.0 in stage 10.0 (TID 1141)
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000087
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000087_0: Committed
15/08/21 19:47:53 INFO TaskSetManager: Finished task 86.0 in stage 10.0 (TID 1135) in 304 ms on localhost (77/200)
15/08/21 19:47:53 INFO Executor: Finished task 87.0 in stage 10.0 (TID 1136). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 93.0 in stage 10.0 (TID 1142, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 93.0 in stage 10.0 (TID 1142)
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,796
15/08/21 19:47:53 INFO TaskSetManager: Finished task 87.0 in stage 10.0 (TID 1136) in 230 ms on localhost (78/200)
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,769B for [ps_partkey] INT32: 476 values, 1,911B raw, 1,733B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,062B for [value] DOUBLE: 476 values, 3,815B raw, 2,018B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000088
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000088_0: Committed
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000075
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000075_0: Committed
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000074
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000074_0: Committed
15/08/21 19:47:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000073
15/08/21 19:47:53 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000073_0: Committed
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:53 INFO Executor: Finished task 88.0 in stage 10.0 (TID 1137). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Finished task 74.0 in stage 10.0 (TID 1123). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 94.0 in stage 10.0 (TID 1143, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Finished task 73.0 in stage 10.0 (TID 1122). 843 bytes result sent to driver
15/08/21 19:47:53 INFO TaskSetManager: Starting task 95.0 in stage 10.0 (TID 1144, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 95.0 in stage 10.0 (TID 1144)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 96.0 in stage 10.0 (TID 1145, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 96.0 in stage 10.0 (TID 1145)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 88.0 in stage 10.0 (TID 1137) in 252 ms on localhost (79/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 74.0 in stage 10.0 (TID 1123) in 723 ms on localhost (80/200)
15/08/21 19:47:53 INFO Executor: Finished task 75.0 in stage 10.0 (TID 1124). 843 bytes result sent to driver
15/08/21 19:47:53 INFO Executor: Running task 94.0 in stage 10.0 (TID 1143)
15/08/21 19:47:53 INFO TaskSetManager: Starting task 97.0 in stage 10.0 (TID 1146, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:53 INFO Executor: Running task 97.0 in stage 10.0 (TID 1146)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 73.0 in stage 10.0 (TID 1122) in 725 ms on localhost (81/200)
15/08/21 19:47:53 INFO TaskSetManager: Finished task 75.0 in stage 10.0 (TID 1124) in 724 ms on localhost (82/200)
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,996
15/08/21 19:47:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,148
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,797B for [ps_partkey] INT32: 486 values, 1,951B raw, 1,761B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 1,822B for [ps_partkey] INT32: 494 values, 1,983B raw, 1,786B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,108B for [value] DOUBLE: 486 values, 3,895B raw, 2,064B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ColumnChunkPageWriteStore: written 2,124B for [value] DOUBLE: 494 values, 3,959B raw, 2,080B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:53 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,408
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,176
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,527B for [ps_partkey] INT32: 407 values, 1,635B raw, 1,491B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,657B for [ps_partkey] INT32: 445 values, 1,787B raw, 1,621B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,786B for [value] DOUBLE: 407 values, 3,263B raw, 1,742B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,936B for [value] DOUBLE: 445 values, 3,567B raw, 1,892B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,540
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000080
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000078
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000080_0: Committed
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000078_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000081
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000081_0: Committed
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,727B for [ps_partkey] INT32: 464 values, 1,863B raw, 1,691B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,995B for [value] DOUBLE: 464 values, 3,719B raw, 1,951B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO Executor: Finished task 81.0 in stage 10.0 (TID 1130). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 78.0 in stage 10.0 (TID 1127). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 80.0 in stage 10.0 (TID 1129). 843 bytes result sent to driver
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,088
15/08/21 19:47:54 INFO TaskSetManager: Starting task 98.0 in stage 10.0 (TID 1147, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 98.0 in stage 10.0 (TID 1147)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 99.0 in stage 10.0 (TID 1148, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 99.0 in stage 10.0 (TID 1148)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 100.0 in stage 10.0 (TID 1149, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,815B for [ps_partkey] INT32: 491 values, 1,971B raw, 1,779B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,126B for [value] DOUBLE: 491 values, 3,935B raw, 2,082B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO TaskSetManager: Finished task 80.0 in stage 10.0 (TID 1129) in 682 ms on localhost (83/200)
15/08/21 19:47:54 INFO Executor: Running task 100.0 in stage 10.0 (TID 1149)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 81.0 in stage 10.0 (TID 1130) in 678 ms on localhost (84/200)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 78.0 in stage 10.0 (TID 1127) in 728 ms on localhost (85/200)
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,836
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,602B for [ps_partkey] INT32: 428 values, 1,719B raw, 1,566B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,861B for [value] DOUBLE: 428 values, 3,431B raw, 1,817B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,536
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,892B for [ps_partkey] INT32: 513 values, 2,059B raw, 1,856B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,191B for [value] DOUBLE: 513 values, 4,111B raw, 2,147B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000094
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000094_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 94.0 in stage 10.0 (TID 1143). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 101.0 in stage 10.0 (TID 1150, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 101.0 in stage 10.0 (TID 1150)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 94.0 in stage 10.0 (TID 1143) in 247 ms on localhost (86/200)
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000082
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000082_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 82.0 in stage 10.0 (TID 1131). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 102.0 in stage 10.0 (TID 1151, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 102.0 in stage 10.0 (TID 1151)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 82.0 in stage 10.0 (TID 1131) in 675 ms on localhost (87/200)
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,944
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,969B for [ps_partkey] INT32: 535 values, 2,147B raw, 1,933B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,259B for [value] DOUBLE: 535 values, 4,287B raw, 2,215B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,528
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,898B for [ps_partkey] INT32: 513 values, 2,059B raw, 1,862B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,216B for [value] DOUBLE: 513 values, 4,111B raw, 2,172B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,076
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,992B for [ps_partkey] INT32: 540 values, 2,167B raw, 1,956B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,292B for [value] DOUBLE: 540 values, 4,327B raw, 2,248B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000085
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000085_0: Committed
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO Executor: Finished task 85.0 in stage 10.0 (TID 1134). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 103.0 in stage 10.0 (TID 1152, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 103.0 in stage 10.0 (TID 1152)
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000084
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000083
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000084_0: Committed
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000083_0: Committed
15/08/21 19:47:54 INFO TaskSetManager: Finished task 85.0 in stage 10.0 (TID 1134) in 787 ms on localhost (88/200)
15/08/21 19:47:54 INFO Executor: Finished task 84.0 in stage 10.0 (TID 1133). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 104.0 in stage 10.0 (TID 1153, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Finished task 83.0 in stage 10.0 (TID 1132). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Running task 104.0 in stage 10.0 (TID 1153)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 105.0 in stage 10.0 (TID 1154, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 105.0 in stage 10.0 (TID 1154)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 84.0 in stage 10.0 (TID 1133) in 805 ms on localhost (89/200)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 83.0 in stage 10.0 (TID 1132) in 809 ms on localhost (90/200)
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000089
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000089_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 89.0 in stage 10.0 (TID 1138). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 106.0 in stage 10.0 (TID 1155, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 106.0 in stage 10.0 (TID 1155)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 89.0 in stage 10.0 (TID 1138) in 676 ms on localhost (91/200)
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,636
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,080B for [ps_partkey] INT32: 568 values, 2,279B raw, 2,044B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,421B for [value] DOUBLE: 568 values, 4,551B raw, 2,377B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,100
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,474B for [ps_partkey] INT32: 392 values, 1,575B raw, 1,438B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,724B for [value] DOUBLE: 392 values, 3,143B raw, 1,680B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000091
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000091_0: Committed
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000101
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000101_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000090
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000090_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 91.0 in stage 10.0 (TID 1140). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 101.0 in stage 10.0 (TID 1150). 843 bytes result sent to driver
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO Executor: Finished task 90.0 in stage 10.0 (TID 1139). 843 bytes result sent to driver
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO TaskSetManager: Starting task 107.0 in stage 10.0 (TID 1156, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO Executor: Running task 107.0 in stage 10.0 (TID 1156)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 108.0 in stage 10.0 (TID 1157, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 108.0 in stage 10.0 (TID 1157)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 109.0 in stage 10.0 (TID 1158, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 109.0 in stage 10.0 (TID 1158)
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO TaskSetManager: Finished task 101.0 in stage 10.0 (TID 1150) in 268 ms on localhost (92/200)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 91.0 in stage 10.0 (TID 1140) in 632 ms on localhost (93/200)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 90.0 in stage 10.0 (TID 1139) in 647 ms on localhost (94/200)
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,240
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000092
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000092_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000093
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000093_0: Committed
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,136
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,669B for [ps_partkey] INT32: 449 values, 1,803B raw, 1,633B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,916B for [value] DOUBLE: 449 values, 3,599B raw, 1,872B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO Executor: Finished task 93.0 in stage 10.0 (TID 1142). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 92.0 in stage 10.0 (TID 1141). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 110.0 in stage 10.0 (TID 1159, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 110.0 in stage 10.0 (TID 1159)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 111.0 in stage 10.0 (TID 1160, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 111.0 in stage 10.0 (TID 1160)
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,473B for [ps_partkey] INT32: 393 values, 1,579B raw, 1,437B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,228
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,705B for [value] DOUBLE: 393 values, 3,151B raw, 1,661B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,408
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,497B for [ps_partkey] INT32: 398 values, 1,599B raw, 1,461B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO TaskSetManager: Finished task 92.0 in stage 10.0 (TID 1141) in 631 ms on localhost (95/200)
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,715B for [value] DOUBLE: 398 values, 3,191B raw, 1,671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO TaskSetManager: Finished task 93.0 in stage 10.0 (TID 1142) in 622 ms on localhost (96/200)
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,707B for [ps_partkey] INT32: 457 values, 1,835B raw, 1,671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,962B for [value] DOUBLE: 457 values, 3,663B raw, 1,918B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000104
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000104_0: Committed
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO Executor: Finished task 104.0 in stage 10.0 (TID 1153). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 112.0 in stage 10.0 (TID 1161, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 112.0 in stage 10.0 (TID 1161)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 104.0 in stage 10.0 (TID 1153) in 197 ms on localhost (97/200)
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000097
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000097_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 97.0 in stage 10.0 (TID 1146). 843 bytes result sent to driver
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000096
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000096_0: Committed
15/08/21 19:47:54 INFO TaskSetManager: Starting task 113.0 in stage 10.0 (TID 1162, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 113.0 in stage 10.0 (TID 1162)
15/08/21 19:47:54 INFO Executor: Finished task 96.0 in stage 10.0 (TID 1145). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Finished task 97.0 in stage 10.0 (TID 1146) in 631 ms on localhost (98/200)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 114.0 in stage 10.0 (TID 1163, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 114.0 in stage 10.0 (TID 1163)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 96.0 in stage 10.0 (TID 1145) in 640 ms on localhost (99/200)
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000095
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000095_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 95.0 in stage 10.0 (TID 1144). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 115.0 in stage 10.0 (TID 1164, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 115.0 in stage 10.0 (TID 1164)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 95.0 in stage 10.0 (TID 1144) in 654 ms on localhost (100/200)
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,068
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,040
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,818B for [ps_partkey] INT32: 490 values, 1,967B raw, 1,782B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,076B for [value] DOUBLE: 490 values, 3,927B raw, 2,032B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,805B for [ps_partkey] INT32: 489 values, 1,963B raw, 1,769B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,081B for [value] DOUBLE: 489 values, 3,919B raw, 2,037B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,636
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,906B for [ps_partkey] INT32: 518 values, 2,079B raw, 1,870B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,197B for [value] DOUBLE: 518 values, 4,151B raw, 2,153B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,156
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,916
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,780B for [ps_partkey] INT32: 482 values, 1,935B raw, 1,744B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,826B for [ps_partkey] INT32: 494 values, 1,983B raw, 1,790B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,057B for [value] DOUBLE: 482 values, 3,863B raw, 2,013B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,124B for [value] DOUBLE: 494 values, 3,959B raw, 2,080B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,456
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,488
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,516
15/08/21 19:47:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,088
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,706B for [ps_partkey] INT32: 459 values, 1,843B raw, 1,670B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,541B for [ps_partkey] INT32: 411 values, 1,651B raw, 1,505B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,976B for [value] DOUBLE: 459 values, 3,679B raw, 1,932B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,748B for [value] DOUBLE: 411 values, 3,295B raw, 1,704B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,889B for [ps_partkey] INT32: 512 values, 2,055B raw, 1,853B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 2,185B for [value] DOUBLE: 512 values, 4,103B raw, 2,141B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,651B for [ps_partkey] INT32: 441 values, 1,771B raw, 1,615B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO ColumnChunkPageWriteStore: written 1,931B for [value] DOUBLE: 441 values, 3,535B raw, 1,887B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000102
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000102_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000100
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000100_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000099
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000099_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000098
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000098_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 102.0 in stage 10.0 (TID 1151). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 98.0 in stage 10.0 (TID 1147). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 100.0 in stage 10.0 (TID 1149). 843 bytes result sent to driver
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000109
15/08/21 19:47:54 INFO Executor: Finished task 99.0 in stage 10.0 (TID 1148). 843 bytes result sent to driver
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000109_0: Committed
15/08/21 19:47:54 INFO TaskSetManager: Starting task 116.0 in stage 10.0 (TID 1165, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Finished task 109.0 in stage 10.0 (TID 1158). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Finished task 102.0 in stage 10.0 (TID 1151) in 769 ms on localhost (101/200)
15/08/21 19:47:54 INFO Executor: Running task 116.0 in stage 10.0 (TID 1165)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 117.0 in stage 10.0 (TID 1166, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 117.0 in stage 10.0 (TID 1166)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 118.0 in stage 10.0 (TID 1167, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 118.0 in stage 10.0 (TID 1167)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 119.0 in stage 10.0 (TID 1168, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 119.0 in stage 10.0 (TID 1168)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 120.0 in stage 10.0 (TID 1169, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 98.0 in stage 10.0 (TID 1147) in 862 ms on localhost (102/200)
15/08/21 19:47:54 INFO Executor: Running task 120.0 in stage 10.0 (TID 1169)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 99.0 in stage 10.0 (TID 1148) in 862 ms on localhost (103/200)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 100.0 in stage 10.0 (TID 1149) in 865 ms on localhost (104/200)
15/08/21 19:47:54 INFO TaskSetManager: Finished task 109.0 in stage 10.0 (TID 1158) in 520 ms on localhost (105/200)
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000105
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000105_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000103
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000103_0: Committed
15/08/21 19:47:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000111
15/08/21 19:47:54 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000111_0: Committed
15/08/21 19:47:54 INFO Executor: Finished task 103.0 in stage 10.0 (TID 1152). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 111.0 in stage 10.0 (TID 1160). 843 bytes result sent to driver
15/08/21 19:47:54 INFO Executor: Finished task 105.0 in stage 10.0 (TID 1154). 843 bytes result sent to driver
15/08/21 19:47:54 INFO TaskSetManager: Starting task 121.0 in stage 10.0 (TID 1170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 121.0 in stage 10.0 (TID 1170)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 122.0 in stage 10.0 (TID 1171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 122.0 in stage 10.0 (TID 1171)
15/08/21 19:47:54 INFO TaskSetManager: Starting task 123.0 in stage 10.0 (TID 1172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:54 INFO Executor: Running task 123.0 in stage 10.0 (TID 1172)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 103.0 in stage 10.0 (TID 1152) in 660 ms on localhost (106/200)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 105.0 in stage 10.0 (TID 1154) in 652 ms on localhost (107/200)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO TaskSetManager: Finished task 111.0 in stage 10.0 (TID 1160) in 518 ms on localhost (108/200)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000106
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000106_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 106.0 in stage 10.0 (TID 1155). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 124.0 in stage 10.0 (TID 1173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 124.0 in stage 10.0 (TID 1173)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 106.0 in stage 10.0 (TID 1155) in 661 ms on localhost (109/200)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,536
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,296
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,568
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,717B for [ps_partkey] INT32: 463 values, 1,859B raw, 1,681B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,986B for [value] DOUBLE: 463 values, 3,711B raw, 1,942B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,511B for [ps_partkey] INT32: 401 values, 1,611B raw, 1,475B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,778B for [value] DOUBLE: 401 values, 3,215B raw, 1,734B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,563B for [ps_partkey] INT32: 415 values, 1,667B raw, 1,527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,780B for [value] DOUBLE: 415 values, 3,327B raw, 1,736B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,536
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,552B for [ps_partkey] INT32: 413 values, 1,659B raw, 1,516B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,798B for [value] DOUBLE: 413 values, 3,311B raw, 1,754B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,588
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,735B for [ps_partkey] INT32: 466 values, 1,871B raw, 1,699B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,006B for [value] DOUBLE: 466 values, 3,735B raw, 1,962B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,396
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,700B for [ps_partkey] INT32: 456 values, 1,831B raw, 1,664B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,972B for [value] DOUBLE: 456 values, 3,655B raw, 1,928B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,956
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,622B for [ps_partkey] INT32: 434 values, 1,743B raw, 1,586B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,901B for [value] DOUBLE: 434 values, 3,479B raw, 1,857B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,756
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,757B for [ps_partkey] INT32: 474 values, 1,903B raw, 1,721B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,019B for [value] DOUBLE: 474 values, 3,799B raw, 1,975B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,288
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,508B for [ps_partkey] INT32: 401 values, 1,611B raw, 1,472B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,725B for [value] DOUBLE: 401 values, 3,215B raw, 1,681B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000108
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000108_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 108.0 in stage 10.0 (TID 1157). 843 bytes result sent to driver
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000107
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000107_0: Committed
15/08/21 19:47:55 INFO TaskSetManager: Starting task 125.0 in stage 10.0 (TID 1174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Finished task 107.0 in stage 10.0 (TID 1156). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Running task 125.0 in stage 10.0 (TID 1174)
15/08/21 19:47:55 INFO TaskSetManager: Starting task 126.0 in stage 10.0 (TID 1175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 126.0 in stage 10.0 (TID 1175)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 108.0 in stage 10.0 (TID 1157) in 798 ms on localhost (110/200)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 107.0 in stage 10.0 (TID 1156) in 802 ms on localhost (111/200)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000124
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000124_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 124.0 in stage 10.0 (TID 1173). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 127.0 in stage 10.0 (TID 1176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 127.0 in stage 10.0 (TID 1176)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 124.0 in stage 10.0 (TID 1173) in 260 ms on localhost (112/200)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000110
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000110_0: Committed
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO Executor: Finished task 110.0 in stage 10.0 (TID 1159). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 128.0 in stage 10.0 (TID 1177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 128.0 in stage 10.0 (TID 1177)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 110.0 in stage 10.0 (TID 1159) in 838 ms on localhost (113/200)
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,036
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,916
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,806B for [ps_partkey] INT32: 488 values, 1,959B raw, 1,770B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,789B for [ps_partkey] INT32: 482 values, 1,935B raw, 1,753B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,061B for [value] DOUBLE: 482 values, 3,863B raw, 2,017B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,057B for [value] DOUBLE: 488 values, 3,911B raw, 2,013B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000112
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000112_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 112.0 in stage 10.0 (TID 1161). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 129.0 in stage 10.0 (TID 1178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 129.0 in stage 10.0 (TID 1178)
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,156
15/08/21 19:47:55 INFO TaskSetManager: Finished task 112.0 in stage 10.0 (TID 1161) in 863 ms on localhost (114/200)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000126
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000126_0: Committed
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,007B for [ps_partkey] INT32: 544 values, 2,183B raw, 1,971B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,294B for [value] DOUBLE: 544 values, 4,359B raw, 2,250B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO Executor: Finished task 126.0 in stage 10.0 (TID 1175). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 130.0 in stage 10.0 (TID 1179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 130.0 in stage 10.0 (TID 1179)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 126.0 in stage 10.0 (TID 1175) in 175 ms on localhost (115/200)
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000113
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000113_0: Committed
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000114
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000114_0: Committed
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO Executor: Finished task 114.0 in stage 10.0 (TID 1163). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Finished task 113.0 in stage 10.0 (TID 1162). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 131.0 in stage 10.0 (TID 1180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 131.0 in stage 10.0 (TID 1180)
15/08/21 19:47:55 INFO TaskSetManager: Starting task 132.0 in stage 10.0 (TID 1181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO Executor: Running task 132.0 in stage 10.0 (TID 1181)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000115
15/08/21 19:47:55 INFO TaskSetManager: Finished task 114.0 in stage 10.0 (TID 1163) in 877 ms on localhost (116/200)
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000115_0: Committed
15/08/21 19:47:55 INFO TaskSetManager: Finished task 113.0 in stage 10.0 (TID 1162) in 880 ms on localhost (117/200)
15/08/21 19:47:55 INFO Executor: Finished task 115.0 in stage 10.0 (TID 1164). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 133.0 in stage 10.0 (TID 1182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 133.0 in stage 10.0 (TID 1182)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 115.0 in stage 10.0 (TID 1164) in 872 ms on localhost (118/200)
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,728
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,414B for [ps_partkey] INT32: 373 values, 1,499B raw, 1,378B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,625B for [value] DOUBLE: 373 values, 2,991B raw, 1,581B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,308
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,528
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,859B for [ps_partkey] INT32: 502 values, 2,015B raw, 1,823B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,143B for [value] DOUBLE: 502 values, 4,023B raw, 2,099B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,059B for [ps_partkey] INT32: 563 values, 2,259B raw, 2,023B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,356B for [value] DOUBLE: 563 values, 4,511B raw, 2,312B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000128
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000128_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 128.0 in stage 10.0 (TID 1177). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 134.0 in stage 10.0 (TID 1183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 134.0 in stage 10.0 (TID 1183)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000130
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000130_0: Committed
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000129
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000129_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 129.0 in stage 10.0 (TID 1178). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Finished task 130.0 in stage 10.0 (TID 1179). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 135.0 in stage 10.0 (TID 1184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 128.0 in stage 10.0 (TID 1177) in 268 ms on localhost (119/200)
15/08/21 19:47:55 INFO Executor: Running task 135.0 in stage 10.0 (TID 1184)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 129.0 in stage 10.0 (TID 1178) in 195 ms on localhost (120/200)
15/08/21 19:47:55 INFO TaskSetManager: Starting task 136.0 in stage 10.0 (TID 1185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 130.0 in stage 10.0 (TID 1179) in 192 ms on localhost (121/200)
15/08/21 19:47:55 INFO Executor: Running task 136.0 in stage 10.0 (TID 1185)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000119
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000119_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 119.0 in stage 10.0 (TID 1168). 843 bytes result sent to driver
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000116
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000120
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000116_0: Committed
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000120_0: Committed
15/08/21 19:47:55 INFO TaskSetManager: Finished task 119.0 in stage 10.0 (TID 1168) in 662 ms on localhost (122/200)
15/08/21 19:47:55 INFO Executor: Finished task 116.0 in stage 10.0 (TID 1165). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Finished task 120.0 in stage 10.0 (TID 1169). 843 bytes result sent to driver
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,648
15/08/21 19:47:55 INFO TaskSetManager: Starting task 137.0 in stage 10.0 (TID 1186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 137.0 in stage 10.0 (TID 1186)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000117
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000117_0: Committed
15/08/21 19:47:55 INFO TaskSetManager: Starting task 138.0 in stage 10.0 (TID 1187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 138.0 in stage 10.0 (TID 1187)
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,276
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,571B for [ps_partkey] INT32: 419 values, 1,683B raw, 1,535B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO Executor: Finished task 117.0 in stage 10.0 (TID 1166). 843 bytes result sent to driver
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,811B for [value] DOUBLE: 419 values, 3,359B raw, 1,767B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,036
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,504B for [ps_partkey] INT32: 400 values, 1,607B raw, 1,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,729B for [value] DOUBLE: 400 values, 3,207B raw, 1,685B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO TaskSetManager: Starting task 139.0 in stage 10.0 (TID 1188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 116.0 in stage 10.0 (TID 1165) in 675 ms on localhost (123/200)
15/08/21 19:47:55 INFO Executor: Running task 139.0 in stage 10.0 (TID 1188)
15/08/21 19:47:55 INFO TaskSetManager: Starting task 140.0 in stage 10.0 (TID 1189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 140.0 in stage 10.0 (TID 1189)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 120.0 in stage 10.0 (TID 1169) in 676 ms on localhost (124/200)
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,637B for [ps_partkey] INT32: 438 values, 1,759B raw, 1,601B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO TaskSetManager: Finished task 117.0 in stage 10.0 (TID 1166) in 682 ms on localhost (125/200)
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,861B for [value] DOUBLE: 438 values, 3,511B raw, 1,817B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000121
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000118
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000121_0: Committed
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000118_0: Committed
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000122
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000122_0: Committed
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000123
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000123_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 123.0 in stage 10.0 (TID 1172). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Finished task 122.0 in stage 10.0 (TID 1171). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Finished task 121.0 in stage 10.0 (TID 1170). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Finished task 118.0 in stage 10.0 (TID 1167). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 141.0 in stage 10.0 (TID 1190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 141.0 in stage 10.0 (TID 1190)
15/08/21 19:47:55 INFO TaskSetManager: Starting task 142.0 in stage 10.0 (TID 1191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 142.0 in stage 10.0 (TID 1191)
15/08/21 19:47:55 INFO TaskSetManager: Starting task 143.0 in stage 10.0 (TID 1192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 122.0 in stage 10.0 (TID 1171) in 676 ms on localhost (126/200)
15/08/21 19:47:55 INFO Executor: Running task 143.0 in stage 10.0 (TID 1192)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO TaskSetManager: Finished task 123.0 in stage 10.0 (TID 1172) in 676 ms on localhost (127/200)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 121.0 in stage 10.0 (TID 1170) in 683 ms on localhost (128/200)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO TaskSetManager: Starting task 144.0 in stage 10.0 (TID 1193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO TaskSetManager: Finished task 118.0 in stage 10.0 (TID 1167) in 724 ms on localhost (129/200)
15/08/21 19:47:55 INFO Executor: Running task 144.0 in stage 10.0 (TID 1193)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000132
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000132_0: Committed
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO Executor: Finished task 132.0 in stage 10.0 (TID 1181). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 145.0 in stage 10.0 (TID 1194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 145.0 in stage 10.0 (TID 1194)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO TaskSetManager: Finished task 132.0 in stage 10.0 (TID 1181) in 252 ms on localhost (130/200)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,956
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,790B for [ps_partkey] INT32: 484 values, 1,943B raw, 1,754B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,588
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,052B for [value] DOUBLE: 484 values, 3,879B raw, 2,008B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,748
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,909B for [ps_partkey] INT32: 516 values, 2,071B raw, 1,873B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,158B for [value] DOUBLE: 516 values, 4,135B raw, 2,114B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,796
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,588B for [ps_partkey] INT32: 424 values, 1,703B raw, 1,552B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,815B for [value] DOUBLE: 424 values, 3,399B raw, 1,771B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,754B for [ps_partkey] INT32: 476 values, 1,911B raw, 1,718B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,596
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,027B for [value] DOUBLE: 476 values, 3,815B raw, 1,983B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,557B for [ps_partkey] INT32: 416 values, 1,671B raw, 1,521B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,806B for [value] DOUBLE: 416 values, 3,335B raw, 1,762B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,768
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,476
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,593B for [ps_partkey] INT32: 425 values, 1,707B raw, 1,557B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,797B for [value] DOUBLE: 425 values, 3,407B raw, 1,753B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,888B for [ps_partkey] INT32: 510 values, 2,047B raw, 1,852B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,149B for [value] DOUBLE: 510 values, 4,087B raw, 2,105B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000125
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000125_0: Committed
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO Executor: Finished task 125.0 in stage 10.0 (TID 1174). 843 bytes result sent to driver
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO TaskSetManager: Starting task 146.0 in stage 10.0 (TID 1195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 146.0 in stage 10.0 (TID 1195)
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO TaskSetManager: Finished task 125.0 in stage 10.0 (TID 1174) in 626 ms on localhost (131/200)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000135
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000135_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 135.0 in stage 10.0 (TID 1184). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 147.0 in stage 10.0 (TID 1196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 147.0 in stage 10.0 (TID 1196)
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000127
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000137
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000127_0: Committed
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000137_0: Committed
15/08/21 19:47:55 INFO TaskSetManager: Finished task 135.0 in stage 10.0 (TID 1184) in 298 ms on localhost (132/200)
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,668
15/08/21 19:47:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,300
15/08/21 19:47:55 INFO Executor: Finished task 127.0 in stage 10.0 (TID 1176). 843 bytes result sent to driver
15/08/21 19:47:55 INFO Executor: Finished task 137.0 in stage 10.0 (TID 1186). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 148.0 in stage 10.0 (TID 1197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 148.0 in stage 10.0 (TID 1197)
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,091B for [ps_partkey] INT32: 570 values, 2,287B raw, 2,055B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,463B for [value] DOUBLE: 570 values, 4,567B raw, 2,419B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO TaskSetManager: Starting task 149.0 in stage 10.0 (TID 1198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,851B for [ps_partkey] INT32: 502 values, 2,015B raw, 1,815B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,101B for [value] DOUBLE: 502 values, 4,023B raw, 2,057B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO Executor: Running task 149.0 in stage 10.0 (TID 1198)
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,000
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,596
15/08/21 19:47:55 INFO TaskSetManager: Finished task 137.0 in stage 10.0 (TID 1186) in 300 ms on localhost (133/200)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 127.0 in stage 10.0 (TID 1176) in 640 ms on localhost (134/200)
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,727B for [ps_partkey] INT32: 466 values, 1,871B raw, 1,691B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,092B for [value] DOUBLE: 466 values, 3,735B raw, 2,048B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,975B for [ps_partkey] INT32: 537 values, 2,155B raw, 1,939B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,244B for [value] DOUBLE: 537 values, 4,303B raw, 2,200B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,956
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 1,794B for [ps_partkey] INT32: 484 values, 1,943B raw, 1,758B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO ColumnChunkPageWriteStore: written 2,131B for [value] DOUBLE: 484 values, 3,879B raw, 2,087B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000134
15/08/21 19:47:55 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000134_0: Committed
15/08/21 19:47:55 INFO Executor: Finished task 134.0 in stage 10.0 (TID 1183). 843 bytes result sent to driver
15/08/21 19:47:55 INFO TaskSetManager: Starting task 150.0 in stage 10.0 (TID 1199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:55 INFO Executor: Running task 150.0 in stage 10.0 (TID 1199)
15/08/21 19:47:55 INFO TaskSetManager: Finished task 134.0 in stage 10.0 (TID 1183) in 393 ms on localhost (135/200)
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:55 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:55 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000142
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000142_0: Committed
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO Executor: Finished task 142.0 in stage 10.0 (TID 1191). 843 bytes result sent to driver
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 19:47:56 INFO TaskSetManager: Starting task 151.0 in stage 10.0 (TID 1200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 151.0 in stage 10.0 (TID 1200)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 142.0 in stage 10.0 (TID 1191) in 374 ms on localhost (136/200)
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,420
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,362B for [ps_partkey] INT32: 358 values, 1,439B raw, 1,326B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,636B for [value] DOUBLE: 358 values, 2,871B raw, 1,592B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,436
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,862B for [ps_partkey] INT32: 508 values, 2,039B raw, 1,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,242B for [value] DOUBLE: 508 values, 4,071B raw, 2,198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,028
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,108
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,803B for [ps_partkey] INT32: 488 values, 1,959B raw, 1,767B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,817B for [ps_partkey] INT32: 492 values, 1,975B raw, 1,781B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,159B for [value] DOUBLE: 488 values, 3,911B raw, 2,115B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,143B for [value] DOUBLE: 492 values, 3,943B raw, 2,099B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000131
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000133
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000131_0: Committed
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000133_0: Committed
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO Executor: Finished task 133.0 in stage 10.0 (TID 1182). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 131.0 in stage 10.0 (TID 1180). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 152.0 in stage 10.0 (TID 1201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 152.0 in stage 10.0 (TID 1201)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 153.0 in stage 10.0 (TID 1202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 153.0 in stage 10.0 (TID 1202)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 131.0 in stage 10.0 (TID 1180) in 680 ms on localhost (137/200)
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO TaskSetManager: Finished task 133.0 in stage 10.0 (TID 1182) in 676 ms on localhost (138/200)
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000149
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000149_0: Committed
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000146
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000146_0: Committed
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000148
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000148_0: Committed
15/08/21 19:47:56 INFO Executor: Finished task 146.0 in stage 10.0 (TID 1195). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 149.0 in stage 10.0 (TID 1198). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 148.0 in stage 10.0 (TID 1197). 843 bytes result sent to driver
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,288
15/08/21 19:47:56 INFO TaskSetManager: Starting task 154.0 in stage 10.0 (TID 1203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 154.0 in stage 10.0 (TID 1203)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 155.0 in stage 10.0 (TID 1204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 149.0 in stage 10.0 (TID 1198) in 238 ms on localhost (139/200)
15/08/21 19:47:56 INFO Executor: Running task 155.0 in stage 10.0 (TID 1204)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 146.0 in stage 10.0 (TID 1195) in 303 ms on localhost (140/200)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 156.0 in stage 10.0 (TID 1205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,681B for [ps_partkey] INT32: 451 values, 1,811B raw, 1,645B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO Executor: Running task 156.0 in stage 10.0 (TID 1205)
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,024B for [value] DOUBLE: 451 values, 3,615B raw, 1,980B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO TaskSetManager: Finished task 148.0 in stage 10.0 (TID 1197) in 246 ms on localhost (141/200)
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,456
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,232B for [ps_partkey] INT32: 609 values, 2,443B raw, 2,196B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,639B for [value] DOUBLE: 609 values, 4,879B raw, 2,595B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000150
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000150_0: Committed
15/08/21 19:47:56 INFO Executor: Finished task 150.0 in stage 10.0 (TID 1199). 843 bytes result sent to driver
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO TaskSetManager: Starting task 157.0 in stage 10.0 (TID 1206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO Executor: Running task 157.0 in stage 10.0 (TID 1206)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 150.0 in stage 10.0 (TID 1199) in 276 ms on localhost (142/200)
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000136
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000136_0: Committed
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000138
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000138_0: Committed
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000139
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000139_0: Committed
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO Executor: Finished task 138.0 in stage 10.0 (TID 1187). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 136.0 in stage 10.0 (TID 1185). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 158.0 in stage 10.0 (TID 1207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 158.0 in stage 10.0 (TID 1207)
15/08/21 19:47:56 INFO Executor: Finished task 139.0 in stage 10.0 (TID 1188). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 159.0 in stage 10.0 (TID 1208, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,140
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO TaskSetManager: Starting task 160.0 in stage 10.0 (TID 1209, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 159.0 in stage 10.0 (TID 1208)
15/08/21 19:47:56 INFO Executor: Running task 160.0 in stage 10.0 (TID 1209)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 138.0 in stage 10.0 (TID 1187) in 649 ms on localhost (143/200)
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO TaskSetManager: Finished task 136.0 in stage 10.0 (TID 1185) in 673 ms on localhost (144/200)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 139.0 in stage 10.0 (TID 1188) in 650 ms on localhost (145/200)
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,826B for [ps_partkey] INT32: 494 values, 1,983B raw, 1,790B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,209B for [value] DOUBLE: 494 values, 3,959B raw, 2,165B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,168
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,654B for [ps_partkey] INT32: 445 values, 1,787B raw, 1,618B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,999B for [value] DOUBLE: 445 values, 3,567B raw, 1,955B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,336
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,368
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000152
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000152_0: Committed
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,697B for [ps_partkey] INT32: 455 values, 1,827B raw, 1,661B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,032B for [value] DOUBLE: 455 values, 3,647B raw, 1,988B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,866B for [ps_partkey] INT32: 505 values, 2,027B raw, 1,830B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,245B for [value] DOUBLE: 505 values, 4,047B raw, 2,201B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,656
15/08/21 19:47:56 INFO Executor: Finished task 152.0 in stage 10.0 (TID 1201). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 161.0 in stage 10.0 (TID 1210, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 161.0 in stage 10.0 (TID 1210)
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,909B for [ps_partkey] INT32: 519 values, 2,083B raw, 1,873B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,273B for [value] DOUBLE: 519 values, 4,159B raw, 2,229B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000153
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000153_0: Committed
15/08/21 19:47:56 INFO TaskSetManager: Finished task 152.0 in stage 10.0 (TID 1201) in 229 ms on localhost (146/200)
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO Executor: Finished task 153.0 in stage 10.0 (TID 1202). 843 bytes result sent to driver
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO TaskSetManager: Starting task 162.0 in stage 10.0 (TID 1211, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO Executor: Running task 162.0 in stage 10.0 (TID 1211)
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO TaskSetManager: Finished task 153.0 in stage 10.0 (TID 1202) in 234 ms on localhost (147/200)
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000140
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000140_0: Committed
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000155
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000155_0: Committed
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000145
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000145_0: Committed
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO Executor: Finished task 140.0 in stage 10.0 (TID 1189). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 155.0 in stage 10.0 (TID 1204). 843 bytes result sent to driver
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO TaskSetManager: Starting task 163.0 in stage 10.0 (TID 1212, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 163.0 in stage 10.0 (TID 1212)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 164.0 in stage 10.0 (TID 1213, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Finished task 145.0 in stage 10.0 (TID 1194). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 165.0 in stage 10.0 (TID 1214, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 155.0 in stage 10.0 (TID 1204) in 238 ms on localhost (148/200)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 140.0 in stage 10.0 (TID 1189) in 765 ms on localhost (149/200)
15/08/21 19:47:56 INFO Executor: Running task 165.0 in stage 10.0 (TID 1214)
15/08/21 19:47:56 INFO Executor: Running task 164.0 in stage 10.0 (TID 1213)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 145.0 in stage 10.0 (TID 1194) in 719 ms on localhost (150/200)
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000141
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000143
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000141_0: Committed
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000143_0: Committed
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,860
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000144
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000144_0: Committed
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,611B for [ps_partkey] INT32: 430 values, 1,727B raw, 1,575B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO Executor: Finished task 143.0 in stage 10.0 (TID 1192). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 141.0 in stage 10.0 (TID 1190). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 144.0 in stage 10.0 (TID 1193). 843 bytes result sent to driver
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,949B for [value] DOUBLE: 430 values, 3,447B raw, 1,905B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO TaskSetManager: Starting task 166.0 in stage 10.0 (TID 1215, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 167.0 in stage 10.0 (TID 1216, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 167.0 in stage 10.0 (TID 1216)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 168.0 in stage 10.0 (TID 1217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 168.0 in stage 10.0 (TID 1217)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 143.0 in stage 10.0 (TID 1192) in 766 ms on localhost (151/200)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 144.0 in stage 10.0 (TID 1193) in 760 ms on localhost (152/200)
15/08/21 19:47:56 INFO Executor: Running task 166.0 in stage 10.0 (TID 1215)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 141.0 in stage 10.0 (TID 1190) in 773 ms on localhost (153/200)
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,068
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,807B for [ps_partkey] INT32: 490 values, 1,967B raw, 1,771B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,155B for [value] DOUBLE: 490 values, 3,927B raw, 2,111B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,596
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,976
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,560B for [ps_partkey] INT32: 416 values, 1,671B raw, 1,524B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,883B for [value] DOUBLE: 416 values, 3,335B raw, 1,839B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,134B for [ps_partkey] INT32: 585 values, 2,347B raw, 2,098B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,547B for [value] DOUBLE: 585 values, 4,687B raw, 2,503B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000159
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000159_0: Committed
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000147
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000147_0: Committed
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO Executor: Finished task 147.0 in stage 10.0 (TID 1196). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 159.0 in stage 10.0 (TID 1208). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 169.0 in stage 10.0 (TID 1218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO Executor: Running task 169.0 in stage 10.0 (TID 1218)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 170.0 in stage 10.0 (TID 1219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 170.0 in stage 10.0 (TID 1219)
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO TaskSetManager: Finished task 159.0 in stage 10.0 (TID 1208) in 272 ms on localhost (154/200)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 147.0 in stage 10.0 (TID 1196) in 663 ms on localhost (155/200)
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,868
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,348
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,033B for [ps_partkey] INT32: 554 values, 2,223B raw, 1,997B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,426B for [value] DOUBLE: 554 values, 4,439B raw, 2,382B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,609B for [ps_partkey] INT32: 430 values, 1,727B raw, 1,573B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,914B for [value] DOUBLE: 430 values, 3,447B raw, 1,870B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,868
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,836
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,608B for [ps_partkey] INT32: 430 values, 1,727B raw, 1,572B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,603B for [ps_partkey] INT32: 428 values, 1,719B raw, 1,567B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,894B for [value] DOUBLE: 430 values, 3,447B raw, 1,850B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,929B for [value] DOUBLE: 428 values, 3,431B raw, 1,885B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,936
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,960B for [ps_partkey] INT32: 533 values, 2,139B raw, 1,924B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,116
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,323B for [value] DOUBLE: 533 values, 4,271B raw, 2,279B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,748
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,653B for [ps_partkey] INT32: 442 values, 1,775B raw, 1,617B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,966B for [value] DOUBLE: 442 values, 3,543B raw, 1,922B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,753B for [ps_partkey] INT32: 474 values, 1,903B raw, 1,717B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,108B for [value] DOUBLE: 474 values, 3,799B raw, 2,064B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,896
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,776B for [ps_partkey] INT32: 481 values, 1,931B raw, 1,740B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,132B for [value] DOUBLE: 481 values, 3,855B raw, 2,088B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000151
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000151_0: Committed
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO Executor: Finished task 151.0 in stage 10.0 (TID 1200). 843 bytes result sent to driver
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO TaskSetManager: Starting task 171.0 in stage 10.0 (TID 1220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 171.0 in stage 10.0 (TID 1220)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 151.0 in stage 10.0 (TID 1200) in 628 ms on localhost (156/200)
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000166
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000166_0: Committed
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO Executor: Finished task 166.0 in stage 10.0 (TID 1215). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 172.0 in stage 10.0 (TID 1221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 172.0 in stage 10.0 (TID 1221)
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000167
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000167_0: Committed
15/08/21 19:47:56 INFO TaskSetManager: Finished task 166.0 in stage 10.0 (TID 1215) in 262 ms on localhost (157/200)
15/08/21 19:47:56 INFO Executor: Finished task 167.0 in stage 10.0 (TID 1216). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 173.0 in stage 10.0 (TID 1222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 173.0 in stage 10.0 (TID 1222)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 167.0 in stage 10.0 (TID 1216) in 270 ms on localhost (158/200)
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,608
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,562B for [ps_partkey] INT32: 417 values, 1,675B raw, 1,526B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,856B for [value] DOUBLE: 417 values, 3,343B raw, 1,812B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,556
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,557B for [ps_partkey] INT32: 414 values, 1,663B raw, 1,521B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,872B for [value] DOUBLE: 414 values, 3,319B raw, 1,828B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000154
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000154_0: Committed
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000156
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000156_0: Committed
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000169
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000169_0: Committed
15/08/21 19:47:56 INFO Executor: Finished task 156.0 in stage 10.0 (TID 1205). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Finished task 154.0 in stage 10.0 (TID 1203). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 174.0 in stage 10.0 (TID 1223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Finished task 169.0 in stage 10.0 (TID 1218). 843 bytes result sent to driver
15/08/21 19:47:56 INFO Executor: Running task 174.0 in stage 10.0 (TID 1223)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 175.0 in stage 10.0 (TID 1224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 175.0 in stage 10.0 (TID 1224)
15/08/21 19:47:56 INFO TaskSetManager: Starting task 176.0 in stage 10.0 (TID 1225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 156.0 in stage 10.0 (TID 1205) in 645 ms on localhost (159/200)
15/08/21 19:47:56 INFO Executor: Running task 176.0 in stage 10.0 (TID 1225)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 154.0 in stage 10.0 (TID 1203) in 649 ms on localhost (160/200)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 169.0 in stage 10.0 (TID 1218) in 266 ms on localhost (161/200)
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,700
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,921B for [ps_partkey] INT32: 522 values, 2,095B raw, 1,885B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,269B for [value] DOUBLE: 522 values, 4,183B raw, 2,225B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,888
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,956B for [ps_partkey] INT32: 531 values, 2,131B raw, 1,920B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,345B for [value] DOUBLE: 531 values, 4,255B raw, 2,301B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,420
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,708B for [ps_partkey] INT32: 458 values, 1,839B raw, 1,672B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,031B for [value] DOUBLE: 458 values, 3,671B raw, 1,987B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000157
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000157_0: Committed
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO Executor: Finished task 157.0 in stage 10.0 (TID 1206). 843 bytes result sent to driver
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO TaskSetManager: Starting task 177.0 in stage 10.0 (TID 1226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 177.0 in stage 10.0 (TID 1226)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 157.0 in stage 10.0 (TID 1206) in 653 ms on localhost (162/200)
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000173
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000173_0: Committed
15/08/21 19:47:56 INFO Executor: Finished task 173.0 in stage 10.0 (TID 1222). 843 bytes result sent to driver
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO TaskSetManager: Starting task 178.0 in stage 10.0 (TID 1227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 178.0 in stage 10.0 (TID 1227)
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO TaskSetManager: Finished task 173.0 in stage 10.0 (TID 1222) in 193 ms on localhost (163/200)
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000160
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000160_0: Committed
15/08/21 19:47:56 INFO Executor: Finished task 160.0 in stage 10.0 (TID 1209). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 179.0 in stage 10.0 (TID 1228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 179.0 in stage 10.0 (TID 1228)
15/08/21 19:47:56 INFO TaskSetManager: Finished task 160.0 in stage 10.0 (TID 1209) in 650 ms on localhost (164/200)
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,108
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,616
15/08/21 19:47:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000158
15/08/21 19:47:56 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000158_0: Committed
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,648B for [ps_partkey] INT32: 442 values, 1,775B raw, 1,612B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,986B for [value] DOUBLE: 442 values, 3,543B raw, 1,942B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,725B for [ps_partkey] INT32: 467 values, 1,875B raw, 1,689B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 2,045B for [value] DOUBLE: 467 values, 3,743B raw, 2,001B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,472
15/08/21 19:47:56 INFO Executor: Finished task 158.0 in stage 10.0 (TID 1207). 843 bytes result sent to driver
15/08/21 19:47:56 INFO TaskSetManager: Starting task 180.0 in stage 10.0 (TID 1229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:56 INFO Executor: Running task 180.0 in stage 10.0 (TID 1229)
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,373B for [ps_partkey] INT32: 361 values, 1,451B raw, 1,337B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO ColumnChunkPageWriteStore: written 1,641B for [value] DOUBLE: 361 values, 2,895B raw, 1,597B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:56 INFO TaskSetManager: Finished task 158.0 in stage 10.0 (TID 1207) in 689 ms on localhost (165/200)
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:56 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:56 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000174
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000174_0: Committed
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO Executor: Finished task 174.0 in stage 10.0 (TID 1223). 843 bytes result sent to driver
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO TaskSetManager: Starting task 181.0 in stage 10.0 (TID 1230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO Executor: Running task 181.0 in stage 10.0 (TID 1230)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 174.0 in stage 10.0 (TID 1223) in 217 ms on localhost (166/200)
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,568
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,728B for [ps_partkey] INT32: 465 values, 1,867B raw, 1,692B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,042B for [value] DOUBLE: 465 values, 3,727B raw, 1,998B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000162
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000162_0: Committed
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000161
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000161_0: Committed
15/08/21 19:47:57 INFO Executor: Finished task 161.0 in stage 10.0 (TID 1210). 843 bytes result sent to driver
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO Executor: Finished task 162.0 in stage 10.0 (TID 1211). 843 bytes result sent to driver
15/08/21 19:47:57 INFO TaskSetManager: Starting task 182.0 in stage 10.0 (TID 1231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO TaskSetManager: Starting task 183.0 in stage 10.0 (TID 1232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 182.0 in stage 10.0 (TID 1231)
15/08/21 19:47:57 INFO Executor: Running task 183.0 in stage 10.0 (TID 1232)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 161.0 in stage 10.0 (TID 1210) in 712 ms on localhost (167/200)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 162.0 in stage 10.0 (TID 1211) in 707 ms on localhost (168/200)
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000168
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000168_0: Committed
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000163
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000163_0: Committed
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,288
15/08/21 19:47:57 INFO Executor: Finished task 168.0 in stage 10.0 (TID 1217). 843 bytes result sent to driver
15/08/21 19:47:57 INFO TaskSetManager: Starting task 184.0 in stage 10.0 (TID 1233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 184.0 in stage 10.0 (TID 1233)
15/08/21 19:47:57 INFO Executor: Finished task 163.0 in stage 10.0 (TID 1212). 843 bytes result sent to driver
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,678B for [ps_partkey] INT32: 451 values, 1,811B raw, 1,642B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,019B for [value] DOUBLE: 451 values, 3,615B raw, 1,975B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO TaskSetManager: Starting task 185.0 in stage 10.0 (TID 1234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,596
15/08/21 19:47:57 INFO TaskSetManager: Finished task 168.0 in stage 10.0 (TID 1217) in 654 ms on localhost (169/200)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 163.0 in stage 10.0 (TID 1212) in 702 ms on localhost (170/200)
15/08/21 19:47:57 INFO Executor: Running task 185.0 in stage 10.0 (TID 1234)
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000164
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000164_0: Committed
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,906B for [ps_partkey] INT32: 516 values, 2,071B raw, 1,870B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,246B for [value] DOUBLE: 516 values, 4,135B raw, 2,202B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO Executor: Finished task 164.0 in stage 10.0 (TID 1213). 843 bytes result sent to driver
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000165
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000165_0: Committed
15/08/21 19:47:57 INFO TaskSetManager: Starting task 186.0 in stage 10.0 (TID 1235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 186.0 in stage 10.0 (TID 1235)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 164.0 in stage 10.0 (TID 1213) in 716 ms on localhost (171/200)
15/08/21 19:47:57 INFO Executor: Finished task 165.0 in stage 10.0 (TID 1214). 843 bytes result sent to driver
15/08/21 19:47:57 INFO TaskSetManager: Starting task 187.0 in stage 10.0 (TID 1236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 187.0 in stage 10.0 (TID 1236)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 165.0 in stage 10.0 (TID 1214) in 720 ms on localhost (172/200)
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,732
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,752B for [ps_partkey] INT32: 474 values, 1,903B raw, 1,716B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,107B for [value] DOUBLE: 474 values, 3,799B raw, 2,063B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000177
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000177_0: Committed
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO Executor: Finished task 177.0 in stage 10.0 (TID 1226). 843 bytes result sent to driver
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO TaskSetManager: Starting task 188.0 in stage 10.0 (TID 1237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 188.0 in stage 10.0 (TID 1237)
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000180
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000180_0: Committed
15/08/21 19:47:57 INFO TaskSetManager: Finished task 177.0 in stage 10.0 (TID 1226) in 279 ms on localhost (173/200)
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000179
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000179_0: Committed
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO Executor: Finished task 180.0 in stage 10.0 (TID 1229). 843 bytes result sent to driver
15/08/21 19:47:57 INFO TaskSetManager: Starting task 189.0 in stage 10.0 (TID 1238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Finished task 179.0 in stage 10.0 (TID 1228). 843 bytes result sent to driver
15/08/21 19:47:57 INFO Executor: Running task 189.0 in stage 10.0 (TID 1238)
15/08/21 19:47:57 INFO TaskSetManager: Starting task 190.0 in stage 10.0 (TID 1239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 190.0 in stage 10.0 (TID 1239)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 179.0 in stage 10.0 (TID 1228) in 260 ms on localhost (174/200)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 180.0 in stage 10.0 (TID 1229) in 216 ms on localhost (175/200)
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000170
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000170_0: Committed
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,112
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,236
15/08/21 19:47:57 INFO Executor: Finished task 170.0 in stage 10.0 (TID 1219). 843 bytes result sent to driver
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,822B for [ps_partkey] INT32: 493 values, 1,979B raw, 1,786B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,671B for [ps_partkey] INT32: 448 values, 1,799B raw, 1,635B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,186B for [value] DOUBLE: 493 values, 3,951B raw, 2,142B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,002B for [value] DOUBLE: 448 values, 3,591B raw, 1,958B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO TaskSetManager: Starting task 191.0 in stage 10.0 (TID 1240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 191.0 in stage 10.0 (TID 1240)
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO TaskSetManager: Finished task 170.0 in stage 10.0 (TID 1219) in 668 ms on localhost (176/200)
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,688
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,400
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,053B for [ps_partkey] INT32: 557 values, 2,235B raw, 2,017B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,408B for [value] DOUBLE: 557 values, 4,463B raw, 2,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,574B for [ps_partkey] INT32: 421 values, 1,691B raw, 1,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,877B for [value] DOUBLE: 421 values, 3,375B raw, 1,833B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,476
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,541B for [ps_partkey] INT32: 410 values, 1,647B raw, 1,505B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,850B for [value] DOUBLE: 410 values, 3,287B raw, 1,806B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000172
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000172_0: Committed
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,888
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000171
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000171_0: Committed
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,068
15/08/21 19:47:57 INFO Executor: Finished task 172.0 in stage 10.0 (TID 1221). 843 bytes result sent to driver
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,609B for [ps_partkey] INT32: 431 values, 1,731B raw, 1,573B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,905B for [value] DOUBLE: 431 values, 3,455B raw, 1,861B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO Executor: Finished task 171.0 in stage 10.0 (TID 1220). 843 bytes result sent to driver
15/08/21 19:47:57 INFO TaskSetManager: Starting task 192.0 in stage 10.0 (TID 1241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:57 INFO Executor: Running task 192.0 in stage 10.0 (TID 1241)
15/08/21 19:47:57 INFO TaskSetManager: Starting task 193.0 in stage 10.0 (TID 1242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 193.0 in stage 10.0 (TID 1242)
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,638B for [ps_partkey] INT32: 440 values, 1,767B raw, 1,602B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,970B for [value] DOUBLE: 440 values, 3,527B raw, 1,926B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO TaskSetManager: Finished task 171.0 in stage 10.0 (TID 1220) in 642 ms on localhost (177/200)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 172.0 in stage 10.0 (TID 1221) in 620 ms on localhost (178/200)
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000181
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000181_0: Committed
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,580
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,328
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,732B for [ps_partkey] INT32: 466 values, 1,871B raw, 1,696B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,079B for [value] DOUBLE: 466 values, 3,735B raw, 2,035B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,508B for [ps_partkey] INT32: 403 values, 1,619B raw, 1,472B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,790B for [value] DOUBLE: 403 values, 3,231B raw, 1,746B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO Executor: Finished task 181.0 in stage 10.0 (TID 1230). 843 bytes result sent to driver
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000187
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000187_0: Committed
15/08/21 19:47:57 INFO TaskSetManager: Starting task 194.0 in stage 10.0 (TID 1243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 194.0 in stage 10.0 (TID 1243)
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO Executor: Finished task 187.0 in stage 10.0 (TID 1236). 843 bytes result sent to driver
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO TaskSetManager: Starting task 195.0 in stage 10.0 (TID 1244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO TaskSetManager: Finished task 181.0 in stage 10.0 (TID 1230) in 352 ms on localhost (179/200)
15/08/21 19:47:57 INFO Executor: Running task 195.0 in stage 10.0 (TID 1244)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 187.0 in stage 10.0 (TID 1236) in 258 ms on localhost (180/200)
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,356
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,509B for [ps_partkey] INT32: 404 values, 1,623B raw, 1,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,778B for [value] DOUBLE: 404 values, 3,239B raw, 1,734B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000175
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000175_0: Committed
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000189
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000189_0: Committed
15/08/21 19:47:57 INFO Executor: Finished task 175.0 in stage 10.0 (TID 1224). 843 bytes result sent to driver
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000186
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000176
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000186_0: Committed
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000176_0: Committed
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO Executor: Finished task 189.0 in stage 10.0 (TID 1238). 843 bytes result sent to driver
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO TaskSetManager: Starting task 196.0 in stage 10.0 (TID 1245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 19:47:57 INFO Executor: Running task 196.0 in stage 10.0 (TID 1245)
15/08/21 19:47:57 INFO Executor: Finished task 186.0 in stage 10.0 (TID 1235). 843 bytes result sent to driver
15/08/21 19:47:57 INFO Executor: Finished task 176.0 in stage 10.0 (TID 1225). 843 bytes result sent to driver
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,000
15/08/21 19:47:57 INFO TaskSetManager: Starting task 197.0 in stage 10.0 (TID 1246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 197.0 in stage 10.0 (TID 1246)
15/08/21 19:47:57 INFO TaskSetManager: Starting task 198.0 in stage 10.0 (TID 1247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO Executor: Running task 198.0 in stage 10.0 (TID 1247)
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO TaskSetManager: Starting task 199.0 in stage 10.0 (TID 1248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,631B for [ps_partkey] INT32: 437 values, 1,755B raw, 1,595B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO Executor: Running task 199.0 in stage 10.0 (TID 1248)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 175.0 in stage 10.0 (TID 1224) in 647 ms on localhost (181/200)
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,899B for [value] DOUBLE: 437 values, 3,503B raw, 1,855B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO TaskSetManager: Finished task 186.0 in stage 10.0 (TID 1235) in 344 ms on localhost (182/200)
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO TaskSetManager: Finished task 176.0 in stage 10.0 (TID 1225) in 648 ms on localhost (183/200)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 189.0 in stage 10.0 (TID 1238) in 284 ms on localhost (184/200)
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,476
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,061B for [ps_partkey] INT32: 560 values, 2,247B raw, 2,025B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,410B for [value] DOUBLE: 560 values, 4,487B raw, 2,366B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000191
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000191_0: Committed
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000188
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000188_0: Committed
15/08/21 19:47:57 INFO Executor: Finished task 191.0 in stage 10.0 (TID 1240). 843 bytes result sent to driver
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 19:47:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000178
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000178_0: Committed
15/08/21 19:47:57 INFO Executor: Finished task 188.0 in stage 10.0 (TID 1237). 843 bytes result sent to driver
15/08/21 19:47:57 INFO Executor: Finished task 178.0 in stage 10.0 (TID 1227). 843 bytes result sent to driver
15/08/21 19:47:57 INFO TaskSetManager: Finished task 188.0 in stage 10.0 (TID 1237) in 371 ms on localhost (185/200)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 191.0 in stage 10.0 (TID 1240) in 326 ms on localhost (186/200)
15/08/21 19:47:57 INFO TaskSetManager: Finished task 178.0 in stage 10.0 (TID 1227) in 637 ms on localhost (187/200)
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,476
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,710B for [ps_partkey] INT32: 460 values, 1,847B raw, 1,674B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,008B for [value] DOUBLE: 460 values, 3,687B raw, 1,964B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000192
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000192_0: Committed
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO Executor: Finished task 192.0 in stage 10.0 (TID 1241). 843 bytes result sent to driver
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,428
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO TaskSetManager: Finished task 192.0 in stage 10.0 (TID 1241) in 271 ms on localhost (188/200)
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,041B for [ps_partkey] INT32: 558 values, 2,239B raw, 2,005B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,385B for [value] DOUBLE: 558 values, 4,471B raw, 2,341B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 19:47:57 INFO CodecConfig: Compression: GZIP
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,248
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 19:47:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 19:47:57 INFO ParquetOutputFormat: Validation is off
15/08/21 19:47:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,672B for [ps_partkey] INT32: 449 values, 1,803B raw, 1,636B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,985B for [value] DOUBLE: 449 values, 3,599B raw, 1,941B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000193
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000193_0: Committed
15/08/21 19:47:57 INFO Executor: Finished task 193.0 in stage 10.0 (TID 1242). 843 bytes result sent to driver
15/08/21 19:47:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 19:47:57 INFO TaskSetManager: Finished task 193.0 in stage 10.0 (TID 1242) in 296 ms on localhost (189/200)
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,376
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,108
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,699B for [ps_partkey] INT32: 455 values, 1,827B raw, 1,663B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,027B for [value] DOUBLE: 455 values, 3,647B raw, 1,983B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 1,995B for [ps_partkey] INT32: 542 values, 2,175B raw, 1,959B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,328B for [value] DOUBLE: 542 values, 4,343B raw, 2,284B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000195
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000195_0: Committed
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,068
15/08/21 19:47:57 INFO Executor: Finished task 195.0 in stage 10.0 (TID 1244). 843 bytes result sent to driver
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000194
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000194_0: Committed
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,161B for [ps_partkey] INT32: 590 values, 2,367B raw, 2,125B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO ColumnChunkPageWriteStore: written 2,565B for [value] DOUBLE: 590 values, 4,727B raw, 2,521B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:57 INFO TaskSetManager: Finished task 195.0 in stage 10.0 (TID 1244) in 307 ms on localhost (190/200)
15/08/21 19:47:57 INFO Executor: Finished task 194.0 in stage 10.0 (TID 1243). 843 bytes result sent to driver
15/08/21 19:47:57 INFO TaskSetManager: Finished task 194.0 in stage 10.0 (TID 1243) in 316 ms on localhost (191/200)
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000182
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000182_0: Committed
15/08/21 19:47:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,576
15/08/21 19:47:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000183
15/08/21 19:47:57 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000183_0: Committed
15/08/21 19:47:57 INFO Executor: Finished task 182.0 in stage 10.0 (TID 1231). 843 bytes result sent to driver
15/08/21 19:47:57 INFO Executor: Finished task 183.0 in stage 10.0 (TID 1232). 843 bytes result sent to driver
15/08/21 19:47:58 INFO ColumnChunkPageWriteStore: written 1,728B for [ps_partkey] INT32: 465 values, 1,867B raw, 1,692B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:58 INFO ColumnChunkPageWriteStore: written 2,027B for [value] DOUBLE: 465 values, 3,727B raw, 1,983B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
15/08/21 19:47:58 INFO TaskSetManager: Finished task 182.0 in stage 10.0 (TID 1231) in 1187 ms on localhost (192/200)
15/08/21 19:47:58 INFO TaskSetManager: Finished task 183.0 in stage 10.0 (TID 1232) in 1186 ms on localhost (193/200)
15/08/21 19:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000196
15/08/21 19:47:58 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000196_0: Committed
15/08/21 19:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000197
15/08/21 19:47:58 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000197_0: Committed
15/08/21 19:47:58 INFO Executor: Finished task 197.0 in stage 10.0 (TID 1246). 843 bytes result sent to driver
15/08/21 19:47:58 INFO Executor: Finished task 196.0 in stage 10.0 (TID 1245). 843 bytes result sent to driver
15/08/21 19:47:58 INFO TaskSetManager: Finished task 197.0 in stage 10.0 (TID 1246) in 819 ms on localhost (194/200)
15/08/21 19:47:58 INFO TaskSetManager: Finished task 196.0 in stage 10.0 (TID 1245) in 823 ms on localhost (195/200)
15/08/21 19:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000184
15/08/21 19:47:58 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000184_0: Committed
15/08/21 19:47:58 INFO Executor: Finished task 184.0 in stage 10.0 (TID 1233). 843 bytes result sent to driver
15/08/21 19:47:58 INFO TaskSetManager: Finished task 184.0 in stage 10.0 (TID 1233) in 1181 ms on localhost (196/200)
15/08/21 19:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000199
15/08/21 19:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000185
15/08/21 19:47:58 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000199_0: Committed
15/08/21 19:47:58 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000185_0: Committed
15/08/21 19:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000190
15/08/21 19:47:58 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000190_0: Committed
15/08/21 19:47:58 INFO Executor: Finished task 199.0 in stage 10.0 (TID 1248). 843 bytes result sent to driver
15/08/21 19:47:58 INFO Executor: Finished task 185.0 in stage 10.0 (TID 1234). 843 bytes result sent to driver
15/08/21 19:47:58 INFO Executor: Finished task 190.0 in stage 10.0 (TID 1239). 843 bytes result sent to driver
15/08/21 19:47:58 INFO TaskSetManager: Finished task 185.0 in stage 10.0 (TID 1234) in 1181 ms on localhost (197/200)
15/08/21 19:47:58 INFO TaskSetManager: Finished task 190.0 in stage 10.0 (TID 1239) in 1105 ms on localhost (198/200)
15/08/21 19:47:58 INFO TaskSetManager: Finished task 199.0 in stage 10.0 (TID 1248) in 828 ms on localhost (199/200)
15/08/21 19:47:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508211947_0010_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508211947_0010_m_000198
15/08/21 19:47:58 INFO SparkHadoopMapRedUtil: attempt_201508211947_0010_m_000198_0: Committed
15/08/21 19:47:58 INFO Executor: Finished task 198.0 in stage 10.0 (TID 1247). 843 bytes result sent to driver
15/08/21 19:47:58 INFO TaskSetManager: Finished task 198.0 in stage 10.0 (TID 1247) in 845 ms on localhost (200/200)
15/08/21 19:47:58 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/21 19:47:58 INFO DAGScheduler: ResultStage 10 (processCmd at CliDriver.java:423) finished in 8.820 s
15/08/21 19:47:58 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@57fdabe
15/08/21 19:47:58 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 17.130393 s
15/08/21 19:47:58 INFO StatsReportListener: task runtime:(count: 200, mean: 685.045000, stdev: 450.044423, max: 2324.000000, min: 175.000000)
15/08/21 19:47:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:58 INFO StatsReportListener: 	175.0 ms	234.0 ms	260.0 ms	326.0 ms	652.0 ms	739.0 ms	1.2 s	1.9 s	2.3 s
15/08/21 19:47:58 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.320000, stdev: 0.712461, max: 5.000000, min: 0.000000)
15/08/21 19:47:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:58 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	5.0 ms
15/08/21 19:47:58 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 19:47:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:58 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 19:47:58 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 19:47:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:58 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 19:47:58 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 79.054683, stdev: 15.269439, max: 96.938776, min: 14.325530)
15/08/21 19:47:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:58 INFO StatsReportListener: 	14 %	59 %	65 %	70 %	85 %	89 %	91 %	94 %	97 %
15/08/21 19:47:58 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.056241, stdev: 0.138106, max: 1.111111, min: 0.000000)
15/08/21 19:47:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:58 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/21 19:47:58 INFO StatsReportListener: other time pct: (count: 200, mean: 20.889076, stdev: 15.264400, max: 85.674470, min: 3.061224)
15/08/21 19:47:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:47:58 INFO StatsReportListener: 	 3 %	 7 %	 9 %	12 %	15 %	30 %	35 %	41 %	86 %
15/08/21 19:47:59 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 19:48:00 INFO DefaultWriterContainer: Job job_201508211947_0000 committed.
15/08/21 19:48:00 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 19:48:00 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_common_metadata
15/08/21 19:48:00 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 19:48:00 INFO DAGScheduler: Got job 5 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 19:48:00 INFO DAGScheduler: Final stage: ResultStage 11(processCmd at CliDriver.java:423)
15/08/21 19:48:00 INFO DAGScheduler: Parents of final stage: List()
15/08/21 19:48:00 INFO DAGScheduler: Missing parents: List()
15/08/21 19:48:00 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 19:48:00 INFO MemoryStore: ensureFreeSpace(2968) called with curMem=1865069, maxMem=22226833244
15/08/21 19:48:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 19:48:00 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=1868037, maxMem=22226833244
15/08/21 19:48:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 1776.0 B, free 20.7 GB)
15/08/21 19:48:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:54384 (size: 1776.0 B, free: 20.7 GB)
15/08/21 19:48:00 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:874
15/08/21 19:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at processCmd at CliDriver.java:423)
15/08/21 19:48:00 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
15/08/21 19:48:00 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 1249, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 19:48:00 INFO Executor: Running task 0.0 in stage 11.0 (TID 1249)
15/08/21 19:48:00 INFO Executor: Finished task 0.0 in stage 11.0 (TID 1249). 606 bytes result sent to driver
15/08/21 19:48:00 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 1249) in 11 ms on localhost (1/1)
15/08/21 19:48:00 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/21 19:48:00 INFO DAGScheduler: ResultStage 11 (processCmd at CliDriver.java:423) finished in 0.011 s
15/08/21 19:48:00 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@328b62f1
15/08/21 19:48:00 INFO StatsReportListener: task runtime:(count: 1, mean: 11.000000, stdev: 0.000000, max: 11.000000, min: 11.000000)
15/08/21 19:48:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:48:00 INFO StatsReportListener: 	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms
15/08/21 19:48:00 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 19:48:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:48:00 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 19:48:00 INFO DAGScheduler: Job 5 finished: processCmd at CliDriver.java:423, took 0.026290 s
15/08/21 19:48:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 19:48:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:48:00 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
Time taken: 24.488 seconds
15/08/21 19:48:00 INFO CliDriver: Time taken: 24.488 seconds
15/08/21 19:48:00 INFO StatsReportListener: other time pct: (count: 1, mean: 100.000000, stdev: 0.000000, max: 100.000000, min: 100.000000)
15/08/21 19:48:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 19:48:00 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/21 19:48:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/21 19:48:00 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/21 19:48:00 INFO DAGScheduler: Stopping DAGScheduler
15/08/21 19:48:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/21 19:48:00 INFO Utils: path = /tmp/spark-07b7866a-0531-4123-9f8f-3926e05d7110/blockmgr-417ef505-7212-4569-8bae-01a6403ef683, already present as root for deletion.
15/08/21 19:48:00 INFO MemoryStore: MemoryStore cleared
15/08/21 19:48:00 INFO BlockManager: BlockManager stopped
15/08/21 19:48:00 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/21 19:48:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/21 19:48:00 INFO SparkContext: Successfully stopped SparkContext
15/08/21 19:48:00 INFO Utils: Shutdown hook called
15/08/21 19:48:00 INFO Utils: Deleting directory /tmp/spark-07b7866a-0531-4123-9f8f-3926e05d7110
15/08/21 19:48:00 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/21 19:48:00 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/21 19:48:00 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/08/21 19:48:01 INFO Utils: Deleting directory /tmp/spark-4b5f431c-4684-4e07-b82a-d6ef449705d2
15/08/21 19:48:01 INFO Utils: Deleting directory /tmp/spark-e42fa52a-1942-4bbb-b286-e21087298751
