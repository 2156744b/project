 -- the query
insert into table q11_part_tmp_par
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
    join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
    join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par) sum_tmp
    join q11_part_tmp_par
where part_value > total_value * 0.000001
order by value desc; 
15/08/21 21:30:17 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 21:30:17 INFO metastore: Connected to metastore.
15/08/21 21:30:18 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/21 21:30:18 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:18 INFO SparkContext: Running Spark version 1.4.1
15/08/21 21:30:18 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:19 INFO SecurityManager: Changing view acls to: hive
15/08/21 21:30:19 INFO SecurityManager: Changing modify acls to: hive
15/08/21 21:30:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/21 21:30:20 INFO Slf4jLogger: Slf4jLogger started
15/08/21 21:30:20 INFO Remoting: Starting remoting
15/08/21 21:30:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:46451]
15/08/21 21:30:20 INFO Utils: Successfully started service 'sparkDriver' on port 46451.
15/08/21 21:30:20 INFO SparkEnv: Registering MapOutputTracker
15/08/21 21:30:20 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:20 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:20 INFO SparkEnv: Registering BlockManagerMaster
15/08/21 21:30:20 INFO DiskBlockManager: Created local directory at /tmp/spark-dd5e7f8a-78c8-4ac9-bff7-401cd2464af3/blockmgr-738bdfcf-d074-4fd8-ae5d-2cf640dc07a9
15/08/21 21:30:20 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/21 21:30:20 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-dd5e7f8a-78c8-4ac9-bff7-401cd2464af3/httpd-e6bbb57d-e364-4051-9cae-634df2179fe2
15/08/21 21:30:20 INFO HttpServer: Starting HTTP Server
15/08/21 21:30:20 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 21:30:20 INFO AbstractConnector: Started SocketConnector@0.0.0.0:48432
15/08/21 21:30:20 INFO Utils: Successfully started service 'HTTP file server' on port 48432.
15/08/21 21:30:20 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/21 21:30:21 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 21:30:21 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/21 21:30:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/21 21:30:21 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/21 21:30:21 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:21 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:21 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:21 INFO Executor: Starting executor ID driver on host localhost
15/08/21 21:30:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52592.
15/08/21 21:30:21 INFO NettyBlockTransferService: Server created on 52592
15/08/21 21:30:21 INFO BlockManagerMaster: Trying to register BlockManager
15/08/21 21:30:21 INFO BlockManagerMasterEndpoint: Registering block manager localhost:52592 with 20.7 GB RAM, BlockManagerId(driver, localhost, 52592)
15/08/21 21:30:21 INFO BlockManagerMaster: Registered BlockManager
15/08/21 21:30:22 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 21:30:22 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/21 21:30:22 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/21 21:30:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/21 21:30:23 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 21:30:23 INFO metastore: Connected to metastore.
15/08/21 21:30:24 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/21 21:30:24 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/21 21:30:24 INFO ParseDriver: Parsing command: -- the query
insert into table q11_part_tmp_par
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
    join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
    join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey
15/08/21 21:30:24 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/21 21:30:28 INFO MemoryStore: ensureFreeSpace(326528) called with curMem=0, maxMem=22226833244
15/08/21 21:30:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 21:30:29 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=326528, maxMem=22226833244
15/08/21 21:30:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 21:30:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:52592 (size: 22.3 KB, free: 20.7 GB)
15/08/21 21:30:29 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/21 21:30:29 INFO MemoryStore: ensureFreeSpace(326584) called with curMem=349321, maxMem=22226833244
15/08/21 21:30:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 21:30:29 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=675905, maxMem=22226833244
15/08/21 21:30:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 21:30:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:52592 (size: 22.3 KB, free: 20.7 GB)
15/08/21 21:30:30 INFO SparkContext: Created broadcast 1 from processCmd at CliDriver.java:423
15/08/21 21:30:30 INFO MemoryStore: ensureFreeSpace(326584) called with curMem=698698, maxMem=22226833244
15/08/21 21:30:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 21:30:30 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1025282, maxMem=22226833244
15/08/21 21:30:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 21:30:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:52592 (size: 22.3 KB, free: 20.7 GB)
15/08/21 21:30:30 INFO SparkContext: Created broadcast 2 from processCmd at CliDriver.java:423
15/08/21 21:30:30 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 21:30:30 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 21:30:30 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/21 21:30:30 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/21 21:30:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 21:30:30 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 21:30:30 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/21 21:30:30 INFO DAGScheduler: Got job 0 (run at ThreadPoolExecutor.java:1145) with 1 output partitions (allowLocal=false)
15/08/21 21:30:30 INFO DAGScheduler: Final stage: ResultStage 0(run at ThreadPoolExecutor.java:1145)
15/08/21 21:30:30 INFO DAGScheduler: Parents of final stage: List()
15/08/21 21:30:30 INFO DAGScheduler: Missing parents: List()
15/08/21 21:30:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/21 21:30:30 INFO MemoryStore: ensureFreeSpace(6144) called with curMem=1048075, maxMem=22226833244
15/08/21 21:30:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.0 KB, free 20.7 GB)
15/08/21 21:30:30 INFO MemoryStore: ensureFreeSpace(3289) called with curMem=1054219, maxMem=22226833244
15/08/21 21:30:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.2 KB, free 20.7 GB)
15/08/21 21:30:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:52592 (size: 3.2 KB, free: 20.7 GB)
15/08/21 21:30:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/21 21:30:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at run at ThreadPoolExecutor.java:1145)
15/08/21 21:30:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/08/21 21:30:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1682 bytes)
15/08/21 21:30:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/21 21:30:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 2330 length: 2330 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:30 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1856 bytes result sent to driver
15/08/21 21:30:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 474 ms on localhost (1/1)
15/08/21 21:30:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/21 21:30:30 INFO DAGScheduler: ResultStage 0 (run at ThreadPoolExecutor.java:1145) finished in 0.494 s
15/08/21 21:30:30 INFO DAGScheduler: Job 0 finished: run at ThreadPoolExecutor.java:1145, took 0.650979 s
15/08/21 21:30:31 INFO MemoryStore: ensureFreeSpace(344) called with curMem=1057508, maxMem=22226833244
15/08/21 21:30:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 344.0 B, free 20.7 GB)
15/08/21 21:30:31 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@e0ae46a
15/08/21 21:30:31 INFO MemoryStore: ensureFreeSpace(176) called with curMem=1057852, maxMem=22226833244
15/08/21 21:30:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 176.0 B, free 20.7 GB)
15/08/21 21:30:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:52592 (size: 176.0 B, free: 20.7 GB)
15/08/21 21:30:31 INFO SparkContext: Created broadcast 4 from run at ThreadPoolExecutor.java:1145
15/08/21 21:30:31 INFO StatsReportListener: task runtime:(count: 1, mean: 474.000000, stdev: 0.000000, max: 474.000000, min: 474.000000)
15/08/21 21:30:31 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:31 INFO StatsReportListener: 	474.0 ms	474.0 ms	474.0 ms	474.0 ms	474.0 ms	474.0 ms	474.0 ms	474.0 ms	474.0 ms
15/08/21 21:30:31 INFO StatsReportListener: task result size:(count: 1, mean: 1856.000000, stdev: 0.000000, max: 1856.000000, min: 1856.000000)
15/08/21 21:30:31 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:31 INFO StatsReportListener: 	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B
15/08/21 21:30:31 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 65.611814, stdev: 0.000000, max: 65.611814, min: 65.611814)
15/08/21 21:30:31 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:31 INFO StatsReportListener: 	66 %	66 %	66 %	66 %	66 %	66 %	66 %	66 %	66 %
15/08/21 21:30:31 INFO StatsReportListener: other time pct: (count: 1, mean: 34.388186, stdev: 0.000000, max: 34.388186, min: 34.388186)
15/08/21 21:30:31 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:31 INFO StatsReportListener: 	34 %	34 %	34 %	34 %	34 %	34 %	34 %	34 %	34 %
15/08/21 21:30:31 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/21 21:30:31 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/21 21:30:31 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/21 21:30:31 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/21 21:30:31 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/21 21:30:31 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 21:30:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:30:31 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 21:30:31 INFO DAGScheduler: Registering RDD 9 (processCmd at CliDriver.java:423)
15/08/21 21:30:31 INFO DAGScheduler: Registering RDD 14 (processCmd at CliDriver.java:423)
15/08/21 21:30:31 INFO DAGScheduler: Registering RDD 20 (processCmd at CliDriver.java:423)
15/08/21 21:30:31 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 21:30:31 INFO DAGScheduler: Final stage: ResultStage 4(processCmd at CliDriver.java:423)
15/08/21 21:30:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
15/08/21 21:30:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
15/08/21 21:30:31 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 21:30:31 INFO MemoryStore: ensureFreeSpace(6536) called with curMem=1058028, maxMem=22226833244
15/08/21 21:30:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.4 KB, free 20.7 GB)
15/08/21 21:30:31 INFO MemoryStore: ensureFreeSpace(3553) called with curMem=1064564, maxMem=22226833244
15/08/21 21:30:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KB, free 20.7 GB)
15/08/21 21:30:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:52592 (size: 3.5 KB, free: 20.7 GB)
15/08/21 21:30:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:874
15/08/21 21:30:31 INFO DAGScheduler: Submitting 39 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423)
15/08/21 21:30:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 39 tasks
15/08/21 21:30:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, ANY, 1694 bytes)
15/08/21 21:30:31 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 21:30:31 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, ANY, 1699 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, ANY, 1708 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, localhost, ANY, 1694 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, localhost, ANY, 1699 bytes)
15/08/21 21:30:31 INFO MemoryStore: ensureFreeSpace(9520) called with curMem=1068117, maxMem=22226833244
15/08/21 21:30:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.3 KB, free 20.7 GB)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, localhost, ANY, 1707 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, localhost, ANY, 1693 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, localhost, ANY, 1699 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, localhost, ANY, 1709 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, localhost, ANY, 1693 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, localhost, ANY, 1699 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, localhost, ANY, 1708 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, localhost, ANY, 1694 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, localhost, ANY, 1699 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, localhost, ANY, 1707 bytes)
15/08/21 21:30:31 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, localhost, ANY, 1694 bytes)
15/08/21 21:30:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/08/21 21:30:31 INFO MemoryStore: ensureFreeSpace(4939) called with curMem=1077637, maxMem=22226833244
15/08/21 21:30:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.8 KB, free 20.7 GB)
15/08/21 21:30:31 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
15/08/21 21:30:31 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
15/08/21 21:30:31 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
15/08/21 21:30:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:52592 (size: 4.8 KB, free: 20.7 GB)
15/08/21 21:30:31 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
15/08/21 21:30:31 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
15/08/21 21:30:31 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
15/08/21 21:30:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:874
15/08/21 21:30:31 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
15/08/21 21:30:31 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
15/08/21 21:30:31 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
15/08/21 21:30:31 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
15/08/21 21:30:31 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
15/08/21 21:30:31 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
15/08/21 21:30:31 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
15/08/21 21:30:31 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
15/08/21 21:30:31 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:423)
15/08/21 21:30:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks
15/08/21 21:30:31 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 268435456 end: 336808364 length: 68372908 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 268435456 end: 334658433 length: 66222977 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 268435456 end: 336837881 length: 68402425 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 268435456 end: 334660800 length: 66225344 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 268435456 end: 335698071 length: 67262615 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: reading another 1 footers
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: reading another 13 footers
21-Aug-2015 21:30:26 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 21:30:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
21-Aug-2015 21:30:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 31 ms. row count = 25
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464967 records.
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465186 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464663 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465498 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464958 records.
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465101 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243981 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243920 records.
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465498 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464813 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203579 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203347 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465634 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465108 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465546 records.
21-Aug-2015 21:30:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1222637 records.
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at r15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:32 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 21:30:38 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2125 bytes result sent to driver
15/08/21 21:30:38 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, localhost, ANY, 1699 bytes)
15/08/21 21:30:38 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
15/08/21 21:30:38 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2125 bytes result sent to driver
15/08/21 21:30:38 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, localhost, ANY, 1707 bytes)
15/08/21 21:30:38 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
15/08/21 21:30:38 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2125 bytes result sent to driver
15/08/21 21:30:38 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, localhost, ANY, 1694 bytes)
15/08/21 21:30:38 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 6823 ms on localhost (1/39)
15/08/21 21:30:38 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
15/08/21 21:30:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:38 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 6852 ms on localhost (2/39)
15/08/21 21:30:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 268435456 end: 336808298 length: 68372842 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:38 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 6849 ms on localhost (3/39)
15/08/21 21:30:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:38 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2125 bytes result sent to driver
15/08/21 21:30:38 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, localhost, ANY, 1699 bytes)
15/08/21 21:30:38 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
15/08/21 21:30:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:38 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 7120 ms on localhost (4/39)
15/08/21 21:30:39 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2125 bytes result sent to driver
15/08/21 21:30:39 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, localhost, ANY, 1710 bytes)
15/08/21 21:30:39 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
15/08/21 21:30:39 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 7904 ms on localhost (5/39)
15/08/21 21:30:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 268435456 end: 333041820 length: 64606364 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, localhost, ANY, 1694 bytes)
15/08/21 21:30:43 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 11441 ms on localhost (6/39)
15/08/21 21:30:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23, localhost, ANY, 1699 bytes)
15/08/21 21:30:43 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11654 ms on localhost (7/39)
15/08/21 21:30:43 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 24, localhost, ANY, 1710 bytes)
15/08/21 21:30:43 INFO Executor: Running task 23.0 in stage 1.0 (TID 24)
15/08/21 21:30:43 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 25, localhost, ANY, 1693 bytes)
15/08/21 21:30:43 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 11712 ms on localhost (8/39)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 268435456 end: 334667494 length: 66232038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO Executor: Running task 24.0 in stage 1.0 (TID 25)
15/08/21 21:30:43 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 11729 ms on localhost (9/39)
15/08/21 21:30:43 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 26, localhost, ANY, 1698 bytes)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO Executor: Running task 25.0 in stage 1.0 (TID 26)
15/08/21 21:30:43 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 11791 ms on localhost (10/39)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 27, localhost, ANY, 1706 bytes)
15/08/21 21:30:43 INFO Executor: Running task 26.0 in stage 1.0 (TID 27)
15/08/21 21:30:43 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 11854 ms on localhost (11/39)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 268435456 end: 338400334 length: 69964878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
ow 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 416 ms. row count = 1243981
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 504 ms. row count = 1243920
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 499 ms. row count = 1222637
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 503 ms. row count = 1203579
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 522 ms. row count = 1203347
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 594 ms. row count = 2465498
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 591 ms. row count = 2465186
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 581 ms. row count = 2465546
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 604 ms. row count = 2465634
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 617 ms. row count = 2465498
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 593 ms. row count = 2464958
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 617 ms. row count = 2465108
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 627 ms. row count = 2464813
21-Aug-2015 21:30:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 632 ms. row count = 2465101
21-Aug-2015 21:30:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 647 ms. row count = 2464663
21-Aug-2015 21:30:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 648 ms. row count = 2464967
21-Aug-2015 21:30:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2463721 records.
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243832 records.
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464953 records.
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464761 records.
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 221 ms. row count = 1243832
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 241 ms. row count = 2463721
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 287 ms. row count = 2464953
21-Aug-2015 21:30:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 217 ms. row count = 2464761
21-Aug-2015 21:30:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1174247 records.
21-Aug-2015 21:30:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 88 ms. row count = 1174247
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2466033 records.
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464261 records.
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 138 ms. row count = 2466033
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203394 records.
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464540 records.
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 134 ms. row count = 2464261
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465074 records.
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 12735415/08/21 21:30:43 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 28, localhost, ANY, 1693 bytes)
15/08/21 21:30:43 INFO Executor: Running task 27.0 in stage 1.0 (TID 28)
15/08/21 21:30:43 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 12100 ms on localhost (12/39)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2125 bytes result sent to driver
15/08/21 21:30:43 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 29, localhost, ANY, 1699 bytes)
15/08/21 21:30:43 INFO Executor: Running task 28.0 in stage 1.0 (TID 29)
15/08/21 21:30:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 12151 ms on localhost (13/39)
15/08/21 21:30:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:52592 in memory (size: 3.2 KB, free: 20.7 GB)
15/08/21 21:30:44 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2125 bytes result sent to driver
15/08/21 21:30:45 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 30, localhost, ANY, 1707 bytes)
15/08/21 21:30:45 INFO Executor: Running task 29.0 in stage 1.0 (TID 30)
15/08/21 21:30:45 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 13253 ms on localhost (14/39)
15/08/21 21:30:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 268435456 end: 336803495 length: 68368039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:45 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2125 bytes result sent to driver
15/08/21 21:30:45 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 31, localhost, ANY, 1693 bytes)
15/08/21 21:30:45 INFO Executor: Running task 30.0 in stage 1.0 (TID 31)
15/08/21 21:30:45 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 13414 ms on localhost (15/39)
15/08/21 21:30:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:45 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2125 bytes result sent to driver
15/08/21 21:30:45 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 32, localhost, ANY, 1699 bytes)
15/08/21 21:30:45 INFO Executor: Running task 31.0 in stage 1.0 (TID 32)
15/08/21 21:30:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:45 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 13736 ms on localhost (16/39)
15/08/21 21:30:45 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2125 bytes result sent to driver
15/08/21 21:30:45 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 33, localhost, ANY, 1707 bytes)
15/08/21 21:30:45 INFO Executor: Running task 32.0 in stage 1.0 (TID 33)
15/08/21 21:30:45 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 6240 ms on localhost (17/39)
15/08/21 21:30:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 268435456 end: 334633553 length: 66198097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:45 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2125 bytes result sent to driver
15/08/21 21:30:45 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 34, localhost, ANY, 1694 bytes)
15/08/21 21:30:45 INFO Executor: Running task 33.0 in stage 1.0 (TID 34)
15/08/21 21:30:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:45 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 7368 ms on localhost (18/39)
15/08/21 21:30:47 INFO Executor: Finished task 26.0 in stage 1.0 (TID 27). 2125 bytes result sent to driver
15/08/21 21:30:47 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 35, localhost, ANY, 1699 bytes)
15/08/21 21:30:47 INFO Executor: Running task 34.0 in stage 1.0 (TID 35)
15/08/21 21:30:47 INFO Executor: Finished task 23.0 in stage 1.0 (TID 24). 2125 bytes result sent to driver
15/08/21 21:30:47 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 36, localhost, ANY, 1710 bytes)
15/08/21 21:30:47 INFO Executor: Running task 35.0 in stage 1.0 (TID 36)
15/08/21 21:30:47 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 27) in 4379 ms on localhost (19/39)
15/08/21 21:30:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 268435456 end: 334670294 length: 66234838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:47 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 24) in 4543 ms on localhost (20/39)
15/08/21 21:30:48 INFO Executor: Finished task 29.0 in stage 1.0 (TID 30). 2125 bytes result sent to driver
15/08/21 21:30:48 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 37, localhost, ANY, 1693 bytes)
15/08/21 21:30:48 INFO Executor: Running task 36.0 in stage 1.0 (TID 37)
15/08/21 21:30:48 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 30) in 3530 ms on localhost (21/39)
15/08/21 21:30:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:48 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2125 bytes result sent to driver
15/08/21 21:30:48 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 38, localhost, ANY, 1699 bytes)
15/08/21 21:30:48 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
15/08/21 21:30:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:48 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 10101 ms on localhost (22/39)
15/08/21 21:30:48 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2125 bytes result sent to driver
15/08/21 21:30:48 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 39, localhost, ANY, 1707 bytes)
15/08/21 21:30:48 INFO Executor: Running task 38.0 in stage 1.0 (TID 39)
15/08/21 21:30:48 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 10165 ms on localhost (23/39)
15/08/21 21:30:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 268435456 end: 336840779 length: 68405323 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:48 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2125 bytes result sent to driver
15/08/21 21:30:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 40, localhost, ANY, 1695 bytes)
15/08/21 21:30:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 40)
15/08/21 21:30:48 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 10142 ms on localhost (24/39)
15/08/21 21:30:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000001_0 start: 0 end: 10091464 length: 10091464 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:48 INFO Executor: Finished task 32.0 in stage 1.0 (TID 33). 2125 bytes result sent to driver
15/08/21 21:30:48 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 41, localhost, ANY, 1696 bytes)
15/08/21 21:30:48 INFO Executor: Running task 1.0 in stage 2.0 (TID 41)
15/08/21 21:30:48 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 33) in 3033 ms on localhost (25/39)
15/08/21 21:30:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000006_0 start: 0 end: 10091200 length: 10091200 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
7 records.
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 95 ms. row count = 1203394
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 105 ms. row count = 1273547
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 191 ms. row count = 2465074
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465823 records.
21-Aug-2015 21:30:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 1159 ms. row count = 2464540
21-Aug-2015 21:30:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464354 records.
21-Aug-2015 21:30:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 995 ms. row count = 2465823
21-Aug-2015 21:30:45 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 177 ms. row count = 2464354
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1244217 records.
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 114 ms. row count = 1244217
21-Aug-2015 21:30:45 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464440 records.
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 144 ms. row count = 2464440
21-Aug-2015 21:30:45 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465446 records.
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 167 ms. row count = 2465446
21-Aug-2015 21:30:45 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203324 records.
21-Aug-2015 21:30:45 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465130 records.
21-Aug-2015 21:30:45 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:46 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 1203324
21-Aug-2015 21:30:46 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 148 ms. row count = 2465130
21-Aug-2015 21:30:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465185 records.
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203179 records.
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 78 ms. row count = 1203179
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 143 ms. row count = 2465185
21-Aug-2015 21:30:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2466709 records.
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465174 records.
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 129 ms. row count = 2466709
21-Aug-2015 21:30:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1244220 records.
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 123 ms. row count = 2465174
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 1244220
21-Aug-2015 21:30:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124899 records.
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 124899
21-Aug-2015 21:30:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputCo15/08/21 21:30:49 INFO Executor: Finished task 1.0 in stage 2.0 (TID 41). 2125 bytes result sent to driver
15/08/21 21:30:49 INFO Executor: Finished task 0.0 in stage 2.0 (TID 40). 2125 bytes result sent to driver
15/08/21 21:30:49 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 42, localhost, ANY, 1696 bytes)
15/08/21 21:30:49 INFO Executor: Running task 2.0 in stage 2.0 (TID 42)
15/08/21 21:30:49 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 43, localhost, ANY, 1694 bytes)
15/08/21 21:30:49 INFO Executor: Running task 3.0 in stage 2.0 (TID 43)
15/08/21 21:30:49 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 41) in 374 ms on localhost (1/8)
15/08/21 21:30:49 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 40) in 441 ms on localhost (2/8)
15/08/21 21:30:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000007_0 start: 0 end: 10090201 length: 10090201 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000005_0 start: 0 end: 10091941 length: 10091941 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:49 INFO Executor: Finished task 3.0 in stage 2.0 (TID 43). 2125 bytes result sent to driver
15/08/21 21:30:49 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 44, localhost, ANY, 1694 bytes)
15/08/21 21:30:49 INFO Executor: Running task 4.0 in stage 2.0 (TID 44)
15/08/21 21:30:49 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 43) in 353 ms on localhost (3/8)
15/08/21 21:30:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000004_0 start: 0 end: 10091082 length: 10091082 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:49 INFO Executor: Finished task 2.0 in stage 2.0 (TID 42). 2125 bytes result sent to driver
15/08/21 21:30:49 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 45, localhost, ANY, 1694 bytes)
15/08/21 21:30:49 INFO Executor: Running task 5.0 in stage 2.0 (TID 45)
15/08/21 21:30:49 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 42) in 398 ms on localhost (4/8)
15/08/21 21:30:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 10153723 length: 10153723 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:49 INFO Executor: Finished task 4.0 in stage 2.0 (TID 44). 2125 bytes result sent to driver
15/08/21 21:30:49 INFO Executor: Finished task 5.0 in stage 2.0 (TID 45). 2125 bytes result sent to driver
15/08/21 21:30:49 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 46, localhost, ANY, 1695 bytes)
15/08/21 21:30:49 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 44) in 369 ms on localhost (5/8)
15/08/21 21:30:49 INFO Executor: Running task 6.0 in stage 2.0 (TID 46)
15/08/21 21:30:50 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 47, localhost, ANY, 1695 bytes)
15/08/21 21:30:50 INFO Executor: Running task 7.0 in stage 2.0 (TID 47)
15/08/21 21:30:50 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 45) in 329 ms on localhost (6/8)
15/08/21 21:30:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000003_0 start: 0 end: 10090247 length: 10090247 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000002_0 start: 0 end: 10086725 length: 10086725 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 21:30:50 INFO Executor: Finished task 7.0 in stage 2.0 (TID 47). 2125 bytes result sent to driver
15/08/21 21:30:50 INFO Executor: Finished task 6.0 in stage 2.0 (TID 46). 2125 bytes result sent to driver
15/08/21 21:30:50 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 47) in 404 ms on localhost (7/8)
15/08/21 21:30:50 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 46) in 419 ms on localhost (8/8)
15/08/21 21:30:50 INFO DAGScheduler: ShuffleMapStage 2 (processCmd at CliDriver.java:423) finished in 18.599 s
15/08/21 21:30:50 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@665b2dfa
15/08/21 21:30:50 INFO DAGScheduler: looking for newly runnable stages
15/08/21 21:30:50 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
15/08/21 21:30:50 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
15/08/21 21:30:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/21 21:30:50 INFO StatsReportListener: task runtime:(count: 33, mean: 7059.727273, stdev: 4710.706859, max: 13736.000000, min: 329.000000)
15/08/21 21:30:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:50 INFO StatsReportListener: 	329.0 ms	353.0 ms	374.0 ms	3.0 s	7.1 s	11.7 s	12.2 s	13.4 s	13.7 s
15/08/21 21:30:50 INFO StatsReportListener: shuffle bytes written:(count: 33, mean: 32464792.939394, stdev: 21911143.073813, max: 55122523.000000, min: 39409.000000)
15/08/21 21:30:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:50 INFO StatsReportListener: 	38.5 KB	38.6 KB	38.7 KB	25.0 MB	26.5 MB	52.5 MB	52.6 MB	52.6 MB	52.6 MB
15/08/21 21:30:50 INFO StatsReportListener: task result size:(count: 33, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 21:30:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:50 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 21:30:50 INFO StatsReportListener: executor (non-fetch) time pct: (count: 33, mean: 96.326484, stdev: 4.111767, max: 99.185252, min: 86.402266)
15/08/21 21:30:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:50 INFO StatsReportListener: 	86 %	88 %	89 %	96 %	99 %	99 %	99 %	99 %	99 %
15/08/21 21:30:50 INFO DAGScheduler: failed: Set()
15/08/21 21:30:50 INFO StatsReportListener: other time pct: (count: 33, mean: 3.673516, stdev: 4.111767, max: 13.597734, min: 0.814748)
15/08/21 21:30:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:50 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 1 %	 4 %	11 %	12 %	14 %
15/08/21 21:30:50 INFO DAGScheduler: Missing parents for ShuffleMapStage 3: List(ShuffleMapStage 1)
15/08/21 21:30:50 INFO DAGScheduler: Missing parents for ResultStage 4: List(ShuffleMapStage 3)
15/08/21 21:30:50 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2125 bytes result sent to driver
15/08/21 21:30:50 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 7452 ms on localhost (26/39)
15/08/21 21:30:50 INFO Executor: Finished task 25.0 in stage 1.0 (TID 26). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 26) in 7470 ms on localhost (27/39)
15/08/21 21:30:51 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO Executor: Finished task 24.0 in stage 1.0 (TID 25). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 7837 ms on localhost (28/39)
15/08/21 21:30:51 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 25) in 7551 ms on localhost (29/39)
15/08/21 21:30:51 INFO Executor: Finished task 35.0 in stage 1.0 (TID 36). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 36) in 3412 ms on localhost (30/39)
15/08/21 21:30:51 INFO Executor: Finished task 27.0 in stage 1.0 (TID 28). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 28) in 7584 ms on localhost (31/39)
15/08/21 21:30:51 INFO Executor: Finished task 30.0 in stage 1.0 (TID 31). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 31) in 6274 ms on localhost (32/39)
15/08/21 21:30:51 INFO Executor: Finished task 28.0 in stage 1.0 (TID 29). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 29) in 7625 ms on localhost (33/39)
15/08/21 21:30:51 INFO Executor: Finished task 31.0 in stage 1.0 (TID 32). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 32) in 6067 ms on localhost (34/39)
15/08/21 21:30:51 INFO Executor: Finished task 38.0 in stage 1.0 (TID 39). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 39) in 3049 ms on localhost (35/39)
15/08/21 21:30:51 INFO Executor: Finished task 33.0 in stage 1.0 (TID 34). 2125 bytes result sent to driver
15/08/21 21:30:51 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 34) in 5922 ms on localhost (36/39)
15/08/21 21:30:53 INFO Executor: Finished task 34.0 in stage 1.0 (TID 35). 2125 bytes result sent to driver
15/08/21 21:30:53 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 35) in 5476 ms on localhost (37/39)
15/08/21 21:30:53 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2125 bytes result sent to driver
15/08/21 21:30:53 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 38) in 4955 ms on localhost (38/39)
15/08/21 21:30:53 INFO Executor: Finished task 36.0 in stage 1.0 (TID 37). 2125 bytes result sent to driver
15/08/21 21:30:53 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 37) in 5186 ms on localhost (39/39)
15/08/21 21:30:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/21 21:30:53 INFO DAGScheduler: ShuffleMapStage 1 (processCmd at CliDriver.java:423) finished in 21.951 s
15/08/21 21:30:53 INFO DAGScheduler: looking for newly runnable stages
15/08/21 21:30:53 INFO DAGScheduler: running: Set()
15/08/21 21:30:53 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
15/08/21 21:30:53 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4fc61255
15/08/21 21:30:53 INFO DAGScheduler: failed: Set()
15/08/21 21:30:53 INFO StatsReportListener: task runtime:(count: 14, mean: 6132.857143, stdev: 1522.988878, max: 7837.000000, min: 3049.000000)
15/08/21 21:30:53 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:53 INFO StatsReportListener: 	3.0 s	3.0 s	3.4 s	5.2 s	6.3 s	7.6 s	7.6 s	7.8 s	7.8 s
15/08/21 21:30:53 INFO StatsReportListener: shuffle bytes written:(count: 14, mean: 50500654.214286, stdev: 9576027.721338, max: 55117260.000000, min: 26900011.000000)
15/08/21 21:30:53 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:53 INFO StatsReportListener: 	25.7 MB	25.7 MB	26.5 MB	49.4 MB	52.5 MB	52.6 MB	52.6 MB	52.6 MB	52.6 MB
15/08/21 21:30:53 INFO StatsReportListener: task result size:(count: 14, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 21:30:53 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:53 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 21:30:53 INFO StatsReportListener: executor (non-fetch) time pct: (count: 14, mean: 99.109985, stdev: 0.643347, max: 99.560959, min: 97.015415)
15/08/21 21:30:53 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:53 INFO StatsReportListener: 	97 %	97 %	98 %	99 %	99 %	99 %	99 %	100 %	100 %
15/08/21 21:30:53 INFO StatsReportListener: other time pct: (count: 14, mean: 0.890015, stdev: 0.643347, max: 2.984585, min: 0.439041)
15/08/21 21:30:53 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:30:53 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 1 %	 2 %	 3 %	 3 %
15/08/21 21:30:53 INFO DAGScheduler: Missing parents for ShuffleMapStage 3: List()
15/08/21 21:30:53 INFO DAGScheduler: Missing parents for ResultStage 4: List(ShuffleMapStage 3)
15/08/21 21:30:53 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[20] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 21:30:53 INFO MemoryStore: ensureFreeSpace(14224) called with curMem=1073143, maxMem=22226833244
15/08/21 21:30:53 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.9 KB, free 20.7 GB)
15/08/21 21:30:53 INFO MemoryStore: ensureFreeSpace(6789) called with curMem=1087367, maxMem=22226833244
15/08/21 21:30:53 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.6 KB, free 20.7 GB)
15/08/21 21:30:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:52592 (size: 6.6 KB, free: 20.7 GB)
15/08/21 21:30:53 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874
15/08/21 21:30:53 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[20] at processCmd at CliDriver.java:423)
15/08/21 21:30:53 INFO TaskSchedulerImpl: Adding task set 3.0 with 200 tasks
15/08/21 21:30:53 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 48, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 49, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 50, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 51, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 52, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 53, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 54, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 55, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 56, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 57, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 58, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 59, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 60, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 61, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 62, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 63, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:53 INFO Executor: Running task 0.0 in stage 3.0 (TID 48)
15/08/21 21:30:53 INFO Executor: Running task 6.0 in stage 3.0 (TID 54)
15/08/21 21:30:53 INFO Executor: Running task 11.0 in stage 3.0 (TID 59)
15/08/21 21:30:53 INFO Executor: Running task 14.0 in stage 3.0 (TID 62)
15/08/21 21:30:53 INFO Executor: Running task 15.0 in stage 3.0 (TID 63)
15/08/21 21:30:53 INFO Executor: Running task 8.0 in stage 3.0 (TID 56)
15/08/21 21:30:53 INFO Executor: Running task 2.0 in stage 3.0 (TID 50)
15/08/21 21:30:53 INFO Executor: Running task 10.0 in stage 3.0 (TID 58)
15/08/21 21:30:53 INFO Executor: Running task 9.0 in stage 3.0 (TID 57)
15/08/21 21:30:53 INFO Executor: Running task 7.0 in stage 3.0 (TID 55)
15/08/21 21:30:53 INFO Executor: Running task 4.0 in stage 3.0 (TID 52)
15/08/21 21:30:53 INFO Executor: Running task 5.0 in stage 3.0 (TID 53)
15/08/21 21:30:53 INFO Executor: Running task 1.0 in stage 3.0 (TID 49)
15/08/21 21:30:53 INFO Executor: Running task 3.0 in stage 3.0 (TID 51)
15/08/21 21:30:53 INFO Executor: Running task 12.0 in stage 3.0 (TID 60)
15/08/21 21:30:53 INFO Executor: Running task 13.0 in stage 3.0 (TID 61)
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 21:30:56 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:52592 in memory (size: 4.8 KB, free: 20.7 GB)
15/08/21 21:30:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:52592 in memory (size: 3.5 KB, free: 20.7 GB)
15/08/21 21:30:57 INFO Executor: Finished task 10.0 in stage 3.0 (TID 58). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 64, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 16.0 in stage 3.0 (TID 64)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 58) in 3636 ms on localhost (1/200)
15/08/21 21:30:57 INFO Executor: Finished task 13.0 in stage 3.0 (TID 61). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 65, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 17.0 in stage 3.0 (TID 65)
15/08/21 21:30:57 INFO Executor: Finished task 11.0 in stage 3.0 (TID 59). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO Executor: Finished task 7.0 in stage 3.0 (TID 55). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 66, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 18.0 in stage 3.0 (TID 66)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 61) in 3736 ms on localhost (2/200)
15/08/21 21:30:57 INFO Executor: Finished task 4.0 in stage 3.0 (TID 52). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 67, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 19.0 in stage 3.0 (TID 67)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 55) in 3747 ms on localhost (3/200)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 59) in 3744 ms on localhost (4/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 68, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 20.0 in stage 3.0 (TID 68)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO Executor: Finished task 1.0 in stage 3.0 (TID 49). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 69, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 21.0 in stage 3.0 (TID 69)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 52) in 3803 ms on localhost (5/200)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 49) in 3816 ms on localhost (6/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO Executor: Finished task 5.0 in stage 3.0 (TID 53). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 48). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 70, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 22.0 in stage 3.0 (TID 70)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 71, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 23.0 in stage 3.0 (TID 71)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 48) in 3868 ms on localhost (7/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO Executor: Finished task 14.0 in stage 3.0 (TID 62). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO Executor: Finished task 8.0 in stage 3.0 (TID 56). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO Executor: Finished task 15.0 in stage 3.0 (TID 63). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 53) in 3869 ms on localhost (8/200)
15/08/21 21:30:57 INFO Executor: Finished task 3.0 in stage 3.0 (TID 51). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 72, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 24.0 in stage 3.0 (TID 72)
15/08/21 21:30:57 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 73, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 25.0 in stage 3.0 (TID 73)
15/08/21 21:30:57 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 74, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 75, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 27.0 in stage 3.0 (TID 75)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 62) in 3877 ms on localhost (9/200)
15/08/21 21:30:57 INFO Executor: Running task 26.0 in stage 3.0 (TID 74)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 63) in 3877 ms on localhost (10/200)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 56) in 3884 ms on localhost (11/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 51) in 3906 ms on localhost (12/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO Executor: Finished task 12.0 in stage 3.0 (TID 60). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO Executor: Finished task 9.0 in stage 3.0 (TID 57). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 76, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 28.0 in stage 3.0 (TID 76)
15/08/21 21:30:57 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 77, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 29.0 in stage 3.0 (TID 77)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 57) in 3925 ms on localhost (13/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 60) in 3926 ms on localhost (14/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO Executor: Finished task 2.0 in stage 3.0 (TID 50). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 78, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 50) in 3966 ms on localhost (15/200)
15/08/21 21:30:57 INFO Executor: Running task 30.0 in stage 3.0 (TID 78)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:57 INFO Executor: Finished task 6.0 in stage 3.0 (TID 54). 1219 bytes result sent to driver
15/08/21 21:30:57 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 79, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:57 INFO Executor: Running task 31.0 in stage 3.0 (TID 79)
15/08/21 21:30:57 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 54) in 4016 ms on localhost (16/200)
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:30:58 INFO Executor: Finished task 16.0 in stage 3.0 (TID 64). 1219 bytes result sent to driver
15/08/21 21:30:58 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 80, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:58 INFO Executor: Running task 32.0 in stage 3.0 (TID 80)
15/08/21 21:30:58 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 64) in 1324 ms on localhost (17/200)
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:58 INFO Executor: Finished task 17.0 in stage 3.0 (TID 65). 1219 bytes result sent to driver
15/08/21 21:30:58 INFO Executor: Finished task 26.0 in stage 3.0 (TID 74). 1219 bytes result sent to driver
15/08/21 21:30:58 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 81, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:58 INFO Executor: Running task 33.0 in stage 3.0 (TID 81)
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:58 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 82, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:58 INFO Executor: Running task 34.0 in stage 3.0 (TID 82)
15/08/21 21:30:58 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 65) in 1340 ms on localhost (18/200)
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:58 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 74) in 1206 ms on localhost (19/200)
15/08/21 21:30:58 INFO Executor: Finished task 25.0 in stage 3.0 (TID 73). 1219 bytes result sent to driver
15/08/21 21:30:58 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 83, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:58 INFO Executor: Running task 35.0 in stage 3.0 (TID 83)
15/08/21 21:30:58 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 73) in 1235 ms on localhost (20/200)
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:58 INFO Executor: Finished task 18.0 in stage 3.0 (TID 66). 1219 bytes result sent to driver
15/08/21 21:30:58 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 84, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:58 INFO Executor: Running task 36.0 in stage 3.0 (TID 84)
15/08/21 21:30:58 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 66) in 1400 ms on localhost (21/200)
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:58 INFO Executor: Finished task 19.0 in stage 3.0 (TID 67). 1219 bytes result sent to driver
15/08/21 21:30:58 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 85, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:58 INFO Executor: Running task 37.0 in stage 3.0 (TID 85)
15/08/21 21:30:58 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 67) in 1431 ms on localhost (22/200)
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO Executor: Finished task 28.0 in stage 3.0 (TID 76). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 86, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 38.0 in stage 3.0 (TID 86)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 76) in 1394 ms on localhost (23/200)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO Executor: Finished task 24.0 in stage 3.0 (TID 72). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 87, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 39.0 in stage 3.0 (TID 87)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 72) in 1481 ms on localhost (24/200)
15/08/21 21:30:59 INFO Executor: Finished task 22.0 in stage 3.0 (TID 70). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 88, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 40.0 in stage 3.0 (TID 88)
15/08/21 21:30:59 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 70) in 1692 ms on localhost (25/200)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO Executor: Finished task 29.0 in stage 3.0 (TID 77). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO Executor: Finished task 23.0 in stage 3.0 (TID 71). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 89, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 41.0 in stage 3.0 (TID 89)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:59 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 90, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 42.0 in stage 3.0 (TID 90)
15/08/21 21:30:59 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 71) in 1756 ms on localhost (26/200)
15/08/21 21:30:59 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 77) in 1678 ms on localhost (27/200)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO Executor: Finished task 20.0 in stage 3.0 (TID 68). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 91, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 43.0 in stage 3.0 (TID 91)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:59 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 68) in 1938 ms on localhost (28/200)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO Executor: Finished task 27.0 in stage 3.0 (TID 75). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO Executor: Finished task 21.0 in stage 3.0 (TID 69). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 92, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 44.0 in stage 3.0 (TID 92)
15/08/21 21:30:59 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 93, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Finished task 30.0 in stage 3.0 (TID 78). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO Executor: Running task 45.0 in stage 3.0 (TID 93)
15/08/21 21:30:59 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 94, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 46.0 in stage 3.0 (TID 94)
15/08/21 21:30:59 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 75) in 1854 ms on localhost (29/200)
15/08/21 21:30:59 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 69) in 1940 ms on localhost (30/200)
15/08/21 21:30:59 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 78) in 1779 ms on localhost (31/200)
15/08/21 21:30:59 INFO Executor: Finished task 31.0 in stage 3.0 (TID 79). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:59 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 95, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 79) in 1754 ms on localhost (32/200)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO Executor: Running task 47.0 in stage 3.0 (TID 95)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO Executor: Finished task 32.0 in stage 3.0 (TID 80). 1219 bytes result sent to driver
15/08/21 21:30:59 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 96, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:30:59 INFO Executor: Running task 48.0 in stage 3.0 (TID 96)
15/08/21 21:30:59 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 80) in 1186 ms on localhost (33/200)
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:30:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:00 INFO Executor: Finished task 34.0 in stage 3.0 (TID 82). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 97, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 49.0 in stage 3.0 (TID 97)
15/08/21 21:31:00 INFO Executor: Finished task 33.0 in stage 3.0 (TID 81). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 82) in 1228 ms on localhost (34/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 98, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 50.0 in stage 3.0 (TID 98)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:00 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 81) in 1246 ms on localhost (35/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO Executor: Finished task 37.0 in stage 3.0 (TID 85). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 99, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 51.0 in stage 3.0 (TID 99)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 85) in 1206 ms on localhost (36/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO Executor: Finished task 35.0 in stage 3.0 (TID 83). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 100, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 52.0 in stage 3.0 (TID 100)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 83) in 1305 ms on localhost (37/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO Executor: Finished task 38.0 in stage 3.0 (TID 86). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 101, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 53.0 in stage 3.0 (TID 101)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 86) in 1158 ms on localhost (38/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO Executor: Finished task 39.0 in stage 3.0 (TID 87). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 102, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 54.0 in stage 3.0 (TID 102)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 87) in 1207 ms on localhost (39/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 21:31:00 INFO Executor: Finished task 36.0 in stage 3.0 (TID 84). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 103, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 55.0 in stage 3.0 (TID 103)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 84) in 1447 ms on localhost (40/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:00 INFO Executor: Finished task 42.0 in stage 3.0 (TID 90). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 104, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 56.0 in stage 3.0 (TID 104)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 90) in 1369 ms on localhost (41/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO Executor: Finished task 40.0 in stage 3.0 (TID 88). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 105, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 57.0 in stage 3.0 (TID 105)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 88) in 1481 ms on localhost (42/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO Executor: Finished task 45.0 in stage 3.0 (TID 93). 1219 bytes result sent to driver
15/08/21 21:31:00 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 106, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:00 INFO Executor: Running task 58.0 in stage 3.0 (TID 106)
15/08/21 21:31:00 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 93) in 1376 ms on localhost (43/200)
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO Executor: Finished task 41.0 in stage 3.0 (TID 89). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 107, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 59.0 in stage 3.0 (TID 107)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 89) in 1655 ms on localhost (44/200)
15/08/21 21:31:01 INFO Executor: Finished task 46.0 in stage 3.0 (TID 94). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 108, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 60.0 in stage 3.0 (TID 108)
15/08/21 21:31:01 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 94) in 1554 ms on localhost (45/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO Executor: Finished task 43.0 in stage 3.0 (TID 91). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 109, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 61.0 in stage 3.0 (TID 109)
15/08/21 21:31:01 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 91) in 1642 ms on localhost (46/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO Executor: Finished task 44.0 in stage 3.0 (TID 92). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 110, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 62.0 in stage 3.0 (TID 110)
15/08/21 21:31:01 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 92) in 1613 ms on localhost (47/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO Executor: Finished task 47.0 in stage 3.0 (TID 95). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO Executor: Finished task 53.0 in stage 3.0 (TID 101). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 111, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 112, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 63.0 in stage 3.0 (TID 111)
15/08/21 21:31:01 INFO Executor: Running task 64.0 in stage 3.0 (TID 112)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:01 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 95) in 1802 ms on localhost (48/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:01 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 101) in 1108 ms on localhost (49/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:01 INFO Executor: Finished task 56.0 in stage 3.0 (TID 104). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 113, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 65.0 in stage 3.0 (TID 113)
15/08/21 21:31:01 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 104) in 723 ms on localhost (50/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:01 INFO Executor: Finished task 49.0 in stage 3.0 (TID 97). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 114, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 66.0 in stage 3.0 (TID 114)
15/08/21 21:31:01 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 97) in 1471 ms on localhost (51/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:01 INFO Executor: Finished task 48.0 in stage 3.0 (TID 96). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 115, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 67.0 in stage 3.0 (TID 115)
15/08/21 21:31:01 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 96) in 1717 ms on localhost (52/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO Executor: Finished task 50.0 in stage 3.0 (TID 98). 1219 bytes result sent to driver
15/08/21 21:31:01 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 116, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:01 INFO Executor: Running task 68.0 in stage 3.0 (TID 116)
15/08/21 21:31:01 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 98) in 1534 ms on localhost (53/200)
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO Executor: Finished task 51.0 in stage 3.0 (TID 99). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 117, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO Executor: Running task 69.0 in stage 3.0 (TID 117)
15/08/21 21:31:02 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 99) in 2360 ms on localhost (54/200)
15/08/21 21:31:02 INFO Executor: Finished task 58.0 in stage 3.0 (TID 106). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 118, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO Executor: Running task 70.0 in stage 3.0 (TID 118)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:02 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 106) in 1640 ms on localhost (55/200)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO Executor: Finished task 52.0 in stage 3.0 (TID 100). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 119, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO Executor: Running task 71.0 in stage 3.0 (TID 119)
15/08/21 21:31:02 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 100) in 2416 ms on localhost (56/200)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO Executor: Finished task 55.0 in stage 3.0 (TID 103). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 120, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO Executor: Running task 72.0 in stage 3.0 (TID 120)
15/08/21 21:31:02 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 103) in 2459 ms on localhost (57/200)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO Executor: Finished task 59.0 in stage 3.0 (TID 107). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 121, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO Executor: Running task 73.0 in stage 3.0 (TID 121)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 107) in 1865 ms on localhost (58/200)
15/08/21 21:31:02 INFO Executor: Finished task 54.0 in stage 3.0 (TID 102). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 122, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 102) in 2561 ms on localhost (59/200)
15/08/21 21:31:02 INFO Executor: Running task 74.0 in stage 3.0 (TID 122)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO Executor: Finished task 61.0 in stage 3.0 (TID 109). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 123, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO Executor: Running task 75.0 in stage 3.0 (TID 123)
15/08/21 21:31:02 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 109) in 1851 ms on localhost (60/200)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:02 INFO Executor: Finished task 60.0 in stage 3.0 (TID 108). 1219 bytes result sent to driver
15/08/21 21:31:02 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 124, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:02 INFO Executor: Running task 76.0 in stage 3.0 (TID 124)
15/08/21 21:31:02 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 108) in 1912 ms on localhost (61/200)
15/08/21 21:31:02 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO Executor: Finished task 62.0 in stage 3.0 (TID 110). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 125, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 77.0 in stage 3.0 (TID 125)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 110) in 2164 ms on localhost (62/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO Executor: Finished task 64.0 in stage 3.0 (TID 112). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 126, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 78.0 in stage 3.0 (TID 126)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 112) in 2034 ms on localhost (63/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO Executor: Finished task 57.0 in stage 3.0 (TID 105). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 127, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 105) in 2607 ms on localhost (64/200)
15/08/21 21:31:03 INFO Executor: Running task 79.0 in stage 3.0 (TID 127)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO Executor: Finished task 65.0 in stage 3.0 (TID 113). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 128, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 80.0 in stage 3.0 (TID 128)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 113) in 2050 ms on localhost (65/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO Executor: Finished task 67.0 in stage 3.0 (TID 115). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 129, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 115) in 1970 ms on localhost (66/200)
15/08/21 21:31:03 INFO Executor: Finished task 68.0 in stage 3.0 (TID 116). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO Executor: Running task 81.0 in stage 3.0 (TID 129)
15/08/21 21:31:03 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 130, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 116) in 1952 ms on localhost (67/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO Executor: Running task 82.0 in stage 3.0 (TID 130)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO Executor: Finished task 70.0 in stage 3.0 (TID 118). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 131, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 83.0 in stage 3.0 (TID 131)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 118) in 1062 ms on localhost (68/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 21:31:03 INFO Executor: Finished task 71.0 in stage 3.0 (TID 119). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 132, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 84.0 in stage 3.0 (TID 132)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 119) in 1035 ms on localhost (69/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO Executor: Finished task 63.0 in stage 3.0 (TID 111). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 133, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 85.0 in stage 3.0 (TID 133)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 111) in 2353 ms on localhost (70/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:03 INFO Executor: Finished task 66.0 in stage 3.0 (TID 114). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 134, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 86.0 in stage 3.0 (TID 134)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 114) in 2218 ms on localhost (71/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO Executor: Finished task 69.0 in stage 3.0 (TID 117). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 135, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 87.0 in stage 3.0 (TID 135)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 117) in 1348 ms on localhost (72/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO Executor: Finished task 73.0 in stage 3.0 (TID 121). 1219 bytes result sent to driver
15/08/21 21:31:03 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 136, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:03 INFO Executor: Running task 88.0 in stage 3.0 (TID 136)
15/08/21 21:31:03 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 121) in 1066 ms on localhost (73/200)
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO Executor: Finished task 74.0 in stage 3.0 (TID 122). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 137, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 89.0 in stage 3.0 (TID 137)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 122) in 1181 ms on localhost (74/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO Executor: Finished task 76.0 in stage 3.0 (TID 124). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 138, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 90.0 in stage 3.0 (TID 138)
15/08/21 21:31:04 INFO Executor: Finished task 72.0 in stage 3.0 (TID 120). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 139, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 91.0 in stage 3.0 (TID 139)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 124) in 1168 ms on localhost (75/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 120) in 1330 ms on localhost (76/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO Executor: Finished task 75.0 in stage 3.0 (TID 123). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 140, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 92.0 in stage 3.0 (TID 140)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 123) in 1281 ms on localhost (77/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO Executor: Finished task 80.0 in stage 3.0 (TID 128). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 141, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 93.0 in stage 3.0 (TID 141)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 128) in 950 ms on localhost (78/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO Executor: Finished task 78.0 in stage 3.0 (TID 126). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 142, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 94.0 in stage 3.0 (TID 142)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 126) in 1145 ms on localhost (79/200)
15/08/21 21:31:04 INFO Executor: Finished task 77.0 in stage 3.0 (TID 125). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 143, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 95.0 in stage 3.0 (TID 143)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 125) in 1292 ms on localhost (80/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO Executor: Finished task 79.0 in stage 3.0 (TID 127). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 144, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 96.0 in stage 3.0 (TID 144)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 127) in 1233 ms on localhost (81/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO Executor: Finished task 83.0 in stage 3.0 (TID 131). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 145, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 97.0 in stage 3.0 (TID 145)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 131) in 1068 ms on localhost (82/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO Executor: Finished task 81.0 in stage 3.0 (TID 129). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 146, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 98.0 in stage 3.0 (TID 146)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 129) in 1120 ms on localhost (83/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO Executor: Finished task 84.0 in stage 3.0 (TID 132). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 147, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 99.0 in stage 3.0 (TID 147)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 132) in 1068 ms on localhost (84/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO Executor: Finished task 82.0 in stage 3.0 (TID 130). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 148, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Finished task 85.0 in stage 3.0 (TID 133). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO Executor: Running task 100.0 in stage 3.0 (TID 148)
15/08/21 21:31:04 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 149, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 101.0 in stage 3.0 (TID 149)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 130) in 1224 ms on localhost (85/200)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 133) in 1107 ms on localhost (86/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO Executor: Finished task 86.0 in stage 3.0 (TID 134). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 150, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 102.0 in stage 3.0 (TID 150)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 134) in 1077 ms on localhost (87/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO Executor: Finished task 87.0 in stage 3.0 (TID 135). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 151, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 103.0 in stage 3.0 (TID 151)
15/08/21 21:31:04 INFO Executor: Finished task 89.0 in stage 3.0 (TID 137). 1219 bytes result sent to driver
15/08/21 21:31:04 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 152, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:04 INFO Executor: Running task 104.0 in stage 3.0 (TID 152)
15/08/21 21:31:04 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 135) in 1063 ms on localhost (88/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 137) in 875 ms on localhost (89/200)
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 88.0 in stage 3.0 (TID 136). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 153, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 105.0 in stage 3.0 (TID 153)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 136) in 1201 ms on localhost (90/200)
15/08/21 21:31:05 INFO Executor: Finished task 90.0 in stage 3.0 (TID 138). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 154, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 106.0 in stage 3.0 (TID 154)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 138) in 980 ms on localhost (91/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 92.0 in stage 3.0 (TID 140). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 155, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 107.0 in stage 3.0 (TID 155)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 140) in 1030 ms on localhost (92/200)
15/08/21 21:31:05 INFO Executor: Finished task 91.0 in stage 3.0 (TID 139). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 156, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 108.0 in stage 3.0 (TID 156)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 139) in 1123 ms on localhost (93/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 94.0 in stage 3.0 (TID 142). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 157, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 109.0 in stage 3.0 (TID 157)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 142) in 1084 ms on localhost (94/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO Executor: Finished task 95.0 in stage 3.0 (TID 143). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 158, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 110.0 in stage 3.0 (TID 158)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 143) in 1019 ms on localhost (95/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 93.0 in stage 3.0 (TID 141). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 159, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 111.0 in stage 3.0 (TID 159)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 141) in 1164 ms on localhost (96/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 97.0 in stage 3.0 (TID 145). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 160, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 112.0 in stage 3.0 (TID 160)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 145) in 1038 ms on localhost (97/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 96.0 in stage 3.0 (TID 144). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 161, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 113.0 in stage 3.0 (TID 161)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 144) in 1165 ms on localhost (98/200)
15/08/21 21:31:05 INFO Executor: Finished task 99.0 in stage 3.0 (TID 147). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 162, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 114.0 in stage 3.0 (TID 162)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 147) in 1101 ms on localhost (99/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 98.0 in stage 3.0 (TID 146). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 163, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 115.0 in stage 3.0 (TID 163)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 146) in 1141 ms on localhost (100/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO Executor: Finished task 100.0 in stage 3.0 (TID 148). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 164, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 116.0 in stage 3.0 (TID 164)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 148) in 1065 ms on localhost (101/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 102.0 in stage 3.0 (TID 150). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO Executor: Finished task 103.0 in stage 3.0 (TID 151). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 165, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 117.0 in stage 3.0 (TID 165)
15/08/21 21:31:05 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 166, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 118.0 in stage 3.0 (TID 166)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 150) in 1075 ms on localhost (102/200)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 151) in 982 ms on localhost (103/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO Executor: Finished task 101.0 in stage 3.0 (TID 149). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 167, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 119.0 in stage 3.0 (TID 167)
15/08/21 21:31:05 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 149) in 1151 ms on localhost (104/200)
15/08/21 21:31:05 INFO Executor: Finished task 104.0 in stage 3.0 (TID 152). 1219 bytes result sent to driver
15/08/21 21:31:05 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:05 INFO Executor: Running task 120.0 in stage 3.0 (TID 168)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 152) in 1019 ms on localhost (105/200)
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:06 INFO Executor: Finished task 105.0 in stage 3.0 (TID 153). 1219 bytes result sent to driver
15/08/21 21:31:06 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:06 INFO Executor: Running task 121.0 in stage 3.0 (TID 169)
15/08/21 21:31:06 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 153) in 907 ms on localhost (106/200)
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:06 INFO Executor: Finished task 107.0 in stage 3.0 (TID 155). 1219 bytes result sent to driver
15/08/21 21:31:06 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 170, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:06 INFO Executor: Running task 122.0 in stage 3.0 (TID 170)
15/08/21 21:31:06 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 155) in 882 ms on localhost (107/200)
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:06 INFO Executor: Finished task 106.0 in stage 3.0 (TID 154). 1219 bytes result sent to driver
15/08/21 21:31:06 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 171, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:06 INFO Executor: Running task 123.0 in stage 3.0 (TID 171)
15/08/21 21:31:06 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 154) in 1071 ms on localhost (108/200)
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:06 INFO Executor: Finished task 108.0 in stage 3.0 (TID 156). 1219 bytes result sent to driver
15/08/21 21:31:06 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 172, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:06 INFO Executor: Running task 124.0 in stage 3.0 (TID 172)
15/08/21 21:31:06 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 156) in 937 ms on localhost (109/200)
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:06 INFO Executor: Finished task 109.0 in stage 3.0 (TID 157). 1219 bytes result sent to driver
15/08/21 21:31:06 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 173, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:06 INFO Executor: Running task 125.0 in stage 3.0 (TID 173)
15/08/21 21:31:06 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 157) in 1327 ms on localhost (110/200)
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:06 INFO Executor: Finished task 110.0 in stage 3.0 (TID 158). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 174, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 126.0 in stage 3.0 (TID 174)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 158) in 1428 ms on localhost (111/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO Executor: Finished task 112.0 in stage 3.0 (TID 160). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 175, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 127.0 in stage 3.0 (TID 175)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 160) in 1412 ms on localhost (112/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 111.0 in stage 3.0 (TID 159). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 176, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 128.0 in stage 3.0 (TID 176)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 159) in 1506 ms on localhost (113/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO Executor: Finished task 114.0 in stage 3.0 (TID 162). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 177, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 162) in 1500 ms on localhost (114/200)
15/08/21 21:31:07 INFO Executor: Running task 129.0 in stage 3.0 (TID 177)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO Executor: Finished task 115.0 in stage 3.0 (TID 163). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 178, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 130.0 in stage 3.0 (TID 178)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 163) in 1712 ms on localhost (115/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 113.0 in stage 3.0 (TID 161). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 179, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 161) in 1815 ms on localhost (116/200)
15/08/21 21:31:07 INFO Executor: Running task 131.0 in stage 3.0 (TID 179)
15/08/21 21:31:07 INFO Executor: Finished task 117.0 in stage 3.0 (TID 165). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 180, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 132.0 in stage 3.0 (TID 180)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 165) in 1700 ms on localhost (117/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 124.0 in stage 3.0 (TID 172). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 181, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 133.0 in stage 3.0 (TID 181)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 172) in 1452 ms on localhost (118/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 119.0 in stage 3.0 (TID 167). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 182, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 134.0 in stage 3.0 (TID 182)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 167) in 1768 ms on localhost (119/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 118.0 in stage 3.0 (TID 166). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 183, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 135.0 in stage 3.0 (TID 183)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 166) in 1903 ms on localhost (120/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 120.0 in stage 3.0 (TID 168). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 184, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 136.0 in stage 3.0 (TID 184)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 1904 ms on localhost (121/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 116.0 in stage 3.0 (TID 164). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 185, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 137.0 in stage 3.0 (TID 185)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 164) in 2038 ms on localhost (122/200)
15/08/21 21:31:07 INFO Executor: Finished task 122.0 in stage 3.0 (TID 170). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 186, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 138.0 in stage 3.0 (TID 186)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 170) in 1768 ms on localhost (123/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:07 INFO Executor: Finished task 121.0 in stage 3.0 (TID 169). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 187, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 139.0 in stage 3.0 (TID 187)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 1941 ms on localhost (124/200)
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:07 INFO Executor: Finished task 126.0 in stage 3.0 (TID 174). 1219 bytes result sent to driver
15/08/21 21:31:07 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 188, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:07 INFO Executor: Running task 140.0 in stage 3.0 (TID 188)
15/08/21 21:31:07 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 174) in 980 ms on localhost (125/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 123.0 in stage 3.0 (TID 171). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 189, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 141.0 in stage 3.0 (TID 189)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 171) in 1863 ms on localhost (126/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO Executor: Finished task 125.0 in stage 3.0 (TID 173). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 190, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 142.0 in stage 3.0 (TID 190)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 173) in 1251 ms on localhost (127/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 132.0 in stage 3.0 (TID 180). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 191, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 143.0 in stage 3.0 (TID 191)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 180) in 606 ms on localhost (128/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 127.0 in stage 3.0 (TID 175). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 192, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 144.0 in stage 3.0 (TID 192)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 175) in 1147 ms on localhost (129/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 129.0 in stage 3.0 (TID 177). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 193, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 145.0 in stage 3.0 (TID 193)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 177) in 1004 ms on localhost (130/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO Executor: Finished task 128.0 in stage 3.0 (TID 176). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 194, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 176) in 1213 ms on localhost (131/200)
15/08/21 21:31:08 INFO Executor: Running task 146.0 in stage 3.0 (TID 194)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 133.0 in stage 3.0 (TID 181). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 195, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 181) in 820 ms on localhost (132/200)
15/08/21 21:31:08 INFO Executor: Running task 147.0 in stage 3.0 (TID 195)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 130.0 in stage 3.0 (TID 178). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 196, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 178) in 1117 ms on localhost (133/200)
15/08/21 21:31:08 INFO Executor: Running task 148.0 in stage 3.0 (TID 196)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 131.0 in stage 3.0 (TID 179). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 197, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 149.0 in stage 3.0 (TID 197)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 179) in 1077 ms on localhost (134/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO Executor: Finished task 137.0 in stage 3.0 (TID 185). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 198, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 150.0 in stage 3.0 (TID 198)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 185) in 805 ms on localhost (135/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO Executor: Finished task 134.0 in stage 3.0 (TID 182). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO Executor: Finished task 135.0 in stage 3.0 (TID 183). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 199, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 151.0 in stage 3.0 (TID 199)
15/08/21 21:31:08 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 200, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 152.0 in stage 3.0 (TID 200)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 183) in 998 ms on localhost (136/200)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 182) in 1094 ms on localhost (137/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 144.0 in stage 3.0 (TID 192). 1219 bytes result sent to driver
15/08/21 21:31:08 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 201, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:08 INFO Executor: Running task 153.0 in stage 3.0 (TID 201)
15/08/21 21:31:08 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 192) in 683 ms on localhost (138/200)
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:08 INFO Executor: Finished task 139.0 in stage 3.0 (TID 187). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 202, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 154.0 in stage 3.0 (TID 202)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 187) in 1070 ms on localhost (139/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 140.0 in stage 3.0 (TID 188). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 203, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 155.0 in stage 3.0 (TID 203)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 188) in 1128 ms on localhost (140/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO Executor: Finished task 141.0 in stage 3.0 (TID 189). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO Executor: Finished task 136.0 in stage 3.0 (TID 184). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 204, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 156.0 in stage 3.0 (TID 204)
15/08/21 21:31:09 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 205, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 157.0 in stage 3.0 (TID 205)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 189) in 1082 ms on localhost (141/200)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 184) in 1284 ms on localhost (142/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 142.0 in stage 3.0 (TID 190). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 206, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 190) in 1078 ms on localhost (143/200)
15/08/21 21:31:09 INFO Executor: Running task 158.0 in stage 3.0 (TID 206)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 147.0 in stage 3.0 (TID 195). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 207, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO Executor: Running task 159.0 in stage 3.0 (TID 207)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 195) in 764 ms on localhost (144/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 145.0 in stage 3.0 (TID 193). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 208, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 160.0 in stage 3.0 (TID 208)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 193) in 1128 ms on localhost (145/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 138.0 in stage 3.0 (TID 186). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 209, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 161.0 in stage 3.0 (TID 209)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 186) in 1533 ms on localhost (146/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO Executor: Finished task 149.0 in stage 3.0 (TID 197). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 210, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 162.0 in stage 3.0 (TID 210)
15/08/21 21:31:09 INFO Executor: Finished task 146.0 in stage 3.0 (TID 194). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 197) in 863 ms on localhost (147/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 211, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 163.0 in stage 3.0 (TID 211)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 194) in 1199 ms on localhost (148/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 151.0 in stage 3.0 (TID 199). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 212, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 164.0 in stage 3.0 (TID 212)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 199) in 785 ms on localhost (149/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 143.0 in stage 3.0 (TID 191). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 213, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 165.0 in stage 3.0 (TID 213)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 191) in 1438 ms on localhost (150/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 150.0 in stage 3.0 (TID 198). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO Executor: Finished task 152.0 in stage 3.0 (TID 200). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 214, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 215, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 167.0 in stage 3.0 (TID 215)
15/08/21 21:31:09 INFO Executor: Running task 166.0 in stage 3.0 (TID 214)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 198) in 981 ms on localhost (151/200)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 200) in 875 ms on localhost (152/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 148.0 in stage 3.0 (TID 196). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 216, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 196) in 1145 ms on localhost (153/200)
15/08/21 21:31:09 INFO Executor: Running task 168.0 in stage 3.0 (TID 216)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 153.0 in stage 3.0 (TID 201). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 217, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 169.0 in stage 3.0 (TID 217)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 201) in 977 ms on localhost (154/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO Executor: Finished task 159.0 in stage 3.0 (TID 207). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 218, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 170.0 in stage 3.0 (TID 218)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 207) in 744 ms on localhost (155/200)
15/08/21 21:31:09 INFO Executor: Finished task 155.0 in stage 3.0 (TID 203). 1219 bytes result sent to driver
15/08/21 21:31:09 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 219, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:09 INFO Executor: Running task 171.0 in stage 3.0 (TID 219)
15/08/21 21:31:09 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 203) in 879 ms on localhost (156/200)
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO Executor: Finished task 161.0 in stage 3.0 (TID 209). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 220, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO Executor: Running task 172.0 in stage 3.0 (TID 220)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 209) in 702 ms on localhost (157/200)
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO Executor: Finished task 154.0 in stage 3.0 (TID 202). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 221, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO Executor: Running task 173.0 in stage 3.0 (TID 221)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 202) in 1181 ms on localhost (158/200)
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO Executor: Finished task 158.0 in stage 3.0 (TID 206). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO Executor: Finished task 157.0 in stage 3.0 (TID 205). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO Executor: Finished task 156.0 in stage 3.0 (TID 204). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 222, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO Executor: Running task 174.0 in stage 3.0 (TID 222)
15/08/21 21:31:10 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 223, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 224, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO Executor: Running task 175.0 in stage 3.0 (TID 223)
15/08/21 21:31:10 INFO Executor: Running task 176.0 in stage 3.0 (TID 224)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 206) in 1173 ms on localhost (159/200)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 204) in 1266 ms on localhost (160/200)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 205) in 1267 ms on localhost (161/200)
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:10 INFO Executor: Finished task 164.0 in stage 3.0 (TID 212). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:10 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 225, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 212) in 826 ms on localhost (162/200)
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO Executor: Running task 177.0 in stage 3.0 (TID 225)
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:10 INFO Executor: Finished task 160.0 in stage 3.0 (TID 208). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 226, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO Executor: Running task 178.0 in stage 3.0 (TID 226)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 208) in 1125 ms on localhost (163/200)
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO Executor: Finished task 162.0 in stage 3.0 (TID 210). 1219 bytes result sent to driver
15/08/21 21:31:10 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 227, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:10 INFO Executor: Running task 179.0 in stage 3.0 (TID 227)
15/08/21 21:31:10 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 210) in 1019 ms on localhost (164/200)
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO Executor: Finished task 166.0 in stage 3.0 (TID 214). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 228, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 180.0 in stage 3.0 (TID 228)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 214) in 1658 ms on localhost (165/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO Executor: Finished task 163.0 in stage 3.0 (TID 211). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 229, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 211) in 1859 ms on localhost (166/200)
15/08/21 21:31:11 INFO Executor: Running task 181.0 in stage 3.0 (TID 229)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO Executor: Finished task 171.0 in stage 3.0 (TID 219). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 230, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 182.0 in stage 3.0 (TID 230)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 219) in 1444 ms on localhost (167/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO Executor: Finished task 165.0 in stage 3.0 (TID 213). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 231, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 183.0 in stage 3.0 (TID 231)
15/08/21 21:31:11 INFO Executor: Finished task 167.0 in stage 3.0 (TID 215). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 213) in 1804 ms on localhost (168/200)
15/08/21 21:31:11 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 232, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 184.0 in stage 3.0 (TID 232)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 215) in 1777 ms on localhost (169/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO Executor: Finished task 168.0 in stage 3.0 (TID 216). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 233, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 185.0 in stage 3.0 (TID 233)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 216) in 1698 ms on localhost (170/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO Executor: Finished task 170.0 in stage 3.0 (TID 218). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 234, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 186.0 in stage 3.0 (TID 234)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 218) in 1552 ms on localhost (171/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO Executor: Finished task 172.0 in stage 3.0 (TID 220). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 235, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 187.0 in stage 3.0 (TID 235)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 220) in 1486 ms on localhost (172/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:11 INFO Executor: Finished task 169.0 in stage 3.0 (TID 217). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 236, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 188.0 in stage 3.0 (TID 236)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 217) in 1732 ms on localhost (173/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:11 INFO Executor: Finished task 173.0 in stage 3.0 (TID 221). 1219 bytes result sent to driver
15/08/21 21:31:11 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 237, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:11 INFO Executor: Running task 189.0 in stage 3.0 (TID 237)
15/08/21 21:31:11 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 221) in 1666 ms on localhost (174/200)
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO Executor: Finished task 177.0 in stage 3.0 (TID 225). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 238, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 190.0 in stage 3.0 (TID 238)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 225) in 1749 ms on localhost (175/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO Executor: Finished task 176.0 in stage 3.0 (TID 224). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 239, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 191.0 in stage 3.0 (TID 239)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 224) in 1779 ms on localhost (176/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO Executor: Finished task 179.0 in stage 3.0 (TID 227). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 240, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 192.0 in stage 3.0 (TID 240)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 227) in 1744 ms on localhost (177/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO Executor: Finished task 180.0 in stage 3.0 (TID 228). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 241, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 193.0 in stage 3.0 (TID 241)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 228) in 1084 ms on localhost (178/200)
15/08/21 21:31:12 INFO Executor: Finished task 175.0 in stage 3.0 (TID 223). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 242, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 194.0 in stage 3.0 (TID 242)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 223) in 2023 ms on localhost (179/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO Executor: Finished task 174.0 in stage 3.0 (TID 222). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 243, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 195.0 in stage 3.0 (TID 243)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 222) in 2154 ms on localhost (180/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO Executor: Finished task 183.0 in stage 3.0 (TID 231). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 244, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 196.0 in stage 3.0 (TID 244)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 231) in 1122 ms on localhost (181/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO Executor: Finished task 184.0 in stage 3.0 (TID 232). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO Executor: Finished task 178.0 in stage 3.0 (TID 226). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 245, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 246, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 198.0 in stage 3.0 (TID 246)
15/08/21 21:31:12 INFO Executor: Running task 197.0 in stage 3.0 (TID 245)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 232) in 1252 ms on localhost (182/200)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 226) in 2182 ms on localhost (183/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:12 INFO Executor: Finished task 181.0 in stage 3.0 (TID 229). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 247, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 21:31:12 INFO Executor: Running task 199.0 in stage 3.0 (TID 247)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 229) in 1360 ms on localhost (184/200)
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 21:31:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:12 INFO Executor: Finished task 185.0 in stage 3.0 (TID 233). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 233) in 1280 ms on localhost (185/200)
15/08/21 21:31:12 INFO Executor: Finished task 182.0 in stage 3.0 (TID 230). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 230) in 1348 ms on localhost (186/200)
15/08/21 21:31:12 INFO Executor: Finished task 187.0 in stage 3.0 (TID 235). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 235) in 1203 ms on localhost (187/200)
15/08/21 21:31:12 INFO Executor: Finished task 189.0 in stage 3.0 (TID 237). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO Executor: Finished task 186.0 in stage 3.0 (TID 234). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 237) in 981 ms on localhost (188/200)
15/08/21 21:31:12 INFO Executor: Finished task 188.0 in stage 3.0 (TID 236). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 234) in 1326 ms on localhost (189/200)
15/08/21 21:31:12 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 236) in 1222 ms on localhost (190/200)
15/08/21 21:31:12 INFO Executor: Finished task 190.0 in stage 3.0 (TID 238). 1219 bytes result sent to driver
15/08/21 21:31:12 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 238) in 792 ms on localhost (191/200)
15/08/21 21:31:13 INFO Executor: Finished task 191.0 in stage 3.0 (TID 239). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 239) in 1104 ms on localhost (192/200)
15/08/21 21:31:13 INFO Executor: Finished task 194.0 in stage 3.0 (TID 242). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 242) in 925 ms on localhost (193/200)
15/08/21 21:31:13 INFO Executor: Finished task 192.0 in stage 3.0 (TID 240). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 240) in 1117 ms on localhost (194/200)
15/08/21 21:31:13 INFO Executor: Finished task 193.0 in stage 3.0 (TID 241). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 241) in 1022 ms on localhost (195/200)
15/08/21 21:31:13 INFO Executor: Finished task 195.0 in stage 3.0 (TID 243). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 243) in 913 ms on localhost (196/200)
15/08/21 21:31:13 INFO Executor: Finished task 198.0 in stage 3.0 (TID 246). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 246) in 842 ms on localhost (197/200)
15/08/21 21:31:13 INFO Executor: Finished task 197.0 in stage 3.0 (TID 245). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 245) in 870 ms on localhost (198/200)
15/08/21 21:31:13 INFO Executor: Finished task 196.0 in stage 3.0 (TID 244). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 244) in 1014 ms on localhost (199/200)
15/08/21 21:31:13 INFO Executor: Finished task 199.0 in stage 3.0 (TID 247). 1219 bytes result sent to driver
15/08/21 21:31:13 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 247) in 873 ms on localhost (200/200)
15/08/21 21:31:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/21 21:31:13 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 19.799 s
15/08/21 21:31:13 INFO DAGScheduler: looking for newly runnable stages
15/08/21 21:31:13 INFO DAGScheduler: running: Set()
15/08/21 21:31:13 INFO DAGScheduler: waiting: Set(ResultStage 4)
15/08/21 21:31:13 INFO DAGScheduler: failed: Set()
15/08/21 21:31:13 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@6d815c42
15/08/21 21:31:13 INFO StatsReportListener: task runtime:(count: 200, mean: 1556.835000, stdev: 783.704490, max: 4016.000000, min: 606.000000)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO StatsReportListener: 	606.0 ms	826.0 ms	913.0 ms	1.1 s	1.3 s	1.8 s	2.4 s	3.9 s	4.0 s
15/08/21 21:31:13 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 243498.685000, stdev: 16472.291857, max: 288923.000000, min: 202930.000000)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO DAGScheduler: Missing parents for ResultStage 4: List()
15/08/21 21:31:13 INFO StatsReportListener: 	198.2 KB	213.0 KB	218.2 KB	225.2 KB	237.2 KB	247.6 KB	260.7 KB	264.8 KB	282.2 KB
15/08/21 21:31:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 21:31:13 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.545000, stdev: 0.963315, max: 9.000000, min: 0.000000)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	2.0 ms	9.0 ms
15/08/21 21:31:13 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 21:31:13 INFO StatsReportListener: task result size:(count: 200, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/21 21:31:13 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.865384, stdev: 0.885714, max: 99.258527, min: 93.913043)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO StatsReportListener: 	94 %	96 %	97 %	97 %	98 %	98 %	99 %	99 %	99 %
15/08/21 21:31:13 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.039321, stdev: 0.074920, max: 0.735294, min: 0.000000)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/21 21:31:13 INFO StatsReportListener: other time pct: (count: 200, mean: 2.095295, stdev: 0.874991, max: 6.086957, min: 0.692042)
15/08/21 21:31:13 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:13 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 2 %	 2 %	 3 %	 3 %	 4 %	 6 %
15/08/21 21:31:13 INFO MemoryStore: ensureFreeSpace(82504) called with curMem=1069608, maxMem=22226833244
15/08/21 21:31:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 80.6 KB, free 20.7 GB)
15/08/21 21:31:13 INFO MemoryStore: ensureFreeSpace(31754) called with curMem=1152112, maxMem=22226833244
15/08/21 21:31:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.0 KB, free 20.7 GB)
15/08/21 21:31:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:52592 (size: 31.0 KB, free: 20.7 GB)
15/08/21 21:31:13 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/21 21:31:13 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at processCmd at CliDriver.java:423)
15/08/21 21:31:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks
15/08/21 21:31:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:13 INFO Executor: Running task 3.0 in stage 4.0 (TID 251)
15/08/21 21:31:13 INFO Executor: Running task 7.0 in stage 4.0 (TID 255)
15/08/21 21:31:13 INFO Executor: Running task 2.0 in stage 4.0 (TID 250)
15/08/21 21:31:13 INFO Executor: Running task 10.0 in stage 4.0 (TID 258)
15/08/21 21:31:13 INFO Executor: Running task 15.0 in stage 4.0 (TID 263)
15/08/21 21:31:13 INFO Executor: Running task 1.0 in stage 4.0 (TID 249)
15/08/21 21:31:13 INFO Executor: Running task 4.0 in stage 4.0 (TID 252)
15/08/21 21:31:13 INFO Executor: Running task 5.0 in stage 4.0 (TID 253)
15/08/21 21:31:13 INFO Executor: Running task 9.0 in stage 4.0 (TID 257)
15/08/21 21:31:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 248)
15/08/21 21:31:13 INFO Executor: Running task 6.0 in stage 4.0 (TID 254)
15/08/21 21:31:13 INFO Executor: Running task 11.0 in stage 4.0 (TID 259)
15/08/21 21:31:13 INFO Executor: Running task 14.0 in stage 4.0 (TID 262)
15/08/21 21:31:13 INFO Executor: Running task 13.0 in stage 4.0 (TID 261)
15/08/21 21:31:13 INFO Executor: Running task 8.0 in stage 4.0 (TID 256)
15/08/21 21:31:13 INFO Executor: Running task 12.0 in stage 4.0 (TID 260)
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
ntext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124880 records.
21-Aug-2015 21:30:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 124880
21-Aug-2015 21:30:49 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:49 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124938 records.
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124895 records.
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 21 ms. row count = 124938
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 124895
21-Aug-2015 21:30:49 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124842 records.
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:49 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 124842
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 125782 records.
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 125782
21-Aug-2015 21:30:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 21:30:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124936 records.
21-Aug-2015 21:30:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124828 records.
21-Aug-2015 21:30:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 21:30:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 20 ms. row count = 124828
21-Aug-2015 21:30:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 124936
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
21-Aug-2015 21:31:14 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_015/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:14 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:14 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:14 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:14 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:14 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:14 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:14 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:14 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,068
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,752
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,948
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,648
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,548
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,540
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,436
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,396
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,048
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,948
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,076
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,452
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,936
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,368
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,848
15/08/21 21:31:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,012
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 48,712B for [ps_partkey] INT32: 14,564 values, 58,264B raw, 48,673B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 51,274B for [ps_partkey] INT32: 15,383 values, 61,540B raw, 51,235B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 48,680B for [ps_partkey] INT32: 14,560 values, 58,248B raw, 48,641B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,162B for [ps_partkey] INT32: 14,740 values, 58,968B raw, 49,123B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 74,922B for [part_value] DOUBLE: 14,564 values, 116,520B raw, 74,875B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 74,857B for [part_value] DOUBLE: 14,560 values, 116,488B raw, 74,810B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 79,068B for [part_value] DOUBLE: 15,383 values, 123,072B raw, 79,021B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 75,844B for [part_value] DOUBLE: 14,740 values, 117,928B raw, 75,797B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 50,251B for [ps_partkey] INT32: 15,038 values, 60,160B raw, 50,212B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,523B for [ps_partkey] INT32: 14,825 values, 59,308B raw, 49,484B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 77,368B for [part_value] DOUBLE: 15,038 values, 120,312B raw, 77,321B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 48,462B for [ps_partkey] INT32: 14,489 values, 57,964B raw, 48,423B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 50,483B for [ps_partkey] INT32: 15,106 values, 60,432B raw, 50,444B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,044B for [ps_partkey] INT32: 14,684 values, 58,744B raw, 49,005B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,596B for [ps_partkey] INT32: 14,829 values, 59,324B raw, 49,557B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,340B for [ps_partkey] INT32: 14,755 values, 59,028B raw, 49,301B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,574B for [ps_partkey] INT32: 14,919 values, 59,684B raw, 49,535B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 48,792B for [ps_partkey] INT32: 14,590 values, 58,368B raw, 48,753B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 76,306B for [part_value] DOUBLE: 14,825 values, 118,608B raw, 76,259B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,481B for [ps_partkey] INT32: 14,814 values, 59,264B raw, 49,442B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 49,185B for [ps_partkey] INT32: 14,784 values, 59,144B raw, 49,146B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 77,727B for [part_value] DOUBLE: 15,106 values, 120,856B raw, 77,680B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 74,523B for [part_value] DOUBLE: 14,489 values, 115,920B raw, 74,476B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 75,515B for [part_value] DOUBLE: 14,684 values, 117,480B raw, 75,468B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 75,891B for [part_value] DOUBLE: 14,755 values, 118,048B raw, 75,844B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 76,082B for [part_value] DOUBLE: 14,784 values, 118,280B raw, 76,035B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 76,230B for [part_value] DOUBLE: 14,829 values, 118,640B raw, 76,183B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 76,702B for [part_value] DOUBLE: 14,919 values, 119,360B raw, 76,655B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 76,190B for [part_value] DOUBLE: 14,814 values, 118,520B raw, 76,143B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 75,037B for [part_value] DOUBLE: 14,590 values, 116,728B raw, 74,990B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 50,296B for [ps_partkey] INT32: 15,058 values, 60,240B raw, 50,257B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO ColumnChunkPageWriteStore: written 77,520B for [part_value] DOUBLE: 15,058 values, 120,472B raw, 77,473B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000015
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000015_0: Committed
15/08/21 21:31:15 INFO Executor: Finished task 15.0 in stage 4.0 (TID 263). 843 bytes result sent to driver
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000007
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000008
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000007_0: Committed
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000000
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000009
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000000_0: Committed
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000008_0: Committed
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000013
15/08/21 21:31:15 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000013_0: Committed
15/08/21 21:31:15 INFO Executor: Finished task 7.0 in stage 4.0 (TID 255). 843 bytes result sent to driver
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000009_0: Committed
15/08/21 21:31:15 INFO Executor: Running task 16.0 in stage 4.0 (TID 264)
15/08/21 21:31:15 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Finished task 8.0 in stage 4.0 (TID 256). 843 bytes result sent to driver
15/08/21 21:31:15 INFO Executor: Running task 17.0 in stage 4.0 (TID 265)
15/08/21 21:31:15 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Running task 18.0 in stage 4.0 (TID 266)
15/08/21 21:31:15 INFO Executor: Finished task 13.0 in stage 4.0 (TID 261). 843 bytes result sent to driver
15/08/21 21:31:15 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 263) in 1873 ms on localhost (1/200)
15/08/21 21:31:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 248). 843 bytes result sent to driver
15/08/21 21:31:15 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 255) in 1879 ms on localhost (2/200)
15/08/21 21:31:15 INFO Executor: Running task 19.0 in stage 4.0 (TID 267)
15/08/21 21:31:15 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 256) in 1881 ms on localhost (3/200)
15/08/21 21:31:15 INFO Executor: Running task 20.0 in stage 4.0 (TID 268)
15/08/21 21:31:15 INFO Executor: Finished task 9.0 in stage 4.0 (TID 257). 843 bytes result sent to driver
15/08/21 21:31:15 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 261) in 1884 ms on localhost (4/200)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 248) in 1892 ms on localhost (5/200)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 257) in 1889 ms on localhost (6/200)
15/08/21 21:31:15 INFO Executor: Running task 21.0 in stage 4.0 (TID 269)
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000004
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000004_0: Committed
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000014
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000014_0: Committed
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000006
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000002
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000002_0: Committed
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000006_0: Committed
15/08/21 21:31:15 INFO Executor: Finished task 4.0 in stage 4.0 (TID 252). 843 bytes result sent to driver
15/08/21 21:31:15 INFO Executor: Finished task 14.0 in stage 4.0 (TID 262). 843 bytes result sent to driver
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000003
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000003_0: Committed
15/08/21 21:31:15 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000010
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000001
15/08/21 21:31:15 INFO Executor: Finished task 3.0 in stage 4.0 (TID 251). 843 bytes result sent to driver
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000010_0: Committed
15/08/21 21:31:15 INFO Executor: Running task 22.0 in stage 4.0 (TID 270)
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000001_0: Committed
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000011
15/08/21 21:31:15 INFO Executor: Finished task 2.0 in stage 4.0 (TID 250). 843 bytes result sent to driver
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000011_0: Committed
15/08/21 21:31:15 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000005
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000005_0: Committed
15/08/21 21:31:15 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Finished task 10.0 in stage 4.0 (TID 258). 843 bytes result sent to driver
15/08/21 21:31:15 INFO Executor: Running task 25.0 in stage 4.0 (TID 273)
15/08/21 21:31:15 INFO Executor: Finished task 1.0 in stage 4.0 (TID 249). 843 bytes result sent to driver
15/08/21 21:31:15 INFO Executor: Finished task 11.0 in stage 4.0 (TID 259). 843 bytes result sent to driver
15/08/21 21:31:15 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Finished task 6.0 in stage 4.0 (TID 254). 843 bytes result sent to driver
15/08/21 21:31:15 INFO Executor: Finished task 5.0 in stage 4.0 (TID 253). 843 bytes result sent to driver
15/08/21 21:31:15 INFO Executor: Running task 23.0 in stage 4.0 (TID 271)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 252) in 1928 ms on localhost (7/200)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 250) in 1929 ms on localhost (8/200)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 251) in 1928 ms on localhost (9/200)
15/08/21 21:31:15 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Running task 24.0 in stage 4.0 (TID 272)
15/08/21 21:31:15 INFO Executor: Running task 26.0 in stage 4.0 (TID 274)
15/08/21 21:31:15 INFO Executor: Running task 27.0 in stage 4.0 (TID 275)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 262) in 1928 ms on localhost (10/200)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 258) in 1931 ms on localhost (11/200)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 249) in 1935 ms on localhost (12/200)
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:15 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Running task 28.0 in stage 4.0 (TID 276)
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:15 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 259) in 1937 ms on localhost (13/200)
15/08/21 21:31:15 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 254) in 1942 ms on localhost (14/200)
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Running task 29.0 in stage 4.0 (TID 277)
15/08/21 21:31:15 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:15 INFO Executor: Running task 30.0 in stage 4.0 (TID 278)
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 253) in 1946 ms on localhost (15/200)
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:15 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:15 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000012
15/08/21 21:31:15 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000012_0: Committed
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO Executor: Finished task 12.0 in stage 4.0 (TID 260). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 31.0 in stage 4.0 (TID 279)
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,836
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 260) in 2272 ms on localhost (16/200)
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,148
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,744,160
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,440
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,156
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,900
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,928
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,840
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,062B for [ps_partkey] INT32: 14,678 values, 58,720B raw, 49,023B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 75,433B for [part_value] DOUBLE: 14,678 values, 117,432B raw, 75,386B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,910B for [ps_partkey] INT32: 14,944 values, 59,784B raw, 49,871B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,840
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 76,958B for [part_value] DOUBLE: 14,944 values, 119,560B raw, 76,911B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,308
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,048
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,596
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,060
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 47,681B for [ps_partkey] INT32: 14,345 values, 57,388B raw, 47,642B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 48,495B for [ps_partkey] INT32: 14,509 values, 58,044B raw, 48,456B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 73,847B for [part_value] DOUBLE: 14,345 values, 114,768B raw, 73,800B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 74,606B for [part_value] DOUBLE: 14,509 values, 116,080B raw, 74,559B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,128
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,293B for [ps_partkey] INT32: 15,344 values, 61,384B raw, 51,254B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 78,932B for [part_value] DOUBLE: 15,344 values, 122,760B raw, 78,885B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,270B for [ps_partkey] INT32: 15,383 values, 61,540B raw, 51,231B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 79,065B for [part_value] DOUBLE: 15,383 values, 123,072B raw, 79,018B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,423B for [ps_partkey] INT32: 15,079 values, 60,324B raw, 50,384B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,549B for [part_value] DOUBLE: 15,079 values, 120,640B raw, 77,502B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,178B for [ps_partkey] INT32: 15,029 values, 60,124B raw, 50,139B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,538B for [ps_partkey] INT32: 15,132 values, 60,536B raw, 50,499B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,329B for [part_value] DOUBLE: 15,029 values, 120,240B raw, 77,282B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,861B for [part_value] DOUBLE: 15,132 values, 121,064B raw, 77,814B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,219B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,180B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,676
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,318B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,271B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,315B for [ps_partkey] INT32: 15,393 values, 61,580B raw, 51,276B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,541B for [ps_partkey] INT32: 14,816 values, 59,272B raw, 49,502B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 48,539B for [ps_partkey] INT32: 14,590 values, 58,368B raw, 48,500B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,154B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 51,115B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 76,158B for [part_value] DOUBLE: 14,816 values, 118,536B raw, 76,111B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 78,754B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,707B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 74,949B for [part_value] DOUBLE: 14,590 values, 116,728B raw, 74,902B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 79,116B for [part_value] DOUBLE: 15,393 values, 123,152B raw, 79,069B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,502B for [ps_partkey] INT32: 15,120 values, 60,488B raw, 50,463B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,808B for [part_value] DOUBLE: 15,120 values, 120,968B raw, 77,761B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000018
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000018_0: Committed
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000019
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000017
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000019_0: Committed
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000023
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000023_0: Committed
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000029
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000029_0: Committed
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000021
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000017_0: Committed
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000021_0: Committed
15/08/21 21:31:16 INFO Executor: Finished task 18.0 in stage 4.0 (TID 266). 843 bytes result sent to driver
15/08/21 21:31:16 INFO Executor: Finished task 19.0 in stage 4.0 (TID 267). 843 bytes result sent to driver
15/08/21 21:31:16 INFO Executor: Finished task 29.0 in stage 4.0 (TID 277). 843 bytes result sent to driver
15/08/21 21:31:16 INFO Executor: Finished task 17.0 in stage 4.0 (TID 265). 843 bytes result sent to driver
15/08/21 21:31:16 INFO Executor: Finished task 23.0 in stage 4.0 (TID 271). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 32.0 in stage 4.0 (TID 280)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 33.0 in stage 4.0 (TID 281)
15/08/21 21:31:16 INFO Executor: Finished task 21.0 in stage 4.0 (TID 269). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 34.0 in stage 4.0 (TID 282)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 35.0 in stage 4.0 (TID 283)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 36.0 in stage 4.0 (TID 284)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 266) in 541 ms on localhost (17/200)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 267) in 537 ms on localhost (18/200)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 285, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 277) in 476 ms on localhost (19/200)
15/08/21 21:31:16 INFO Executor: Running task 37.0 in stage 4.0 (TID 285)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 271) in 498 ms on localhost (20/200)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 265) in 545 ms on localhost (21/200)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 269) in 535 ms on localhost (22/200)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000016
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000028
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000028_0: Committed
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000022
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000016_0: Committed
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000022_0: Committed
15/08/21 21:31:16 INFO Executor: Finished task 28.0 in stage 4.0 (TID 276). 843 bytes result sent to driver
15/08/21 21:31:16 INFO Executor: Finished task 22.0 in stage 4.0 (TID 270). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 286, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 38.0 in stage 4.0 (TID 286)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 287, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 39.0 in stage 4.0 (TID 287)
15/08/21 21:31:16 INFO Executor: Finished task 16.0 in stage 4.0 (TID 264). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 288, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 270) in 533 ms on localhost (23/200)
15/08/21 21:31:16 INFO Executor: Running task 40.0 in stage 4.0 (TID 288)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 276) in 515 ms on localhost (24/200)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000027
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000027_0: Committed
15/08/21 21:31:16 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 264) in 588 ms on localhost (25/200)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000025
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000025_0: Committed
15/08/21 21:31:16 INFO Executor: Finished task 27.0 in stage 4.0 (TID 275). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 289, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 41.0 in stage 4.0 (TID 289)
15/08/21 21:31:16 INFO Executor: Finished task 25.0 in stage 4.0 (TID 273). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 290, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 42.0 in stage 4.0 (TID 290)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 275) in 530 ms on localhost (26/200)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 273) in 537 ms on localhost (27/200)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000030
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000030_0: Committed
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000026
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000026_0: Committed
15/08/21 21:31:16 INFO Executor: Finished task 30.0 in stage 4.0 (TID 278). 843 bytes result sent to driver
15/08/21 21:31:16 INFO Executor: Finished task 26.0 in stage 4.0 (TID 274). 843 bytes result sent to driver
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000024
15/08/21 21:31:16 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 291, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000024_0: Committed
15/08/21 21:31:16 INFO Executor: Running task 43.0 in stage 4.0 (TID 291)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 292, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 44.0 in stage 4.0 (TID 292)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 278) in 529 ms on localhost (28/200)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 274) in 550 ms on localhost (29/200)
15/08/21 21:31:16 INFO Executor: Finished task 24.0 in stage 4.0 (TID 272). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 293, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 45.0 in stage 4.0 (TID 293)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 272) in 555 ms on localhost (30/200)
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,136
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,591B for [ps_partkey] INT32: 14,843 values, 59,380B raw, 49,552B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 76,334B for [part_value] DOUBLE: 14,843 values, 118,752B raw, 76,287B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000031
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000031_0: Committed
15/08/21 21:31:16 INFO Executor: Finished task 31.0 in stage 4.0 (TID 279). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 294, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 46.0 in stage 4.0 (TID 294)
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 279) in 379 ms on localhost (31/200)
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,796
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,256
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,328
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000020
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000020_0: Committed
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,188
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,173B for [ps_partkey] INT32: 15,049 values, 60,204B raw, 50,134B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,362B for [ps_partkey] INT32: 14,776 values, 59,112B raw, 49,323B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 75,995B for [part_value] DOUBLE: 14,776 values, 118,216B raw, 75,948B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,439B for [part_value] DOUBLE: 15,049 values, 120,400B raw, 77,392B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO Executor: Finished task 20.0 in stage 4.0 (TID 268). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 295, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 47.0 in stage 4.0 (TID 295)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 268) in 914 ms on localhost (32/200)
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,548
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,088
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,774B for [ps_partkey] INT32: 15,203 values, 60,820B raw, 50,735B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 78,172B for [part_value] DOUBLE: 15,203 values, 121,632B raw, 78,125B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,301B for [ps_partkey] INT32: 15,046 values, 60,192B raw, 50,262B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,328
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,375B for [part_value] DOUBLE: 15,046 values, 120,376B raw, 77,328B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,168
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,670B for [ps_partkey] INT32: 15,164 values, 60,664B raw, 50,631B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 78,004B for [part_value] DOUBLE: 15,164 values, 121,320B raw, 77,957B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000032
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000032_0: Committed
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,175B for [ps_partkey] INT32: 14,791 values, 59,172B raw, 49,136B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 75,932B for [part_value] DOUBLE: 14,791 values, 118,336B raw, 75,885B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO Executor: Finished task 32.0 in stage 4.0 (TID 280). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 296, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000034
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000034_0: Committed
15/08/21 21:31:16 INFO Executor: Running task 48.0 in stage 4.0 (TID 296)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 280) in 442 ms on localhost (33/200)
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,996
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 52,094B for [ps_partkey] INT32: 15,603 values, 62,420B raw, 52,055B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 80,396B for [part_value] DOUBLE: 15,603 values, 124,832B raw, 80,349B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO Executor: Finished task 34.0 in stage 4.0 (TID 282). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 297, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000039
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000039_0: Committed
15/08/21 21:31:16 INFO Executor: Running task 49.0 in stage 4.0 (TID 297)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 282) in 457 ms on localhost (34/200)
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,600B for [ps_partkey] INT32: 15,145 values, 60,588B raw, 50,561B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO Executor: Finished task 39.0 in stage 4.0 (TID 287). 843 bytes result sent to driver
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 77,934B for [part_value] DOUBLE: 15,145 values, 121,168B raw, 77,887B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 298, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 50.0 in stage 4.0 (TID 298)
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,688
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000036
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000036_0: Committed
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,968
15/08/21 21:31:16 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 287) in 436 ms on localhost (35/200)
15/08/21 21:31:16 INFO Executor: Finished task 36.0 in stage 4.0 (TID 284). 843 bytes result sent to driver
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 284) in 469 ms on localhost (36/200)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 299, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 51.0 in stage 4.0 (TID 299)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000033
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000033_0: Committed
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000043
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000043_0: Committed
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,365B for [ps_partkey] INT32: 15,386 values, 61,552B raw, 51,326B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO Executor: Finished task 43.0 in stage 4.0 (TID 291). 843 bytes result sent to driver
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 79,089B for [part_value] DOUBLE: 15,386 values, 123,096B raw, 79,042B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO Executor: Finished task 33.0 in stage 4.0 (TID 281). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 300, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 52.0 in stage 4.0 (TID 300)
15/08/21 21:31:16 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 301, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 53.0 in stage 4.0 (TID 301)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 291) in 427 ms on localhost (37/200)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 281) in 487 ms on localhost (38/200)
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,394B for [ps_partkey] INT32: 14,771 values, 59,092B raw, 49,355B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,100
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,416
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 75,957B for [part_value] DOUBLE: 14,771 values, 118,176B raw, 75,910B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,700
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,581B for [ps_partkey] INT32: 15,235 values, 60,948B raw, 50,542B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000044
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000044_0: Committed
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 78,357B for [part_value] DOUBLE: 15,235 values, 121,888B raw, 78,310B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO Executor: Finished task 44.0 in stage 4.0 (TID 292). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 302, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 54.0 in stage 4.0 (TID 302)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 292) in 461 ms on localhost (39/200)
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000035
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000035_0: Committed
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,580B for [ps_partkey] INT32: 14,842 values, 59,376B raw, 49,541B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 50,045B for [ps_partkey] INT32: 14,972 values, 59,896B raw, 50,006B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 76,973B for [part_value] DOUBLE: 14,972 values, 119,784B raw, 76,926B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 76,384B for [part_value] DOUBLE: 14,842 values, 118,744B raw, 76,337B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO Executor: Finished task 35.0 in stage 4.0 (TID 283). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 303, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO Executor: Running task 55.0 in stage 4.0 (TID 303)
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 283) in 542 ms on localhost (40/200)
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,806B for [ps_partkey] INT32: 15,557 values, 62,236B raw, 51,767B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 79,910B for [part_value] DOUBLE: 15,557 values, 124,464B raw, 79,863B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000037
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000037_0: Committed
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO Executor: Finished task 37.0 in stage 4.0 (TID 285). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 304, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 56.0 in stage 4.0 (TID 304)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 285) in 561 ms on localhost (41/200)
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000041
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000041_0: Committed
15/08/21 21:31:16 INFO Executor: Finished task 41.0 in stage 4.0 (TID 289). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 305, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 57.0 in stage 4.0 (TID 305)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 289) in 538 ms on localhost (42/200)
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,920
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,228B for [ps_partkey] INT32: 15,333 values, 61,340B raw, 51,189B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 78,917B for [part_value] DOUBLE: 15,333 values, 122,672B raw, 78,870B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000046
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000046_0: Committed
15/08/21 21:31:16 INFO Executor: Finished task 46.0 in stage 4.0 (TID 294). 843 bytes result sent to driver
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 306, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 58.0 in stage 4.0 (TID 306)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 294) in 459 ms on localhost (43/200)
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,396
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 49,817B for [ps_partkey] INT32: 14,906 values, 59,632B raw, 49,778B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 76,596B for [part_value] DOUBLE: 14,906 values, 119,256B raw, 76,549B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,956
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,396
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 51,391B for [ps_partkey] INT32: 15,384 values, 61,544B raw, 51,352B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO ColumnChunkPageWriteStore: written 79,099B for [part_value] DOUBLE: 15,384 values, 123,080B raw, 79,052B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,116
15/08/21 21:31:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000047
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000047_0: Committed
15/08/21 21:31:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:16 INFO Executor: Finished task 47.0 in stage 4.0 (TID 295). 843 bytes result sent to driver
15/08/21 21:31:16 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 307, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:16 INFO Executor: Running task 59.0 in stage 4.0 (TID 307)
15/08/21 21:31:16 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 295) in 456 ms on localhost (44/200)
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,516
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 49,962B for [ps_partkey] INT32: 14,992 values, 59,976B raw, 49,923B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 49,751B for [ps_partkey] INT32: 14,956 values, 59,832B raw, 49,712B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 76,847B for [part_value] DOUBLE: 14,956 values, 119,656B raw, 76,800B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 77,067B for [part_value] DOUBLE: 14,992 values, 119,944B raw, 77,020B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,280
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,020
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 48,826B for [ps_partkey] INT32: 14,612 values, 58,456B raw, 48,787B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 75,159B for [part_value] DOUBLE: 14,612 values, 116,904B raw, 75,112B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,504
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,508
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 51,758B for [ps_partkey] INT32: 15,501 values, 62,012B raw, 51,719B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 79,721B for [part_value] DOUBLE: 15,501 values, 124,016B raw, 79,674B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000048
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000048_0: Committed
15/08/21 21:31:17 INFO Executor: Finished task 48.0 in stage 4.0 (TID 296). 843 bytes result sent to driver
15/08/21 21:31:17 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 308, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO Executor: Running task 60.0 in stage 4.0 (TID 308)
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 50,137B for [ps_partkey] INT32: 15,013 values, 60,060B raw, 50,098B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 77,255B for [part_value] DOUBLE: 15,013 values, 120,112B raw, 77,208B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 296) in 1142 ms on localhost (45/200)
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 50,882B for [ps_partkey] INT32: 15,238 values, 60,960B raw, 50,843B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 50,331B for [ps_partkey] INT32: 15,062 values, 60,256B raw, 50,292B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 78,305B for [part_value] DOUBLE: 15,238 values, 121,912B raw, 78,258B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 77,320B for [part_value] DOUBLE: 15,062 values, 120,504B raw, 77,273B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000038
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000038_0: Committed
15/08/21 21:31:17 INFO Executor: Finished task 38.0 in stage 4.0 (TID 286). 843 bytes result sent to driver
15/08/21 21:31:17 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 309, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:52592 in memory (size: 6.6 KB, free: 20.7 GB)
15/08/21 21:31:17 INFO Executor: Running task 61.0 in stage 4.0 (TID 309)
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000040
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000040_0: Committed
15/08/21 21:31:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:17 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 286) in 1570 ms on localhost (46/200)
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000042
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000042_0: Committed
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO Executor: Finished task 40.0 in stage 4.0 (TID 288). 843 bytes result sent to driver
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:17 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 310, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Running task 62.0 in stage 4.0 (TID 310)
15/08/21 21:31:17 INFO Executor: Finished task 42.0 in stage 4.0 (TID 290). 843 bytes result sent to driver
15/08/21 21:31:17 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 311, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 288) in 1577 ms on localhost (47/200)
15/08/21 21:31:17 INFO Executor: Running task 63.0 in stage 4.0 (TID 311)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 290) in 1571 ms on localhost (48/200)
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,716
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 49,109B for [ps_partkey] INT32: 14,772 values, 59,096B raw, 49,070B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 75,834B for [part_value] DOUBLE: 14,772 values, 118,184B raw, 75,787B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,296
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 50,523B for [ps_partkey] INT32: 15,101 values, 60,412B raw, 50,484B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 77,680B for [part_value] DOUBLE: 15,101 values, 120,816B raw, 77,633B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000045
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000045_0: Committed
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000050
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000050_0: Committed
15/08/21 21:31:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:17 INFO Executor: Finished task 50.0 in stage 4.0 (TID 298). 843 bytes result sent to driver
15/08/21 21:31:17 INFO Executor: Finished task 45.0 in stage 4.0 (TID 293). 843 bytes result sent to driver
15/08/21 21:31:17 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 312, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 313, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Running task 65.0 in stage 4.0 (TID 313)
15/08/21 21:31:17 INFO Executor: Running task 64.0 in stage 4.0 (TID 312)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 293) in 1638 ms on localhost (49/200)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 298) in 1237 ms on localhost (50/200)
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000054
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000054_0: Committed
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000053
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000053_0: Committed
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000052
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000052_0: Committed
15/08/21 21:31:17 INFO Executor: Finished task 54.0 in stage 4.0 (TID 302). 843 bytes result sent to driver
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000055
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000055_0: Committed
15/08/21 21:31:17 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 314, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Finished task 52.0 in stage 4.0 (TID 300). 843 bytes result sent to driver
15/08/21 21:31:17 INFO Executor: Finished task 53.0 in stage 4.0 (TID 301). 843 bytes result sent to driver
15/08/21 21:31:17 INFO Executor: Running task 66.0 in stage 4.0 (TID 314)
15/08/21 21:31:17 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 315, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Running task 67.0 in stage 4.0 (TID 315)
15/08/21 21:31:17 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 316, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Running task 68.0 in stage 4.0 (TID 316)
15/08/21 21:31:17 INFO Executor: Finished task 55.0 in stage 4.0 (TID 303). 843 bytes result sent to driver
15/08/21 21:31:17 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 302) in 1199 ms on localhost (51/200)
15/08/21 21:31:17 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 317, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Running task 69.0 in stage 4.0 (TID 317)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 300) in 1235 ms on localhost (52/200)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 301) in 1237 ms on localhost (53/200)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 303) in 1180 ms on localhost (54/200)
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000057
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000057_0: Committed
15/08/21 21:31:17 INFO Executor: Finished task 57.0 in stage 4.0 (TID 305). 843 bytes result sent to driver
15/08/21 21:31:17 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 318, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Running task 70.0 in stage 4.0 (TID 318)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 305) in 1159 ms on localhost (55/200)
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,088
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 50,476B for [ps_partkey] INT32: 15,141 values, 60,572B raw, 50,437B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ColumnChunkPageWriteStore: written 77,823B for [part_value] DOUBLE: 15,141 values, 121,136B raw, 77,776B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000058
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000058_0: Committed
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:17 INFO Executor: Finished task 58.0 in stage 4.0 (TID 306). 843 bytes result sent to driver
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,356
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:17 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 319, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:17 INFO Executor: Running task 71.0 in stage 4.0 (TID 319)
15/08/21 21:31:17 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 306) in 1146 ms on localhost (56/200)
15/08/21 21:31:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:17 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000056
15/08/21 21:31:17 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:17 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000056_0: Committed
15/08/21 21:31:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO Executor: Finished task 56.0 in stage 4.0 (TID 304). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 320, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 72.0 in stage 4.0 (TID 320)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 304) in 1286 ms on localhost (57/200)
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,650B for [ps_partkey] INT32: 14,854 values, 59,424B raw, 49,611B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,656
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 76,442B for [part_value] DOUBLE: 14,854 values, 118,840B raw, 76,395B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,372
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,530B for [ps_partkey] INT32: 14,819 values, 59,284B raw, 49,491B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 76,342B for [part_value] DOUBLE: 14,819 values, 118,560B raw, 76,295B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,048
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000059
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000059_0: Committed
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,040
15/08/21 21:31:18 INFO Executor: Finished task 59.0 in stage 4.0 (TID 307). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 321, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 51,124B for [ps_partkey] INT32: 15,306 values, 61,232B raw, 51,085B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO Executor: Running task 73.0 in stage 4.0 (TID 321)
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 78,692B for [part_value] DOUBLE: 15,306 values, 122,456B raw, 78,645B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 307) in 1104 ms on localhost (58/200)
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,691B for [ps_partkey] INT32: 15,189 values, 60,764B raw, 50,652B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 78,111B for [part_value] DOUBLE: 15,189 values, 121,520B raw, 78,064B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,219B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,180B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 77,415B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,368B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000060
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000060_0: Committed
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000062
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000062_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 60.0 in stage 4.0 (TID 308). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 322, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Finished task 62.0 in stage 4.0 (TID 310). 843 bytes result sent to driver
15/08/21 21:31:18 INFO Executor: Running task 74.0 in stage 4.0 (TID 322)
15/08/21 21:31:18 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 323, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO Executor: Running task 75.0 in stage 4.0 (TID 323)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 308) in 417 ms on localhost (59/200)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 310) in 387 ms on localhost (60/200)
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000063
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000063_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 63.0 in stage 4.0 (TID 311). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 324, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 76.0 in stage 4.0 (TID 324)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 311) in 429 ms on localhost (61/200)
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000049
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000049_0: Committed
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,528
15/08/21 21:31:18 INFO Executor: Finished task 49.0 in stage 4.0 (TID 297). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 325, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 77.0 in stage 4.0 (TID 325)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 297) in 1601 ms on localhost (62/200)
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,556
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,960
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,196
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,088
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 48,837B for [ps_partkey] INT32: 14,613 values, 58,460B raw, 48,798B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,851B for [ps_partkey] INT32: 14,914 values, 59,664B raw, 49,812B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 75,206B for [part_value] DOUBLE: 14,613 values, 116,912B raw, 75,159B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 76,734B for [part_value] DOUBLE: 14,914 values, 119,320B raw, 76,687B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,908
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,955B for [ps_partkey] INT32: 14,985 values, 59,948B raw, 49,916B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 77,033B for [part_value] DOUBLE: 14,985 values, 119,888B raw, 76,986B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,476B for [ps_partkey] INT32: 14,896 values, 59,592B raw, 49,437B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000061
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000061_0: Committed
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 76,602B for [part_value] DOUBLE: 14,896 values, 119,176B raw, 76,555B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,100B for [ps_partkey] INT32: 14,991 values, 59,972B raw, 50,061B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO Executor: Finished task 61.0 in stage 4.0 (TID 309). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 326, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 77,153B for [part_value] DOUBLE: 14,991 values, 119,936B raw, 77,106B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO Executor: Running task 78.0 in stage 4.0 (TID 326)
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,034B for [ps_partkey] INT32: 14,982 values, 59,936B raw, 49,995B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 77,103B for [part_value] DOUBLE: 14,982 values, 119,864B raw, 77,056B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 309) in 513 ms on localhost (63/200)
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000051
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000051_0: Committed
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,708
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO Executor: Finished task 51.0 in stage 4.0 (TID 299). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 327, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 79.0 in stage 4.0 (TID 327)
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000068
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000068_0: Committed
15/08/21 21:31:18 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 299) in 1661 ms on localhost (64/200)
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000066
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000066_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 68.0 in stage 4.0 (TID 316). 843 bytes result sent to driver
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 328, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Finished task 66.0 in stage 4.0 (TID 314). 843 bytes result sent to driver
15/08/21 21:31:18 INFO Executor: Running task 80.0 in stage 4.0 (TID 328)
15/08/21 21:31:18 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 329, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 81.0 in stage 4.0 (TID 329)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 316) in 433 ms on localhost (65/200)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 314) in 437 ms on localhost (66/200)
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 51,007B for [ps_partkey] INT32: 15,272 values, 61,096B raw, 50,968B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000065
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000065_0: Committed
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,636
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 78,724B for [part_value] DOUBLE: 15,272 values, 122,184B raw, 78,677B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO Executor: Finished task 65.0 in stage 4.0 (TID 313). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 330, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000067
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000067_0: Committed
15/08/21 21:31:18 INFO Executor: Running task 82.0 in stage 4.0 (TID 330)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 313) in 481 ms on localhost (67/200)
15/08/21 21:31:18 INFO Executor: Finished task 67.0 in stage 4.0 (TID 315). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 331, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,620
15/08/21 21:31:18 INFO Executor: Running task 83.0 in stage 4.0 (TID 331)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 315) in 469 ms on localhost (68/200)
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 51,296B for [ps_partkey] INT32: 15,368 values, 61,480B raw, 51,257B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 79,001B for [part_value] DOUBLE: 15,368 values, 122,952B raw, 78,954B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,207B for [ps_partkey] INT32: 15,018 values, 60,080B raw, 50,168B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 77,277B for [part_value] DOUBLE: 15,018 values, 120,152B raw, 77,230B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000070
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000070_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 70.0 in stage 4.0 (TID 318). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 332, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 84.0 in stage 4.0 (TID 332)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 318) in 495 ms on localhost (69/200)
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,636
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000072
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000072_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 72.0 in stage 4.0 (TID 320). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 333, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 85.0 in stage 4.0 (TID 333)
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 320) in 436 ms on localhost (70/200)
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 48,889B for [ps_partkey] INT32: 14,718 values, 58,880B raw, 48,850B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 75,723B for [part_value] DOUBLE: 14,718 values, 117,752B raw, 75,676B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000071
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000071_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 71.0 in stage 4.0 (TID 319). 843 bytes result sent to driver
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 334, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 86.0 in stage 4.0 (TID 334)
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 319) in 516 ms on localhost (71/200)
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,892
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,888B for [ps_partkey] INT32: 15,282 values, 61,136B raw, 50,849B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 78,626B for [part_value] DOUBLE: 15,282 values, 122,264B raw, 78,579B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,736
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,771,796
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000074
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000074_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 74.0 in stage 4.0 (TID 322). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 335, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,878B for [ps_partkey] INT32: 14,923 values, 59,700B raw, 49,839B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 76,795B for [part_value] DOUBLE: 14,923 values, 119,392B raw, 76,748B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO Executor: Running task 87.0 in stage 4.0 (TID 335)
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 322) in 445 ms on localhost (72/200)
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 52,486B for [ps_partkey] INT32: 15,726 values, 62,912B raw, 52,447B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 80,849B for [part_value] DOUBLE: 15,726 values, 125,816B raw, 80,802B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,116
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,056
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,616
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000075
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000075_0: Committed
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO Executor: Finished task 75.0 in stage 4.0 (TID 323). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 336, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 88.0 in stage 4.0 (TID 336)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 323) in 497 ms on localhost (73/200)
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,270B for [ps_partkey] INT32: 15,142 values, 60,576B raw, 50,231B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 77,924B for [part_value] DOUBLE: 15,142 values, 121,144B raw, 77,877B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000076
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000076_0: Committed
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 50,002B for [ps_partkey] INT32: 14,967 values, 59,876B raw, 49,963B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,928B for [ps_partkey] INT32: 14,939 values, 59,764B raw, 49,889B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 76,865B for [part_value] DOUBLE: 14,939 values, 119,520B raw, 76,818B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 77,120B for [part_value] DOUBLE: 14,967 values, 119,744B raw, 77,073B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO Executor: Finished task 76.0 in stage 4.0 (TID 324). 843 bytes result sent to driver
15/08/21 21:31:18 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 337, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 89.0 in stage 4.0 (TID 337)
15/08/21 21:31:18 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 324) in 482 ms on localhost (74/200)
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,676
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000064
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000064_0: Committed
15/08/21 21:31:18 INFO Executor: Finished task 64.0 in stage 4.0 (TID 312). 843 bytes result sent to driver
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,428
15/08/21 21:31:18 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 338, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 90.0 in stage 4.0 (TID 338)
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 312) in 846 ms on localhost (75/200)
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 51,686B for [ps_partkey] INT32: 15,470 values, 61,888B raw, 51,647B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 79,465B for [part_value] DOUBLE: 15,470 values, 123,768B raw, 79,418B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 51,142B for [ps_partkey] INT32: 15,358 values, 61,440B raw, 51,103B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 78,917B for [part_value] DOUBLE: 15,358 values, 122,872B raw, 78,870B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,548
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 49,611B for [ps_partkey] INT32: 14,864 values, 59,464B raw, 49,572B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO ColumnChunkPageWriteStore: written 76,384B for [part_value] DOUBLE: 14,864 values, 118,920B raw, 76,337B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:18 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:18 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000079
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000079_0: Committed
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO Executor: Finished task 79.0 in stage 4.0 (TID 327). 843 bytes result sent to driver
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 339, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:18 INFO Executor: Running task 91.0 in stage 4.0 (TID 339)
15/08/21 21:31:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:18 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 327) in 687 ms on localhost (76/200)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000069
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000069_0: Committed
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000073
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000073_0: Committed
15/08/21 21:31:19 INFO Executor: Finished task 69.0 in stage 4.0 (TID 317). 843 bytes result sent to driver
15/08/21 21:31:19 INFO Executor: Finished task 73.0 in stage 4.0 (TID 321). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 340, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 92.0 in stage 4.0 (TID 340)
15/08/21 21:31:19 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 341, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 93.0 in stage 4.0 (TID 341)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 321) in 950 ms on localhost (77/200)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 317) in 1164 ms on localhost (78/200)
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,536
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,072
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,548
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,316
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,296
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,036
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,188
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,755B for [ps_partkey] INT32: 15,191 values, 60,772B raw, 50,716B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 78,053B for [part_value] DOUBLE: 15,191 values, 121,536B raw, 78,006B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,088
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 52,016B for [ps_partkey] INT32: 15,614 values, 62,464B raw, 51,977B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 80,268B for [part_value] DOUBLE: 15,614 values, 124,920B raw, 80,221B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,836B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 50,797B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 51,622B for [ps_partkey] INT32: 15,463 values, 61,860B raw, 51,583B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,082B for [ps_partkey] INT32: 15,001 values, 60,012B raw, 50,043B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 78,688B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,641B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,129B for [part_value] DOUBLE: 15,001 values, 120,016B raw, 77,082B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 79,556B for [part_value] DOUBLE: 15,463 values, 123,712B raw, 79,509B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 51,121B for [ps_partkey] INT32: 15,296 values, 61,192B raw, 51,082B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 78,667B for [part_value] DOUBLE: 15,296 values, 122,376B raw, 78,620B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,216B for [ps_partkey] INT32: 15,038 values, 60,160B raw, 50,177B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,257B for [part_value] DOUBLE: 15,038 values, 120,312B raw, 77,210B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,730B for [ps_partkey] INT32: 14,891 values, 59,572B raw, 49,691B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 76,638B for [part_value] DOUBLE: 14,891 values, 119,136B raw, 76,591B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000081
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000081_0: Committed
15/08/21 21:31:19 INFO Executor: Finished task 81.0 in stage 4.0 (TID 329). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 342, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000080
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000080_0: Committed
15/08/21 21:31:19 INFO Executor: Running task 94.0 in stage 4.0 (TID 342)
15/08/21 21:31:19 INFO Executor: Finished task 80.0 in stage 4.0 (TID 328). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 329) in 831 ms on localhost (79/200)
15/08/21 21:31:19 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 343, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 95.0 in stage 4.0 (TID 343)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 328) in 834 ms on localhost (80/200)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000090
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000090_0: Committed
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000088
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000088_0: Committed
15/08/21 21:31:19 INFO Executor: Finished task 90.0 in stage 4.0 (TID 338). 843 bytes result sent to driver
15/08/21 21:31:19 INFO Executor: Finished task 88.0 in stage 4.0 (TID 336). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 344, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 96.0 in stage 4.0 (TID 344)
15/08/21 21:31:19 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 345, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 97.0 in stage 4.0 (TID 345)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000085
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000085_0: Committed
15/08/21 21:31:19 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 336) in 500 ms on localhost (81/200)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 338) in 449 ms on localhost (82/200)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000084
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000084_0: Committed
15/08/21 21:31:19 INFO Executor: Finished task 85.0 in stage 4.0 (TID 333). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 346, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 98.0 in stage 4.0 (TID 346)
15/08/21 21:31:19 INFO Executor: Finished task 84.0 in stage 4.0 (TID 332). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 347, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 99.0 in stage 4.0 (TID 347)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 333) in 719 ms on localhost (83/200)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 332) in 769 ms on localhost (84/200)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000086
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000086_0: Committed
15/08/21 21:31:19 INFO Executor: Finished task 86.0 in stage 4.0 (TID 334). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 348, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 100.0 in stage 4.0 (TID 348)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 334) in 673 ms on localhost (85/200)
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,360
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,969B for [ps_partkey] INT32: 15,255 values, 61,028B raw, 50,930B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 78,402B for [part_value] DOUBLE: 15,255 values, 122,048B raw, 78,355B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,356
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,413B for [ps_partkey] INT32: 15,104 values, 60,424B raw, 50,374B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,613B for [part_value] DOUBLE: 15,104 values, 120,840B raw, 77,566B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,268
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 51,764B for [ps_partkey] INT32: 15,500 values, 62,008B raw, 51,725B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 79,825B for [part_value] DOUBLE: 15,500 values, 124,008B raw, 79,778B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000091
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000091_0: Committed
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000082
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000082_0: Committed
15/08/21 21:31:19 INFO Executor: Finished task 91.0 in stage 4.0 (TID 339). 843 bytes result sent to driver
15/08/21 21:31:19 INFO Executor: Finished task 82.0 in stage 4.0 (TID 330). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 349, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000083
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000077
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000077_0: Committed
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000083_0: Committed
15/08/21 21:31:19 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 350, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Finished task 83.0 in stage 4.0 (TID 331). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 330) in 1153 ms on localhost (86/200)
15/08/21 21:31:19 INFO Executor: Running task 102.0 in stage 4.0 (TID 350)
15/08/21 21:31:19 INFO Executor: Finished task 77.0 in stage 4.0 (TID 325). 843 bytes result sent to driver
15/08/21 21:31:19 INFO Executor: Running task 101.0 in stage 4.0 (TID 349)
15/08/21 21:31:19 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 351, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 339) in 530 ms on localhost (87/200)
15/08/21 21:31:19 INFO Executor: Running task 103.0 in stage 4.0 (TID 351)
15/08/21 21:31:19 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 352, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 331) in 1158 ms on localhost (88/200)
15/08/21 21:31:19 INFO Executor: Running task 104.0 in stage 4.0 (TID 352)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 325) in 1298 ms on localhost (89/200)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000092
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000092_0: Committed
15/08/21 21:31:19 INFO Executor: Finished task 92.0 in stage 4.0 (TID 340). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 353, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 105.0 in stage 4.0 (TID 353)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000093
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000093_0: Committed
15/08/21 21:31:19 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 340) in 490 ms on localhost (90/200)
15/08/21 21:31:19 INFO Executor: Finished task 93.0 in stage 4.0 (TID 341). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 354, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 106.0 in stage 4.0 (TID 354)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 341) in 509 ms on localhost (91/200)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000087
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000087_0: Committed
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000078
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000078_0: Committed
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,188
15/08/21 21:31:19 INFO Executor: Finished task 78.0 in stage 4.0 (TID 326). 843 bytes result sent to driver
15/08/21 21:31:19 INFO Executor: Finished task 87.0 in stage 4.0 (TID 335). 843 bytes result sent to driver
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000089
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000089_0: Committed
15/08/21 21:31:19 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 355, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 356, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 108.0 in stage 4.0 (TID 356)
15/08/21 21:31:19 INFO Executor: Running task 107.0 in stage 4.0 (TID 355)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 326) in 1292 ms on localhost (92/200)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 335) in 966 ms on localhost (93/200)
15/08/21 21:31:19 INFO Executor: Finished task 89.0 in stage 4.0 (TID 337). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 357, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 337) in 886 ms on localhost (94/200)
15/08/21 21:31:19 INFO Executor: Running task 109.0 in stage 4.0 (TID 357)
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,773,160
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,328
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,296
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,588
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,436B for [ps_partkey] INT32: 15,096 values, 60,392B raw, 50,397B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,569B for [part_value] DOUBLE: 15,096 values, 120,776B raw, 77,522B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,436
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,348
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,466B for [ps_partkey] INT32: 15,103 values, 60,420B raw, 50,427B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 52,751B for [ps_partkey] INT32: 15,795 values, 63,188B raw, 52,712B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 81,317B for [part_value] DOUBLE: 15,795 values, 126,368B raw, 81,270B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,323B for [ps_partkey] INT32: 14,751 values, 59,012B raw, 49,284B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,626B for [part_value] DOUBLE: 15,103 values, 120,832B raw, 77,579B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 75,816B for [part_value] DOUBLE: 14,751 values, 118,016B raw, 75,769B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 51,665B for [ps_partkey] INT32: 15,566 values, 62,272B raw, 51,626B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 79,967B for [part_value] DOUBLE: 15,566 values, 124,536B raw, 79,920B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 48,414B for [ps_partkey] INT32: 14,508 values, 58,040B raw, 48,375B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 51,460B for [ps_partkey] INT32: 15,404 values, 61,624B raw, 51,421B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 74,748B for [part_value] DOUBLE: 14,508 values, 116,072B raw, 74,701B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 79,159B for [part_value] DOUBLE: 15,404 values, 123,240B raw, 79,112B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:19 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:19 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000099
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000099_0: Committed
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO Executor: Finished task 99.0 in stage 4.0 (TID 347). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 358, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 110.0 in stage 4.0 (TID 358)
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000100
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000100_0: Committed
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,096
15/08/21 21:31:19 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 347) in 655 ms on localhost (95/200)
15/08/21 21:31:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:19 INFO Executor: Finished task 100.0 in stage 4.0 (TID 348). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 359, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 111.0 in stage 4.0 (TID 359)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 348) in 653 ms on localhost (96/200)
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,813B for [ps_partkey] INT32: 14,991 values, 59,972B raw, 49,774B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,080B for [part_value] DOUBLE: 14,991 values, 119,936B raw, 77,033B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,236
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,648
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000095
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000095_0: Committed
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,836
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,076
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO Executor: Finished task 95.0 in stage 4.0 (TID 343). 843 bytes result sent to driver
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,748
15/08/21 21:31:19 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 360, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 112.0 in stage 4.0 (TID 360)
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,028
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,756
15/08/21 21:31:19 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 343) in 764 ms on localhost (97/200)
15/08/21 21:31:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,888
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 48,714B for [ps_partkey] INT32: 14,569 values, 58,284B raw, 48,675B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 75,126B for [part_value] DOUBLE: 14,569 values, 116,560B raw, 75,079B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,492B for [ps_partkey] INT32: 14,848 values, 59,400B raw, 49,453B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 76,418B for [part_value] DOUBLE: 14,848 values, 118,792B raw, 76,371B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,681B for [ps_partkey] INT32: 14,878 values, 59,520B raw, 49,642B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 76,614B for [part_value] DOUBLE: 14,878 values, 119,032B raw, 76,567B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,711B for [ps_partkey] INT32: 14,874 values, 59,504B raw, 49,672B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 76,490B for [part_value] DOUBLE: 14,874 values, 119,000B raw, 76,443B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,599B for [ps_partkey] INT32: 14,838 values, 59,360B raw, 49,560B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,464B for [ps_partkey] INT32: 15,090 values, 60,368B raw, 50,425B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 76,341B for [part_value] DOUBLE: 14,838 values, 118,712B raw, 76,294B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,675B for [part_value] DOUBLE: 15,090 values, 120,728B raw, 77,628B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 49,529B for [ps_partkey] INT32: 14,824 values, 59,304B raw, 49,490B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 76,382B for [part_value] DOUBLE: 14,824 values, 118,600B raw, 76,335B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 50,561B for [ps_partkey] INT32: 15,131 values, 60,532B raw, 50,522B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO ColumnChunkPageWriteStore: written 77,778B for [part_value] DOUBLE: 15,131 values, 121,056B raw, 77,731B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000104
15/08/21 21:31:19 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000104_0: Committed
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:19 INFO Executor: Finished task 104.0 in stage 4.0 (TID 352). 843 bytes result sent to driver
15/08/21 21:31:19 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 361, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:19 INFO Executor: Running task 113.0 in stage 4.0 (TID 361)
15/08/21 21:31:19 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 352) in 462 ms on localhost (98/200)
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000105
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000105_0: Committed
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO Executor: Finished task 105.0 in stage 4.0 (TID 353). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 362, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 114.0 in stage 4.0 (TID 362)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 353) in 544 ms on localhost (99/200)
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,268
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000108
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000108_0: Committed
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,200
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000101
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000109
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000101_0: Committed
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000107
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000109_0: Committed
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000107_0: Committed
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000103
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000103_0: Committed
15/08/21 21:31:20 INFO Executor: Finished task 108.0 in stage 4.0 (TID 356). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 363, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Finished task 107.0 in stage 4.0 (TID 355). 843 bytes result sent to driver
15/08/21 21:31:20 INFO Executor: Running task 115.0 in stage 4.0 (TID 363)
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO Executor: Finished task 109.0 in stage 4.0 (TID 357). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 364, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 116.0 in stage 4.0 (TID 364)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 365, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 356) in 534 ms on localhost (100/200)
15/08/21 21:31:20 INFO Executor: Finished task 103.0 in stage 4.0 (TID 351). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 366, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 118.0 in stage 4.0 (TID 366)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 355) in 539 ms on localhost (101/200)
15/08/21 21:31:20 INFO Executor: Finished task 101.0 in stage 4.0 (TID 349). 843 bytes result sent to driver
15/08/21 21:31:20 INFO Executor: Running task 117.0 in stage 4.0 (TID 365)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 367, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 357) in 536 ms on localhost (102/200)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 351) in 605 ms on localhost (103/200)
15/08/21 21:31:20 INFO Executor: Running task 119.0 in stage 4.0 (TID 367)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 349) in 617 ms on localhost (104/200)
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000098
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000098_0: Committed
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000096
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000096_0: Committed
15/08/21 21:31:20 INFO Executor: Finished task 98.0 in stage 4.0 (TID 346). 843 bytes result sent to driver
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000097
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000094
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000097_0: Committed
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000094_0: Committed
15/08/21 21:31:20 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 368, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 120.0 in stage 4.0 (TID 368)
15/08/21 21:31:20 INFO Executor: Finished task 94.0 in stage 4.0 (TID 342). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 346) in 956 ms on localhost (105/200)
15/08/21 21:31:20 INFO Executor: Finished task 96.0 in stage 4.0 (TID 344). 843 bytes result sent to driver
15/08/21 21:31:20 INFO Executor: Finished task 97.0 in stage 4.0 (TID 345). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 369, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 121.0 in stage 4.0 (TID 369)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 370, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 371, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 123.0 in stage 4.0 (TID 371)
15/08/21 21:31:20 INFO Executor: Running task 122.0 in stage 4.0 (TID 370)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 342) in 989 ms on localhost (106/200)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 345) in 967 ms on localhost (107/200)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 344) in 968 ms on localhost (108/200)
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,305B for [ps_partkey] INT32: 14,750 values, 59,008B raw, 49,266B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 75,859B for [part_value] DOUBLE: 14,750 values, 118,008B raw, 75,812B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 48,805B for [ps_partkey] INT32: 14,597 values, 58,396B raw, 48,766B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 75,162B for [part_value] DOUBLE: 14,597 values, 116,784B raw, 75,115B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,116
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,965B for [ps_partkey] INT32: 14,942 values, 59,776B raw, 49,926B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,683B for [part_value] DOUBLE: 14,942 values, 119,544B raw, 76,636B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,860
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,133B for [ps_partkey] INT32: 14,780 values, 59,128B raw, 49,094B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,143B for [part_value] DOUBLE: 14,780 values, 118,248B raw, 76,096B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000113
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000113_0: Committed
15/08/21 21:31:20 INFO Executor: Finished task 113.0 in stage 4.0 (TID 361). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 372, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 124.0 in stage 4.0 (TID 372)
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 361) in 352 ms on localhost (109/200)
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000106
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000106_0: Committed
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000102
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000102_0: Committed
15/08/21 21:31:20 INFO Executor: Finished task 106.0 in stage 4.0 (TID 354). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 373, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 125.0 in stage 4.0 (TID 373)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 354) in 860 ms on localhost (110/200)
15/08/21 21:31:20 INFO Executor: Finished task 102.0 in stage 4.0 (TID 350). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 374, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 126.0 in stage 4.0 (TID 374)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 350) in 919 ms on localhost (111/200)
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,276
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,744,856
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,528
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,956
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,332
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,288
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,300
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,008
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 48,681B for [ps_partkey] INT32: 14,550 values, 58,208B raw, 48,642B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 74,857B for [part_value] DOUBLE: 14,550 values, 116,408B raw, 74,810B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 47,957B for [ps_partkey] INT32: 14,379 values, 57,524B raw, 47,918B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 73,881B for [part_value] DOUBLE: 14,379 values, 115,040B raw, 73,834B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,902B for [ps_partkey] INT32: 14,934 values, 59,744B raw, 49,863B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,801B for [ps_partkey] INT32: 14,913 values, 59,660B raw, 49,762B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,631B for [part_value] DOUBLE: 14,913 values, 119,312B raw, 76,584B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,798B for [part_value] DOUBLE: 14,934 values, 119,480B raw, 76,751B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,971B for [ps_partkey] INT32: 14,951 values, 59,812B raw, 49,932B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,980B for [ps_partkey] INT32: 14,954 values, 59,824B raw, 49,941B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,964B for [part_value] DOUBLE: 14,951 values, 119,616B raw, 76,917B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,859B for [part_value] DOUBLE: 14,954 values, 119,640B raw, 76,812B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,105B for [ps_partkey] INT32: 14,687 values, 58,756B raw, 49,066B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,235B for [ps_partkey] INT32: 14,802 values, 59,216B raw, 49,196B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 75,641B for [part_value] DOUBLE: 14,687 values, 117,504B raw, 75,594B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,170B for [part_value] DOUBLE: 14,802 values, 118,424B raw, 76,123B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,746,052
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,746,856
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 48,143B for [ps_partkey] INT32: 14,440 values, 57,768B raw, 48,104B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 74,116B for [part_value] DOUBLE: 14,440 values, 115,528B raw, 74,069B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 48,423B for [ps_partkey] INT32: 14,479 values, 57,924B raw, 48,384B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 74,557B for [part_value] DOUBLE: 14,479 values, 115,840B raw, 74,510B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,476
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000110
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000111
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000110_0: Committed
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000111_0: Committed
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,525B for [ps_partkey] INT32: 14,810 values, 59,248B raw, 49,486B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,193B for [part_value] DOUBLE: 14,810 values, 118,488B raw, 76,146B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO Executor: Finished task 111.0 in stage 4.0 (TID 359). 843 bytes result sent to driver
15/08/21 21:31:20 INFO Executor: Finished task 110.0 in stage 4.0 (TID 358). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 375, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 376, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 127.0 in stage 4.0 (TID 375)
15/08/21 21:31:20 INFO Executor: Running task 128.0 in stage 4.0 (TID 376)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 359) in 843 ms on localhost (112/200)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 358) in 858 ms on localhost (113/200)
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000116
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000119
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000114
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000119_0: Committed
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000118
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000114_0: Committed
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000118_0: Committed
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000116_0: Committed
15/08/21 21:31:20 INFO Executor: Finished task 118.0 in stage 4.0 (TID 366). 843 bytes result sent to driver
15/08/21 21:31:20 INFO Executor: Finished task 114.0 in stage 4.0 (TID 362). 843 bytes result sent to driver
15/08/21 21:31:20 INFO Executor: Finished task 119.0 in stage 4.0 (TID 367). 843 bytes result sent to driver
15/08/21 21:31:20 INFO Executor: Finished task 116.0 in stage 4.0 (TID 364). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 377, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 129.0 in stage 4.0 (TID 377)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 378, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 130.0 in stage 4.0 (TID 378)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 379, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 131.0 in stage 4.0 (TID 379)
15/08/21 21:31:20 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 380, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 366) in 583 ms on localhost (114/200)
15/08/21 21:31:20 INFO Executor: Running task 132.0 in stage 4.0 (TID 380)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 367) in 580 ms on localhost (115/200)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 362) in 651 ms on localhost (116/200)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 364) in 589 ms on localhost (117/200)
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000112
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000112_0: Committed
15/08/21 21:31:20 INFO Executor: Finished task 112.0 in stage 4.0 (TID 360). 843 bytes result sent to driver
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000124
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000124_0: Committed
15/08/21 21:31:20 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 381, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 133.0 in stage 4.0 (TID 381)
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,896
15/08/21 21:31:20 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 360) in 808 ms on localhost (118/200)
15/08/21 21:31:20 INFO Executor: Finished task 124.0 in stage 4.0 (TID 372). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 382, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 134.0 in stage 4.0 (TID 382)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 372) in 403 ms on localhost (119/200)
15/08/21 21:31:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,416
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 49,574B for [ps_partkey] INT32: 14,831 values, 59,332B raw, 49,535B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 76,286B for [part_value] DOUBLE: 14,831 values, 118,656B raw, 76,239B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 50,147B for [ps_partkey] INT32: 15,007 values, 60,036B raw, 50,108B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ColumnChunkPageWriteStore: written 77,232B for [part_value] DOUBLE: 15,007 values, 120,064B raw, 77,185B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000126
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000126_0: Committed
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:20 INFO Executor: Finished task 126.0 in stage 4.0 (TID 374). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 383, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 135.0 in stage 4.0 (TID 383)
15/08/21 21:31:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000125
15/08/21 21:31:20 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000125_0: Committed
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:20 INFO Executor: Finished task 125.0 in stage 4.0 (TID 373). 843 bytes result sent to driver
15/08/21 21:31:20 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 384, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:20 INFO Executor: Running task 136.0 in stage 4.0 (TID 384)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 374) in 573 ms on localhost (120/200)
15/08/21 21:31:20 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 373) in 584 ms on localhost (121/200)
15/08/21 21:31:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:20 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:20 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,088
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,136
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,848
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000123
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000123_0: Committed
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000121
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000120
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000121_0: Committed
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000120_0: Committed
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,136
15/08/21 21:31:21 INFO Executor: Finished task 123.0 in stage 4.0 (TID 371). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Finished task 121.0 in stage 4.0 (TID 369). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 385, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,300
15/08/21 21:31:21 INFO Executor: Running task 137.0 in stage 4.0 (TID 385)
15/08/21 21:31:21 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 386, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,368
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO Executor: Finished task 120.0 in stage 4.0 (TID 368). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 371) in 965 ms on localhost (122/200)
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:21 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 387, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 139.0 in stage 4.0 (TID 387)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 369) in 969 ms on localhost (123/200)
15/08/21 21:31:21 INFO Executor: Running task 138.0 in stage 4.0 (TID 386)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,270B for [ps_partkey] INT32: 14,741 values, 58,972B raw, 49,231B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 368) in 978 ms on localhost (124/200)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 75,768B for [part_value] DOUBLE: 14,741 values, 117,936B raw, 75,721B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000115
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000115_0: Committed
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000122
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000122_0: Committed
15/08/21 21:31:21 INFO Executor: Finished task 115.0 in stage 4.0 (TID 363). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 388, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 140.0 in stage 4.0 (TID 388)
15/08/21 21:31:21 INFO Executor: Finished task 122.0 in stage 4.0 (TID 370). 843 bytes result sent to driver
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000117
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000117_0: Committed
15/08/21 21:31:21 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 363) in 1000 ms on localhost (125/200)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,112B for [ps_partkey] INT32: 14,693 values, 58,780B raw, 49,073B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 389, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 141.0 in stage 4.0 (TID 389)
15/08/21 21:31:21 INFO Executor: Finished task 117.0 in stage 4.0 (TID 365). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 390, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 370) in 990 ms on localhost (126/200)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 75,560B for [part_value] DOUBLE: 14,693 values, 117,552B raw, 75,513B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,216
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,502B for [ps_partkey] INT32: 14,879 values, 59,524B raw, 49,463B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO Executor: Running task 142.0 in stage 4.0 (TID 390)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 76,540B for [part_value] DOUBLE: 14,879 values, 119,040B raw, 76,493B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 365) in 1013 ms on localhost (127/200)
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,676
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 50,257B for [ps_partkey] INT32: 15,043 values, 60,180B raw, 50,218B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 77,384B for [part_value] DOUBLE: 15,043 values, 120,352B raw, 77,337B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 50,319B for [ps_partkey] INT32: 15,055 values, 60,228B raw, 50,280B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 77,437B for [part_value] DOUBLE: 15,055 values, 120,448B raw, 77,390B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 48,404B for [ps_partkey] INT32: 14,502 values, 58,016B raw, 48,365B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 74,467B for [part_value] DOUBLE: 14,502 values, 116,024B raw, 74,420B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 51,267B for [ps_partkey] INT32: 15,347 values, 61,396B raw, 51,228B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 78,878B for [part_value] DOUBLE: 15,347 values, 122,784B raw, 78,831B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 52,003B for [ps_partkey] INT32: 15,570 values, 62,288B raw, 51,964B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 80,039B for [part_value] DOUBLE: 15,570 values, 124,568B raw, 79,992B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000131
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000131_0: Committed
15/08/21 21:31:21 INFO Executor: Finished task 131.0 in stage 4.0 (TID 379). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 391, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 143.0 in stage 4.0 (TID 391)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 379) in 513 ms on localhost (128/200)
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000128
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000134
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000127
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000127_0: Committed
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000128_0: Committed
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000134_0: Committed
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000133
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000133_0: Committed
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO Executor: Finished task 134.0 in stage 4.0 (TID 382). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Finished task 127.0 in stage 4.0 (TID 375). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Finished task 128.0 in stage 4.0 (TID 376). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Finished task 133.0 in stage 4.0 (TID 381). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 392, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 144.0 in stage 4.0 (TID 392)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 382) in 560 ms on localhost (129/200)
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000130
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000130_0: Committed
15/08/21 21:31:21 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 393, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 394, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 146.0 in stage 4.0 (TID 394)
15/08/21 21:31:21 INFO Executor: Running task 145.0 in stage 4.0 (TID 393)
15/08/21 21:31:21 INFO Executor: Finished task 130.0 in stage 4.0 (TID 378). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 395, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 375) in 626 ms on localhost (130/200)
15/08/21 21:31:21 INFO Executor: Running task 147.0 in stage 4.0 (TID 395)
15/08/21 21:31:21 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 396, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 148.0 in stage 4.0 (TID 396)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 376) in 629 ms on localhost (131/200)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 378) in 610 ms on localhost (132/200)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 381) in 584 ms on localhost (133/200)
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,468
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 50,105B for [ps_partkey] INT32: 15,010 values, 60,048B raw, 50,066B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 77,136B for [part_value] DOUBLE: 15,010 values, 120,088B raw, 77,089B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,856
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,556
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,594B for [ps_partkey] INT32: 14,829 values, 59,324B raw, 49,555B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 76,110B for [part_value] DOUBLE: 14,829 values, 118,640B raw, 76,063B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 50,222B for [ps_partkey] INT32: 15,114 values, 60,464B raw, 50,183B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 77,686B for [part_value] DOUBLE: 15,114 values, 120,920B raw, 77,639B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000135
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000135_0: Committed
15/08/21 21:31:21 INFO Executor: Finished task 135.0 in stage 4.0 (TID 383). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 397, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 149.0 in stage 4.0 (TID 397)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 383) in 584 ms on localhost (134/200)
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,028
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,176
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,770,848
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,912
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,437B for [ps_partkey] INT32: 14,788 values, 59,160B raw, 49,398B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 76,001B for [part_value] DOUBLE: 14,788 values, 118,312B raw, 75,954B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,514B for [ps_partkey] INT32: 14,845 values, 59,388B raw, 49,475B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 76,410B for [part_value] DOUBLE: 14,845 values, 118,768B raw, 76,363B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 52,405B for [ps_partkey] INT32: 15,679 values, 62,724B raw, 52,366B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 80,646B for [part_value] DOUBLE: 15,679 values, 125,440B raw, 80,599B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,480
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 51,320B for [ps_partkey] INT32: 15,361 values, 61,452B raw, 51,281B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 79,054B for [part_value] DOUBLE: 15,361 values, 122,896B raw, 79,007B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,724B for [ps_partkey] INT32: 14,883 values, 59,540B raw, 49,685B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 76,569B for [part_value] DOUBLE: 14,883 values, 119,072B raw, 76,522B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000137
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000137_0: Committed
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000136
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000136_0: Committed
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO Executor: Finished task 137.0 in stage 4.0 (TID 385). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Finished task 136.0 in stage 4.0 (TID 384). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 398, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 150.0 in stage 4.0 (TID 398)
15/08/21 21:31:21 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 399, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 151.0 in stage 4.0 (TID 399)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 384) in 716 ms on localhost (135/200)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 385) in 614 ms on localhost (136/200)
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000129
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000129_0: Committed
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000132
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000132_0: Committed
15/08/21 21:31:21 INFO Executor: Finished task 132.0 in stage 4.0 (TID 380). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Finished task 129.0 in stage 4.0 (TID 377). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 400, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 152.0 in stage 4.0 (TID 400)
15/08/21 21:31:21 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 401, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 380) in 1079 ms on localhost (137/200)
15/08/21 21:31:21 INFO Executor: Running task 153.0 in stage 4.0 (TID 401)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 377) in 1083 ms on localhost (138/200)
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,308
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,908
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,168
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,548
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,816
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,448
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 51,096B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 51,057B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 51,925B for [ps_partkey] INT32: 15,532 values, 62,136B raw, 51,886B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 78,697B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,650B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 80,023B for [part_value] DOUBLE: 15,532 values, 124,264B raw, 79,976B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000141
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000141_0: Committed
15/08/21 21:31:21 INFO Executor: Finished task 141.0 in stage 4.0 (TID 389). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 402, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 154.0 in stage 4.0 (TID 402)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 389) in 714 ms on localhost (139/200)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 51,652B for [ps_partkey] INT32: 15,545 values, 62,188B raw, 51,613B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000142
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000142_0: Committed
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 79,880B for [part_value] DOUBLE: 15,545 values, 124,368B raw, 79,833B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000139
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000139_0: Committed
15/08/21 21:31:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:21 INFO Executor: Finished task 139.0 in stage 4.0 (TID 387). 843 bytes result sent to driver
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 51,781B for [ps_partkey] INT32: 15,509 values, 62,044B raw, 51,742B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000140
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,094B for [ps_partkey] INT32: 14,677 values, 58,716B raw, 49,055B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 403, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000138
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 79,704B for [part_value] DOUBLE: 15,509 values, 124,080B raw, 79,657B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000138_0: Committed
15/08/21 21:31:21 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000140_0: Committed
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 75,416B for [part_value] DOUBLE: 14,677 values, 117,424B raw, 75,369B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO Executor: Finished task 142.0 in stage 4.0 (TID 390). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Running task 155.0 in stage 4.0 (TID 403)
15/08/21 21:31:21 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 404, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 49,700B for [ps_partkey] INT32: 14,914 values, 59,664B raw, 49,661B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO Executor: Running task 156.0 in stage 4.0 (TID 404)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 387) in 747 ms on localhost (140/200)
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 76,716B for [part_value] DOUBLE: 14,914 values, 119,320B raw, 76,669B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO Executor: Finished task 138.0 in stage 4.0 (TID 386). 843 bytes result sent to driver
15/08/21 21:31:21 INFO Executor: Finished task 140.0 in stage 4.0 (TID 388). 843 bytes result sent to driver
15/08/21 21:31:21 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 405, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 157.0 in stage 4.0 (TID 405)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 386) in 754 ms on localhost (141/200)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 390) in 729 ms on localhost (142/200)
15/08/21 21:31:21 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 406, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:21 INFO Executor: Running task 158.0 in stage 4.0 (TID 406)
15/08/21 21:31:21 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 388) in 746 ms on localhost (143/200)
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,736
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 50,196B for [ps_partkey] INT32: 15,023 values, 60,100B raw, 50,157B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO ColumnChunkPageWriteStore: written 77,252B for [part_value] DOUBLE: 15,023 values, 120,192B raw, 77,205B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:21 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:21 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000145
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000145_0: Committed
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000148
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000144
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000148_0: Committed
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000144_0: Committed
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO Executor: Finished task 145.0 in stage 4.0 (TID 393). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 144.0 in stage 4.0 (TID 392). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 148.0 in stage 4.0 (TID 396). 843 bytes result sent to driver
15/08/21 21:31:22 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 407, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO Executor: Running task 159.0 in stage 4.0 (TID 407)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 408, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 160.0 in stage 4.0 (TID 408)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 409, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 161.0 in stage 4.0 (TID 409)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 392) in 787 ms on localhost (144/200)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 393) in 777 ms on localhost (145/200)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 396) in 772 ms on localhost (146/200)
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000149
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000149_0: Committed
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO Executor: Finished task 149.0 in stage 4.0 (TID 397). 843 bytes result sent to driver
15/08/21 21:31:22 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 410, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 162.0 in stage 4.0 (TID 410)
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 397) in 547 ms on localhost (147/200)
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,456
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,448
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,520
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,996
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,936
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 49,504B for [ps_partkey] INT32: 14,809 values, 59,244B raw, 49,465B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 50,311B for [ps_partkey] INT32: 15,063 values, 60,260B raw, 50,272B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 51,023B for [ps_partkey] INT32: 15,359 values, 61,444B raw, 50,984B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 78,933B for [part_value] DOUBLE: 15,359 values, 122,880B raw, 78,886B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 77,535B for [part_value] DOUBLE: 15,063 values, 120,512B raw, 77,488B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 76,281B for [part_value] DOUBLE: 14,809 values, 118,480B raw, 76,234B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 52,190B for [ps_partkey] INT32: 15,636 values, 62,552B raw, 52,151B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 80,404B for [part_value] DOUBLE: 15,636 values, 125,096B raw, 80,357B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 50,652B for [ps_partkey] INT32: 15,183 values, 60,740B raw, 50,613B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 78,131B for [part_value] DOUBLE: 15,183 values, 121,472B raw, 78,084B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,684
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 50,997B for [ps_partkey] INT32: 15,272 values, 61,096B raw, 50,958B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 78,449B for [part_value] DOUBLE: 15,272 values, 122,184B raw, 78,402B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,756
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,770,436
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,256
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 51,914B for [ps_partkey] INT32: 15,549 values, 62,204B raw, 51,875B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 79,983B for [part_value] DOUBLE: 15,549 values, 124,400B raw, 79,936B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 52,299B for [ps_partkey] INT32: 15,658 values, 62,640B raw, 52,260B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 51,005B for [ps_partkey] INT32: 15,274 values, 61,104B raw, 50,966B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 80,491B for [part_value] DOUBLE: 15,658 values, 125,272B raw, 80,444B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 78,604B for [part_value] DOUBLE: 15,274 values, 122,200B raw, 78,557B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000151
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000151_0: Committed
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000152
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000152_0: Committed
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000153
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000146
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000153_0: Committed
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000146_0: Committed
15/08/21 21:31:22 INFO Executor: Finished task 151.0 in stage 4.0 (TID 399). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 153.0 in stage 4.0 (TID 401). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 152.0 in stage 4.0 (TID 400). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 146.0 in stage 4.0 (TID 394). 843 bytes result sent to driver
15/08/21 21:31:22 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 411, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 163.0 in stage 4.0 (TID 411)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 412, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 164.0 in stage 4.0 (TID 412)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 413, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 165.0 in stage 4.0 (TID 413)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 414, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 166.0 in stage 4.0 (TID 414)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 399) in 690 ms on localhost (148/200)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 401) in 616 ms on localhost (149/200)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 400) in 637 ms on localhost (150/200)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 394) in 1093 ms on localhost (151/200)
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000147
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000147_0: Committed
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO Executor: Finished task 147.0 in stage 4.0 (TID 395). 843 bytes result sent to driver
15/08/21 21:31:22 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 415, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 167.0 in stage 4.0 (TID 415)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 395) in 1102 ms on localhost (152/200)
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000143
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000143_0: Committed
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000156
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000156_0: Committed
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,756
15/08/21 21:31:22 INFO Executor: Finished task 143.0 in stage 4.0 (TID 391). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 156.0 in stage 4.0 (TID 404). 843 bytes result sent to driver
15/08/21 21:31:22 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 416, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 168.0 in stage 4.0 (TID 416)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 417, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 169.0 in stage 4.0 (TID 417)
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,988
15/08/21 21:31:22 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 391) in 1287 ms on localhost (153/200)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 404) in 647 ms on localhost (154/200)
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000157
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000157_0: Committed
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000158
15/08/21 21:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000155
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000158_0: Committed
15/08/21 21:31:22 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000155_0: Committed
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,748
15/08/21 21:31:22 INFO Executor: Finished task 157.0 in stage 4.0 (TID 405). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 158.0 in stage 4.0 (TID 406). 843 bytes result sent to driver
15/08/21 21:31:22 INFO Executor: Finished task 155.0 in stage 4.0 (TID 403). 843 bytes result sent to driver
15/08/21 21:31:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,240
15/08/21 21:31:22 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 418, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 170.0 in stage 4.0 (TID 418)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 419, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO Executor: Running task 171.0 in stage 4.0 (TID 419)
15/08/21 21:31:22 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 420, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 405) in 656 ms on localhost (155/200)
15/08/21 21:31:22 INFO Executor: Running task 172.0 in stage 4.0 (TID 420)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 403) in 664 ms on localhost (156/200)
15/08/21 21:31:22 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 406) in 651 ms on localhost (157/200)
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 49,271B for [ps_partkey] INT32: 14,736 values, 58,952B raw, 49,232B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 75,793B for [part_value] DOUBLE: 14,736 values, 117,896B raw, 75,746B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 51,721B for [ps_partkey] INT32: 15,524 values, 62,104B raw, 51,682B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 79,875B for [part_value] DOUBLE: 15,524 values, 124,200B raw, 79,828B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 50,149B for [ps_partkey] INT32: 15,099 values, 60,404B raw, 50,110B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 51,705B for [ps_partkey] INT32: 15,474 values, 61,904B raw, 51,666B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 77,711B for [part_value] DOUBLE: 15,099 values, 120,800B raw, 77,664B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ColumnChunkPageWriteStore: written 79,616B for [part_value] DOUBLE: 15,474 values, 123,800B raw, 79,569B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:22 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:22 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000160
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000154
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000160_0: Committed
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000161
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000161_0: Committed
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000150
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000150_0: Committed
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000154_0: Committed
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO Executor: Finished task 154.0 in stage 4.0 (TID 402). 843 bytes result sent to driver
15/08/21 21:31:23 INFO Executor: Finished task 150.0 in stage 4.0 (TID 398). 843 bytes result sent to driver
15/08/21 21:31:23 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 421, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Finished task 161.0 in stage 4.0 (TID 409). 843 bytes result sent to driver
15/08/21 21:31:23 INFO Executor: Running task 173.0 in stage 4.0 (TID 421)
15/08/21 21:31:23 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 422, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 423, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 175.0 in stage 4.0 (TID 423)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 402) in 1641 ms on localhost (158/200)
15/08/21 21:31:23 INFO Executor: Running task 174.0 in stage 4.0 (TID 422)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 398) in 1771 ms on localhost (159/200)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 409) in 1405 ms on localhost (160/200)
15/08/21 21:31:23 INFO Executor: Finished task 160.0 in stage 4.0 (TID 408). 843 bytes result sent to driver
15/08/21 21:31:23 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 424, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 408) in 1415 ms on localhost (161/200)
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,244
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,788
15/08/21 21:31:23 INFO Executor: Running task 176.0 in stage 4.0 (TID 424)
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,048
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,200
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,888
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 49,558B for [ps_partkey] INT32: 14,826 values, 59,312B raw, 49,519B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 76,257B for [part_value] DOUBLE: 14,826 values, 118,616B raw, 76,210B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000162
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000162_0: Committed
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 50,579B for [ps_partkey] INT32: 15,150 values, 60,608B raw, 50,540B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 77,838B for [part_value] DOUBLE: 15,150 values, 121,208B raw, 77,791B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000159
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000159_0: Committed
15/08/21 21:31:23 INFO Executor: Finished task 162.0 in stage 4.0 (TID 410). 843 bytes result sent to driver
15/08/21 21:31:23 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 425, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 177.0 in stage 4.0 (TID 425)
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 50,076B for [ps_partkey] INT32: 14,989 values, 59,964B raw, 50,037B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 410) in 1438 ms on localhost (162/200)
15/08/21 21:31:23 INFO Executor: Finished task 159.0 in stage 4.0 (TID 407). 843 bytes result sent to driver
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 77,123B for [part_value] DOUBLE: 14,989 values, 119,920B raw, 77,076B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 50,594B for [ps_partkey] INT32: 15,147 values, 60,596B raw, 50,555B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 426, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 178.0 in stage 4.0 (TID 426)
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 77,989B for [part_value] DOUBLE: 15,147 values, 121,184B raw, 77,942B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 407) in 1479 ms on localhost (163/200)
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 49,221B for [ps_partkey] INT32: 14,731 values, 58,932B raw, 49,182B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 75,709B for [part_value] DOUBLE: 14,731 values, 117,856B raw, 75,662B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,236
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,608
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,036
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,956
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 50,438B for [ps_partkey] INT32: 15,098 values, 60,400B raw, 50,399B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 77,641B for [part_value] DOUBLE: 15,098 values, 120,792B raw, 77,594B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 49,662B for [ps_partkey] INT32: 14,938 values, 59,760B raw, 49,623B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 50,882B for [ps_partkey] INT32: 15,267 values, 61,076B raw, 50,843B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 76,717B for [part_value] DOUBLE: 14,938 values, 119,512B raw, 76,670B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 78,518B for [part_value] DOUBLE: 15,267 values, 122,144B raw, 78,471B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,432
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 49,777B for [ps_partkey] INT32: 14,884 values, 59,544B raw, 49,738B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 76,597B for [part_value] DOUBLE: 14,884 values, 119,080B raw, 76,550B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 52,093B for [ps_partkey] INT32: 15,609 values, 62,444B raw, 52,054B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 80,224B for [part_value] DOUBLE: 15,609 values, 124,880B raw, 80,177B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000163
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000166
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000166_0: Committed
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000165
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000165_0: Committed
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000163_0: Committed
15/08/21 21:31:23 INFO Executor: Finished task 165.0 in stage 4.0 (TID 413). 843 bytes result sent to driver
15/08/21 21:31:23 INFO Executor: Finished task 166.0 in stage 4.0 (TID 414). 843 bytes result sent to driver
15/08/21 21:31:23 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 427, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Finished task 163.0 in stage 4.0 (TID 411). 843 bytes result sent to driver
15/08/21 21:31:23 INFO Executor: Running task 179.0 in stage 4.0 (TID 427)
15/08/21 21:31:23 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 428, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 180.0 in stage 4.0 (TID 428)
15/08/21 21:31:23 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 429, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 181.0 in stage 4.0 (TID 429)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 413) in 1337 ms on localhost (164/200)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 414) in 1337 ms on localhost (165/200)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 411) in 1339 ms on localhost (166/200)
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000171
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000171_0: Committed
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO Executor: Finished task 171.0 in stage 4.0 (TID 419). 843 bytes result sent to driver
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 430, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 182.0 in stage 4.0 (TID 430)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 419) in 1395 ms on localhost (167/200)
15/08/21 21:31:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:23 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:23 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000170
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000170_0: Committed
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000169
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000169_0: Committed
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000168
15/08/21 21:31:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000172
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000168_0: Committed
15/08/21 21:31:23 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000172_0: Committed
15/08/21 21:31:23 INFO Executor: Finished task 170.0 in stage 4.0 (TID 418). 843 bytes result sent to driver
15/08/21 21:31:23 INFO Executor: Finished task 169.0 in stage 4.0 (TID 417). 843 bytes result sent to driver
15/08/21 21:31:23 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 431, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 183.0 in stage 4.0 (TID 431)
15/08/21 21:31:23 INFO Executor: Finished task 168.0 in stage 4.0 (TID 416). 843 bytes result sent to driver
15/08/21 21:31:23 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 432, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Finished task 172.0 in stage 4.0 (TID 420). 843 bytes result sent to driver
15/08/21 21:31:23 INFO Executor: Running task 184.0 in stage 4.0 (TID 432)
15/08/21 21:31:23 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 433, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 434, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:23 INFO Executor: Running task 185.0 in stage 4.0 (TID 433)
15/08/21 21:31:23 INFO Executor: Running task 186.0 in stage 4.0 (TID 434)
15/08/21 21:31:23 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 417) in 1452 ms on localhost (168/200)
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 416) in 1453 ms on localhost (169/200)
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 420) in 1444 ms on localhost (170/200)
15/08/21 21:31:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:23 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 418) in 1446 ms on localhost (171/200)
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,536
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,788
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,856
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,788
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,556
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 49,687B for [ps_partkey] INT32: 14,876 values, 59,512B raw, 49,648B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 76,407B for [part_value] DOUBLE: 14,876 values, 119,016B raw, 76,360B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 51,029B for [ps_partkey] INT32: 15,313 values, 61,260B raw, 50,990B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 50,524B for [ps_partkey] INT32: 15,129 values, 60,524B raw, 50,485B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 78,700B for [part_value] DOUBLE: 15,313 values, 122,512B raw, 78,653B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 77,969B for [part_value] DOUBLE: 15,129 values, 121,040B raw, 77,922B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 49,299B for [ps_partkey] INT32: 14,826 values, 59,312B raw, 49,260B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 76,279B for [part_value] DOUBLE: 14,826 values, 118,616B raw, 76,232B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,056
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 51,348B for [ps_partkey] INT32: 15,364 values, 61,464B raw, 51,309B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 79,172B for [part_value] DOUBLE: 15,364 values, 122,920B raw, 79,125B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,028
15/08/21 21:31:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,048
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 50,231B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,192B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:23 INFO ColumnChunkPageWriteStore: written 77,408B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,361B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,996
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 49,423B for [ps_partkey] INT32: 14,788 values, 59,160B raw, 49,384B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 76,170B for [part_value] DOUBLE: 14,788 values, 118,312B raw, 76,123B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 51,700B for [ps_partkey] INT32: 15,489 values, 61,964B raw, 51,661B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 79,563B for [part_value] DOUBLE: 15,489 values, 123,920B raw, 79,516B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 52,053B for [ps_partkey] INT32: 15,586 values, 62,352B raw, 52,014B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 80,219B for [part_value] DOUBLE: 15,586 values, 124,696B raw, 80,172B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000164
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000164_0: Committed
15/08/21 21:31:24 INFO Executor: Finished task 164.0 in stage 4.0 (TID 412). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 435, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 187.0 in stage 4.0 (TID 435)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 412) in 1736 ms on localhost (172/200)
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000167
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000167_0: Committed
15/08/21 21:31:24 INFO Executor: Finished task 167.0 in stage 4.0 (TID 415). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 436, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 188.0 in stage 4.0 (TID 436)
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 415) in 1789 ms on localhost (173/200)
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000174
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000174_0: Committed
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000176
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000176_0: Committed
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO Executor: Finished task 174.0 in stage 4.0 (TID 422). 843 bytes result sent to driver
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 437, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 189.0 in stage 4.0 (TID 437)
15/08/21 21:31:24 INFO Executor: Finished task 176.0 in stage 4.0 (TID 424). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 438, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 190.0 in stage 4.0 (TID 438)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 424) in 731 ms on localhost (174/200)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 422) in 741 ms on localhost (175/200)
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000173
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000173_0: Committed
15/08/21 21:31:24 INFO Executor: Finished task 173.0 in stage 4.0 (TID 421). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 439, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 191.0 in stage 4.0 (TID 439)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 421) in 790 ms on localhost (176/200)
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000181
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000181_0: Committed
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO Executor: Finished task 181.0 in stage 4.0 (TID 429). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 440, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 192.0 in stage 4.0 (TID 440)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 429) in 550 ms on localhost (177/200)
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000180
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000180_0: Committed
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO Executor: Finished task 180.0 in stage 4.0 (TID 428). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 441, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 193.0 in stage 4.0 (TID 441)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 428) in 594 ms on localhost (178/200)
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,948
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,488
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,308
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,600
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 49,666B for [ps_partkey] INT32: 14,952 values, 59,816B raw, 49,627B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 49,428B for [ps_partkey] INT32: 14,784 values, 59,144B raw, 49,389B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 76,926B for [part_value] DOUBLE: 14,952 values, 119,624B raw, 76,879B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 76,037B for [part_value] DOUBLE: 14,784 values, 118,280B raw, 75,990B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 50,836B for [ps_partkey] INT32: 15,211 values, 60,852B raw, 50,797B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 78,247B for [part_value] DOUBLE: 15,211 values, 121,696B raw, 78,200B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,008
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 50,512B for [ps_partkey] INT32: 15,117 values, 60,476B raw, 50,473B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 77,798B for [part_value] DOUBLE: 15,117 values, 120,944B raw, 77,751B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 50,452B for [ps_partkey] INT32: 15,137 values, 60,556B raw, 50,413B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 77,897B for [part_value] DOUBLE: 15,137 values, 121,104B raw, 77,850B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,168
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,416
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 49,470B for [ps_partkey] INT32: 14,795 values, 59,188B raw, 49,431B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 75,985B for [part_value] DOUBLE: 14,795 values, 118,368B raw, 75,938B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 50,785B for [ps_partkey] INT32: 15,207 values, 60,836B raw, 50,746B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 78,386B for [part_value] DOUBLE: 15,207 values, 121,664B raw, 78,339B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,448
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,416
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,768
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,120
15/08/21 21:31:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,068
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 50,186B for [ps_partkey] INT32: 15,009 values, 60,044B raw, 50,147B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 52,194B for [ps_partkey] INT32: 15,625 values, 62,508B raw, 52,155B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 49,166B for [ps_partkey] INT32: 14,793 values, 59,180B raw, 49,127B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 50,514B for [ps_partkey] INT32: 15,107 values, 60,436B raw, 50,475B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 77,274B for [part_value] DOUBLE: 15,009 values, 120,080B raw, 77,227B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 75,993B for [part_value] DOUBLE: 14,793 values, 118,352B raw, 75,946B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 77,758B for [part_value] DOUBLE: 15,107 values, 120,864B raw, 77,711B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 80,315B for [part_value] DOUBLE: 15,625 values, 125,008B raw, 80,268B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 51,297B for [ps_partkey] INT32: 15,340 values, 61,368B raw, 51,258B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO ColumnChunkPageWriteStore: written 78,833B for [part_value] DOUBLE: 15,340 values, 122,728B raw, 78,786B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000179
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000179_0: Committed
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000175
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000175_0: Committed
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000185
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000178
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000185_0: Committed
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000186
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000178_0: Committed
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000177
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000186_0: Committed
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000177_0: Committed
15/08/21 21:31:24 INFO Executor: Finished task 178.0 in stage 4.0 (TID 426). 843 bytes result sent to driver
15/08/21 21:31:24 INFO Executor: Finished task 186.0 in stage 4.0 (TID 434). 843 bytes result sent to driver
15/08/21 21:31:24 INFO Executor: Finished task 177.0 in stage 4.0 (TID 425). 843 bytes result sent to driver
15/08/21 21:31:24 INFO Executor: Finished task 185.0 in stage 4.0 (TID 433). 843 bytes result sent to driver
15/08/21 21:31:24 INFO Executor: Finished task 179.0 in stage 4.0 (TID 427). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 442, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Finished task 175.0 in stage 4.0 (TID 423). 843 bytes result sent to driver
15/08/21 21:31:24 INFO Executor: Running task 194.0 in stage 4.0 (TID 442)
15/08/21 21:31:24 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 443, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 195.0 in stage 4.0 (TID 443)
15/08/21 21:31:24 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 444, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 196.0 in stage 4.0 (TID 444)
15/08/21 21:31:24 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 445, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 426) in 1189 ms on localhost (179/200)
15/08/21 21:31:24 INFO Executor: Running task 197.0 in stage 4.0 (TID 445)
15/08/21 21:31:24 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 446, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 434) in 788 ms on localhost (180/200)
15/08/21 21:31:24 INFO Executor: Running task 198.0 in stage 4.0 (TID 446)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 425) in 1197 ms on localhost (181/200)
15/08/21 21:31:24 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 447, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:24 INFO Executor: Running task 199.0 in stage 4.0 (TID 447)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 433) in 791 ms on localhost (182/200)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 427) in 1004 ms on localhost (183/200)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 423) in 1267 ms on localhost (184/200)
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000183
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000183_0: Committed
15/08/21 21:31:24 INFO Executor: Finished task 183.0 in stage 4.0 (TID 431). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 431) in 862 ms on localhost (185/200)
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000187
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000189
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000187_0: Committed
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000189_0: Committed
15/08/21 21:31:24 INFO Executor: Finished task 189.0 in stage 4.0 (TID 437). 843 bytes result sent to driver
15/08/21 21:31:24 INFO Executor: Finished task 187.0 in stage 4.0 (TID 435). 843 bytes result sent to driver
15/08/21 21:31:24 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 435) in 749 ms on localhost (186/200)
15/08/21 21:31:24 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 437) in 677 ms on localhost (187/200)
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:24 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:24 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000188
15/08/21 21:31:24 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000188_0: Committed
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:25 INFO Executor: Finished task 188.0 in stage 4.0 (TID 436). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 436) in 853 ms on localhost (188/200)
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000190
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000192
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000191
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000190_0: Committed
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000191_0: Committed
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000192_0: Committed
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000182
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000184
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000182_0: Committed
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000184_0: Committed
15/08/21 21:31:25 INFO Executor: Finished task 190.0 in stage 4.0 (TID 438). 843 bytes result sent to driver
15/08/21 21:31:25 INFO Executor: Finished task 192.0 in stage 4.0 (TID 440). 843 bytes result sent to driver
15/08/21 21:31:25 INFO Executor: Finished task 191.0 in stage 4.0 (TID 439). 843 bytes result sent to driver
15/08/21 21:31:25 INFO Executor: Finished task 184.0 in stage 4.0 (TID 432). 843 bytes result sent to driver
15/08/21 21:31:25 INFO Executor: Finished task 182.0 in stage 4.0 (TID 430). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 438) in 886 ms on localhost (189/200)
15/08/21 21:31:25 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 439) in 834 ms on localhost (190/200)
15/08/21 21:31:25 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 440) in 816 ms on localhost (191/200)
15/08/21 21:31:25 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 432) in 1152 ms on localhost (192/200)
15/08/21 21:31:25 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 430) in 1201 ms on localhost (193/200)
15/08/21 21:31:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,996
15/08/21 21:31:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,088
15/08/21 21:31:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,868
15/08/21 21:31:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,196
15/08/21 21:31:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,688
15/08/21 21:31:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,908
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 49,578B for [ps_partkey] INT32: 14,836 values, 59,352B raw, 49,539B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 49,411B for [ps_partkey] INT32: 14,791 values, 59,172B raw, 49,372B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 76,124B for [part_value] DOUBLE: 14,791 values, 118,336B raw, 76,077B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 76,367B for [part_value] DOUBLE: 14,836 values, 118,696B raw, 76,320B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 51,188B for [ps_partkey] INT32: 15,330 values, 61,328B raw, 51,149B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 78,891B for [part_value] DOUBLE: 15,330 values, 122,648B raw, 78,844B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 51,393B for [ps_partkey] INT32: 15,396 values, 61,592B raw, 51,354B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 79,186B for [part_value] DOUBLE: 15,396 values, 123,176B raw, 79,139B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 51,976B for [ps_partkey] INT32: 15,571 values, 62,292B raw, 51,937B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 79,945B for [part_value] DOUBLE: 15,571 values, 124,576B raw, 79,898B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 51,427B for [ps_partkey] INT32: 15,432 values, 61,736B raw, 51,388B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO ColumnChunkPageWriteStore: written 79,261B for [part_value] DOUBLE: 15,432 values, 123,464B raw, 79,214B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000199
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000199_0: Committed
15/08/21 21:31:25 INFO Executor: Finished task 199.0 in stage 4.0 (TID 447). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 447) in 629 ms on localhost (194/200)
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000194
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000194_0: Committed
15/08/21 21:31:25 INFO Executor: Finished task 194.0 in stage 4.0 (TID 442). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 442) in 640 ms on localhost (195/200)
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000193
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000193_0: Committed
15/08/21 21:31:25 INFO Executor: Finished task 193.0 in stage 4.0 (TID 441). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 441) in 1132 ms on localhost (196/200)
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000196
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000196_0: Committed
15/08/21 21:31:25 INFO Executor: Finished task 196.0 in stage 4.0 (TID 444). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 444) in 1105 ms on localhost (197/200)
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000195
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000197
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000197_0: Committed
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000195_0: Committed
15/08/21 21:31:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0004_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212131_0004_m_000198
15/08/21 21:31:25 INFO SparkHadoopMapRedUtil: attempt_201508212131_0004_m_000198_0: Committed
15/08/21 21:31:25 INFO Executor: Finished task 195.0 in stage 4.0 (TID 443). 843 bytes result sent to driver
15/08/21 21:31:25 INFO Executor: Finished task 197.0 in stage 4.0 (TID 445). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 443) in 1180 ms on localhost (198/200)
15/08/21 21:31:25 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 445) in 1179 ms on localhost (199/200)
15/08/21 21:31:25 INFO Executor: Finished task 198.0 in stage 4.0 (TID 446). 843 bytes result sent to driver
15/08/21 21:31:25 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 446) in 1185 ms on localhost (200/200)
15/08/21 21:31:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/21 21:31:25 INFO DAGScheduler: ResultStage 4 (processCmd at CliDriver.java:423) finished in 12.140 s
15/08/21 21:31:25 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@9cc9505
15/08/21 21:31:25 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 54.420315 s
15/08/21 21:31:25 INFO StatsReportListener: task runtime:(count: 200, mean: 926.375000, stdev: 452.426706, max: 2272.000000, min: 352.000000)
15/08/21 21:31:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:25 INFO StatsReportListener: 	352.0 ms	437.0 ms	469.0 ms	547.0 ms	787.0 ms	1.2 s	1.7 s	1.9 s	2.3 s
15/08/21 21:31:25 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.385000, stdev: 0.752845, max: 7.000000, min: 0.000000)
15/08/21 21:31:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:25 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	1.0 ms	7.0 ms
15/08/21 21:31:25 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 21:31:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:25 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 21:31:25 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 21:31:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:25 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 21:31:25 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 88.105518, stdev: 6.361908, max: 96.183206, min: 29.076087)
15/08/21 21:31:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:25 INFO StatsReportListener: 	29 %	81 %	82 %	84 %	88 %	92 %	95 %	95 %	96 %
15/08/21 21:31:25 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.053667, stdev: 0.116617, max: 1.190476, min: 0.000000)
15/08/21 21:31:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:25 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/21 21:31:25 INFO StatsReportListener: other time pct: (count: 200, mean: 11.840815, stdev: 6.353973, max: 70.923913, min: 3.816794)
15/08/21 21:31:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:25 INFO StatsReportListener: 	 4 %	 5 %	 5 %	 8 %	11 %	15 %	18 %	19 %	71 %
15/08/21 21:31:29 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 21:31:30 INFO DefaultWriterContainer: Job job_201508212130_0000 committed.
15/08/21 21:31:30 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 21:31:30 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_common_metadata
15/08/21 21:31:30 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 21:31:30 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 21:31:30 INFO DAGScheduler: Final stage: ResultStage 5(processCmd at CliDriver.java:423)
15/08/21 21:31:30 INFO DAGScheduler: Parents of final stage: List()
15/08/21 21:31:30 INFO DAGScheduler: Missing parents: List()
15/08/21 21:31:30 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 21:31:30 INFO MemoryStore: ensureFreeSpace(2968) called with curMem=1162853, maxMem=22226833244
15/08/21 21:31:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 21:31:30 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=1165821, maxMem=22226833244
15/08/21 21:31:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1776.0 B, free 20.7 GB)
15/08/21 21:31:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:52592 (size: 1776.0 B, free: 20.7 GB)
15/08/21 21:31:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874
15/08/21 21:31:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/21 21:31:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/08/21 21:31:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 448, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 21:31:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 448)
15/08/21 21:31:30 INFO Executor: Finished task 0.0 in stage 5.0 (TID 448). 606 bytes result sent to driver
15/08/21 21:31:30 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 448) in 339 ms on localhost (1/1)
15/08/21 21:31:30 INFO DAGScheduler: ResultStage 5 (processCmd at CliDriver.java:423) finished in 0.340 s
15/08/21 21:31:30 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/21 21:31:30 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@61af38b9
15/08/21 21:31:30 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 0.359060 s
15/08/21 21:31:30 INFO StatsReportListener: task runtime:(count: 1, mean: 339.000000, stdev: 0.000000, max: 339.000000, min: 339.000000)
15/08/21 21:31:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:30 INFO StatsReportListener: 	339.0 ms	339.0 ms	339.0 ms	339.0 ms	339.0 ms	339.0 ms	339.0 ms	339.0 ms	339.0 ms
15/08/21 21:31:30 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 21:31:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:30 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 21:31:30 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 32.743363, stdev: 0.000000, max: 32.743363, min: 32.743363)
15/08/21 21:31:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:30 INFO StatsReportListener: 	33 %	33 %	33 %	33 %	33 %	33 %	33 %	33 %	33 %
15/08/21 21:31:30 INFO StatsReportListener: other time pct: (count: 1, mean: 67.256637, stdev: 0.000000, max: 67.256637, min: 67.256637)
15/08/21 21:31:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:30 INFO StatsReportListener: 	67 %	67 %	67 %	67 %	67 %	67 %	67 %	67 %	67 %
Time taken: 66.406 seconds
15/08/21 21:31:30 INFO CliDriver: Time taken: 66.406 seconds
15/08/21 21:31:30 INFO ParseDriver: Parsing command: insert into table q11_important_stock_par
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par) sum_tmp
    join q11_part_tmp_par
where part_value > total_value * 0.000001
order by value desc
15/08/21 21:31:30 INFO ParseDriver: Parse Completed
15/08/21 21:31:30 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 21:31:30 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1167597, maxMem=22226833244
15/08/21 21:31:30 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 21:31:30 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1494205, maxMem=22226833244
15/08/21 21:31:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 21:31:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:52592 (size: 22.3 KB, free: 20.7 GB)
15/08/21 21:31:30 INFO SparkContext: Created broadcast 10 from processCmd at CliDriver.java:423
15/08/21 21:31:30 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1516998, maxMem=22226833244
15/08/21 21:31:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 21:31:31 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1843606, maxMem=22226833244
15/08/21 21:31:31 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 21:31:31 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:52592 (size: 22.3 KB, free: 20.7 GB)
15/08/21 21:31:31 INFO SparkContext: Created broadcast 11 from processCmd at CliDriver.java:423
15/08/21 21:31:31 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 21:31:31 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 21:31:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 21:31:31 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 21:31:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 21:31:32 INFO DAGScheduler: Registering RDD 31 (processCmd at CliDriver.java:423)
15/08/21 21:31:32 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 21:31:32 INFO DAGScheduler: Final stage: ResultStage 7(processCmd at CliDriver.java:423)
15/08/21 21:31:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
15/08/21 21:31:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)
15/08/21 21:31:32 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 21:31:32 INFO MemoryStore: ensureFreeSpace(7688) called with curMem=1866399, maxMem=22226833244
15/08/21 21:31:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.5 KB, free 20.7 GB)
15/08/21 21:31:32 INFO MemoryStore: ensureFreeSpace(3920) called with curMem=1874087, maxMem=22226833244
15/08/21 21:31:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.8 KB, free 20.7 GB)
15/08/21 21:31:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:52592 (size: 3.8 KB, free: 20.7 GB)
15/08/21 21:31:32 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/21 21:31:32 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at processCmd at CliDriver.java:423)
15/08/21 21:31:32 INFO TaskSchedulerImpl: Adding task set 6.0 with 200 tasks
15/08/21 21:31:32 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 449, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 450, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 451, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 452, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 453, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 454, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 455, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 456, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 457, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 458, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 459, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 460, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 461, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 462, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 463, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 464, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO Executor: Running task 1.0 in stage 6.0 (TID 450)
15/08/21 21:31:32 INFO Executor: Running task 5.0 in stage 6.0 (TID 454)
15/08/21 21:31:32 INFO Executor: Running task 12.0 in stage 6.0 (TID 461)
15/08/21 21:31:32 INFO Executor: Running task 11.0 in stage 6.0 (TID 460)
15/08/21 21:31:32 INFO Executor: Running task 6.0 in stage 6.0 (TID 455)
15/08/21 21:31:32 INFO Executor: Running task 2.0 in stage 6.0 (TID 451)
15/08/21 21:31:32 INFO Executor: Running task 8.0 in stage 6.0 (TID 457)
15/08/21 21:31:32 INFO Executor: Running task 0.0 in stage 6.0 (TID 449)
15/08/21 21:31:32 INFO Executor: Running task 4.0 in stage 6.0 (TID 453)
15/08/21 21:31:32 INFO Executor: Running task 13.0 in stage 6.0 (TID 462)
15/08/21 21:31:32 INFO Executor: Running task 3.0 in stage 6.0 (TID 452)
15/08/21 21:31:32 INFO Executor: Running task 10.0 in stage 6.0 (TID 459)
15/08/21 21:31:32 INFO Executor: Running task 9.0 in stage 6.0 (TID 458)
15/08/21 21:31:32 INFO Executor: Running task 15.0 in stage 6.0 (TID 464)
15/08/21 21:31:32 INFO Executor: Running task 7.0 in stage 6.0 (TID 456)
15/08/21 21:31:32 INFO Executor: Running task 14.0 in stage 6.0 (TID 463)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15039
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15545
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 14788
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 15142
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 15524
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 15049
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15062
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15046
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14590
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14731
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14829
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15007
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 15570
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 14824
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 15106
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 15104
15/08/21 21:31:32 INFO Executor: Finished task 7.0 in stage 6.0 (TID 456). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 12.0 in stage 6.0 (TID 461). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 5.0 in stage 6.0 (TID 454). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 13.0 in stage 6.0 (TID 462). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 10.0 in stage 6.0 (TID 459). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 15.0 in stage 6.0 (TID 464). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 8.0 in stage 6.0 (TID 457). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 2.0 in stage 6.0 (TID 451). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 11.0 in stage 6.0 (TID 460). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 465, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 466, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 14.0 in stage 6.0 (TID 463). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 467, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 0.0 in stage 6.0 (TID 449). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 17.0 in stage 6.0 (TID 466)
15/08/21 21:31:32 INFO Executor: Finished task 3.0 in stage 6.0 (TID 452). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 18.0 in stage 6.0 (TID 467)
15/08/21 21:31:32 INFO Executor: Finished task 6.0 in stage 6.0 (TID 455). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 468, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO Executor: Running task 16.0 in stage 6.0 (TID 465)
15/08/21 21:31:32 INFO Executor: Finished task 9.0 in stage 6.0 (TID 458). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 1.0 in stage 6.0 (TID 450). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 19.0 in stage 6.0 (TID 468)
15/08/21 21:31:32 INFO Executor: Finished task 4.0 in stage 6.0 (TID 453). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 456) in 134 ms on localhost (1/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 461) in 131 ms on localhost (2/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 454) in 139 ms on localhost (3/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 469, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 470, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 20.0 in stage 6.0 (TID 469)
15/08/21 21:31:32 INFO Executor: Running task 21.0 in stage 6.0 (TID 470)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 471, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 22.0 in stage 6.0 (TID 471)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 462) in 139 ms on localhost (4/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 464) in 144 ms on localhost (5/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 472, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 457) in 151 ms on localhost (6/200)
15/08/21 21:31:32 INFO Executor: Running task 23.0 in stage 6.0 (TID 472)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 459) in 151 ms on localhost (7/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 473, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 24.0 in stage 6.0 (TID 473)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15120
15/08/21 21:31:32 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 474, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14693
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15470
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14755
15/08/21 21:31:32 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 475, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14906
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 26.0 in stage 6.0 (TID 475)
15/08/21 21:31:32 INFO Executor: Running task 25.0 in stage 6.0 (TID 474)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15625
15/08/21 21:31:32 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 476, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 21:31:32 INFO Executor: Running task 27.0 in stage 6.0 (TID 476)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15313
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 451) in 179 ms on localhost (8/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 477, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO Executor: Running task 28.0 in stage 6.0 (TID 477)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 478, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 29.0 in stage 6.0 (TID 478)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 479, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 30.0 in stage 6.0 (TID 479)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 21:31:32 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 480, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15147
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 460) in 185 ms on localhost (9/200)
15/08/21 21:31:32 INFO Executor: Running task 31.0 in stage 6.0 (TID 480)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14831
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 455) in 193 ms on localhost (10/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 16.0 in stage 6.0 (TID 465). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14776
15/08/21 21:31:32 INFO Executor: Finished task 21.0 in stage 6.0 (TID 470). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15114
15/08/21 21:31:32 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 481, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 22.0 in stage 6.0 (TID 471). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 463) in 196 ms on localhost (11/200)
15/08/21 21:31:32 INFO Executor: Running task 32.0 in stage 6.0 (TID 481)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 452) in 204 ms on localhost (12/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14612
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15344
15/08/21 21:31:32 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 449) in 208 ms on localhost (13/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15129
15/08/21 21:31:32 INFO Executor: Finished task 19.0 in stage 6.0 (TID 468). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14826
15/08/21 21:31:32 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 482, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 483, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 484, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 34.0 in stage 6.0 (TID 483)
15/08/21 21:31:32 INFO Executor: Running task 33.0 in stage 6.0 (TID 482)
15/08/21 21:31:32 INFO Executor: Running task 35.0 in stage 6.0 (TID 484)
15/08/21 21:31:32 INFO Executor: Finished task 18.0 in stage 6.0 (TID 467). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 485, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 21:31:32 INFO Executor: Finished task 23.0 in stage 6.0 (TID 472). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 15383
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 36.0 in stage 6.0 (TID 485)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 458) in 210 ms on localhost (14/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15063
15/08/21 21:31:32 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 465) in 103 ms on localhost (15/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 453) in 222 ms on localhost (16/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 450) in 224 ms on localhost (17/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 17.0 in stage 6.0 (TID 466). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 25.0 in stage 6.0 (TID 474). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 486, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 20.0 in stage 6.0 (TID 469). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 37.0 in stage 6.0 (TID 486)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 487, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 21:31:32 INFO Executor: Running task 38.0 in stage 6.0 (TID 487)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 30.0 in stage 6.0 (TID 479). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 488, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 21:31:32 INFO Executor: Running task 39.0 in stage 6.0 (TID 488)
15/08/21 21:31:32 INFO Executor: Finished task 28.0 in stage 6.0 (TID 477). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 489, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 40.0 in stage 6.0 (TID 489)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 490, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 471) in 90 ms on localhost (18/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14952
15/08/21 21:31:32 INFO Executor: Running task 41.0 in stage 6.0 (TID 490)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 468) in 105 ms on localhost (19/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14809
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 467) in 108 ms on localhost (20/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 26.0 in stage 6.0 (TID 475). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 470) in 99 ms on localhost (21/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 491, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 492, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO Executor: Running task 43.0 in stage 6.0 (TID 492)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 42.0 in stage 6.0 (TID 491)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 472) in 94 ms on localhost (22/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 466) in 118 ms on localhost (23/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14750
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 31.0 in stage 6.0 (TID 480). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 29.0 in stage 6.0 (TID 478). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 474) in 85 ms on localhost (24/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15393
15/08/21 21:31:32 INFO Executor: Finished task 27.0 in stage 6.0 (TID 476). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 469) in 111 ms on localhost (25/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 21:31:32 INFO Executor: Finished task 24.0 in stage 6.0 (TID 473). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14718
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15099
15/08/21 21:31:32 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 493, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Running task 44.0 in stage 6.0 (TID 493)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 494, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 479) in 75 ms on localhost (26/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 35.0 in stage 6.0 (TID 484). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15384
15/08/21 21:31:32 INFO Executor: Running task 45.0 in stage 6.0 (TID 494)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 495, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15571
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14954
15/08/21 21:31:32 INFO Executor: Running task 46.0 in stage 6.0 (TID 495)
15/08/21 21:31:32 INFO Executor: Finished task 34.0 in stage 6.0 (TID 483). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 496, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 497, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Running task 48.0 in stage 6.0 (TID 497)
15/08/21 21:31:32 INFO Executor: Running task 47.0 in stage 6.0 (TID 496)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 498, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14741
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 49.0 in stage 6.0 (TID 498)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 475) in 105 ms on localhost (27/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 478) in 101 ms on localhost (28/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 477) in 104 ms on localhost (29/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 33.0 in stage 6.0 (TID 482). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 32.0 in stage 6.0 (TID 481). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 50.0 in stage 6.0 (TID 499, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 37.0 in stage 6.0 (TID 486). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 50.0 in stage 6.0 (TID 499)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 51.0 in stage 6.0 (TID 500, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 484) in 82 ms on localhost (30/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 476) in 122 ms on localhost (31/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 473) in 137 ms on localhost (32/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 480) in 108 ms on localhost (33/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 43.0 in stage 6.0 (TID 492). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14479
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15361
15/08/21 21:31:32 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 483) in 88 ms on localhost (34/200)
15/08/21 21:31:32 INFO Executor: Running task 51.0 in stage 6.0 (TID 500)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 52.0 in stage 6.0 (TID 501, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 52.0 in stage 6.0 (TID 501)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15145
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 53.0 in stage 6.0 (TID 502, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 53.0 in stage 6.0 (TID 502)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 482) in 93 ms on localhost (35/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 481) in 106 ms on localhost (36/200)
15/08/21 21:31:32 INFO Executor: Finished task 36.0 in stage 6.0 (TID 485). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 38.0 in stage 6.0 (TID 487). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 14810
15/08/21 21:31:32 INFO Executor: Finished task 41.0 in stage 6.0 (TID 490). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 54.0 in stage 6.0 (TID 503, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 54.0 in stage 6.0 (TID 503)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 55.0 in stage 6.0 (TID 504, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 14914
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 15013
15/08/21 21:31:32 INFO Executor: Running task 55.0 in stage 6.0 (TID 504)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 40.0 in stage 6.0 (TID 489). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 56.0 in stage 6.0 (TID 505, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO Executor: Running task 56.0 in stage 6.0 (TID 505)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 57.0 in stage 6.0 (TID 506, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 57.0 in stage 6.0 (TID 506)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14613
15/08/21 21:31:32 INFO Executor: Finished task 49.0 in stage 6.0 (TID 498). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14780
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 42.0 in stage 6.0 (TID 491). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 485) in 116 ms on localhost (37/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15296
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15132
15/08/21 21:31:32 INFO Executor: Finished task 48.0 in stage 6.0 (TID 497). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15207
15/08/21 21:31:32 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 486) in 108 ms on localhost (38/200)
15/08/21 21:31:32 INFO Executor: Finished task 46.0 in stage 6.0 (TID 495). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 44.0 in stage 6.0 (TID 493). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 492) in 93 ms on localhost (39/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 58.0 in stage 6.0 (TID 507, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 489) in 107 ms on localhost (40/200)
15/08/21 21:31:32 INFO Executor: Running task 58.0 in stage 6.0 (TID 507)
15/08/21 21:31:32 INFO Executor: Finished task 39.0 in stage 6.0 (TID 488). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 45.0 in stage 6.0 (TID 494). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 59.0 in stage 6.0 (TID 508, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 59.0 in stage 6.0 (TID 508)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14967
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 60.0 in stage 6.0 (TID 509, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 60.0 in stage 6.0 (TID 509)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 61.0 in stage 6.0 (TID 510, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 61.0 in stage 6.0 (TID 510)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 62.0 in stage 6.0 (TID 511, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 498) in 86 ms on localhost (41/200)
15/08/21 21:31:32 INFO Executor: Running task 62.0 in stage 6.0 (TID 511)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15274
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 487) in 133 ms on localhost (42/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14795
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 497) in 92 ms on localhost (43/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 491) in 120 ms on localhost (44/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 495) in 105 ms on localhost (45/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 51.0 in stage 6.0 (TID 500). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 493) in 114 ms on localhost (46/200)
15/08/21 21:31:32 INFO Executor: Finished task 47.0 in stage 6.0 (TID 496). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 50.0 in stage 6.0 (TID 499). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 490) in 136 ms on localhost (47/200)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 63.0 in stage 6.0 (TID 512, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 63.0 in stage 6.0 (TID 512)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 64.0 in stage 6.0 (TID 513, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 53.0 in stage 6.0 (TID 502). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 52.0 in stage 6.0 (TID 501). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 64.0 in stage 6.0 (TID 513)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 65.0 in stage 6.0 (TID 514, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 66.0 in stage 6.0 (TID 515, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 66.0 in stage 6.0 (TID 515)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 67.0 in stage 6.0 (TID 516, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 67.0 in stage 6.0 (TID 516)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 68.0 in stage 6.0 (TID 517, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Running task 68.0 in stage 6.0 (TID 517)
15/08/21 21:31:32 INFO Executor: Running task 65.0 in stage 6.0 (TID 514)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 69.0 in stage 6.0 (TID 518, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 21:31:32 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 488) in 149 ms on localhost (48/200)
15/08/21 21:31:32 INFO Executor: Running task 69.0 in stage 6.0 (TID 518)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14972
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14771
15/08/21 21:31:32 INFO TaskSetManager: Finished task 50.0 in stage 6.0 (TID 499) in 96 ms on localhost (49/200)
15/08/21 21:31:32 INFO Executor: Finished task 57.0 in stage 6.0 (TID 506). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 70.0 in stage 6.0 (TID 519, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 70.0 in stage 6.0 (TID 519)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 53.0 in stage 6.0 (TID 502) in 88 ms on localhost (50/200)
15/08/21 21:31:32 INFO Executor: Finished task 55.0 in stage 6.0 (TID 504). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 56.0 in stage 6.0 (TID 505). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 52.0 in stage 6.0 (TID 501) in 92 ms on localhost (51/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 494) in 138 ms on localhost (52/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14502
15/08/21 21:31:32 INFO TaskSetManager: Starting task 71.0 in stage 6.0 (TID 520, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 54.0 in stage 6.0 (TID 503). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 72.0 in stage 6.0 (TID 521, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 72.0 in stage 6.0 (TID 521)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 51.0 in stage 6.0 (TID 500) in 107 ms on localhost (53/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 71.0 in stage 6.0 (TID 520)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15340
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14864
15/08/21 21:31:32 INFO Executor: Finished task 60.0 in stage 6.0 (TID 509). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15386
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14982
15/08/21 21:31:32 INFO TaskSetManager: Starting task 73.0 in stage 6.0 (TID 522, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 496) in 147 ms on localhost (54/200)
15/08/21 21:31:32 INFO Executor: Running task 73.0 in stage 6.0 (TID 522)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15636
15/08/21 21:31:32 INFO TaskSetManager: Starting task 74.0 in stage 6.0 (TID 523, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 58.0 in stage 6.0 (TID 507). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 55.0 in stage 6.0 (TID 504) in 105 ms on localhost (55/200)
15/08/21 21:31:32 INFO Executor: Finished task 59.0 in stage 6.0 (TID 508). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15347
15/08/21 21:31:32 INFO Executor: Running task 74.0 in stage 6.0 (TID 523)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 57.0 in stage 6.0 (TID 506) in 92 ms on localhost (56/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 15131
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 75.0 in stage 6.0 (TID 524, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14489
15/08/21 21:31:32 INFO Executor: Running task 75.0 in stage 6.0 (TID 524)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 56.0 in stage 6.0 (TID 505) in 113 ms on localhost (57/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 60.0 in stage 6.0 (TID 509) in 71 ms on localhost (58/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 54.0 in stage 6.0 (TID 503) in 118 ms on localhost (59/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 76.0 in stage 6.0 (TID 525, localhost, ANY, 1683 bytes)
15/08/21 21:31:32 INFO Executor: Running task 76.0 in stage 6.0 (TID 525)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14989
15/08/21 21:31:32 INFO TaskSetManager: Finished task 58.0 in stage 6.0 (TID 507) in 88 ms on localhost (60/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 67.0 in stage 6.0 (TID 516). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15039
15/08/21 21:31:32 INFO Executor: Finished task 61.0 in stage 6.0 (TID 510). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 59.0 in stage 6.0 (TID 508) in 91 ms on localhost (61/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 62.0 in stage 6.0 (TID 511). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 77.0 in stage 6.0 (TID 526, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Running task 77.0 in stage 6.0 (TID 526)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 78.0 in stage 6.0 (TID 527, localhost, ANY, 1682 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 64.0 in stage 6.0 (TID 513). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14590
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 78.0 in stage 6.0 (TID 527)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 67.0 in stage 6.0 (TID 516) in 67 ms on localhost (62/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 61.0 in stage 6.0 (TID 510) in 89 ms on localhost (63/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 62.0 in stage 6.0 (TID 511) in 93 ms on localhost (64/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 79.0 in stage 6.0 (TID 528, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 21:31:32 INFO Executor: Running task 79.0 in stage 6.0 (TID 528)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14509
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15509
15/08/21 21:31:32 INFO Executor: Finished task 65.0 in stage 6.0 (TID 514). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 70.0 in stage 6.0 (TID 519). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 80.0 in stage 6.0 (TID 529, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 66.0 in stage 6.0 (TID 515). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 68.0 in stage 6.0 (TID 517). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 63.0 in stage 6.0 (TID 512). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 80.0 in stage 6.0 (TID 529)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 81.0 in stage 6.0 (TID 530, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 82.0 in stage 6.0 (TID 531, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 81.0 in stage 6.0 (TID 530)
15/08/21 21:31:32 INFO Executor: Finished task 69.0 in stage 6.0 (TID 518). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 64.0 in stage 6.0 (TID 513) in 89 ms on localhost (65/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15603
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 82.0 in stage 6.0 (TID 531)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 21:31:32 INFO TaskSetManager: Starting task 83.0 in stage 6.0 (TID 532, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 14944
15/08/21 21:31:32 INFO TaskSetManager: Finished task 65.0 in stage 6.0 (TID 514) in 98 ms on localhost (66/200)
15/08/21 21:31:32 INFO Executor: Running task 83.0 in stage 6.0 (TID 532)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14678
15/08/21 21:31:32 INFO TaskSetManager: Starting task 84.0 in stage 6.0 (TID 533, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 84.0 in stage 6.0 (TID 533)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 71.0 in stage 6.0 (TID 520). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 85.0 in stage 6.0 (TID 534, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 72.0 in stage 6.0 (TID 521). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 70.0 in stage 6.0 (TID 519) in 96 ms on localhost (67/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15489
15/08/21 21:31:32 INFO Executor: Running task 85.0 in stage 6.0 (TID 534)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15090
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 76.0 in stage 6.0 (TID 525). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14854
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15333
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 66.0 in stage 6.0 (TID 515) in 108 ms on localhost (68/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 68.0 in stage 6.0 (TID 517) in 113 ms on localhost (69/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 63.0 in stage 6.0 (TID 512) in 119 ms on localhost (70/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15501
15/08/21 21:31:32 INFO Executor: Finished task 75.0 in stage 6.0 (TID 524). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 86.0 in stage 6.0 (TID 535, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 86.0 in stage 6.0 (TID 535)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 87.0 in stage 6.0 (TID 536, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 87.0 in stage 6.0 (TID 536)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15043
15/08/21 21:31:32 INFO Executor: Finished task 74.0 in stage 6.0 (TID 523). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 21:31:32 INFO Executor: Finished task 73.0 in stage 6.0 (TID 522). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 77.0 in stage 6.0 (TID 526). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 78.0 in stage 6.0 (TID 527). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 88.0 in stage 6.0 (TID 537, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15189
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15368
15/08/21 21:31:32 INFO TaskSetManager: Finished task 71.0 in stage 6.0 (TID 520) in 111 ms on localhost (71/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 69.0 in stage 6.0 (TID 518) in 127 ms on localhost (72/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 72.0 in stage 6.0 (TID 521) in 112 ms on localhost (73/200)
15/08/21 21:31:32 INFO Executor: Running task 88.0 in stage 6.0 (TID 537)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14842
15/08/21 21:31:32 INFO TaskSetManager: Starting task 89.0 in stage 6.0 (TID 538, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 79.0 in stage 6.0 (TID 528). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 80.0 in stage 6.0 (TID 529). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 89.0 in stage 6.0 (TID 538)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14939
15/08/21 21:31:32 INFO TaskSetManager: Starting task 90.0 in stage 6.0 (TID 539, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 76.0 in stage 6.0 (TID 525) in 102 ms on localhost (74/200)
15/08/21 21:31:32 INFO Executor: Running task 90.0 in stage 6.0 (TID 539)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 91.0 in stage 6.0 (TID 540, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 91.0 in stage 6.0 (TID 540)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 75.0 in stage 6.0 (TID 524) in 115 ms on localhost (75/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 92.0 in stage 6.0 (TID 541, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 81.0 in stage 6.0 (TID 530). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 74.0 in stage 6.0 (TID 523) in 128 ms on localhost (76/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 73.0 in stage 6.0 (TID 522) in 141 ms on localhost (77/200)
15/08/21 21:31:32 INFO Executor: Running task 92.0 in stage 6.0 (TID 541)
15/08/21 21:31:32 INFO Executor: Finished task 83.0 in stage 6.0 (TID 532). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 85.0 in stage 6.0 (TID 534). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 84.0 in stage 6.0 (TID 533). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 93.0 in stage 6.0 (TID 542, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 93.0 in stage 6.0 (TID 542)
15/08/21 21:31:32 INFO Executor: Finished task 82.0 in stage 6.0 (TID 531). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15795
15/08/21 21:31:32 INFO TaskSetManager: Starting task 94.0 in stage 6.0 (TID 543, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 86.0 in stage 6.0 (TID 535). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 77.0 in stage 6.0 (TID 526) in 105 ms on localhost (78/200)
15/08/21 21:31:32 INFO Executor: Running task 94.0 in stage 6.0 (TID 543)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 95.0 in stage 6.0 (TID 544, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15023
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15396
15/08/21 21:31:32 INFO Executor: Running task 95.0 in stage 6.0 (TID 544)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 21:31:32 INFO TaskSetManager: Starting task 96.0 in stage 6.0 (TID 545, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 96.0 in stage 6.0 (TID 545)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 78.0 in stage 6.0 (TID 527) in 114 ms on localhost (79/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 87.0 in stage 6.0 (TID 536). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 80.0 in stage 6.0 (TID 529) in 104 ms on localhost (80/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 97.0 in stage 6.0 (TID 546, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14740
15/08/21 21:31:32 INFO Executor: Running task 97.0 in stage 6.0 (TID 546)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 98.0 in stage 6.0 (TID 547, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 98.0 in stage 6.0 (TID 547)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 99.0 in stage 6.0 (TID 548, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 79.0 in stage 6.0 (TID 528) in 111 ms on localhost (81/200)
15/08/21 21:31:32 INFO Executor: Running task 99.0 in stage 6.0 (TID 548)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 100.0 in stage 6.0 (TID 549, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 100.0 in stage 6.0 (TID 549)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 81.0 in stage 6.0 (TID 530) in 108 ms on localhost (82/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 85.0 in stage 6.0 (TID 534) in 91 ms on localhost (83/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 101.0 in stage 6.0 (TID 550, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15117
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15614
15/08/21 21:31:32 INFO Executor: Running task 101.0 in stage 6.0 (TID 550)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 84.0 in stage 6.0 (TID 533) in 94 ms on localhost (84/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 83.0 in stage 6.0 (TID 532) in 98 ms on localhost (85/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 82.0 in stage 6.0 (TID 531) in 110 ms on localhost (86/200)
15/08/21 21:31:32 INFO Executor: Finished task 88.0 in stage 6.0 (TID 537). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 14843
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15364
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15038
15/08/21 21:31:32 INFO Executor: Finished task 90.0 in stage 6.0 (TID 539). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 102.0 in stage 6.0 (TID 551, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14440
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 86.0 in stage 6.0 (TID 535) in 86 ms on localhost (87/200)
15/08/21 21:31:32 INFO Executor: Running task 102.0 in stage 6.0 (TID 551)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14991
15/08/21 21:31:32 INFO TaskSetManager: Starting task 103.0 in stage 6.0 (TID 552, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 103.0 in stage 6.0 (TID 552)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 104.0 in stage 6.0 (TID 553, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 104.0 in stage 6.0 (TID 553)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 105.0 in stage 6.0 (TID 554, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Running task 105.0 in stage 6.0 (TID 554)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 87.0 in stage 6.0 (TID 536) in 90 ms on localhost (88/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 91.0 in stage 6.0 (TID 540). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 89.0 in stage 6.0 (TID 538). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14677
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 92.0 in stage 6.0 (TID 541). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 106.0 in stage 6.0 (TID 555, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 94.0 in stage 6.0 (TID 543). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 88.0 in stage 6.0 (TID 537) in 94 ms on localhost (89/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 90.0 in stage 6.0 (TID 539) in 61 ms on localhost (90/200)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 107.0 in stage 6.0 (TID 556, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Running task 107.0 in stage 6.0 (TID 556)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 108.0 in stage 6.0 (TID 557, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 89.0 in stage 6.0 (TID 538) in 86 ms on localhost (91/200)
15/08/21 21:31:32 INFO Executor: Finished task 95.0 in stage 6.0 (TID 544). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 106.0 in stage 6.0 (TID 555)
15/08/21 21:31:32 INFO Executor: Running task 108.0 in stage 6.0 (TID 557)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 91.0 in stage 6.0 (TID 540) in 66 ms on localhost (92/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 92.0 in stage 6.0 (TID 541) in 65 ms on localhost (93/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 109.0 in stage 6.0 (TID 558, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 109.0 in stage 6.0 (TID 558)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 15404
15/08/21 21:31:32 INFO TaskSetManager: Starting task 110.0 in stage 6.0 (TID 559, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14687
15/08/21 21:31:32 INFO Executor: Running task 110.0 in stage 6.0 (TID 559)
15/08/21 21:31:32 INFO Executor: Finished task 93.0 in stage 6.0 (TID 542). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 111.0 in stage 6.0 (TID 560, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 95.0 in stage 6.0 (TID 544) in 59 ms on localhost (94/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 96.0 in stage 6.0 (TID 545). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 94.0 in stage 6.0 (TID 543) in 64 ms on localhost (95/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15009
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 100.0 in stage 6.0 (TID 549). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 97.0 in stage 6.0 (TID 546). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 21:31:32 INFO TaskSetManager: Starting task 112.0 in stage 6.0 (TID 561, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 111.0 in stage 6.0 (TID 560)
15/08/21 21:31:32 INFO Executor: Running task 112.0 in stage 6.0 (TID 561)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 113.0 in stage 6.0 (TID 562, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 21:31:32 INFO TaskSetManager: Finished task 93.0 in stage 6.0 (TID 542) in 78 ms on localhost (96/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15038
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15098
15/08/21 21:31:32 INFO Executor: Running task 113.0 in stage 6.0 (TID 562)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15500
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 101.0 in stage 6.0 (TID 550). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 99.0 in stage 6.0 (TID 548). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 96.0 in stage 6.0 (TID 545) in 71 ms on localhost (97/200)
15/08/21 21:31:32 INFO Executor: Finished task 98.0 in stage 6.0 (TID 547). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 104.0 in stage 6.0 (TID 553). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 114.0 in stage 6.0 (TID 563, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 100.0 in stage 6.0 (TID 549) in 66 ms on localhost (98/200)
15/08/21 21:31:32 INFO Executor: Running task 114.0 in stage 6.0 (TID 563)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 97.0 in stage 6.0 (TID 546) in 74 ms on localhost (99/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 115.0 in stage 6.0 (TID 564, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 115.0 in stage 6.0 (TID 564)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 21:31:32 INFO Executor: Finished task 105.0 in stage 6.0 (TID 554). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 101.0 in stage 6.0 (TID 550) in 68 ms on localhost (100/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15141
15/08/21 21:31:32 INFO Executor: Finished task 103.0 in stage 6.0 (TID 552). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 102.0 in stage 6.0 (TID 551). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 99.0 in stage 6.0 (TID 548) in 76 ms on localhost (101/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15566
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 116.0 in stage 6.0 (TID 565, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14597
15/08/21 21:31:32 INFO Executor: Running task 116.0 in stage 6.0 (TID 565)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 117.0 in stage 6.0 (TID 566, localhost, ANY, 1683 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 106.0 in stage 6.0 (TID 555). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14985
15/08/21 21:31:32 INFO Executor: Running task 117.0 in stage 6.0 (TID 566)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 118.0 in stage 6.0 (TID 567, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 98.0 in stage 6.0 (TID 547) in 93 ms on localhost (102/200)
15/08/21 21:31:32 INFO Executor: Running task 118.0 in stage 6.0 (TID 567)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14784
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 21:31:32 INFO Executor: Finished task 107.0 in stage 6.0 (TID 556). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 119.0 in stage 6.0 (TID 568, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 104.0 in stage 6.0 (TID 553) in 72 ms on localhost (103/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 105.0 in stage 6.0 (TID 554) in 72 ms on localhost (104/200)
15/08/21 21:31:32 INFO Executor: Finished task 108.0 in stage 6.0 (TID 557). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 120.0 in stage 6.0 (TID 569, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 103.0 in stage 6.0 (TID 552) in 76 ms on localhost (105/200)
15/08/21 21:31:32 INFO Executor: Running task 120.0 in stage 6.0 (TID 569)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 21:31:32 INFO TaskSetManager: Finished task 102.0 in stage 6.0 (TID 551) in 87 ms on localhost (106/200)
15/08/21 21:31:32 INFO Executor: Running task 119.0 in stage 6.0 (TID 568)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 121.0 in stage 6.0 (TID 570, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 121.0 in stage 6.0 (TID 570)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 122.0 in stage 6.0 (TID 571, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO Executor: Running task 122.0 in stage 6.0 (TID 571)
15/08/21 21:31:32 INFO Executor: Finished task 110.0 in stage 6.0 (TID 559). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 123.0 in stage 6.0 (TID 572, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 123.0 in stage 6.0 (TID 572)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14919
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15211
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 124.0 in stage 6.0 (TID 573, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 124.0 in stage 6.0 (TID 573)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 125.0 in stage 6.0 (TID 574, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 107.0 in stage 6.0 (TID 556) in 82 ms on localhost (107/200)
15/08/21 21:31:32 INFO Executor: Running task 125.0 in stage 6.0 (TID 574)
15/08/21 21:31:32 INFO Executor: Finished task 109.0 in stage 6.0 (TID 558). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 106.0 in stage 6.0 (TID 555) in 86 ms on localhost (108/200)
15/08/21 21:31:32 INFO Executor: Finished task 112.0 in stage 6.0 (TID 561). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 114.0 in stage 6.0 (TID 563). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 126.0 in stage 6.0 (TID 575, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 111.0 in stage 6.0 (TID 560). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 113.0 in stage 6.0 (TID 562). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 126.0 in stage 6.0 (TID 575)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 127.0 in stage 6.0 (TID 576, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 109.0 in stage 6.0 (TID 558) in 78 ms on localhost (109/200)
15/08/21 21:31:32 INFO Executor: Running task 127.0 in stage 6.0 (TID 576)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 110.0 in stage 6.0 (TID 559) in 78 ms on localhost (110/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15267
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Finished task 112.0 in stage 6.0 (TID 561) in 73 ms on localhost (111/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 108.0 in stage 6.0 (TID 557) in 90 ms on localhost (112/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 128.0 in stage 6.0 (TID 577, localhost, ANY, 1684 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14884
15/08/21 21:31:32 INFO TaskSetManager: Starting task 129.0 in stage 6.0 (TID 578, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Running task 129.0 in stage 6.0 (TID 578)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 130.0 in stage 6.0 (TID 579, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 130.0 in stage 6.0 (TID 579)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 114.0 in stage 6.0 (TID 563) in 60 ms on localhost (113/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 111.0 in stage 6.0 (TID 560) in 80 ms on localhost (114/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 113.0 in stage 6.0 (TID 562) in 73 ms on localhost (115/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15255
15/08/21 21:31:32 INFO Executor: Running task 128.0 in stage 6.0 (TID 577)
15/08/21 21:31:32 INFO Executor: Finished task 116.0 in stage 6.0 (TID 565). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 131.0 in stage 6.0 (TID 580, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 115.0 in stage 6.0 (TID 564). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 131.0 in stage 6.0 (TID 580)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 132.0 in stage 6.0 (TID 581, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 116.0 in stage 6.0 (TID 565) in 57 ms on localhost (116/200)
15/08/21 21:31:32 INFO Executor: Running task 132.0 in stage 6.0 (TID 581)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15557
15/08/21 21:31:32 INFO Executor: Finished task 117.0 in stage 6.0 (TID 566). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14736
15/08/21 21:31:32 INFO TaskSetManager: Finished task 115.0 in stage 6.0 (TID 564) in 63 ms on localhost (117/200)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 117.0 in stage 6.0 (TID 566) in 57 ms on localhost (118/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15191
15/08/21 21:31:32 INFO TaskSetManager: Starting task 133.0 in stage 6.0 (TID 582, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 133.0 in stage 6.0 (TID 582)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15432
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 119.0 in stage 6.0 (TID 568). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14942
15/08/21 21:31:32 INFO TaskSetManager: Starting task 134.0 in stage 6.0 (TID 583, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 134.0 in stage 6.0 (TID 583)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15272
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 119.0 in stage 6.0 (TID 568) in 62 ms on localhost (119/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14825
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15183
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 118.0 in stage 6.0 (TID 567). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 135.0 in stage 6.0 (TID 584, localhost, ANY, 1683 bytes)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 121.0 in stage 6.0 (TID 570). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 21:31:32 INFO Executor: Running task 135.0 in stage 6.0 (TID 584)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 118.0 in stage 6.0 (TID 567) in 80 ms on localhost (120/200)
15/08/21 21:31:32 INFO Executor: Finished task 127.0 in stage 6.0 (TID 576). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15150
15/08/21 21:31:32 INFO TaskSetManager: Finished task 121.0 in stage 6.0 (TID 570) in 68 ms on localhost (121/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 126.0 in stage 6.0 (TID 575). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15359
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15532
15/08/21 21:31:32 INFO Executor: Finished task 122.0 in stage 6.0 (TID 571). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 136.0 in stage 6.0 (TID 585, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 125.0 in stage 6.0 (TID 574). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 136.0 in stage 6.0 (TID 585)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 120.0 in stage 6.0 (TID 569). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 14550
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15302
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15609
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 137.0 in stage 6.0 (TID 586, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO Executor: Running task 137.0 in stage 6.0 (TID 586)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 138.0 in stage 6.0 (TID 587, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 138.0 in stage 6.0 (TID 587)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 127.0 in stage 6.0 (TID 576) in 64 ms on localhost (122/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 139.0 in stage 6.0 (TID 588, localhost, ANY, 1688 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 123.0 in stage 6.0 (TID 572). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 126.0 in stage 6.0 (TID 575) in 75 ms on localhost (123/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 122.0 in stage 6.0 (TID 571) in 86 ms on localhost (124/200)
15/08/21 21:31:32 INFO Executor: Running task 139.0 in stage 6.0 (TID 588)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 140.0 in stage 6.0 (TID 589, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 140.0 in stage 6.0 (TID 589)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 141.0 in stage 6.0 (TID 590, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 141.0 in stage 6.0 (TID 590)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 142.0 in stage 6.0 (TID 591, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 142.0 in stage 6.0 (TID 591)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14814
15/08/21 21:31:32 INFO Executor: Finished task 128.0 in stage 6.0 (TID 577). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14951
15/08/21 21:31:32 INFO Executor: Finished task 134.0 in stage 6.0 (TID 583). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 124.0 in stage 6.0 (TID 573). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 130.0 in stage 6.0 (TID 579). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14836
15/08/21 21:31:32 INFO TaskSetManager: Finished task 125.0 in stage 6.0 (TID 574) in 90 ms on localhost (125/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 133.0 in stage 6.0 (TID 582). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO Executor: Finished task 132.0 in stage 6.0 (TID 581). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 120.0 in stage 6.0 (TID 569) in 113 ms on localhost (126/200)
15/08/21 21:31:32 INFO Executor: Finished task 129.0 in stage 6.0 (TID 578). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 131.0 in stage 6.0 (TID 580). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 21:31:32 INFO TaskSetManager: Starting task 143.0 in stage 6.0 (TID 592, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 128.0 in stage 6.0 (TID 577) in 91 ms on localhost (127/200)
15/08/21 21:31:32 INFO Executor: Running task 143.0 in stage 6.0 (TID 592)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15358
15/08/21 21:31:32 INFO TaskSetManager: Finished task 123.0 in stage 6.0 (TID 572) in 111 ms on localhost (128/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14802
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 14788
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 15018
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 144.0 in stage 6.0 (TID 593, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14913
15/08/21 21:31:32 INFO Executor: Running task 144.0 in stage 6.0 (TID 593)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 134.0 in stage 6.0 (TID 583) in 82 ms on localhost (129/200)
15/08/21 21:31:32 INFO Executor: Finished task 135.0 in stage 6.0 (TID 584). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 145.0 in stage 6.0 (TID 594, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 145.0 in stage 6.0 (TID 594)
15/08/21 21:31:32 INFO Executor: Finished task 137.0 in stage 6.0 (TID 586). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Starting task 146.0 in stage 6.0 (TID 595, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14564
15/08/21 21:31:32 INFO Executor: Running task 146.0 in stage 6.0 (TID 595)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 130.0 in stage 6.0 (TID 579) in 119 ms on localhost (130/200)
15/08/21 21:31:32 INFO Executor: Finished task 136.0 in stage 6.0 (TID 585). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 142.0 in stage 6.0 (TID 591). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Finished task 133.0 in stage 6.0 (TID 582) in 107 ms on localhost (131/200)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 147.0 in stage 6.0 (TID 596, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Running task 147.0 in stage 6.0 (TID 596)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 148.0 in stage 6.0 (TID 597, localhost, ANY, 1685 bytes)
15/08/21 21:31:32 INFO Executor: Running task 148.0 in stage 6.0 (TID 597)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 138.0 in stage 6.0 (TID 587). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO Executor: Finished task 141.0 in stage 6.0 (TID 590). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO TaskSetManager: Starting task 149.0 in stage 6.0 (TID 598, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Finished task 140.0 in stage 6.0 (TID 589). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15079
15/08/21 21:31:32 INFO Executor: Finished task 139.0 in stage 6.0 (TID 588). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 149.0 in stage 6.0 (TID 598)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 150.0 in stage 6.0 (TID 599, localhost, ANY, 1686 bytes)
15/08/21 21:31:32 INFO Executor: Running task 150.0 in stage 6.0 (TID 599)
15/08/21 21:31:32 INFO TaskSetManager: Starting task 151.0 in stage 6.0 (TID 600, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Running task 151.0 in stage 6.0 (TID 600)
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 21:31:32 INFO TaskSetManager: Starting task 152.0 in stage 6.0 (TID 601, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO Executor: Running task 152.0 in stage 6.0 (TID 601)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14508
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15107
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14923
15/08/21 21:31:32 INFO Executor: Finished task 143.0 in stage 6.0 (TID 592). 1925 bytes result sent to driver
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 153.0 in stage 6.0 (TID 602, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 124.0 in stage 6.0 (TID 573) in 165 ms on localhost (132/200)
15/08/21 21:31:32 INFO Executor: Running task 153.0 in stage 6.0 (TID 602)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Finished task 129.0 in stage 6.0 (TID 578) in 145 ms on localhost (133/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 132.0 in stage 6.0 (TID 581) in 138 ms on localhost (134/200)
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 21:31:32 INFO TaskSetManager: Finished task 135.0 in stage 6.0 (TID 584) in 115 ms on localhost (135/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 131.0 in stage 6.0 (TID 580) in 143 ms on localhost (136/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO TaskSetManager: Starting task 154.0 in stage 6.0 (TID 603, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14560
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO TaskSetManager: Finished task 142.0 in stage 6.0 (TID 591) in 87 ms on localhost (137/200)
15/08/21 21:31:32 INFO Executor: Running task 154.0 in stage 6.0 (TID 603)
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14848
15/08/21 21:31:32 INFO TaskSetManager: Finished task 137.0 in stage 6.0 (TID 586) in 110 ms on localhost (138/200)
15/08/21 21:31:32 INFO TaskSetManager: Finished task 136.0 in stage 6.0 (TID 585) in 115 ms on localhost (139/200)
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14829
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 21:31:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14569
15/08/21 21:31:32 INFO TaskSetManager: Finished task 138.0 in stage 6.0 (TID 587) in 116 ms on localhost (140/200)
15/08/21 21:31:32 INFO Executor: Finished task 146.0 in stage 6.0 (TID 595). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 155.0 in stage 6.0 (TID 604, localhost, ANY, 1687 bytes)
15/08/21 21:31:32 INFO Executor: Finished task 145.0 in stage 6.0 (TID 594). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Finished task 147.0 in stage 6.0 (TID 596). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO Executor: Running task 155.0 in stage 6.0 (TID 604)
15/08/21 21:31:32 INFO Executor: Finished task 144.0 in stage 6.0 (TID 593). 1925 bytes result sent to driver
15/08/21 21:31:32 INFO TaskSetManager: Starting task 156.0 in stage 6.0 (TID 605, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO Executor: Running task 156.0 in stage 6.0 (TID 605)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 141.0 in stage 6.0 (TID 590) in 110 ms on localhost (141/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 140.0 in stage 6.0 (TID 589) in 113 ms on localhost (142/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 157.0 in stage 6.0 (TID 606, localhost, ANY, 1686 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14838
15/08/21 21:31:33 INFO Executor: Running task 157.0 in stage 6.0 (TID 606)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 14878
15/08/21 21:31:33 INFO Executor: Finished task 149.0 in stage 6.0 (TID 598). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 158.0 in stage 6.0 (TID 607, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 151.0 in stage 6.0 (TID 600). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 159.0 in stage 6.0 (TID 608, localhost, ANY, 1684 bytes)
15/08/21 21:31:33 INFO Executor: Running task 158.0 in stage 6.0 (TID 607)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 21:31:33 INFO Executor: Finished task 148.0 in stage 6.0 (TID 597). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 139.0 in stage 6.0 (TID 588) in 126 ms on localhost (143/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 21:31:33 INFO Executor: Running task 159.0 in stage 6.0 (TID 608)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 143.0 in stage 6.0 (TID 592) in 105 ms on localhost (144/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 160.0 in stage 6.0 (TID 609, localhost, ANY, 1688 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 160.0 in stage 6.0 (TID 609)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15001
15/08/21 21:31:33 INFO Executor: Finished task 150.0 in stage 6.0 (TID 599). 1925 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 152.0 in stage 6.0 (TID 601). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 146.0 in stage 6.0 (TID 595) in 78 ms on localhost (145/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 145.0 in stage 6.0 (TID 594) in 83 ms on localhost (146/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 161.0 in stage 6.0 (TID 610, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO Executor: Running task 161.0 in stage 6.0 (TID 610)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15586
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14793
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15203
15/08/21 21:31:33 INFO TaskSetManager: Starting task 162.0 in stage 6.0 (TID 611, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 147.0 in stage 6.0 (TID 596) in 85 ms on localhost (147/200)
15/08/21 21:31:33 INFO Executor: Running task 162.0 in stage 6.0 (TID 611)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 153.0 in stage 6.0 (TID 602). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 163.0 in stage 6.0 (TID 612, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15029
15/08/21 21:31:33 INFO Executor: Running task 163.0 in stage 6.0 (TID 612)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 164.0 in stage 6.0 (TID 613, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO Executor: Running task 164.0 in stage 6.0 (TID 613)
15/08/21 21:31:33 INFO Executor: Finished task 154.0 in stage 6.0 (TID 603). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 155.0 in stage 6.0 (TID 604). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 165.0 in stage 6.0 (TID 614, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO Executor: Running task 165.0 in stage 6.0 (TID 614)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 144.0 in stage 6.0 (TID 593) in 110 ms on localhost (148/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 166.0 in stage 6.0 (TID 615, localhost, ANY, 1688 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 149.0 in stage 6.0 (TID 598) in 87 ms on localhost (149/200)
15/08/21 21:31:33 INFO Executor: Running task 166.0 in stage 6.0 (TID 615)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 151.0 in stage 6.0 (TID 600) in 80 ms on localhost (150/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 156.0 in stage 6.0 (TID 605). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 148.0 in stage 6.0 (TID 597) in 99 ms on localhost (151/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14816
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 167.0 in stage 6.0 (TID 616, localhost, ANY, 1683 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 168.0 in stage 6.0 (TID 617, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO Executor: Running task 167.0 in stage 6.0 (TID 616)
15/08/21 21:31:33 INFO Executor: Running task 168.0 in stage 6.0 (TID 617)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15383
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14934
15/08/21 21:31:33 INFO TaskSetManager: Finished task 152.0 in stage 6.0 (TID 601) in 93 ms on localhost (152/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15058
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 158.0 in stage 6.0 (TID 607). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 150.0 in stage 6.0 (TID 599) in 99 ms on localhost (153/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 21:31:33 INFO Executor: Finished task 159.0 in stage 6.0 (TID 608). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 160.0 in stage 6.0 (TID 609). 1925 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 169.0 in stage 6.0 (TID 618, localhost, ANY, 1686 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14876
15/08/21 21:31:33 INFO Executor: Running task 169.0 in stage 6.0 (TID 618)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 170.0 in stage 6.0 (TID 619, localhost, ANY, 1688 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 171.0 in stage 6.0 (TID 620, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 171.0 in stage 6.0 (TID 620)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 172.0 in stage 6.0 (TID 621, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 21:31:33 INFO Executor: Running task 170.0 in stage 6.0 (TID 619)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 157.0 in stage 6.0 (TID 606). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 153.0 in stage 6.0 (TID 602) in 109 ms on localhost (154/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15474
15/08/21 21:31:33 INFO TaskSetManager: Finished task 154.0 in stage 6.0 (TID 603) in 97 ms on localhost (155/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 172.0 in stage 6.0 (TID 621)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 155.0 in stage 6.0 (TID 604) in 78 ms on localhost (156/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 156.0 in stage 6.0 (TID 605) in 77 ms on localhost (157/200)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 173.0 in stage 6.0 (TID 622, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 173.0 in stage 6.0 (TID 622)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15679
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 158.0 in stage 6.0 (TID 607) in 76 ms on localhost (158/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 161.0 in stage 6.0 (TID 610). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 174.0 in stage 6.0 (TID 623, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 21:31:33 INFO TaskSetManager: Finished task 159.0 in stage 6.0 (TID 608) in 83 ms on localhost (159/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15282
15/08/21 21:31:33 INFO TaskSetManager: Starting task 175.0 in stage 6.0 (TID 624, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 164.0 in stage 6.0 (TID 613). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 174.0 in stage 6.0 (TID 623)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15055
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 176.0 in stage 6.0 (TID 625, localhost, ANY, 1684 bytes)
15/08/21 21:31:33 INFO Executor: Running task 176.0 in stage 6.0 (TID 625)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 177.0 in stage 6.0 (TID 626, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15096
15/08/21 21:31:33 INFO Executor: Running task 177.0 in stage 6.0 (TID 626)
15/08/21 21:31:33 INFO Executor: Finished task 166.0 in stage 6.0 (TID 615). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 162.0 in stage 6.0 (TID 611). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 165.0 in stage 6.0 (TID 614). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14883
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14819
15/08/21 21:31:33 INFO TaskSetManager: Finished task 160.0 in stage 6.0 (TID 609) in 82 ms on localhost (160/200)
15/08/21 21:31:33 INFO Executor: Running task 175.0 in stage 6.0 (TID 624)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 168.0 in stage 6.0 (TID 617). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 157.0 in stage 6.0 (TID 606) in 100 ms on localhost (161/200)
15/08/21 21:31:33 INFO Executor: Finished task 163.0 in stage 6.0 (TID 612). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 178.0 in stage 6.0 (TID 627, localhost, ANY, 1686 bytes)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 178.0 in stage 6.0 (TID 627)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 161.0 in stage 6.0 (TID 610) in 86 ms on localhost (162/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 179.0 in stage 6.0 (TID 628, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14879
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15549
15/08/21 21:31:33 INFO Executor: Running task 179.0 in stage 6.0 (TID 628)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 164.0 in stage 6.0 (TID 613) in 83 ms on localhost (163/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14751
15/08/21 21:31:33 INFO TaskSetManager: Finished task 166.0 in stage 6.0 (TID 615) in 80 ms on localhost (164/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 162.0 in stage 6.0 (TID 611) in 94 ms on localhost (165/200)
15/08/21 21:31:33 INFO Executor: Finished task 171.0 in stage 6.0 (TID 620). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 180.0 in stage 6.0 (TID 629, localhost, ANY, 1683 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 170.0 in stage 6.0 (TID 619). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 167.0 in stage 6.0 (TID 616). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 169.0 in stage 6.0 (TID 618). 1925 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 180.0 in stage 6.0 (TID 629)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 181.0 in stage 6.0 (TID 630, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 173.0 in stage 6.0 (TID 622). 1925 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15238
15/08/21 21:31:33 INFO TaskSetManager: Starting task 182.0 in stage 6.0 (TID 631, localhost, ANY, 1686 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 165.0 in stage 6.0 (TID 614) in 98 ms on localhost (166/200)
15/08/21 21:31:33 INFO Executor: Running task 182.0 in stage 6.0 (TID 631)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 21:31:33 INFO Executor: Running task 181.0 in stage 6.0 (TID 630)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 168.0 in stage 6.0 (TID 617) in 91 ms on localhost (167/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15658
15/08/21 21:31:33 INFO TaskSetManager: Starting task 183.0 in stage 6.0 (TID 632, localhost, ANY, 1686 bytes)
15/08/21 21:31:33 INFO Executor: Running task 183.0 in stage 6.0 (TID 632)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 184.0 in stage 6.0 (TID 633, localhost, ANY, 1686 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 177.0 in stage 6.0 (TID 626). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 175.0 in stage 6.0 (TID 624). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 172.0 in stage 6.0 (TID 621). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14992
15/08/21 21:31:33 INFO TaskSetManager: Finished task 163.0 in stage 6.0 (TID 612) in 110 ms on localhost (168/200)
15/08/21 21:31:33 INFO Executor: Running task 184.0 in stage 6.0 (TID 633)
15/08/21 21:31:33 INFO Executor: Finished task 174.0 in stage 6.0 (TID 623). 1925 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 185.0 in stage 6.0 (TID 634, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO Executor: Running task 185.0 in stage 6.0 (TID 634)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 170.0 in stage 6.0 (TID 619) in 89 ms on localhost (169/200)
15/08/21 21:31:33 INFO Executor: Finished task 178.0 in stage 6.0 (TID 627). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 171.0 in stage 6.0 (TID 620) in 88 ms on localhost (170/200)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 186.0 in stage 6.0 (TID 635, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Running task 186.0 in stage 6.0 (TID 635)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 187.0 in stage 6.0 (TID 636, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14938
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 187.0 in stage 6.0 (TID 636)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 188.0 in stage 6.0 (TID 637, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 188.0 in stage 6.0 (TID 637)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14891
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14896
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 167.0 in stage 6.0 (TID 616) in 116 ms on localhost (171/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14784
15/08/21 21:31:33 INFO Executor: Finished task 180.0 in stage 6.0 (TID 629). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 176.0 in stage 6.0 (TID 625). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15235
15/08/21 21:31:33 INFO TaskSetManager: Finished task 169.0 in stage 6.0 (TID 618) in 112 ms on localhost (172/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 189.0 in stage 6.0 (TID 638, localhost, ANY, 1683 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 177.0 in stage 6.0 (TID 626) in 80 ms on localhost (173/200)
15/08/21 21:31:33 INFO Executor: Running task 189.0 in stage 6.0 (TID 638)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14379
15/08/21 21:31:33 INFO TaskSetManager: Finished task 173.0 in stage 6.0 (TID 622) in 99 ms on localhost (174/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 190.0 in stage 6.0 (TID 639, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO Executor: Running task 190.0 in stage 6.0 (TID 639)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15330
15/08/21 21:31:33 INFO TaskSetManager: Finished task 175.0 in stage 6.0 (TID 624) in 89 ms on localhost (175/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 172.0 in stage 6.0 (TID 621) in 113 ms on localhost (176/200)
15/08/21 21:31:33 INFO Executor: Finished task 179.0 in stage 6.0 (TID 628). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 191.0 in stage 6.0 (TID 640, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO Executor: Running task 191.0 in stage 6.0 (TID 640)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 192.0 in stage 6.0 (TID 641, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 192.0 in stage 6.0 (TID 641)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 21:31:33 INFO TaskSetManager: Finished task 174.0 in stage 6.0 (TID 623) in 97 ms on localhost (177/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 178.0 in stage 6.0 (TID 627) in 89 ms on localhost (178/200)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 193.0 in stage 6.0 (TID 642, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 193.0 in stage 6.0 (TID 642)
15/08/21 21:31:33 INFO Executor: Finished task 181.0 in stage 6.0 (TID 630). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15101
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 194.0 in stage 6.0 (TID 643, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 183.0 in stage 6.0 (TID 632). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 194.0 in stage 6.0 (TID 643)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14845
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 21:31:33 INFO TaskSetManager: Starting task 195.0 in stage 6.0 (TID 644, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 180.0 in stage 6.0 (TID 629) in 79 ms on localhost (179/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 176.0 in stage 6.0 (TID 625) in 109 ms on localhost (180/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15010
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15463
15/08/21 21:31:33 INFO Executor: Running task 195.0 in stage 6.0 (TID 644)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 196.0 in stage 6.0 (TID 645, localhost, ANY, 1688 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 185.0 in stage 6.0 (TID 634). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 186.0 in stage 6.0 (TID 635). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 179.0 in stage 6.0 (TID 628) in 91 ms on localhost (181/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 21:31:33 INFO Executor: Running task 196.0 in stage 6.0 (TID 645)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 197.0 in stage 6.0 (TID 646, localhost, ANY, 1684 bytes)
15/08/21 21:31:33 INFO Executor: Running task 197.0 in stage 6.0 (TID 646)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 198.0 in stage 6.0 (TID 647, localhost, ANY, 1685 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 182.0 in stage 6.0 (TID 631). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 198.0 in stage 6.0 (TID 647)
15/08/21 21:31:33 INFO Executor: Finished task 184.0 in stage 6.0 (TID 633). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 187.0 in stage 6.0 (TID 636). 1925 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 199.0 in stage 6.0 (TID 648, localhost, ANY, 1687 bytes)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 181.0 in stage 6.0 (TID 630) in 87 ms on localhost (182/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 185.0 in stage 6.0 (TID 634) in 62 ms on localhost (183/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 199.0 in stage 6.0 (TID 648)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15103
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15039
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 183.0 in stage 6.0 (TID 632) in 71 ms on localhost (184/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15164
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15272
15/08/21 21:31:33 INFO TaskSetManager: Finished task 182.0 in stage 6.0 (TID 631) in 92 ms on localhost (185/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 187.0 in stage 6.0 (TID 636) in 68 ms on localhost (186/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 186.0 in stage 6.0 (TID 635) in 73 ms on localhost (187/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 184.0 in stage 6.0 (TID 633) in 83 ms on localhost (188/200)
15/08/21 21:31:33 INFO Executor: Finished task 190.0 in stage 6.0 (TID 639). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 191.0 in stage 6.0 (TID 640). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14772
15/08/21 21:31:33 INFO TaskSetManager: Finished task 190.0 in stage 6.0 (TID 639) in 56 ms on localhost (189/200)
15/08/21 21:31:33 INFO Executor: Finished task 188.0 in stage 6.0 (TID 637). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 191.0 in stage 6.0 (TID 640) in 64 ms on localhost (190/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15137
15/08/21 21:31:33 INFO TaskSetManager: Finished task 188.0 in stage 6.0 (TID 637) in 87 ms on localhost (191/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15726
15/08/21 21:31:33 INFO Executor: Finished task 189.0 in stage 6.0 (TID 638). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 189.0 in stage 6.0 (TID 638) in 78 ms on localhost (192/200)
15/08/21 21:31:33 INFO Executor: Finished task 194.0 in stage 6.0 (TID 643). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 198.0 in stage 6.0 (TID 647). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 193.0 in stage 6.0 (TID 642). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 195.0 in stage 6.0 (TID 644). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 192.0 in stage 6.0 (TID 641). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 196.0 in stage 6.0 (TID 645). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 194.0 in stage 6.0 (TID 643) in 76 ms on localhost (193/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 195.0 in stage 6.0 (TID 644) in 73 ms on localhost (194/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 198.0 in stage 6.0 (TID 647) in 64 ms on localhost (195/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 193.0 in stage 6.0 (TID 642) in 79 ms on localhost (196/200)
15/08/21 21:31:33 INFO Executor: Finished task 199.0 in stage 6.0 (TID 648). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 196.0 in stage 6.0 (TID 645) in 70 ms on localhost (197/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 192.0 in stage 6.0 (TID 641) in 88 ms on localhost (198/200)
15/08/21 21:31:33 INFO Executor: Finished task 197.0 in stage 6.0 (TID 646). 1925 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 199.0 in stage 6.0 (TID 648) in 68 ms on localhost (199/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 197.0 in stage 6.0 (TID 646) in 72 ms on localhost (200/200)
15/08/21 21:31:33 INFO DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:423) finished in 1.139 s
15/08/21 21:31:33 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/21 21:31:33 INFO DAGScheduler: looking for newly runnable stages
15/08/21 21:31:33 INFO DAGScheduler: running: Set()
15/08/21 21:31:33 INFO DAGScheduler: waiting: Set(ResultStage 7)
15/08/21 21:31:33 INFO DAGScheduler: failed: Set()
15/08/21 21:31:33 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7ebfa6cf
15/08/21 21:31:33 INFO StatsReportListener: task runtime:(count: 200, mean: 100.645000, stdev: 31.166632, max: 224.000000, min: 56.000000)
15/08/21 21:31:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:33 INFO StatsReportListener: 	56.0 ms	64.0 ms	68.0 ms	80.0 ms	93.0 ms	112.0 ms	139.0 ms	165.0 ms	224.0 ms
15/08/21 21:31:33 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 31.000000, stdev: 0.000000, max: 31.000000, min: 31.000000)
15/08/21 21:31:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:33 INFO StatsReportListener: 	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B
15/08/21 21:31:33 INFO StatsReportListener: task result size:(count: 200, mean: 1925.000000, stdev: 0.000000, max: 1925.000000, min: 1925.000000)
15/08/21 21:31:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:33 INFO StatsReportListener: 	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B
15/08/21 21:31:33 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 56.793495, stdev: 10.221432, max: 88.888889, min: 32.608696)
15/08/21 21:31:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:33 INFO StatsReportListener: 	33 %	43 %	45 %	48 %	56 %	63 %	72 %	77 %	89 %
15/08/21 21:31:33 INFO StatsReportListener: other time pct: (count: 200, mean: 43.206505, stdev: 10.221432, max: 67.391304, min: 11.111111)
15/08/21 21:31:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:33 INFO StatsReportListener: 	11 %	24 %	29 %	37 %	44 %	52 %	55 %	57 %	67 %
15/08/21 21:31:33 INFO DAGScheduler: Missing parents for ResultStage 7: List()
15/08/21 21:31:33 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[43] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 21:31:33 INFO MemoryStore: ensureFreeSpace(12056) called with curMem=1878007, maxMem=22226833244
15/08/21 21:31:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 11.8 KB, free 20.7 GB)
15/08/21 21:31:33 INFO MemoryStore: ensureFreeSpace(5719) called with curMem=1890063, maxMem=22226833244
15/08/21 21:31:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.6 KB, free 20.7 GB)
15/08/21 21:31:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:52592 (size: 5.6 KB, free: 20.7 GB)
15/08/21 21:31:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/21 21:31:33 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 7 (MapPartitionsRDD[43] at processCmd at CliDriver.java:423)
15/08/21 21:31:33 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/21 21:31:33 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 649, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 650, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 651, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 652, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 653, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 654, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 655, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 656, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 657, localhost, ANY, 1983 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 658, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 659, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 660, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 661, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 662, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 663, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 664, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 0.0 in stage 7.0 (TID 649)
15/08/21 21:31:33 INFO Executor: Running task 6.0 in stage 7.0 (TID 655)
15/08/21 21:31:33 INFO Executor: Running task 7.0 in stage 7.0 (TID 656)
15/08/21 21:31:33 INFO Executor: Running task 13.0 in stage 7.0 (TID 662)
15/08/21 21:31:33 INFO Executor: Running task 5.0 in stage 7.0 (TID 654)
15/08/21 21:31:33 INFO Executor: Running task 10.0 in stage 7.0 (TID 659)
15/08/21 21:31:33 INFO Executor: Running task 3.0 in stage 7.0 (TID 652)
15/08/21 21:31:33 INFO Executor: Running task 1.0 in stage 7.0 (TID 650)
15/08/21 21:31:33 INFO Executor: Running task 2.0 in stage 7.0 (TID 651)
15/08/21 21:31:33 INFO Executor: Running task 12.0 in stage 7.0 (TID 661)
15/08/21 21:31:33 INFO Executor: Running task 15.0 in stage 7.0 (TID 664)
15/08/21 21:31:33 INFO Executor: Running task 11.0 in stage 7.0 (TID 660)
15/08/21 21:31:33 INFO Executor: Running task 8.0 in stage 7.0 (TID 657)
15/08/21 21:31:33 INFO Executor: Running task 9.0 in stage 7.0 (TID 658)
15/08/21 21:31:33 INFO Executor: Running task 4.0 in stage 7.0 (TID 653)
15/08/21 21:31:33 INFO Executor: Running task 14.0 in stage 7.0 (TID 663)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15062
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15046
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14731
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15104
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15142
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15545
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15524
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14824
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15570
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15106
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15007
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15049
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15039
15/08/21 21:31:33 INFO Executor: Finished task 8.0 in stage 7.0 (TID 657). 3233 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 649). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 2.0 in stage 7.0 (TID 651). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 665, localhost, ANY, 1979 bytes)
15/08/21 21:31:33 INFO Executor: Running task 16.0 in stage 7.0 (TID 665)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 666, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 13.0 in stage 7.0 (TID 662). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 17.0 in stage 7.0 (TID 666)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 667, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Running task 18.0 in stage 7.0 (TID 667)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 651) in 197 ms on localhost (1/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 649) in 203 ms on localhost (2/200)
15/08/21 21:31:33 INFO Executor: Finished task 1.0 in stage 7.0 (TID 650). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 11.0 in stage 7.0 (TID 660). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 668, localhost, ANY, 1983 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 14.0 in stage 7.0 (TID 663). 3228 bytes result sent to driver
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO Executor: Finished task 7.0 in stage 7.0 (TID 656). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 19.0 in stage 7.0 (TID 668)
15/08/21 21:31:33 INFO Executor: Finished task 5.0 in stage 7.0 (TID 654). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 3.0 in stage 7.0 (TID 652). 3229 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 15.0 in stage 7.0 (TID 664). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 662) in 193 ms on localhost (3/200)
15/08/21 21:31:33 INFO Executor: Finished task 4.0 in stage 7.0 (TID 653). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 6.0 in stage 7.0 (TID 655). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 669, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 20.0 in stage 7.0 (TID 669)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 670, localhost, ANY, 1979 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 9.0 in stage 7.0 (TID 658). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 21.0 in stage 7.0 (TID 670)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 671, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 10.0 in stage 7.0 (TID 659). 3227 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 12.0 in stage 7.0 (TID 661). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 672, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Running task 23.0 in stage 7.0 (TID 672)
15/08/21 21:31:33 INFO Executor: Running task 22.0 in stage 7.0 (TID 671)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 673, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 674, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 650) in 211 ms on localhost (4/200)
15/08/21 21:31:33 INFO Executor: Running task 24.0 in stage 7.0 (TID 673)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 657) in 209 ms on localhost (5/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 654) in 212 ms on localhost (6/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 656) in 212 ms on localhost (7/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:33 INFO Executor: Running task 25.0 in stage 7.0 (TID 674)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 663) in 207 ms on localhost (8/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 652) in 220 ms on localhost (9/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 664) in 213 ms on localhost (10/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 675, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 26.0 in stage 7.0 (TID 675)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 676, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO Executor: Running task 27.0 in stage 7.0 (TID 676)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 677, localhost, ANY, 1979 bytes)
15/08/21 21:31:33 INFO Executor: Running task 28.0 in stage 7.0 (TID 677)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 678, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Running task 29.0 in stage 7.0 (TID 678)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 653) in 233 ms on localhost (11/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 655) in 233 ms on localhost (12/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 660) in 234 ms on localhost (13/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 658) in 238 ms on localhost (14/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15625
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15120
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 679, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Running task 30.0 in stage 7.0 (TID 679)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14755
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14693
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 680, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 21:31:33 INFO Executor: Running task 31.0 in stage 7.0 (TID 680)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 659) in 258 ms on localhost (15/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15313
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15470
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14906
15/08/21 21:31:33 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 661) in 262 ms on localhost (16/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15147
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 17.0 in stage 7.0 (TID 666). 3230 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 681, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 32.0 in stage 7.0 (TID 681)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 666) in 87 ms on localhost (17/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 21:31:33 INFO Executor: Finished task 16.0 in stage 7.0 (TID 665). 3229 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 682, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 33.0 in stage 7.0 (TID 682)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15114
15/08/21 21:31:33 INFO Executor: Finished task 22.0 in stage 7.0 (TID 671). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 683, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14831
15/08/21 21:31:33 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 665) in 98 ms on localhost (18/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Running task 34.0 in stage 7.0 (TID 683)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14612
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 671) in 89 ms on localhost (19/200)
15/08/21 21:31:33 INFO Executor: Finished task 20.0 in stage 7.0 (TID 669). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 684, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 19.0 in stage 7.0 (TID 668). 3233 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 21.0 in stage 7.0 (TID 670). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 35.0 in stage 7.0 (TID 684)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 685, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO Executor: Running task 36.0 in stage 7.0 (TID 685)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 686, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 37.0 in stage 7.0 (TID 686)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 669) in 97 ms on localhost (20/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 670) in 98 ms on localhost (21/200)
15/08/21 21:31:33 INFO Executor: Finished task 18.0 in stage 7.0 (TID 667). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 687, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 668) in 104 ms on localhost (22/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 24.0 in stage 7.0 (TID 673). 3228 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15344
15/08/21 21:31:33 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 688, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14776
15/08/21 21:31:33 INFO Executor: Running task 39.0 in stage 7.0 (TID 688)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 667) in 114 ms on localhost (23/200)
15/08/21 21:31:33 INFO Executor: Running task 38.0 in stage 7.0 (TID 687)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 673) in 105 ms on localhost (24/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15129
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO Executor: Finished task 26.0 in stage 7.0 (TID 675). 3229 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 689, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO Executor: Running task 40.0 in stage 7.0 (TID 689)
15/08/21 21:31:33 INFO Executor: Finished task 25.0 in stage 7.0 (TID 674). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 23.0 in stage 7.0 (TID 672). 3226 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 690, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO Executor: Running task 41.0 in stage 7.0 (TID 690)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 675) in 99 ms on localhost (25/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 691, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 674) in 118 ms on localhost (26/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 672) in 121 ms on localhost (27/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Running task 42.0 in stage 7.0 (TID 691)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15063
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 29.0 in stage 7.0 (TID 678). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 692, localhost, ANY, 1979 bytes)
15/08/21 21:31:33 INFO Executor: Running task 43.0 in stage 7.0 (TID 692)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14952
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 678) in 105 ms on localhost (28/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 30.0 in stage 7.0 (TID 679). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 693, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO Executor: Running task 44.0 in stage 7.0 (TID 693)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 28.0 in stage 7.0 (TID 677). 3229 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14809
15/08/21 21:31:33 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 679) in 96 ms on localhost (29/200)
15/08/21 21:31:33 INFO Executor: Finished task 27.0 in stage 7.0 (TID 676). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 694, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 45.0 in stage 7.0 (TID 694)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 695, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Running task 46.0 in stage 7.0 (TID 695)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 677) in 124 ms on localhost (30/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 21:31:33 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 676) in 126 ms on localhost (31/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15571
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO Executor: Finished task 31.0 in stage 7.0 (TID 680). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 696, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 47.0 in stage 7.0 (TID 696)
15/08/21 21:31:33 INFO Executor: Finished task 33.0 in stage 7.0 (TID 682). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15383
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 697, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 48.0 in stage 7.0 (TID 697)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 680) in 108 ms on localhost (32/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15393
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO Executor: Finished task 35.0 in stage 7.0 (TID 684). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO Executor: Finished task 34.0 in stage 7.0 (TID 683). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14750
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 39.0 in stage 7.0 (TID 688). 3229 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 682) in 97 ms on localhost (33/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15099
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 698, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 684) in 101 ms on localhost (34/200)
15/08/21 21:31:33 INFO Executor: Running task 49.0 in stage 7.0 (TID 698)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 683) in 110 ms on localhost (35/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 699, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 50.0 in stage 7.0 (TID 699)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 700, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 51.0 in stage 7.0 (TID 700)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 688) in 99 ms on localhost (36/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 36.0 in stage 7.0 (TID 685). 3227 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 701, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 52.0 in stage 7.0 (TID 701)
15/08/21 21:31:33 INFO Executor: Finished task 37.0 in stage 7.0 (TID 686). 3232 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 702, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Running task 53.0 in stage 7.0 (TID 702)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 32.0 in stage 7.0 (TID 681). 3229 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 685) in 117 ms on localhost (37/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 703, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 686) in 118 ms on localhost (38/200)
15/08/21 21:31:33 INFO Executor: Running task 54.0 in stage 7.0 (TID 703)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:33 INFO Executor: Finished task 41.0 in stage 7.0 (TID 690). 3233 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 681) in 147 ms on localhost (39/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14954
15/08/21 21:31:33 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 704, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 55.0 in stage 7.0 (TID 704)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 690) in 99 ms on localhost (40/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14741
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14718
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14479
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14810
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15361
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15384
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15013
15/08/21 21:31:33 INFO Executor: Finished task 42.0 in stage 7.0 (TID 691). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 705, localhost, ANY, 1983 bytes)
15/08/21 21:31:33 INFO Executor: Running task 56.0 in stage 7.0 (TID 705)
15/08/21 21:31:33 INFO Executor: Finished task 40.0 in stage 7.0 (TID 689). 3227 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 706, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 57.0 in stage 7.0 (TID 706)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 691) in 126 ms on localhost (41/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14914
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15296
15/08/21 21:31:33 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 689) in 136 ms on localhost (42/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 48.0 in stage 7.0 (TID 697). 3233 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 707, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15145
15/08/21 21:31:33 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 697) in 100 ms on localhost (43/200)
15/08/21 21:31:33 INFO Executor: Running task 58.0 in stage 7.0 (TID 707)
15/08/21 21:31:33 INFO Executor: Finished task 45.0 in stage 7.0 (TID 694). 3228 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14613
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 708, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 59.0 in stage 7.0 (TID 708)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 44.0 in stage 7.0 (TID 693). 3230 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 38.0 in stage 7.0 (TID 687). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 694) in 119 ms on localhost (44/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 709, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 710, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14967
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 693) in 135 ms on localhost (45/200)
15/08/21 21:31:33 INFO Executor: Running task 61.0 in stage 7.0 (TID 710)
15/08/21 21:31:33 INFO Executor: Finished task 51.0 in stage 7.0 (TID 700). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 60.0 in stage 7.0 (TID 709)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14780
15/08/21 21:31:33 INFO Executor: Finished task 46.0 in stage 7.0 (TID 695). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 47.0 in stage 7.0 (TID 696). 3233 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15132
15/08/21 21:31:33 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 687) in 175 ms on localhost (46/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 43.0 in stage 7.0 (TID 692). 3230 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 695) in 132 ms on localhost (47/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 711, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14795
15/08/21 21:31:33 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 712, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 62.0 in stage 7.0 (TID 711)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 49.0 in stage 7.0 (TID 698). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 52.0 in stage 7.0 (TID 701). 3232 bytes result sent to driver
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 63.0 in stage 7.0 (TID 712)
15/08/21 21:31:33 INFO Executor: Finished task 54.0 in stage 7.0 (TID 703). 3228 bytes result sent to driver
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15207
15/08/21 21:31:33 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 696) in 134 ms on localhost (48/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 692) in 164 ms on localhost (49/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 50.0 in stage 7.0 (TID 699). 3226 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 713, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 714, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 65.0 in stage 7.0 (TID 714)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Running task 64.0 in stage 7.0 (TID 713)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15274
15/08/21 21:31:33 INFO Executor: Finished task 56.0 in stage 7.0 (TID 705). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 700) in 107 ms on localhost (50/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 53.0 in stage 7.0 (TID 702). 3228 bytes result sent to driver
15/08/21 21:31:33 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 715, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 66.0 in stage 7.0 (TID 715)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 716, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 55.0 in stage 7.0 (TID 704). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 67.0 in stage 7.0 (TID 716)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 698) in 127 ms on localhost (51/200)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 701) in 107 ms on localhost (52/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 717, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 718, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14972
15/08/21 21:31:33 INFO Executor: Running task 69.0 in stage 7.0 (TID 718)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 719, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 68.0 in stage 7.0 (TID 717)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 703) in 105 ms on localhost (53/200)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14502
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 699) in 125 ms on localhost (54/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 705) in 83 ms on localhost (55/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO Executor: Finished task 57.0 in stage 7.0 (TID 706). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Running task 70.0 in stage 7.0 (TID 719)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14771
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO Executor: Finished task 60.0 in stage 7.0 (TID 709). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14489
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15340
15/08/21 21:31:33 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 720, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO Executor: Running task 71.0 in stage 7.0 (TID 720)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 721, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Finished task 59.0 in stage 7.0 (TID 708). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14982
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15131
15/08/21 21:31:33 INFO Executor: Running task 72.0 in stage 7.0 (TID 721)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 722, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 73.0 in stage 7.0 (TID 722)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 723, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 704) in 132 ms on localhost (56/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 702) in 140 ms on localhost (57/200)
15/08/21 21:31:33 INFO Executor: Running task 74.0 in stage 7.0 (TID 723)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 21:31:33 INFO Executor: Finished task 58.0 in stage 7.0 (TID 707). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Finished task 61.0 in stage 7.0 (TID 710). 3231 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 724, localhost, ANY, 1980 bytes)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 709) in 96 ms on localhost (58/200)
15/08/21 21:31:33 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 706) in 117 ms on localhost (59/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Finished task 62.0 in stage 7.0 (TID 711). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO Executor: Finished task 64.0 in stage 7.0 (TID 713). 3232 bytes result sent to driver
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:33 INFO Executor: Running task 75.0 in stage 7.0 (TID 724)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 725, localhost, ANY, 1978 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 76.0 in stage 7.0 (TID 725)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 726, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO Executor: Running task 77.0 in stage 7.0 (TID 726)
15/08/21 21:31:33 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 727, localhost, ANY, 1978 bytes)
15/08/21 21:31:33 INFO Executor: Running task 78.0 in stage 7.0 (TID 727)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 728, localhost, ANY, 1981 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 708) in 116 ms on localhost (60/200)
15/08/21 21:31:33 INFO Executor: Running task 79.0 in stage 7.0 (TID 728)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 21:31:33 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 707) in 120 ms on localhost (61/200)
15/08/21 21:31:33 INFO Executor: Finished task 63.0 in stage 7.0 (TID 712). 3230 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15347
15/08/21 21:31:33 INFO Executor: Finished task 65.0 in stage 7.0 (TID 714). 3233 bytes result sent to driver
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 711) in 101 ms on localhost (62/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15386
15/08/21 21:31:33 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 713) in 90 ms on localhost (63/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 710) in 115 ms on localhost (64/200)
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 712) in 106 ms on localhost (65/200)
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 729, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO Executor: Running task 80.0 in stage 7.0 (TID 729)
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 21:31:33 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 730, localhost, ANY, 1979 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14509
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15636
15/08/21 21:31:33 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 714) in 110 ms on localhost (66/200)
15/08/21 21:31:33 INFO Executor: Finished task 66.0 in stage 7.0 (TID 715). 3227 bytes result sent to driver
15/08/21 21:31:33 INFO Executor: Running task 81.0 in stage 7.0 (TID 730)
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:33 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 731, localhost, ANY, 1982 bytes)
15/08/21 21:31:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14864
15/08/21 21:31:33 INFO Executor: Running task 82.0 in stage 7.0 (TID 731)
15/08/21 21:31:33 INFO Executor: Finished task 69.0 in stage 7.0 (TID 718). 3234 bytes result sent to driver
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 21:31:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14989
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15039
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14944
15/08/21 21:31:34 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 715) in 123 ms on localhost (67/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 732, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 718) in 119 ms on localhost (68/200)
15/08/21 21:31:34 INFO Executor: Running task 83.0 in stage 7.0 (TID 732)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15090
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15509
15/08/21 21:31:34 INFO Executor: Finished task 73.0 in stage 7.0 (TID 722). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO Executor: Finished task 68.0 in stage 7.0 (TID 717). 3231 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 71.0 in stage 7.0 (TID 720). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 70.0 in stage 7.0 (TID 719). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15603
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 722) in 113 ms on localhost (69/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 76.0 in stage 7.0 (TID 725). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 733, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 79.0 in stage 7.0 (TID 728). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Running task 84.0 in stage 7.0 (TID 733)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 734, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 85.0 in stage 7.0 (TID 734)
15/08/21 21:31:34 INFO Executor: Finished task 74.0 in stage 7.0 (TID 723). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 735, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO Executor: Running task 86.0 in stage 7.0 (TID 735)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 736, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 72.0 in stage 7.0 (TID 721). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 737, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO Executor: Running task 88.0 in stage 7.0 (TID 737)
15/08/21 21:31:34 INFO Executor: Running task 87.0 in stage 7.0 (TID 736)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 21:31:34 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 738, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 89.0 in stage 7.0 (TID 738)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 717) in 150 ms on localhost (70/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 725) in 102 ms on localhost (71/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 67.0 in stage 7.0 (TID 716). 3235 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 719) in 151 ms on localhost (72/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 728) in 95 ms on localhost (73/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15489
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15189
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14678
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14854
15/08/21 21:31:34 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 720) in 131 ms on localhost (74/200)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 739, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 90.0 in stage 7.0 (TID 739)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 740, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 91.0 in stage 7.0 (TID 740)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 723) in 132 ms on localhost (75/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 721) in 142 ms on localhost (76/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 741, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 92.0 in stage 7.0 (TID 741)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 716) in 186 ms on localhost (77/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 75.0 in stage 7.0 (TID 724). 3232 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 742, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO Executor: Running task 93.0 in stage 7.0 (TID 742)
15/08/21 21:31:34 INFO Executor: Finished task 78.0 in stage 7.0 (TID 727). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 724) in 147 ms on localhost (78/200)
15/08/21 21:31:34 INFO Executor: Finished task 80.0 in stage 7.0 (TID 729). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO Executor: Finished task 77.0 in stage 7.0 (TID 726). 3232 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 82.0 in stage 7.0 (TID 731). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 743, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 94.0 in stage 7.0 (TID 743)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 744, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO Executor: Running task 95.0 in stage 7.0 (TID 744)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 745, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15333
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15043
15/08/21 21:31:34 INFO Executor: Running task 96.0 in stage 7.0 (TID 745)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 726) in 154 ms on localhost (79/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 729) in 144 ms on localhost (80/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 727) in 162 ms on localhost (81/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15501
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 746, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 731) in 135 ms on localhost (82/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 97.0 in stage 7.0 (TID 746)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15368
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15795
15/08/21 21:31:34 INFO Executor: Finished task 83.0 in stage 7.0 (TID 732). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 85.0 in stage 7.0 (TID 734). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 747, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 98.0 in stage 7.0 (TID 747)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 21:31:34 INFO Executor: Finished task 81.0 in stage 7.0 (TID 730). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 732) in 125 ms on localhost (83/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 21:31:34 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 734) in 102 ms on localhost (84/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14842
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15023
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 748, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 99.0 in stage 7.0 (TID 748)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14939
15/08/21 21:31:34 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 749, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 14843
15/08/21 21:31:34 INFO Executor: Running task 100.0 in stage 7.0 (TID 749)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 730) in 180 ms on localhost (85/200)
15/08/21 21:31:34 INFO Executor: Finished task 88.0 in stage 7.0 (TID 737). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 750, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 84.0 in stage 7.0 (TID 733). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15396
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 21:31:34 INFO Executor: Running task 101.0 in stage 7.0 (TID 750)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 89.0 in stage 7.0 (TID 738). 3227 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 751, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14740
15/08/21 21:31:34 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 752, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15117
15/08/21 21:31:34 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 737) in 127 ms on localhost (86/200)
15/08/21 21:31:34 INFO Executor: Running task 103.0 in stage 7.0 (TID 752)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 733) in 136 ms on localhost (87/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 102.0 in stage 7.0 (TID 751)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 738) in 131 ms on localhost (88/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 91.0 in stage 7.0 (TID 740). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 753, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 104.0 in stage 7.0 (TID 753)
15/08/21 21:31:34 INFO Executor: Finished task 93.0 in stage 7.0 (TID 742). 3228 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 754, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 105.0 in stage 7.0 (TID 754)
15/08/21 21:31:34 INFO Executor: Finished task 86.0 in stage 7.0 (TID 735). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 87.0 in stage 7.0 (TID 736). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 740) in 125 ms on localhost (89/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 755, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 106.0 in stage 7.0 (TID 755)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15364
15/08/21 21:31:34 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 756, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 107.0 in stage 7.0 (TID 756)
15/08/21 21:31:34 INFO Executor: Finished task 94.0 in stage 7.0 (TID 743). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14440
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15614
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 735) in 152 ms on localhost (90/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 742) in 121 ms on localhost (91/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 90.0 in stage 7.0 (TID 739). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 757, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 108.0 in stage 7.0 (TID 757)
15/08/21 21:31:34 INFO Executor: Finished task 92.0 in stage 7.0 (TID 741). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 736) in 164 ms on localhost (92/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14677
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 758, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 759, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 110.0 in stage 7.0 (TID 759)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 743) in 123 ms on localhost (93/200)
15/08/21 21:31:34 INFO Executor: Finished task 96.0 in stage 7.0 (TID 745). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 739) in 158 ms on localhost (94/200)
15/08/21 21:31:34 INFO Executor: Running task 109.0 in stage 7.0 (TID 758)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 741) in 145 ms on localhost (95/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO Executor: Finished task 97.0 in stage 7.0 (TID 746). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 760, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 111.0 in stage 7.0 (TID 760)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 745) in 122 ms on localhost (96/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 95.0 in stage 7.0 (TID 744). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 761, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15038
15/08/21 21:31:34 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 746) in 105 ms on localhost (97/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO Executor: Running task 112.0 in stage 7.0 (TID 761)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 762, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 113.0 in stage 7.0 (TID 762)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 21:31:34 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 744) in 138 ms on localhost (98/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 98.0 in stage 7.0 (TID 747). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 14991
15/08/21 21:31:34 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 763, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO Executor: Running task 114.0 in stage 7.0 (TID 763)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15404
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 747) in 119 ms on localhost (99/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 99.0 in stage 7.0 (TID 748). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 100.0 in stage 7.0 (TID 749). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15009
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 764, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 14791
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 115.0 in stage 7.0 (TID 764)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15566
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 765, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 116.0 in stage 7.0 (TID 765)
15/08/21 21:31:34 INFO Executor: Finished task 105.0 in stage 7.0 (TID 754). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 748) in 131 ms on localhost (100/200)
15/08/21 21:31:34 INFO Executor: Finished task 101.0 in stage 7.0 (TID 750). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 766, localhost, ANY, 1978 bytes)
15/08/21 21:31:34 INFO Executor: Running task 117.0 in stage 7.0 (TID 766)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 767, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 754) in 100 ms on localhost (101/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 21:31:34 INFO Executor: Running task 118.0 in stage 7.0 (TID 767)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 750) in 120 ms on localhost (102/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 749) in 138 ms on localhost (103/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15500
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO Executor: Finished task 103.0 in stage 7.0 (TID 752). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 768, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15141
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14687
15/08/21 21:31:34 INFO Executor: Finished task 110.0 in stage 7.0 (TID 759). 3229 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 102.0 in stage 7.0 (TID 751). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 752) in 127 ms on localhost (104/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 769, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 120.0 in stage 7.0 (TID 769)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 770, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 759) in 96 ms on localhost (105/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 119.0 in stage 7.0 (TID 768)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15098
15/08/21 21:31:34 INFO Executor: Finished task 107.0 in stage 7.0 (TID 756). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14985
15/08/21 21:31:34 INFO Executor: Running task 121.0 in stage 7.0 (TID 770)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 771, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 122.0 in stage 7.0 (TID 771)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 104.0 in stage 7.0 (TID 753). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15038
15/08/21 21:31:34 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 756) in 122 ms on localhost (106/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 751) in 150 ms on localhost (107/200)
15/08/21 21:31:34 INFO Executor: Finished task 112.0 in stage 7.0 (TID 761). 3235 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 772, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Running task 123.0 in stage 7.0 (TID 772)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 773, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 753) in 148 ms on localhost (108/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 124.0 in stage 7.0 (TID 773)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15302
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14597
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 761) in 109 ms on localhost (109/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 21:31:34 INFO Executor: Finished task 109.0 in stage 7.0 (TID 758). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 774, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 114.0 in stage 7.0 (TID 763). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Running task 125.0 in stage 7.0 (TID 774)
15/08/21 21:31:34 INFO Executor: Finished task 108.0 in stage 7.0 (TID 757). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 775, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 758) in 136 ms on localhost (110/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Running task 126.0 in stage 7.0 (TID 775)
15/08/21 21:31:34 INFO Executor: Finished task 106.0 in stage 7.0 (TID 755). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 776, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO Executor: Running task 127.0 in stage 7.0 (TID 776)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 777, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 21:31:34 INFO Executor: Running task 128.0 in stage 7.0 (TID 777)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 763) in 110 ms on localhost (111/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 757) in 156 ms on localhost (112/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14919
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 755) in 173 ms on localhost (113/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO Executor: Finished task 111.0 in stage 7.0 (TID 760). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 113.0 in stage 7.0 (TID 762). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14884
15/08/21 21:31:34 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 778, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15211
15/08/21 21:31:34 INFO Executor: Finished task 115.0 in stage 7.0 (TID 764). 3235 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Running task 129.0 in stage 7.0 (TID 778)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 779, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 780, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 130.0 in stage 7.0 (TID 779)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 760) in 157 ms on localhost (114/200)
15/08/21 21:31:34 INFO Executor: Running task 131.0 in stage 7.0 (TID 780)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 762) in 147 ms on localhost (115/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 764) in 111 ms on localhost (116/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 117.0 in stage 7.0 (TID 766). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15432
15/08/21 21:31:34 INFO Executor: Finished task 118.0 in stage 7.0 (TID 767). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 781, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 782, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 133.0 in stage 7.0 (TID 782)
15/08/21 21:31:34 INFO Executor: Finished task 116.0 in stage 7.0 (TID 765). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 766) in 107 ms on localhost (117/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 767) in 107 ms on localhost (118/200)
15/08/21 21:31:34 INFO Executor: Running task 132.0 in stage 7.0 (TID 781)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 783, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 134.0 in stage 7.0 (TID 783)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 121.0 in stage 7.0 (TID 770). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15557
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15267
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15191
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15532
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 765) in 125 ms on localhost (119/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 784, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 21:31:34 INFO Executor: Running task 135.0 in stage 7.0 (TID 784)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 770) in 106 ms on localhost (120/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15272
15/08/21 21:31:34 INFO Executor: Finished task 122.0 in stage 7.0 (TID 771). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14736
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 120.0 in stage 7.0 (TID 769). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15183
15/08/21 21:31:34 INFO Executor: Finished task 128.0 in stage 7.0 (TID 777). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 21:31:34 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 785, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Running task 136.0 in stage 7.0 (TID 785)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 21:31:34 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 786, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 126.0 in stage 7.0 (TID 775). 3232 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 137.0 in stage 7.0 (TID 786)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15150
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 15359
15/08/21 21:31:34 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 787, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 119.0 in stage 7.0 (TID 768). 3227 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14825
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 788, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO Executor: Running task 138.0 in stage 7.0 (TID 787)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 789, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 140.0 in stage 7.0 (TID 789)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14550
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14942
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 769) in 137 ms on localhost (121/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 777) in 85 ms on localhost (122/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 771) in 132 ms on localhost (123/200)
15/08/21 21:31:34 INFO Executor: Running task 139.0 in stage 7.0 (TID 788)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 768) in 149 ms on localhost (124/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO Executor: Finished task 123.0 in stage 7.0 (TID 772). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 775) in 105 ms on localhost (125/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 790, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 141.0 in stage 7.0 (TID 790)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 129.0 in stage 7.0 (TID 778). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15609
15/08/21 21:31:34 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 772) in 135 ms on localhost (126/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 130.0 in stage 7.0 (TID 779). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 127.0 in stage 7.0 (TID 776). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 131.0 in stage 7.0 (TID 780). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 791, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 133.0 in stage 7.0 (TID 782). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Running task 142.0 in stage 7.0 (TID 791)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 792, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 793, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 144.0 in stage 7.0 (TID 793)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 778) in 104 ms on localhost (127/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Running task 143.0 in stage 7.0 (TID 792)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 779) in 105 ms on localhost (128/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 776) in 116 ms on localhost (129/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14814
15/08/21 21:31:34 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 780) in 104 ms on localhost (130/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 125.0 in stage 7.0 (TID 774). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 794, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 795, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 146.0 in stage 7.0 (TID 795)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 796, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 147.0 in stage 7.0 (TID 796)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 21:31:34 INFO Executor: Finished task 132.0 in stage 7.0 (TID 781). 3229 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 145.0 in stage 7.0 (TID 794)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14951
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 124.0 in stage 7.0 (TID 773). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 782) in 97 ms on localhost (131/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 774) in 149 ms on localhost (132/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 781) in 115 ms on localhost (133/200)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 797, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO Executor: Running task 148.0 in stage 7.0 (TID 797)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 798, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 149.0 in stage 7.0 (TID 798)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 773) in 185 ms on localhost (134/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14802
15/08/21 21:31:34 INFO Executor: Finished task 136.0 in stage 7.0 (TID 785). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 799, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 150.0 in stage 7.0 (TID 799)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14788
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15018
15/08/21 21:31:34 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 785) in 100 ms on localhost (135/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14836
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 135.0 in stage 7.0 (TID 784). 3232 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14913
15/08/21 21:31:34 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 800, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO Executor: Finished task 134.0 in stage 7.0 (TID 783). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 784) in 130 ms on localhost (136/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14564
15/08/21 21:31:34 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 801, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Running task 152.0 in stage 7.0 (TID 801)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Running task 151.0 in stage 7.0 (TID 800)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 783) in 146 ms on localhost (137/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO Executor: Finished task 138.0 in stage 7.0 (TID 787). 3229 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15107
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 137.0 in stage 7.0 (TID 786). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 787) in 118 ms on localhost (138/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 802, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15079
15/08/21 21:31:34 INFO Executor: Running task 153.0 in stage 7.0 (TID 802)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14508
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 139.0 in stage 7.0 (TID 788). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 21:31:34 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 786) in 135 ms on localhost (139/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 803, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 154.0 in stage 7.0 (TID 803)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO Executor: Finished task 141.0 in stage 7.0 (TID 790). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15358
15/08/21 21:31:34 INFO Executor: Finished task 143.0 in stage 7.0 (TID 792). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 788) in 129 ms on localhost (140/200)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 804, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 155.0 in stage 7.0 (TID 804)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 805, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 147.0 in stage 7.0 (TID 796). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 140.0 in stage 7.0 (TID 789). 3235 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 806, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 790) in 118 ms on localhost (141/200)
15/08/21 21:31:34 INFO Executor: Running task 156.0 in stage 7.0 (TID 805)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 792) in 109 ms on localhost (142/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 146.0 in stage 7.0 (TID 795). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 157.0 in stage 7.0 (TID 806)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 807, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14923
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15302
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 796) in 100 ms on localhost (143/200)
15/08/21 21:31:34 INFO Executor: Running task 158.0 in stage 7.0 (TID 807)
15/08/21 21:31:34 INFO Executor: Finished task 142.0 in stage 7.0 (TID 791). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 144.0 in stage 7.0 (TID 793). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 789) in 145 ms on localhost (144/200)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 808, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO Executor: Running task 159.0 in stage 7.0 (TID 808)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 809, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14560
15/08/21 21:31:34 INFO Executor: Running task 160.0 in stage 7.0 (TID 809)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 810, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 161.0 in stage 7.0 (TID 810)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 811, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 21:31:34 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 795) in 108 ms on localhost (145/200)
15/08/21 21:31:34 INFO Executor: Running task 162.0 in stage 7.0 (TID 811)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 21:31:34 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 791) in 127 ms on localhost (146/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14829
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 793) in 126 ms on localhost (147/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14569
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 145.0 in stage 7.0 (TID 794). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 794) in 124 ms on localhost (148/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14848
15/08/21 21:31:34 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 812, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 163.0 in stage 7.0 (TID 812)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14878
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 149.0 in stage 7.0 (TID 798). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14838
15/08/21 21:31:34 INFO Executor: Finished task 150.0 in stage 7.0 (TID 799). 3227 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 152.0 in stage 7.0 (TID 801). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 813, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 21:31:34 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 814, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 164.0 in stage 7.0 (TID 813)
15/08/21 21:31:34 INFO Executor: Running task 165.0 in stage 7.0 (TID 814)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15001
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 148.0 in stage 7.0 (TID 797). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 151.0 in stage 7.0 (TID 800). 3227 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14793
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 815, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO Executor: Running task 166.0 in stage 7.0 (TID 815)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 799) in 114 ms on localhost (149/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 798) in 125 ms on localhost (150/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15029
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15586
15/08/21 21:31:34 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 816, localhost, ANY, 1978 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 153.0 in stage 7.0 (TID 802). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 797) in 132 ms on localhost (151/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 801) in 95 ms on localhost (152/200)
15/08/21 21:31:34 INFO Executor: Running task 167.0 in stage 7.0 (TID 816)
15/08/21 21:31:34 INFO Executor: Finished task 156.0 in stage 7.0 (TID 805). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 817, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 154.0 in stage 7.0 (TID 803). 3229 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 155.0 in stage 7.0 (TID 804). 3235 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14816
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 800) in 105 ms on localhost (153/200)
15/08/21 21:31:34 INFO Executor: Running task 168.0 in stage 7.0 (TID 817)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 818, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 21:31:34 INFO Executor: Finished task 157.0 in stage 7.0 (TID 806). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15383
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 169.0 in stage 7.0 (TID 818)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 819, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15203
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 820, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 171.0 in stage 7.0 (TID 820)
15/08/21 21:31:34 INFO Executor: Running task 170.0 in stage 7.0 (TID 819)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 802) in 93 ms on localhost (154/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 805) in 79 ms on localhost (155/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 803) in 87 ms on localhost (156/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 821, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 804) in 86 ms on localhost (157/200)
15/08/21 21:31:34 INFO Executor: Running task 172.0 in stage 7.0 (TID 821)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 159.0 in stage 7.0 (TID 808). 3232 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 161.0 in stage 7.0 (TID 810). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO Executor: Finished task 160.0 in stage 7.0 (TID 809). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO Executor: Finished task 162.0 in stage 7.0 (TID 811). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 822, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 158.0 in stage 7.0 (TID 807). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 823, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 21:31:34 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 806) in 95 ms on localhost (158/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15058
15/08/21 21:31:34 INFO Executor: Running task 174.0 in stage 7.0 (TID 823)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 173.0 in stage 7.0 (TID 822)
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 824, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14991
15/08/21 21:31:34 INFO Executor: Running task 175.0 in stage 7.0 (TID 824)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14934
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 825, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO Executor: Running task 176.0 in stage 7.0 (TID 825)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15679
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 808) in 92 ms on localhost (159/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14876
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 810) in 101 ms on localhost (160/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 809) in 103 ms on localhost (161/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15096
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15282
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 163.0 in stage 7.0 (TID 812). 3233 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 21:31:34 INFO Executor: Finished task 164.0 in stage 7.0 (TID 813). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15055
15/08/21 21:31:34 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 826, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 177.0 in stage 7.0 (TID 826)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 811) in 110 ms on localhost (162/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 827, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14883
15/08/21 21:31:34 INFO Executor: Running task 178.0 in stage 7.0 (TID 827)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15474
15/08/21 21:31:34 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 812) in 99 ms on localhost (163/200)
15/08/21 21:31:34 INFO Executor: Finished task 167.0 in stage 7.0 (TID 816). 3229 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 166.0 in stage 7.0 (TID 815). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 165.0 in stage 7.0 (TID 814). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 828, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 807) in 133 ms on localhost (164/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 829, localhost, ANY, 1978 bytes)
15/08/21 21:31:34 INFO Executor: Running task 180.0 in stage 7.0 (TID 829)
15/08/21 21:31:34 INFO Executor: Running task 179.0 in stage 7.0 (TID 828)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 169.0 in stage 7.0 (TID 818). 3231 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 830, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 813) in 98 ms on localhost (165/200)
15/08/21 21:31:34 INFO Executor: Running task 181.0 in stage 7.0 (TID 830)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14751
15/08/21 21:31:34 INFO Executor: Finished task 170.0 in stage 7.0 (TID 819). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 171.0 in stage 7.0 (TID 820). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 831, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 182.0 in stage 7.0 (TID 831)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 816) in 87 ms on localhost (166/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14879
15/08/21 21:31:34 INFO Executor: Finished task 172.0 in stage 7.0 (TID 821). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15238
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14819
15/08/21 21:31:34 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 832, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 833, localhost, ANY, 1981 bytes)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 815) in 96 ms on localhost (167/200)
15/08/21 21:31:34 INFO Executor: Running task 184.0 in stage 7.0 (TID 833)
15/08/21 21:31:34 INFO Executor: Finished task 168.0 in stage 7.0 (TID 817). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Running task 183.0 in stage 7.0 (TID 832)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 814) in 107 ms on localhost (168/200)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 834, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 185.0 in stage 7.0 (TID 834)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 835, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 186.0 in stage 7.0 (TID 835)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 836, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Running task 187.0 in stage 7.0 (TID 836)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 837, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 188.0 in stage 7.0 (TID 837)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 818) in 91 ms on localhost (169/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 820) in 88 ms on localhost (170/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 819) in 89 ms on localhost (171/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 821) in 85 ms on localhost (172/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 817) in 101 ms on localhost (173/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 173.0 in stage 7.0 (TID 822). 3232 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 838, localhost, ANY, 1978 bytes)
15/08/21 21:31:34 INFO Executor: Running task 189.0 in stage 7.0 (TID 838)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 176.0 in stage 7.0 (TID 825). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 175.0 in stage 7.0 (TID 824). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 839, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 190.0 in stage 7.0 (TID 839)
15/08/21 21:31:34 INFO Executor: Finished task 174.0 in stage 7.0 (TID 823). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 840, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15549
15/08/21 21:31:34 INFO Executor: Running task 191.0 in stage 7.0 (TID 840)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 825) in 96 ms on localhost (174/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 822) in 108 ms on localhost (175/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15658
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 21:31:34 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 841, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO Executor: Running task 192.0 in stage 7.0 (TID 841)
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14379
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 21:31:34 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 824) in 108 ms on localhost (176/200)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 823) in 120 ms on localhost (177/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14992
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15330
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14914
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15235
15/08/21 21:31:34 INFO Executor: Finished task 177.0 in stage 7.0 (TID 826). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 842, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Running task 193.0 in stage 7.0 (TID 842)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 826) in 106 ms on localhost (178/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14896
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Finished task 181.0 in stage 7.0 (TID 830). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 180.0 in stage 7.0 (TID 829). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 186.0 in stage 7.0 (TID 835). 3235 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14938
15/08/21 21:31:34 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 843, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14891
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO Executor: Running task 194.0 in stage 7.0 (TID 843)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 844, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14845
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15101
15/08/21 21:31:34 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 830) in 100 ms on localhost (179/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 829) in 102 ms on localhost (180/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 835) in 84 ms on localhost (181/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 845, localhost, ANY, 1983 bytes)
15/08/21 21:31:34 INFO Executor: Running task 195.0 in stage 7.0 (TID 844)
15/08/21 21:31:34 INFO Executor: Running task 196.0 in stage 7.0 (TID 845)
15/08/21 21:31:34 INFO Executor: Finished task 185.0 in stage 7.0 (TID 834). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 179.0 in stage 7.0 (TID 828). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 846, localhost, ANY, 1979 bytes)
15/08/21 21:31:34 INFO Executor: Finished task 178.0 in stage 7.0 (TID 827). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 187.0 in stage 7.0 (TID 836). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 847, localhost, ANY, 1980 bytes)
15/08/21 21:31:34 INFO Executor: Running task 197.0 in stage 7.0 (TID 846)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15463
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO Executor: Finished task 184.0 in stage 7.0 (TID 833). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Running task 198.0 in stage 7.0 (TID 847)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15010
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 183.0 in stage 7.0 (TID 832). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15103
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO Executor: Finished task 182.0 in stage 7.0 (TID 831). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 834) in 100 ms on localhost (182/200)
15/08/21 21:31:34 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 848, localhost, ANY, 1982 bytes)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 828) in 135 ms on localhost (183/200)
15/08/21 21:31:34 INFO Executor: Finished task 190.0 in stage 7.0 (TID 839). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Running task 199.0 in stage 7.0 (TID 848)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 827) in 142 ms on localhost (184/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 836) in 113 ms on localhost (185/200)
15/08/21 21:31:34 INFO Executor: Finished task 188.0 in stage 7.0 (TID 837). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:34 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 833) in 129 ms on localhost (186/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 832) in 130 ms on localhost (187/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 839) in 93 ms on localhost (188/200)
15/08/21 21:31:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:34 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 831) in 134 ms on localhost (189/200)
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 191.0 in stage 7.0 (TID 840). 3233 bytes result sent to driver
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 837) in 120 ms on localhost (190/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 840) in 92 ms on localhost (191/200)
15/08/21 21:31:34 INFO Executor: Finished task 192.0 in stage 7.0 (TID 841). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 189.0 in stage 7.0 (TID 838). 3233 bytes result sent to driver
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 841) in 94 ms on localhost (192/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 838) in 122 ms on localhost (193/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15272
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14772
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15039
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15726
15/08/21 21:31:34 INFO Executor: Finished task 194.0 in stage 7.0 (TID 843). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 21:31:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:34 INFO Executor: Finished task 193.0 in stage 7.0 (TID 842). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15164
15/08/21 21:31:34 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 843) in 87 ms on localhost (194/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 842) in 103 ms on localhost (195/200)
15/08/21 21:31:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 21:31:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:34 INFO Executor: Finished task 196.0 in stage 7.0 (TID 845). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO Executor: Finished task 195.0 in stage 7.0 (TID 844). 3234 bytes result sent to driver
15/08/21 21:31:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15137
15/08/21 21:31:34 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 844) in 95 ms on localhost (196/200)
15/08/21 21:31:34 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 845) in 91 ms on localhost (197/200)
15/08/21 21:31:34 INFO Executor: Finished task 197.0 in stage 7.0 (TID 846). 3230 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 846) in 95 ms on localhost (198/200)
15/08/21 21:31:34 INFO Executor: Finished task 198.0 in stage 7.0 (TID 847). 3232 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 847) in 97 ms on localhost (199/200)
15/08/21 21:31:34 INFO Executor: Finished task 199.0 in stage 7.0 (TID 848). 3231 bytes result sent to driver
15/08/21 21:31:34 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 848) in 80 ms on localhost (200/200)
15/08/21 21:31:34 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/21 21:31:34 INFO DAGScheduler: ResultStage 7 (processCmd at CliDriver.java:423) finished in 1.540 s
15/08/21 21:31:34 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 3.287144 s
15/08/21 21:31:34 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@15463093
15/08/21 21:31:34 INFO StatsReportListener: task runtime:(count: 200, mean: 125.825000, stdev: 35.648343, max: 262.000000, min: 79.000000)
15/08/21 21:31:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:34 INFO StatsReportListener: 	79.0 ms	87.0 ms	93.0 ms	101.0 ms	118.0 ms	136.0 ms	175.0 ms	212.0 ms	262.0 ms
15/08/21 21:31:34 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.285000, stdev: 0.560156, max: 4.000000, min: 0.000000)
15/08/21 21:31:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:34 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	4.0 ms
15/08/21 21:31:34 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 21:31:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:34 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 21:31:34 INFO StatsReportListener: task result size:(count: 200, mean: 3231.525000, stdev: 1.931159, max: 3235.000000, min: 3226.000000)
15/08/21 21:31:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:34 INFO StatsReportListener: 	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB
15/08/21 21:31:34 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 74.969201, stdev: 8.315826, max: 90.155440, min: 50.925926)
15/08/21 21:31:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:34 INFO StatsReportListener: 	51 %	60 %	63 %	70 %	76 %	82 %	85 %	87 %	90 %
15/08/21 21:31:34 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.229381, stdev: 0.475052, max: 3.278689, min: 0.000000)
15/08/21 21:31:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:34 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 3 %
15/08/21 21:31:34 INFO StatsReportListener: other time pct: (count: 200, mean: 24.801419, stdev: 8.355008, max: 49.074074, min: 9.016393)
15/08/21 21:31:34 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:34 INFO StatsReportListener: 	 9 %	13 %	15 %	18 %	24 %	30 %	37 %	40 %	49 %
15/08/21 21:31:35 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 21:31:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:35 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 21:31:35 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 197 bytes
15/08/21 21:31:35 INFO DAGScheduler: Registering RDD 44 (processCmd at CliDriver.java:423)
15/08/21 21:31:35 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 21:31:35 INFO DAGScheduler: Final stage: ResultStage 10(processCmd at CliDriver.java:423)
15/08/21 21:31:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
15/08/21 21:31:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
15/08/21 21:31:35 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 21:31:35 INFO MemoryStore: ensureFreeSpace(16104) called with curMem=1895782, maxMem=22226833244
15/08/21 21:31:35 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.7 KB, free 20.7 GB)
15/08/21 21:31:35 INFO MemoryStore: ensureFreeSpace(8465) called with curMem=1911886, maxMem=22226833244
15/08/21 21:31:35 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.3 KB, free 20.7 GB)
15/08/21 21:31:35 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:52592 (size: 8.3 KB, free: 20.7 GB)
15/08/21 21:31:35 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/21 21:31:35 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423)
15/08/21 21:31:35 INFO TaskSchedulerImpl: Adding task set 9.0 with 200 tasks
15/08/21 21:31:35 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 849, localhost, ANY, 1971 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 850, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 851, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 852, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 853, localhost, ANY, 1970 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 854, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 855, localhost, ANY, 1971 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 856, localhost, ANY, 1971 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 857, localhost, ANY, 1972 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 858, localhost, ANY, 1971 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 859, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 860, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 861, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 862, localhost, ANY, 1970 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 863, localhost, ANY, 1969 bytes)
15/08/21 21:31:35 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 864, localhost, ANY, 1971 bytes)
15/08/21 21:31:35 INFO Executor: Running task 0.0 in stage 9.0 (TID 849)
15/08/21 21:31:35 INFO Executor: Running task 1.0 in stage 9.0 (TID 850)
15/08/21 21:31:35 INFO Executor: Running task 2.0 in stage 9.0 (TID 851)
15/08/21 21:31:35 INFO Executor: Running task 5.0 in stage 9.0 (TID 854)
15/08/21 21:31:35 INFO Executor: Running task 9.0 in stage 9.0 (TID 858)
15/08/21 21:31:35 INFO Executor: Running task 13.0 in stage 9.0 (TID 862)
15/08/21 21:31:35 INFO Executor: Running task 4.0 in stage 9.0 (TID 853)
15/08/21 21:31:35 INFO Executor: Running task 7.0 in stage 9.0 (TID 856)
15/08/21 21:31:35 INFO Executor: Running task 8.0 in stage 9.0 (TID 857)
15/08/21 21:31:35 INFO Executor: Running task 6.0 in stage 9.0 (TID 855)
15/08/21 21:31:35 INFO Executor: Running task 3.0 in stage 9.0 (TID 852)
15/08/21 21:31:35 INFO Executor: Running task 10.0 in stage 9.0 (TID 859)
15/08/21 21:31:35 INFO Executor: Running task 15.0 in stage 9.0 (TID 864)
15/08/21 21:31:35 INFO Executor: Running task 11.0 in stage 9.0 (TID 860)
15/08/21 21:31:35 INFO Executor: Running task 12.0 in stage 9.0 (TID 861)
15/08/21 21:31:35 INFO Executor: Running task 14.0 in stage 9.0 (TID 863)
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15062
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15524
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15142
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15046
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15104
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15570
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15039
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15106
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14788
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15545
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14824
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15007
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15049
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 14731
15/08/21 21:31:35 INFO Executor: Finished task 11.0 in stage 9.0 (TID 860). 2341 bytes result sent to driver
15/08/21 21:31:35 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 865, localhost, ANY, 1968 bytes)
15/08/21 21:31:35 INFO Executor: Running task 16.0 in stage 9.0 (TID 865)
15/08/21 21:31:35 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 860) in 638 ms on localhost (1/200)
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 21:31:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15625
15/08/21 21:31:35 INFO Executor: Finished task 12.0 in stage 9.0 (TID 861). 2341 bytes result sent to driver
15/08/21 21:31:35 INFO Executor: Finished task 13.0 in stage 9.0 (TID 862). 2341 bytes result sent to driver
15/08/21 21:31:35 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 866, localhost, ANY, 1970 bytes)
15/08/21 21:31:35 INFO Executor: Running task 17.0 in stage 9.0 (TID 866)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 867, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 861) in 764 ms on localhost (2/200)
15/08/21 21:31:36 INFO Executor: Running task 18.0 in stage 9.0 (TID 867)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 862) in 768 ms on localhost (3/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15120
15/08/21 21:31:36 INFO Executor: Finished task 2.0 in stage 9.0 (TID 851). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 868, localhost, ANY, 1972 bytes)
15/08/21 21:31:36 INFO Executor: Running task 19.0 in stage 9.0 (TID 868)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 851) in 828 ms on localhost (4/200)
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14755
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO Executor: Finished task 7.0 in stage 9.0 (TID 856). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 869, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO Executor: Running task 20.0 in stage 9.0 (TID 869)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 856) in 858 ms on localhost (5/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14693
15/08/21 21:31:36 INFO Executor: Finished task 8.0 in stage 9.0 (TID 857). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 870, localhost, ANY, 1968 bytes)
15/08/21 21:31:36 INFO Executor: Running task 21.0 in stage 9.0 (TID 870)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 857) in 901 ms on localhost (6/200)
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO Executor: Finished task 10.0 in stage 9.0 (TID 859). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 9.0 in stage 9.0 (TID 858). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14906
15/08/21 21:31:36 INFO Executor: Finished task 14.0 in stage 9.0 (TID 863). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 871, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO Executor: Running task 22.0 in stage 9.0 (TID 871)
15/08/21 21:31:36 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 872, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 859) in 925 ms on localhost (7/200)
15/08/21 21:31:36 INFO Executor: Running task 23.0 in stage 9.0 (TID 872)
15/08/21 21:31:36 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 873, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 858) in 931 ms on localhost (8/200)
15/08/21 21:31:36 INFO Executor: Running task 24.0 in stage 9.0 (TID 873)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 863) in 929 ms on localhost (9/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO Executor: Finished task 3.0 in stage 9.0 (TID 852). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 1.0 in stage 9.0 (TID 850). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 4.0 in stage 9.0 (TID 853). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 15.0 in stage 9.0 (TID 864). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 0.0 in stage 9.0 (TID 849). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 874, localhost, ANY, 1969 bytes)
15/08/21 21:31:36 INFO Executor: Running task 25.0 in stage 9.0 (TID 874)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO Executor: Finished task 6.0 in stage 9.0 (TID 855). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 5.0 in stage 9.0 (TID 854). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 875, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 852) in 965 ms on localhost (10/200)
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO Executor: Running task 26.0 in stage 9.0 (TID 875)
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15470
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 876, localhost, ANY, 1969 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 850) in 978 ms on localhost (11/200)
15/08/21 21:31:36 INFO Executor: Running task 27.0 in stage 9.0 (TID 876)
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 877, localhost, ANY, 1968 bytes)
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15313
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 853) in 1003 ms on localhost (12/200)
15/08/21 21:31:36 INFO Executor: Running task 28.0 in stage 9.0 (TID 877)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 864) in 1002 ms on localhost (13/200)
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14831
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15147
15/08/21 21:31:36 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 878, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO Executor: Running task 29.0 in stage 9.0 (TID 878)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 879, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO Executor: Running task 30.0 in stage 9.0 (TID 879)
15/08/21 21:31:36 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 880, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 849) in 1035 ms on localhost (14/200)
15/08/21 21:31:36 INFO Executor: Running task 31.0 in stage 9.0 (TID 880)
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 854) in 1034 ms on localhost (15/200)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 855) in 1036 ms on localhost (16/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15114
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14612
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14776
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15129
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15344
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15063
15/08/21 21:31:36 INFO Executor: Finished task 16.0 in stage 9.0 (TID 865). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 881, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 865) in 534 ms on localhost (17/200)
15/08/21 21:31:36 INFO Executor: Running task 32.0 in stage 9.0 (TID 881)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15383
15/08/21 21:31:36 INFO Executor: Finished task 17.0 in stage 9.0 (TID 866). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 882, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO Executor: Running task 33.0 in stage 9.0 (TID 882)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 866) in 535 ms on localhost (18/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO Executor: Finished task 18.0 in stage 9.0 (TID 867). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 883, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO Executor: Running task 34.0 in stage 9.0 (TID 883)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 867) in 539 ms on localhost (19/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14952
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14809
15/08/21 21:31:36 INFO Executor: Finished task 19.0 in stage 9.0 (TID 868). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 884, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO Executor: Running task 35.0 in stage 9.0 (TID 884)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 868) in 555 ms on localhost (20/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:36 INFO Executor: Finished task 20.0 in stage 9.0 (TID 869). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 21:31:36 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 885, localhost, ANY, 1969 bytes)
15/08/21 21:31:36 INFO Executor: Running task 36.0 in stage 9.0 (TID 885)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 869) in 591 ms on localhost (21/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO Executor: Finished task 21.0 in stage 9.0 (TID 870). 2341 bytes result sent to driver
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15393
15/08/21 21:31:36 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 886, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO Executor: Running task 37.0 in stage 9.0 (TID 886)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 870) in 621 ms on localhost (22/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO Executor: Finished task 24.0 in stage 9.0 (TID 873). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 22.0 in stage 9.0 (TID 871). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 23.0 in stage 9.0 (TID 872). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 887, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO Executor: Running task 38.0 in stage 9.0 (TID 887)
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 888, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 873) in 649 ms on localhost (23/200)
15/08/21 21:31:36 INFO Executor: Running task 39.0 in stage 9.0 (TID 888)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14750
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 889, localhost, ANY, 1969 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 871) in 680 ms on localhost (24/200)
15/08/21 21:31:36 INFO Executor: Running task 40.0 in stage 9.0 (TID 889)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 872) in 672 ms on localhost (25/200)
15/08/21 21:31:36 INFO Executor: Finished task 25.0 in stage 9.0 (TID 874). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 890, localhost, ANY, 1969 bytes)
15/08/21 21:31:36 INFO Executor: Running task 41.0 in stage 9.0 (TID 890)
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 874) in 695 ms on localhost (26/200)
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO Executor: Finished task 27.0 in stage 9.0 (TID 876). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO Executor: Finished task 26.0 in stage 9.0 (TID 875). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14718
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15571
15/08/21 21:31:36 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 891, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO Executor: Running task 42.0 in stage 9.0 (TID 891)
15/08/21 21:31:36 INFO Executor: Finished task 29.0 in stage 9.0 (TID 878). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO Executor: Finished task 30.0 in stage 9.0 (TID 879). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 892, localhost, ANY, 1968 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 876) in 708 ms on localhost (27/200)
15/08/21 21:31:36 INFO Executor: Running task 43.0 in stage 9.0 (TID 892)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO Executor: Finished task 31.0 in stage 9.0 (TID 880). 2341 bytes result sent to driver
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO Executor: Finished task 28.0 in stage 9.0 (TID 877). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 893, localhost, ANY, 1969 bytes)
15/08/21 21:31:36 INFO Executor: Running task 44.0 in stage 9.0 (TID 893)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 875) in 730 ms on localhost (28/200)
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 878) in 681 ms on localhost (29/200)
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14741
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 894, localhost, ANY, 1971 bytes)
15/08/21 21:31:36 INFO Executor: Running task 45.0 in stage 9.0 (TID 894)
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 895, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 879) in 694 ms on localhost (30/200)
15/08/21 21:31:36 INFO Executor: Running task 46.0 in stage 9.0 (TID 895)
15/08/21 21:31:36 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 880) in 690 ms on localhost (31/200)
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO Executor: Finished task 32.0 in stage 9.0 (TID 881). 2341 bytes result sent to driver
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14954
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15099
15/08/21 21:31:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:36 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 896, localhost, ANY, 1970 bytes)
15/08/21 21:31:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 21:31:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:36 INFO Executor: Running task 47.0 in stage 9.0 (TID 896)
15/08/21 21:31:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15384
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 897, localhost, ANY, 1969 bytes)
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 877) in 794 ms on localhost (32/200)
15/08/21 21:31:37 INFO Executor: Running task 48.0 in stage 9.0 (TID 897)
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 881) in 638 ms on localhost (33/200)
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15361
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14810
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15013
15/08/21 21:31:37 INFO Executor: Finished task 33.0 in stage 9.0 (TID 882). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 898, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 49.0 in stage 9.0 (TID 898)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 882) in 587 ms on localhost (34/200)
15/08/21 21:31:37 INFO Executor: Finished task 34.0 in stage 9.0 (TID 883). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO TaskSetManager: Starting task 50.0 in stage 9.0 (TID 899, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 50.0 in stage 9.0 (TID 899)
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 21:31:37 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 883) in 561 ms on localhost (35/200)
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14479
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO Executor: Finished task 35.0 in stage 9.0 (TID 884). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15145
15/08/21 21:31:37 INFO TaskSetManager: Starting task 51.0 in stage 9.0 (TID 900, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 51.0 in stage 9.0 (TID 900)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 884) in 540 ms on localhost (36/200)
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO Executor: Finished task 36.0 in stage 9.0 (TID 885). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14780
15/08/21 21:31:37 INFO TaskSetManager: Starting task 52.0 in stage 9.0 (TID 901, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 52.0 in stage 9.0 (TID 901)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 885) in 501 ms on localhost (37/200)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15296
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO Executor: Finished task 37.0 in stage 9.0 (TID 886). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14613
15/08/21 21:31:37 INFO TaskSetManager: Starting task 53.0 in stage 9.0 (TID 902, localhost, ANY, 1970 bytes)
15/08/21 21:31:37 INFO Executor: Running task 53.0 in stage 9.0 (TID 902)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 886) in 498 ms on localhost (38/200)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15132
15/08/21 21:31:37 INFO Executor: Finished task 38.0 in stage 9.0 (TID 887). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO Executor: Finished task 39.0 in stage 9.0 (TID 888). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 54.0 in stage 9.0 (TID 903, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 54.0 in stage 9.0 (TID 903)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO TaskSetManager: Starting task 55.0 in stage 9.0 (TID 904, localhost, ANY, 1970 bytes)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 887) in 604 ms on localhost (39/200)
15/08/21 21:31:37 INFO Executor: Running task 55.0 in stage 9.0 (TID 904)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 888) in 595 ms on localhost (40/200)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14967
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO Executor: Finished task 40.0 in stage 9.0 (TID 889). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15207
15/08/21 21:31:37 INFO TaskSetManager: Starting task 56.0 in stage 9.0 (TID 905, localhost, ANY, 1972 bytes)
15/08/21 21:31:37 INFO Executor: Running task 56.0 in stage 9.0 (TID 905)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 889) in 650 ms on localhost (41/200)
15/08/21 21:31:37 INFO Executor: Finished task 42.0 in stage 9.0 (TID 891). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO Executor: Finished task 41.0 in stage 9.0 (TID 890). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 57.0 in stage 9.0 (TID 906, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 57.0 in stage 9.0 (TID 906)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO Executor: Finished task 43.0 in stage 9.0 (TID 892). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 58.0 in stage 9.0 (TID 907, localhost, ANY, 1969 bytes)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 891) in 627 ms on localhost (42/200)
15/08/21 21:31:37 INFO Executor: Running task 58.0 in stage 9.0 (TID 907)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO TaskSetManager: Starting task 59.0 in stage 9.0 (TID 908, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 890) in 677 ms on localhost (43/200)
15/08/21 21:31:37 INFO Executor: Running task 59.0 in stage 9.0 (TID 908)
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14795
15/08/21 21:31:37 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 892) in 633 ms on localhost (44/200)
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15274
15/08/21 21:31:37 INFO Executor: Finished task 45.0 in stage 9.0 (TID 894). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 60.0 in stage 9.0 (TID 909, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 60.0 in stage 9.0 (TID 909)
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 894) in 637 ms on localhost (45/200)
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14771
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO Executor: Finished task 44.0 in stage 9.0 (TID 893). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO Executor: Finished task 46.0 in stage 9.0 (TID 895). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 21:31:37 INFO TaskSetManager: Starting task 61.0 in stage 9.0 (TID 910, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 61.0 in stage 9.0 (TID 910)
15/08/21 21:31:37 INFO Executor: Finished task 47.0 in stage 9.0 (TID 896). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO TaskSetManager: Starting task 62.0 in stage 9.0 (TID 911, localhost, ANY, 1970 bytes)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 893) in 710 ms on localhost (46/200)
15/08/21 21:31:37 INFO Executor: Running task 62.0 in stage 9.0 (TID 911)
15/08/21 21:31:37 INFO Executor: Finished task 48.0 in stage 9.0 (TID 897). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO TaskSetManager: Starting task 63.0 in stage 9.0 (TID 912, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 895) in 688 ms on localhost (47/200)
15/08/21 21:31:37 INFO Executor: Running task 63.0 in stage 9.0 (TID 912)
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 INFO TaskSetManager: Starting task 64.0 in stage 9.0 (TID 913, localhost, ANY, 1970 bytes)
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14972
15/08/21 21:31:37 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 896) in 687 ms on localhost (48/200)
15/08/21 21:31:37 INFO Executor: Running task 64.0 in stage 9.0 (TID 913)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 897) in 668 ms on localhost (49/200)
15/08/21 21:31:37 INFO Executor: Finished task 49.0 in stage 9.0 (TID 898). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO TaskSetManager: Starting task 65.0 in stage 9.0 (TID 914, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 65.0 in stage 9.0 (TID 914)
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 898) in 626 ms on localhost (50/200)
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14502
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO Executor: Finished task 50.0 in stage 9.0 (TID 899). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15340
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15131
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO TaskSetManager: Starting task 66.0 in stage 9.0 (TID 915, localhost, ANY, 1969 bytes)
15/08/21 21:31:37 INFO Executor: Running task 66.0 in stage 9.0 (TID 915)
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO TaskSetManager: Finished task 50.0 in stage 9.0 (TID 899) in 642 ms on localhost (51/200)
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14982
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14489
15/08/21 21:31:37 INFO Executor: Finished task 51.0 in stage 9.0 (TID 900). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 67.0 in stage 9.0 (TID 916, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 51.0 in stage 9.0 (TID 900) in 639 ms on localhost (52/200)
15/08/21 21:31:37 INFO Executor: Finished task 52.0 in stage 9.0 (TID 901). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO Executor: Running task 67.0 in stage 9.0 (TID 916)
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO TaskSetManager: Starting task 68.0 in stage 9.0 (TID 917, localhost, ANY, 1969 bytes)
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15386
15/08/21 21:31:37 INFO Executor: Running task 68.0 in stage 9.0 (TID 917)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 52.0 in stage 9.0 (TID 901) in 635 ms on localhost (53/200)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO Executor: Finished task 53.0 in stage 9.0 (TID 902). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 69.0 in stage 9.0 (TID 918, localhost, ANY, 1969 bytes)
15/08/21 21:31:37 INFO Executor: Running task 69.0 in stage 9.0 (TID 918)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 53.0 in stage 9.0 (TID 902) in 601 ms on localhost (54/200)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15636
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14864
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15347
15/08/21 21:31:37 INFO Executor: Finished task 54.0 in stage 9.0 (TID 903). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 70.0 in stage 9.0 (TID 919, localhost, ANY, 1971 bytes)
15/08/21 21:31:37 INFO Executor: Running task 70.0 in stage 9.0 (TID 919)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 54.0 in stage 9.0 (TID 903) in 569 ms on localhost (55/200)
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:37 INFO Executor: Finished task 55.0 in stage 9.0 (TID 904). 2341 bytes result sent to driver
15/08/21 21:31:37 INFO TaskSetManager: Starting task 71.0 in stage 9.0 (TID 920, localhost, ANY, 1970 bytes)
15/08/21 21:31:37 INFO Executor: Running task 71.0 in stage 9.0 (TID 920)
15/08/21 21:31:37 INFO TaskSetManager: Finished task 55.0 in stage 9.0 (TID 904) in 584 ms on localhost (56/200)
15/08/21 21:31:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 21:31:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14989
15/08/21 21:31:38 INFO Executor: Finished task 56.0 in stage 9.0 (TID 905). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 21:31:38 INFO TaskSetManager: Starting task 72.0 in stage 9.0 (TID 921, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Running task 72.0 in stage 9.0 (TID 921)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 56.0 in stage 9.0 (TID 905) in 588 ms on localhost (57/200)
15/08/21 21:31:38 INFO Executor: Finished task 57.0 in stage 9.0 (TID 906). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO TaskSetManager: Starting task 73.0 in stage 9.0 (TID 922, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Finished task 58.0 in stage 9.0 (TID 907). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO Executor: Running task 73.0 in stage 9.0 (TID 922)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO TaskSetManager: Starting task 74.0 in stage 9.0 (TID 923, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 57.0 in stage 9.0 (TID 906) in 609 ms on localhost (58/200)
15/08/21 21:31:38 INFO Executor: Running task 74.0 in stage 9.0 (TID 923)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 58.0 in stage 9.0 (TID 907) in 586 ms on localhost (59/200)
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 21:31:38 INFO Executor: Finished task 59.0 in stage 9.0 (TID 908). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Starting task 75.0 in stage 9.0 (TID 924, localhost, ANY, 1969 bytes)
15/08/21 21:31:38 INFO Executor: Running task 75.0 in stage 9.0 (TID 924)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 59.0 in stage 9.0 (TID 908) in 611 ms on localhost (60/200)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14509
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14944
15/08/21 21:31:38 INFO Executor: Finished task 60.0 in stage 9.0 (TID 909). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Starting task 76.0 in stage 9.0 (TID 925, localhost, ANY, 1967 bytes)
15/08/21 21:31:38 INFO Executor: Running task 76.0 in stage 9.0 (TID 925)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 60.0 in stage 9.0 (TID 909) in 617 ms on localhost (61/200)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15603
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO Executor: Finished task 61.0 in stage 9.0 (TID 910). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO Executor: Finished task 62.0 in stage 9.0 (TID 911). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO TaskSetManager: Starting task 77.0 in stage 9.0 (TID 926, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Finished task 63.0 in stage 9.0 (TID 912). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO Executor: Running task 77.0 in stage 9.0 (TID 926)
15/08/21 21:31:38 INFO TaskSetManager: Starting task 78.0 in stage 9.0 (TID 927, localhost, ANY, 1967 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 61.0 in stage 9.0 (TID 910) in 635 ms on localhost (62/200)
15/08/21 21:31:38 INFO Executor: Running task 78.0 in stage 9.0 (TID 927)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO Executor: Finished task 64.0 in stage 9.0 (TID 913). 2341 bytes result sent to driver
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO Executor: Finished task 65.0 in stage 9.0 (TID 914). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO TaskSetManager: Starting task 79.0 in stage 9.0 (TID 928, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO Executor: Running task 79.0 in stage 9.0 (TID 928)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15509
15/08/21 21:31:38 INFO TaskSetManager: Starting task 80.0 in stage 9.0 (TID 929, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Running task 80.0 in stage 9.0 (TID 929)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO Executor: Finished task 66.0 in stage 9.0 (TID 915). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO TaskSetManager: Starting task 81.0 in stage 9.0 (TID 930, localhost, ANY, 1968 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 62.0 in stage 9.0 (TID 911) in 663 ms on localhost (63/200)
15/08/21 21:31:38 INFO Executor: Running task 81.0 in stage 9.0 (TID 930)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 63.0 in stage 9.0 (TID 912) in 649 ms on localhost (64/200)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 65.0 in stage 9.0 (TID 914) in 615 ms on localhost (65/200)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:38 INFO TaskSetManager: Finished task 66.0 in stage 9.0 (TID 915) in 588 ms on localhost (66/200)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO TaskSetManager: Starting task 82.0 in stage 9.0 (TID 931, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Running task 82.0 in stage 9.0 (TID 931)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 64.0 in stage 9.0 (TID 913) in 644 ms on localhost (67/200)
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14678
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15489
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15090
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14854
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15501
15/08/21 21:31:38 INFO Executor: Finished task 68.0 in stage 9.0 (TID 917). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO Executor: Finished task 67.0 in stage 9.0 (TID 916). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO TaskSetManager: Starting task 83.0 in stage 9.0 (TID 932, localhost, ANY, 1969 bytes)
15/08/21 21:31:38 INFO Executor: Running task 83.0 in stage 9.0 (TID 932)
15/08/21 21:31:38 INFO TaskSetManager: Starting task 84.0 in stage 9.0 (TID 933, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 68.0 in stage 9.0 (TID 917) in 608 ms on localhost (68/200)
15/08/21 21:31:38 INFO Executor: Running task 84.0 in stage 9.0 (TID 933)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 67.0 in stage 9.0 (TID 916) in 632 ms on localhost (69/200)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO Executor: Finished task 69.0 in stage 9.0 (TID 918). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO TaskSetManager: Starting task 85.0 in stage 9.0 (TID 934, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO Executor: Running task 85.0 in stage 9.0 (TID 934)
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Finished task 69.0 in stage 9.0 (TID 918) in 583 ms on localhost (70/200)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15189
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO Executor: Finished task 70.0 in stage 9.0 (TID 919). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO TaskSetManager: Starting task 86.0 in stage 9.0 (TID 935, localhost, ANY, 1972 bytes)
15/08/21 21:31:38 INFO Executor: Running task 86.0 in stage 9.0 (TID 935)
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Finished task 70.0 in stage 9.0 (TID 919) in 506 ms on localhost (71/200)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15333
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15368
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO Executor: Finished task 71.0 in stage 9.0 (TID 920). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO TaskSetManager: Starting task 87.0 in stage 9.0 (TID 936, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Running task 87.0 in stage 9.0 (TID 936)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 71.0 in stage 9.0 (TID 920) in 495 ms on localhost (72/200)
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14842
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15043
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO Executor: Finished task 72.0 in stage 9.0 (TID 921). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO Executor: Finished task 73.0 in stage 9.0 (TID 922). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14939
15/08/21 21:31:38 INFO TaskSetManager: Starting task 88.0 in stage 9.0 (TID 937, localhost, ANY, 1968 bytes)
15/08/21 21:31:38 INFO Executor: Running task 88.0 in stage 9.0 (TID 937)
15/08/21 21:31:38 INFO Executor: Finished task 74.0 in stage 9.0 (TID 923). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO TaskSetManager: Starting task 89.0 in stage 9.0 (TID 938, localhost, ANY, 1969 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 72.0 in stage 9.0 (TID 921) in 503 ms on localhost (73/200)
15/08/21 21:31:38 INFO Executor: Running task 89.0 in stage 9.0 (TID 938)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 73.0 in stage 9.0 (TID 922) in 486 ms on localhost (74/200)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO TaskSetManager: Starting task 90.0 in stage 9.0 (TID 939, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 74.0 in stage 9.0 (TID 923) in 472 ms on localhost (75/200)
15/08/21 21:31:38 INFO Executor: Running task 90.0 in stage 9.0 (TID 939)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15795
15/08/21 21:31:38 INFO Executor: Finished task 75.0 in stage 9.0 (TID 924). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 21:31:38 INFO TaskSetManager: Starting task 91.0 in stage 9.0 (TID 940, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO Executor: Running task 91.0 in stage 9.0 (TID 940)
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Finished task 75.0 in stage 9.0 (TID 924) in 474 ms on localhost (76/200)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15396
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO Executor: Finished task 76.0 in stage 9.0 (TID 925). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15023
15/08/21 21:31:38 INFO TaskSetManager: Starting task 92.0 in stage 9.0 (TID 941, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO Executor: Running task 92.0 in stage 9.0 (TID 941)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 76.0 in stage 9.0 (TID 925) in 505 ms on localhost (77/200)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO Executor: Finished task 77.0 in stage 9.0 (TID 926). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Starting task 93.0 in stage 9.0 (TID 942, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO Executor: Running task 93.0 in stage 9.0 (TID 942)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 77.0 in stage 9.0 (TID 926) in 514 ms on localhost (78/200)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14740
15/08/21 21:31:38 INFO Executor: Finished task 78.0 in stage 9.0 (TID 927). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO Executor: Finished task 79.0 in stage 9.0 (TID 928). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO TaskSetManager: Starting task 94.0 in stage 9.0 (TID 943, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO Executor: Running task 94.0 in stage 9.0 (TID 943)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO TaskSetManager: Starting task 95.0 in stage 9.0 (TID 944, localhost, ANY, 1968 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 78.0 in stage 9.0 (TID 927) in 550 ms on localhost (79/200)
15/08/21 21:31:38 INFO Executor: Running task 95.0 in stage 9.0 (TID 944)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 79.0 in stage 9.0 (TID 928) in 546 ms on localhost (80/200)
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO Executor: Finished task 80.0 in stage 9.0 (TID 929). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Starting task 96.0 in stage 9.0 (TID 945, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO Executor: Running task 96.0 in stage 9.0 (TID 945)
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO TaskSetManager: Finished task 80.0 in stage 9.0 (TID 929) in 551 ms on localhost (81/200)
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO Executor: Finished task 81.0 in stage 9.0 (TID 930). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14843
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15117
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO TaskSetManager: Starting task 97.0 in stage 9.0 (TID 946, localhost, ANY, 1970 bytes)
15/08/21 21:31:38 INFO Executor: Running task 97.0 in stage 9.0 (TID 946)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 81.0 in stage 9.0 (TID 930) in 557 ms on localhost (82/200)
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15614
15/08/21 21:31:38 INFO Executor: Finished task 82.0 in stage 9.0 (TID 931). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Starting task 98.0 in stage 9.0 (TID 947, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Running task 98.0 in stage 9.0 (TID 947)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 82.0 in stage 9.0 (TID 931) in 580 ms on localhost (83/200)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15364
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO Executor: Finished task 83.0 in stage 9.0 (TID 932). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14440
15/08/21 21:31:38 INFO TaskSetManager: Starting task 99.0 in stage 9.0 (TID 948, localhost, ANY, 1969 bytes)
15/08/21 21:31:38 INFO Executor: Running task 99.0 in stage 9.0 (TID 948)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 83.0 in stage 9.0 (TID 932) in 573 ms on localhost (84/200)
15/08/21 21:31:38 INFO Executor: Finished task 84.0 in stage 9.0 (TID 933). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 INFO TaskSetManager: Starting task 100.0 in stage 9.0 (TID 949, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Running task 100.0 in stage 9.0 (TID 949)
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Finished task 84.0 in stage 9.0 (TID 933) in 573 ms on localhost (85/200)
15/08/21 21:31:38 INFO Executor: Finished task 86.0 in stage 9.0 (TID 935). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14677
15/08/21 21:31:38 INFO TaskSetManager: Starting task 101.0 in stage 9.0 (TID 950, localhost, ANY, 1972 bytes)
15/08/21 21:31:38 INFO Executor: Running task 101.0 in stage 9.0 (TID 950)
15/08/21 21:31:38 INFO Executor: Finished task 85.0 in stage 9.0 (TID 934). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:38 INFO TaskSetManager: Starting task 102.0 in stage 9.0 (TID 951, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO Executor: Finished task 87.0 in stage 9.0 (TID 936). 2341 bytes result sent to driver
15/08/21 21:31:38 INFO Executor: Running task 102.0 in stage 9.0 (TID 951)
15/08/21 21:31:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 21:31:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15038
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:38 INFO TaskSetManager: Starting task 103.0 in stage 9.0 (TID 952, localhost, ANY, 1971 bytes)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 86.0 in stage 9.0 (TID 935) in 596 ms on localhost (86/200)
15/08/21 21:31:38 INFO Executor: Running task 103.0 in stage 9.0 (TID 952)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 85.0 in stage 9.0 (TID 934) in 622 ms on localhost (87/200)
15/08/21 21:31:38 INFO TaskSetManager: Finished task 87.0 in stage 9.0 (TID 936) in 569 ms on localhost (88/200)
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14991
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO Executor: Finished task 88.0 in stage 9.0 (TID 937). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15009
15/08/21 21:31:39 INFO Executor: Finished task 89.0 in stage 9.0 (TID 938). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Starting task 104.0 in stage 9.0 (TID 953, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO Executor: Running task 104.0 in stage 9.0 (TID 953)
15/08/21 21:31:39 INFO Executor: Finished task 90.0 in stage 9.0 (TID 939). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Starting task 105.0 in stage 9.0 (TID 954, localhost, ANY, 1970 bytes)
15/08/21 21:31:39 INFO Executor: Running task 105.0 in stage 9.0 (TID 954)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 88.0 in stage 9.0 (TID 937) in 598 ms on localhost (89/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 106.0 in stage 9.0 (TID 955, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 89.0 in stage 9.0 (TID 938) in 599 ms on localhost (90/200)
15/08/21 21:31:39 INFO Executor: Running task 106.0 in stage 9.0 (TID 955)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 90.0 in stage 9.0 (TID 939) in 578 ms on localhost (91/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14687
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO Executor: Finished task 91.0 in stage 9.0 (TID 940). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15038
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15404
15/08/21 21:31:39 INFO TaskSetManager: Starting task 107.0 in stage 9.0 (TID 956, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 INFO Executor: Running task 107.0 in stage 9.0 (TID 956)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 91.0 in stage 9.0 (TID 940) in 605 ms on localhost (92/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO Executor: Finished task 92.0 in stage 9.0 (TID 941). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Starting task 108.0 in stage 9.0 (TID 957, localhost, ANY, 1970 bytes)
15/08/21 21:31:39 INFO Executor: Running task 108.0 in stage 9.0 (TID 957)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 92.0 in stage 9.0 (TID 941) in 546 ms on localhost (93/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15500
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15098
15/08/21 21:31:39 INFO Executor: Finished task 93.0 in stage 9.0 (TID 942). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO Executor: Finished task 94.0 in stage 9.0 (TID 943). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Starting task 109.0 in stage 9.0 (TID 958, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO Executor: Running task 109.0 in stage 9.0 (TID 958)
15/08/21 21:31:39 INFO Executor: Finished task 95.0 in stage 9.0 (TID 944). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Starting task 110.0 in stage 9.0 (TID 959, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 INFO Executor: Running task 110.0 in stage 9.0 (TID 959)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 93.0 in stage 9.0 (TID 942) in 603 ms on localhost (94/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 111.0 in stage 9.0 (TID 960, localhost, ANY, 1970 bytes)
15/08/21 21:31:39 INFO Executor: Running task 111.0 in stage 9.0 (TID 960)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 94.0 in stage 9.0 (TID 943) in 612 ms on localhost (95/200)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 95.0 in stage 9.0 (TID 944) in 588 ms on localhost (96/200)
15/08/21 21:31:39 INFO Executor: Finished task 96.0 in stage 9.0 (TID 945). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO TaskSetManager: Starting task 112.0 in stage 9.0 (TID 961, localhost, ANY, 1970 bytes)
15/08/21 21:31:39 INFO Executor: Running task 112.0 in stage 9.0 (TID 961)
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 INFO TaskSetManager: Finished task 96.0 in stage 9.0 (TID 945) in 590 ms on localhost (97/200)
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15141
15/08/21 21:31:39 INFO Executor: Finished task 97.0 in stage 9.0 (TID 946). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15566
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 113.0 in stage 9.0 (TID 962, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO Executor: Running task 113.0 in stage 9.0 (TID 962)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 97.0 in stage 9.0 (TID 946) in 587 ms on localhost (98/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14597
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 21:31:39 INFO Executor: Finished task 98.0 in stage 9.0 (TID 947). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO TaskSetManager: Starting task 114.0 in stage 9.0 (TID 963, localhost, ANY, 1972 bytes)
15/08/21 21:31:39 INFO Executor: Running task 114.0 in stage 9.0 (TID 963)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 98.0 in stage 9.0 (TID 947) in 609 ms on localhost (99/200)
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 21:31:39 INFO Executor: Finished task 99.0 in stage 9.0 (TID 948). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 115.0 in stage 9.0 (TID 964, localhost, ANY, 1970 bytes)
15/08/21 21:31:39 INFO Executor: Running task 115.0 in stage 9.0 (TID 964)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 99.0 in stage 9.0 (TID 948) in 592 ms on localhost (100/200)
15/08/21 21:31:39 INFO Executor: Finished task 101.0 in stage 9.0 (TID 950). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO Executor: Finished task 100.0 in stage 9.0 (TID 949). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Starting task 116.0 in stage 9.0 (TID 965, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 INFO Executor: Running task 116.0 in stage 9.0 (TID 965)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 INFO Executor: Finished task 102.0 in stage 9.0 (TID 951). 2341 bytes result sent to driver
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO TaskSetManager: Starting task 117.0 in stage 9.0 (TID 966, localhost, ANY, 1967 bytes)
15/08/21 21:31:39 INFO Executor: Running task 117.0 in stage 9.0 (TID 966)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 101.0 in stage 9.0 (TID 950) in 581 ms on localhost (101/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14985
15/08/21 21:31:39 INFO Executor: Finished task 103.0 in stage 9.0 (TID 952). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 118.0 in stage 9.0 (TID 967, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 INFO Executor: Running task 118.0 in stage 9.0 (TID 967)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 100.0 in stage 9.0 (TID 949) in 610 ms on localhost (102/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 119.0 in stage 9.0 (TID 968, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 102.0 in stage 9.0 (TID 951) in 603 ms on localhost (103/200)
15/08/21 21:31:39 INFO Executor: Running task 119.0 in stage 9.0 (TID 968)
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO TaskSetManager: Finished task 103.0 in stage 9.0 (TID 952) in 580 ms on localhost (104/200)
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15211
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14919
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14884
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO Executor: Finished task 104.0 in stage 9.0 (TID 953). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15267
15/08/21 21:31:39 INFO TaskSetManager: Starting task 120.0 in stage 9.0 (TID 969, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO Executor: Running task 120.0 in stage 9.0 (TID 969)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 104.0 in stage 9.0 (TID 953) in 559 ms on localhost (105/200)
15/08/21 21:31:39 INFO Executor: Finished task 105.0 in stage 9.0 (TID 954). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO Executor: Finished task 106.0 in stage 9.0 (TID 955). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 121.0 in stage 9.0 (TID 970, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO Executor: Running task 121.0 in stage 9.0 (TID 970)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 122.0 in stage 9.0 (TID 971, localhost, ANY, 1968 bytes)
15/08/21 21:31:39 INFO Executor: Running task 122.0 in stage 9.0 (TID 971)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 105.0 in stage 9.0 (TID 954) in 587 ms on localhost (106/200)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 106.0 in stage 9.0 (TID 955) in 578 ms on localhost (107/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO Executor: Finished task 107.0 in stage 9.0 (TID 956). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15191
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 21:31:39 INFO TaskSetManager: Starting task 123.0 in stage 9.0 (TID 972, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO Executor: Running task 123.0 in stage 9.0 (TID 972)
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 INFO Executor: Finished task 108.0 in stage 9.0 (TID 957). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Finished task 107.0 in stage 9.0 (TID 956) in 571 ms on localhost (108/200)
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15432
15/08/21 21:31:39 INFO TaskSetManager: Starting task 124.0 in stage 9.0 (TID 973, localhost, ANY, 1970 bytes)
15/08/21 21:31:39 INFO Executor: Running task 124.0 in stage 9.0 (TID 973)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 108.0 in stage 9.0 (TID 957) in 575 ms on localhost (109/200)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15272
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14825
15/08/21 21:31:39 INFO Executor: Finished task 109.0 in stage 9.0 (TID 958). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO Executor: Finished task 110.0 in stage 9.0 (TID 959). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO TaskSetManager: Starting task 125.0 in stage 9.0 (TID 974, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 INFO Executor: Running task 125.0 in stage 9.0 (TID 974)
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 126.0 in stage 9.0 (TID 975, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 109.0 in stage 9.0 (TID 958) in 587 ms on localhost (110/200)
15/08/21 21:31:39 INFO Executor: Running task 126.0 in stage 9.0 (TID 975)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 110.0 in stage 9.0 (TID 959) in 579 ms on localhost (111/200)
15/08/21 21:31:39 INFO Executor: Finished task 111.0 in stage 9.0 (TID 960). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO TaskSetManager: Starting task 127.0 in stage 9.0 (TID 976, localhost, ANY, 1971 bytes)
15/08/21 21:31:39 INFO Executor: Running task 127.0 in stage 9.0 (TID 976)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 111.0 in stage 9.0 (TID 960) in 599 ms on localhost (112/200)
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14942
15/08/21 21:31:39 INFO Executor: Finished task 112.0 in stage 9.0 (TID 961). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO TaskSetManager: Starting task 128.0 in stage 9.0 (TID 977, localhost, ANY, 1968 bytes)
15/08/21 21:31:39 INFO Executor: Running task 128.0 in stage 9.0 (TID 977)
15/08/21 21:31:39 INFO Executor: Finished task 113.0 in stage 9.0 (TID 962). 2341 bytes result sent to driver
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15557
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:39 INFO TaskSetManager: Starting task 129.0 in stage 9.0 (TID 978, localhost, ANY, 1969 bytes)
15/08/21 21:31:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:39 INFO Executor: Running task 129.0 in stage 9.0 (TID 978)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 112.0 in stage 9.0 (TID 961) in 631 ms on localhost (113/200)
15/08/21 21:31:39 INFO TaskSetManager: Finished task 113.0 in stage 9.0 (TID 962) in 594 ms on localhost (114/200)
15/08/21 21:31:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 21:31:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14736
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO Executor: Finished task 114.0 in stage 9.0 (TID 963). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15532
15/08/21 21:31:40 INFO TaskSetManager: Starting task 130.0 in stage 9.0 (TID 979, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 130.0 in stage 9.0 (TID 979)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 114.0 in stage 9.0 (TID 963) in 586 ms on localhost (115/200)
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15150
15/08/21 21:31:40 INFO Executor: Finished task 115.0 in stage 9.0 (TID 964). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 131.0 in stage 9.0 (TID 980, localhost, ANY, 1969 bytes)
15/08/21 21:31:40 INFO Executor: Running task 131.0 in stage 9.0 (TID 980)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 115.0 in stage 9.0 (TID 964) in 600 ms on localhost (116/200)
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO Executor: Finished task 117.0 in stage 9.0 (TID 966). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO Executor: Finished task 116.0 in stage 9.0 (TID 965). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15183
15/08/21 21:31:40 INFO TaskSetManager: Starting task 132.0 in stage 9.0 (TID 981, localhost, ANY, 1970 bytes)
15/08/21 21:31:40 INFO Executor: Running task 132.0 in stage 9.0 (TID 981)
15/08/21 21:31:40 INFO Executor: Finished task 118.0 in stage 9.0 (TID 967). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO TaskSetManager: Starting task 133.0 in stage 9.0 (TID 982, localhost, ANY, 1970 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 117.0 in stage 9.0 (TID 966) in 591 ms on localhost (117/200)
15/08/21 21:31:40 INFO Executor: Running task 133.0 in stage 9.0 (TID 982)
15/08/21 21:31:40 INFO Executor: Finished task 119.0 in stage 9.0 (TID 968). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO TaskSetManager: Starting task 134.0 in stage 9.0 (TID 983, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 134.0 in stage 9.0 (TID 983)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 116.0 in stage 9.0 (TID 965) in 618 ms on localhost (118/200)
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15359
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO TaskSetManager: Starting task 135.0 in stage 9.0 (TID 984, localhost, ANY, 1968 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 118.0 in stage 9.0 (TID 967) in 614 ms on localhost (119/200)
15/08/21 21:31:40 INFO Executor: Running task 135.0 in stage 9.0 (TID 984)
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO TaskSetManager: Finished task 119.0 in stage 9.0 (TID 968) in 605 ms on localhost (120/200)
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15609
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14550
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15302
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14814
15/08/21 21:31:40 INFO Executor: Finished task 120.0 in stage 9.0 (TID 969). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO Executor: Finished task 121.0 in stage 9.0 (TID 970). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 136.0 in stage 9.0 (TID 985, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 136.0 in stage 9.0 (TID 985)
15/08/21 21:31:40 INFO Executor: Finished task 122.0 in stage 9.0 (TID 971). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 137.0 in stage 9.0 (TID 986, localhost, ANY, 1972 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 120.0 in stage 9.0 (TID 969) in 602 ms on localhost (121/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO Executor: Running task 137.0 in stage 9.0 (TID 986)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:40 INFO TaskSetManager: Starting task 138.0 in stage 9.0 (TID 987, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 121.0 in stage 9.0 (TID 970) in 600 ms on localhost (122/200)
15/08/21 21:31:40 INFO Executor: Running task 138.0 in stage 9.0 (TID 987)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 122.0 in stage 9.0 (TID 971) in 592 ms on localhost (123/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO Executor: Finished task 123.0 in stage 9.0 (TID 972). 2341 bytes result sent to driver
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14951
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14836
15/08/21 21:31:40 INFO TaskSetManager: Starting task 139.0 in stage 9.0 (TID 988, localhost, ANY, 1972 bytes)
15/08/21 21:31:40 INFO Executor: Running task 139.0 in stage 9.0 (TID 988)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 123.0 in stage 9.0 (TID 972) in 588 ms on localhost (124/200)
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO Executor: Finished task 124.0 in stage 9.0 (TID 973). 2341 bytes result sent to driver
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14802
15/08/21 21:31:40 INFO TaskSetManager: Starting task 140.0 in stage 9.0 (TID 989, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 140.0 in stage 9.0 (TID 989)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 124.0 in stage 9.0 (TID 973) in 606 ms on localhost (125/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15018
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO Executor: Finished task 125.0 in stage 9.0 (TID 974). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14788
15/08/21 21:31:40 INFO TaskSetManager: Starting task 141.0 in stage 9.0 (TID 990, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 141.0 in stage 9.0 (TID 990)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 125.0 in stage 9.0 (TID 974) in 572 ms on localhost (126/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO Executor: Finished task 126.0 in stage 9.0 (TID 975). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 142.0 in stage 9.0 (TID 991, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 142.0 in stage 9.0 (TID 991)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 126.0 in stage 9.0 (TID 975) in 583 ms on localhost (127/200)
15/08/21 21:31:40 INFO Executor: Finished task 127.0 in stage 9.0 (TID 976). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO Executor: Finished task 128.0 in stage 9.0 (TID 977). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO TaskSetManager: Starting task 143.0 in stage 9.0 (TID 992, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 143.0 in stage 9.0 (TID 992)
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14913
15/08/21 21:31:40 INFO Executor: Finished task 129.0 in stage 9.0 (TID 978). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:40 INFO TaskSetManager: Starting task 144.0 in stage 9.0 (TID 993, localhost, ANY, 1970 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 127.0 in stage 9.0 (TID 976) in 606 ms on localhost (128/200)
15/08/21 21:31:40 INFO Executor: Running task 144.0 in stage 9.0 (TID 993)
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO TaskSetManager: Starting task 145.0 in stage 9.0 (TID 994, localhost, ANY, 1970 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 128.0 in stage 9.0 (TID 977) in 585 ms on localhost (129/200)
15/08/21 21:31:40 INFO Executor: Running task 145.0 in stage 9.0 (TID 994)
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15358
15/08/21 21:31:40 INFO TaskSetManager: Finished task 129.0 in stage 9.0 (TID 978) in 560 ms on localhost (130/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO Executor: Finished task 130.0 in stage 9.0 (TID 979). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14564
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO TaskSetManager: Starting task 146.0 in stage 9.0 (TID 995, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 146.0 in stage 9.0 (TID 995)
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15079
15/08/21 21:31:40 INFO TaskSetManager: Finished task 130.0 in stage 9.0 (TID 979) in 549 ms on localhost (131/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14923
15/08/21 21:31:40 INFO Executor: Finished task 131.0 in stage 9.0 (TID 980). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO Executor: Finished task 132.0 in stage 9.0 (TID 981). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 147.0 in stage 9.0 (TID 996, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 147.0 in stage 9.0 (TID 996)
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14508
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO TaskSetManager: Starting task 148.0 in stage 9.0 (TID 997, localhost, ANY, 1968 bytes)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO TaskSetManager: Finished task 131.0 in stage 9.0 (TID 980) in 597 ms on localhost (132/200)
15/08/21 21:31:40 INFO Executor: Running task 148.0 in stage 9.0 (TID 997)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 132.0 in stage 9.0 (TID 981) in 578 ms on localhost (133/200)
15/08/21 21:31:40 INFO Executor: Finished task 135.0 in stage 9.0 (TID 984). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO Executor: Finished task 133.0 in stage 9.0 (TID 982). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO Executor: Finished task 134.0 in stage 9.0 (TID 983). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 149.0 in stage 9.0 (TID 998, localhost, ANY, 1970 bytes)
15/08/21 21:31:40 INFO Executor: Running task 149.0 in stage 9.0 (TID 998)
15/08/21 21:31:40 INFO TaskSetManager: Starting task 150.0 in stage 9.0 (TID 999, localhost, ANY, 1970 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 135.0 in stage 9.0 (TID 984) in 567 ms on localhost (134/200)
15/08/21 21:31:40 INFO Executor: Running task 150.0 in stage 9.0 (TID 999)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO TaskSetManager: Starting task 151.0 in stage 9.0 (TID 1000, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 133.0 in stage 9.0 (TID 982) in 611 ms on localhost (135/200)
15/08/21 21:31:40 INFO Executor: Running task 151.0 in stage 9.0 (TID 1000)
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO TaskSetManager: Finished task 134.0 in stage 9.0 (TID 983) in 601 ms on localhost (136/200)
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15107
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15302
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14560
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14569
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14848
15/08/21 21:31:40 INFO Executor: Finished task 137.0 in stage 9.0 (TID 986). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO Executor: Finished task 136.0 in stage 9.0 (TID 985). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 152.0 in stage 9.0 (TID 1001, localhost, ANY, 1972 bytes)
15/08/21 21:31:40 INFO Executor: Running task 152.0 in stage 9.0 (TID 1001)
15/08/21 21:31:40 INFO Executor: Finished task 138.0 in stage 9.0 (TID 987). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 153.0 in stage 9.0 (TID 1002, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 153.0 in stage 9.0 (TID 1002)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 137.0 in stage 9.0 (TID 986) in 622 ms on localhost (137/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO TaskSetManager: Starting task 154.0 in stage 9.0 (TID 1003, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 136.0 in stage 9.0 (TID 985) in 640 ms on localhost (138/200)
15/08/21 21:31:40 INFO Executor: Running task 154.0 in stage 9.0 (TID 1003)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 138.0 in stage 9.0 (TID 987) in 620 ms on localhost (139/200)
15/08/21 21:31:40 INFO Executor: Finished task 139.0 in stage 9.0 (TID 988). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO TaskSetManager: Starting task 155.0 in stage 9.0 (TID 1004, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO Executor: Running task 155.0 in stage 9.0 (TID 1004)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 139.0 in stage 9.0 (TID 988) in 584 ms on localhost (140/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14878
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14838
15/08/21 21:31:40 INFO Executor: Finished task 140.0 in stage 9.0 (TID 989). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO TaskSetManager: Starting task 156.0 in stage 9.0 (TID 1005, localhost, ANY, 1971 bytes)
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 21:31:40 INFO Executor: Running task 156.0 in stage 9.0 (TID 1005)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 140.0 in stage 9.0 (TID 989) in 606 ms on localhost (141/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO Executor: Finished task 141.0 in stage 9.0 (TID 990). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO TaskSetManager: Starting task 157.0 in stage 9.0 (TID 1006, localhost, ANY, 1969 bytes)
15/08/21 21:31:40 INFO Executor: Running task 157.0 in stage 9.0 (TID 1006)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 141.0 in stage 9.0 (TID 990) in 564 ms on localhost (142/200)
15/08/21 21:31:40 INFO Executor: Finished task 142.0 in stage 9.0 (TID 991). 2341 bytes result sent to driver
15/08/21 21:31:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 21:31:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15001
15/08/21 21:31:40 INFO TaskSetManager: Starting task 158.0 in stage 9.0 (TID 1007, localhost, ANY, 1969 bytes)
15/08/21 21:31:40 INFO Executor: Running task 158.0 in stage 9.0 (TID 1007)
15/08/21 21:31:40 INFO TaskSetManager: Finished task 142.0 in stage 9.0 (TID 991) in 568 ms on localhost (143/200)
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:40 INFO Executor: Finished task 143.0 in stage 9.0 (TID 992). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO Executor: Finished task 144.0 in stage 9.0 (TID 993). 2341 bytes result sent to driver
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO TaskSetManager: Starting task 159.0 in stage 9.0 (TID 1008, localhost, ANY, 1968 bytes)
15/08/21 21:31:41 INFO Executor: Running task 159.0 in stage 9.0 (TID 1008)
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14793
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 160.0 in stage 9.0 (TID 1009, localhost, ANY, 1972 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 143.0 in stage 9.0 (TID 992) in 569 ms on localhost (144/200)
15/08/21 21:31:41 INFO Executor: Running task 160.0 in stage 9.0 (TID 1009)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 144.0 in stage 9.0 (TID 993) in 558 ms on localhost (145/200)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15203
15/08/21 21:31:41 INFO Executor: Finished task 145.0 in stage 9.0 (TID 994). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 161.0 in stage 9.0 (TID 1010, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO Executor: Running task 161.0 in stage 9.0 (TID 1010)
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO TaskSetManager: Finished task 145.0 in stage 9.0 (TID 994) in 570 ms on localhost (146/200)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15586
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15029
15/08/21 21:31:41 INFO Executor: Finished task 146.0 in stage 9.0 (TID 995). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 162.0 in stage 9.0 (TID 1011, localhost, ANY, 1970 bytes)
15/08/21 21:31:41 INFO Executor: Running task 162.0 in stage 9.0 (TID 1011)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 146.0 in stage 9.0 (TID 995) in 552 ms on localhost (147/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO Executor: Finished task 147.0 in stage 9.0 (TID 996). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14816
15/08/21 21:31:41 INFO TaskSetManager: Starting task 163.0 in stage 9.0 (TID 1012, localhost, ANY, 1969 bytes)
15/08/21 21:31:41 INFO Executor: Running task 163.0 in stage 9.0 (TID 1012)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO TaskSetManager: Finished task 147.0 in stage 9.0 (TID 996) in 518 ms on localhost (148/200)
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 21:31:41 INFO Executor: Finished task 148.0 in stage 9.0 (TID 997). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 164.0 in stage 9.0 (TID 1013, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 164.0 in stage 9.0 (TID 1013)
15/08/21 21:31:41 INFO Executor: Finished task 150.0 in stage 9.0 (TID 999). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Finished task 148.0 in stage 9.0 (TID 997) in 542 ms on localhost (149/200)
15/08/21 21:31:41 INFO Executor: Finished task 149.0 in stage 9.0 (TID 998). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 165.0 in stage 9.0 (TID 1014, localhost, ANY, 1970 bytes)
15/08/21 21:31:41 INFO Executor: Running task 165.0 in stage 9.0 (TID 1014)
15/08/21 21:31:41 INFO Executor: Finished task 151.0 in stage 9.0 (TID 1000). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 166.0 in stage 9.0 (TID 1015, localhost, ANY, 1970 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 150.0 in stage 9.0 (TID 999) in 526 ms on localhost (150/200)
15/08/21 21:31:41 INFO Executor: Running task 166.0 in stage 9.0 (TID 1015)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 167.0 in stage 9.0 (TID 1016, localhost, ANY, 1967 bytes)
15/08/21 21:31:41 INFO Executor: Running task 167.0 in stage 9.0 (TID 1016)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 149.0 in stage 9.0 (TID 998) in 569 ms on localhost (151/200)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 151.0 in stage 9.0 (TID 1000) in 541 ms on localhost (152/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14934
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15058
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14876
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15679
15/08/21 21:31:41 INFO Executor: Finished task 153.0 in stage 9.0 (TID 1002). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO Executor: Finished task 152.0 in stage 9.0 (TID 1001). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 168.0 in stage 9.0 (TID 1017, localhost, ANY, 1969 bytes)
15/08/21 21:31:41 INFO Executor: Running task 168.0 in stage 9.0 (TID 1017)
15/08/21 21:31:41 INFO Executor: Finished task 154.0 in stage 9.0 (TID 1003). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 169.0 in stage 9.0 (TID 1018, localhost, ANY, 1970 bytes)
15/08/21 21:31:41 INFO Executor: Running task 169.0 in stage 9.0 (TID 1018)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 153.0 in stage 9.0 (TID 1002) in 556 ms on localhost (153/200)
15/08/21 21:31:41 INFO Executor: Finished task 155.0 in stage 9.0 (TID 1004). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 170.0 in stage 9.0 (TID 1019, localhost, ANY, 1972 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 152.0 in stage 9.0 (TID 1001) in 578 ms on localhost (154/200)
15/08/21 21:31:41 INFO Executor: Running task 170.0 in stage 9.0 (TID 1019)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 171.0 in stage 9.0 (TID 1020, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 171.0 in stage 9.0 (TID 1020)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 154.0 in stage 9.0 (TID 1003) in 564 ms on localhost (155/200)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 155.0 in stage 9.0 (TID 1004) in 552 ms on localhost (156/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15282
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15474
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15055
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15096
15/08/21 21:31:41 INFO Executor: Finished task 156.0 in stage 9.0 (TID 1005). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 172.0 in stage 9.0 (TID 1021, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 172.0 in stage 9.0 (TID 1021)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 156.0 in stage 9.0 (TID 1005) in 559 ms on localhost (157/200)
15/08/21 21:31:41 INFO Executor: Finished task 157.0 in stage 9.0 (TID 1006). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO Executor: Finished task 158.0 in stage 9.0 (TID 1007). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 173.0 in stage 9.0 (TID 1022, localhost, ANY, 1969 bytes)
15/08/21 21:31:41 INFO Executor: Running task 173.0 in stage 9.0 (TID 1022)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 157.0 in stage 9.0 (TID 1006) in 539 ms on localhost (158/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 174.0 in stage 9.0 (TID 1023, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 158.0 in stage 9.0 (TID 1007) in 549 ms on localhost (159/200)
15/08/21 21:31:41 INFO Executor: Running task 174.0 in stage 9.0 (TID 1023)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO Executor: Finished task 160.0 in stage 9.0 (TID 1009). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14883
15/08/21 21:31:41 INFO Executor: Finished task 159.0 in stage 9.0 (TID 1008). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14819
15/08/21 21:31:41 INFO TaskSetManager: Starting task 175.0 in stage 9.0 (TID 1024, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 175.0 in stage 9.0 (TID 1024)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO TaskSetManager: Starting task 176.0 in stage 9.0 (TID 1025, localhost, ANY, 1968 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 160.0 in stage 9.0 (TID 1009) in 545 ms on localhost (160/200)
15/08/21 21:31:41 INFO Executor: Running task 176.0 in stage 9.0 (TID 1025)
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO TaskSetManager: Finished task 159.0 in stage 9.0 (TID 1008) in 556 ms on localhost (161/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14879
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO Executor: Finished task 161.0 in stage 9.0 (TID 1010). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Finished task 161.0 in stage 9.0 (TID 1010) in 546 ms on localhost (162/200)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO Executor: Finished task 162.0 in stage 9.0 (TID 1011). 2341 bytes result sent to driver
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO TaskSetManager: Starting task 177.0 in stage 9.0 (TID 1026, localhost, ANY, 1969 bytes)
15/08/21 21:31:41 INFO Executor: Running task 177.0 in stage 9.0 (TID 1026)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15238
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14751
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 178.0 in stage 9.0 (TID 1027, localhost, ANY, 1970 bytes)
15/08/21 21:31:41 INFO Executor: Running task 178.0 in stage 9.0 (TID 1027)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 162.0 in stage 9.0 (TID 1011) in 538 ms on localhost (163/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO Executor: Finished task 163.0 in stage 9.0 (TID 1012). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 179.0 in stage 9.0 (TID 1028, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 179.0 in stage 9.0 (TID 1028)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 163.0 in stage 9.0 (TID 1012) in 534 ms on localhost (164/200)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15549
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO Executor: Finished task 164.0 in stage 9.0 (TID 1013). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14914
15/08/21 21:31:41 INFO Executor: Finished task 165.0 in stage 9.0 (TID 1014). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 180.0 in stage 9.0 (TID 1029, localhost, ANY, 1967 bytes)
15/08/21 21:31:41 INFO Executor: Running task 180.0 in stage 9.0 (TID 1029)
15/08/21 21:31:41 INFO Executor: Finished task 166.0 in stage 9.0 (TID 1015). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO Executor: Finished task 167.0 in stage 9.0 (TID 1016). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 181.0 in stage 9.0 (TID 1030, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO Executor: Running task 181.0 in stage 9.0 (TID 1030)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 164.0 in stage 9.0 (TID 1013) in 542 ms on localhost (165/200)
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 182.0 in stage 9.0 (TID 1031, localhost, ANY, 1969 bytes)
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO Executor: Running task 182.0 in stage 9.0 (TID 1031)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 165.0 in stage 9.0 (TID 1014) in 537 ms on localhost (166/200)
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14992
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 183.0 in stage 9.0 (TID 1032, localhost, ANY, 1970 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 166.0 in stage 9.0 (TID 1015) in 537 ms on localhost (167/200)
15/08/21 21:31:41 INFO Executor: Running task 183.0 in stage 9.0 (TID 1032)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 167.0 in stage 9.0 (TID 1016) in 519 ms on localhost (168/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15658
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14938
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14891
15/08/21 21:31:41 INFO Executor: Finished task 168.0 in stage 9.0 (TID 1017). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO Executor: Finished task 169.0 in stage 9.0 (TID 1018). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 184.0 in stage 9.0 (TID 1033, localhost, ANY, 1970 bytes)
15/08/21 21:31:41 INFO Executor: Running task 184.0 in stage 9.0 (TID 1033)
15/08/21 21:31:41 INFO Executor: Finished task 171.0 in stage 9.0 (TID 1020). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO Executor: Finished task 170.0 in stage 9.0 (TID 1019). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 185.0 in stage 9.0 (TID 1034, localhost, ANY, 1969 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 168.0 in stage 9.0 (TID 1017) in 520 ms on localhost (169/200)
15/08/21 21:31:41 INFO Executor: Running task 185.0 in stage 9.0 (TID 1034)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 169.0 in stage 9.0 (TID 1018) in 498 ms on localhost (170/200)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 171.0 in stage 9.0 (TID 1020) in 473 ms on localhost (171/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 186.0 in stage 9.0 (TID 1035, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 186.0 in stage 9.0 (TID 1035)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 187.0 in stage 9.0 (TID 1036, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 187.0 in stage 9.0 (TID 1036)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 170.0 in stage 9.0 (TID 1019) in 518 ms on localhost (172/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14896
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14379
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15235
15/08/21 21:31:41 INFO Executor: Finished task 172.0 in stage 9.0 (TID 1021). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 21:31:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15330
15/08/21 21:31:41 INFO Executor: Finished task 173.0 in stage 9.0 (TID 1022). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 188.0 in stage 9.0 (TID 1037, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 188.0 in stage 9.0 (TID 1037)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO TaskSetManager: Starting task 189.0 in stage 9.0 (TID 1038, localhost, ANY, 1967 bytes)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 172.0 in stage 9.0 (TID 1021) in 526 ms on localhost (173/200)
15/08/21 21:31:41 INFO Executor: Running task 189.0 in stage 9.0 (TID 1038)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 173.0 in stage 9.0 (TID 1022) in 502 ms on localhost (174/200)
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:41 INFO Executor: Finished task 174.0 in stage 9.0 (TID 1023). 2341 bytes result sent to driver
15/08/21 21:31:41 INFO TaskSetManager: Starting task 190.0 in stage 9.0 (TID 1039, localhost, ANY, 1971 bytes)
15/08/21 21:31:41 INFO Executor: Running task 190.0 in stage 9.0 (TID 1039)
15/08/21 21:31:41 INFO TaskSetManager: Finished task 174.0 in stage 9.0 (TID 1023) in 507 ms on localhost (175/200)
15/08/21 21:31:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15101
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15463
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO Executor: Finished task 175.0 in stage 9.0 (TID 1024). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO Executor: Finished task 176.0 in stage 9.0 (TID 1025). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14845
15/08/21 21:31:42 INFO TaskSetManager: Starting task 191.0 in stage 9.0 (TID 1040, localhost, ANY, 1972 bytes)
15/08/21 21:31:42 INFO Executor: Running task 191.0 in stage 9.0 (TID 1040)
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:42 INFO TaskSetManager: Starting task 192.0 in stage 9.0 (TID 1041, localhost, ANY, 1971 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 175.0 in stage 9.0 (TID 1024) in 543 ms on localhost (176/200)
15/08/21 21:31:42 INFO Executor: Running task 192.0 in stage 9.0 (TID 1041)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 176.0 in stage 9.0 (TID 1025) in 532 ms on localhost (177/200)
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO Executor: Finished task 177.0 in stage 9.0 (TID 1026). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15010
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15103
15/08/21 21:31:42 INFO Executor: Finished task 178.0 in stage 9.0 (TID 1027). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Starting task 193.0 in stage 9.0 (TID 1042, localhost, ANY, 1969 bytes)
15/08/21 21:31:42 INFO Executor: Running task 193.0 in stage 9.0 (TID 1042)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 177.0 in stage 9.0 (TID 1026) in 527 ms on localhost (178/200)
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:42 INFO TaskSetManager: Starting task 194.0 in stage 9.0 (TID 1043, localhost, ANY, 1969 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 178.0 in stage 9.0 (TID 1027) in 535 ms on localhost (179/200)
15/08/21 21:31:42 INFO Executor: Running task 194.0 in stage 9.0 (TID 1043)
15/08/21 21:31:42 INFO Executor: Finished task 179.0 in stage 9.0 (TID 1028). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:42 INFO TaskSetManager: Starting task 195.0 in stage 9.0 (TID 1044, localhost, ANY, 1971 bytes)
15/08/21 21:31:42 INFO Executor: Running task 195.0 in stage 9.0 (TID 1044)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 179.0 in stage 9.0 (TID 1028) in 517 ms on localhost (180/200)
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO Executor: Finished task 181.0 in stage 9.0 (TID 1030). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15272
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO TaskSetManager: Starting task 196.0 in stage 9.0 (TID 1045, localhost, ANY, 1972 bytes)
15/08/21 21:31:42 INFO Executor: Running task 196.0 in stage 9.0 (TID 1045)
15/08/21 21:31:42 INFO Executor: Finished task 180.0 in stage 9.0 (TID 1029). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 21:31:42 INFO Executor: Finished task 182.0 in stage 9.0 (TID 1031). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:42 INFO TaskSetManager: Starting task 197.0 in stage 9.0 (TID 1046, localhost, ANY, 1968 bytes)
15/08/21 21:31:42 INFO Executor: Running task 197.0 in stage 9.0 (TID 1046)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 181.0 in stage 9.0 (TID 1030) in 527 ms on localhost (181/200)
15/08/21 21:31:42 INFO Executor: Finished task 183.0 in stage 9.0 (TID 1032). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:42 INFO TaskSetManager: Starting task 198.0 in stage 9.0 (TID 1047, localhost, ANY, 1969 bytes)
15/08/21 21:31:42 INFO Executor: Running task 198.0 in stage 9.0 (TID 1047)
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO TaskSetManager: Starting task 199.0 in stage 9.0 (TID 1048, localhost, ANY, 1971 bytes)
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:42 INFO TaskSetManager: Finished task 180.0 in stage 9.0 (TID 1029) in 555 ms on localhost (182/200)
15/08/21 21:31:42 INFO Executor: Running task 199.0 in stage 9.0 (TID 1048)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 182.0 in stage 9.0 (TID 1031) in 544 ms on localhost (183/200)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 183.0 in stage 9.0 (TID 1032) in 534 ms on localhost (184/200)
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14772
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15726
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15164
15/08/21 21:31:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-df96c873-c3d8-4d10-af4d-3cecae8ed47c.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 21:31:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 21:31:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 21:31:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 21:31:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15137
15/08/21 21:31:42 INFO Executor: Finished task 184.0 in stage 9.0 (TID 1033). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO Executor: Finished task 185.0 in stage 9.0 (TID 1034). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO Executor: Finished task 186.0 in stage 9.0 (TID 1035). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 184.0 in stage 9.0 (TID 1033) in 510 ms on localhost (185/200)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 185.0 in stage 9.0 (TID 1034) in 504 ms on localhost (186/200)
15/08/21 21:31:42 INFO Executor: Finished task 188.0 in stage 9.0 (TID 1037). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 186.0 in stage 9.0 (TID 1035) in 491 ms on localhost (187/200)
15/08/21 21:31:42 INFO Executor: Finished task 187.0 in stage 9.0 (TID 1036). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 188.0 in stage 9.0 (TID 1037) in 414 ms on localhost (188/200)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 187.0 in stage 9.0 (TID 1036) in 484 ms on localhost (189/200)
15/08/21 21:31:42 INFO Executor: Finished task 189.0 in stage 9.0 (TID 1038). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 189.0 in stage 9.0 (TID 1038) in 468 ms on localhost (190/200)
15/08/21 21:31:42 INFO Executor: Finished task 190.0 in stage 9.0 (TID 1039). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 190.0 in stage 9.0 (TID 1039) in 444 ms on localhost (191/200)
15/08/21 21:31:42 INFO Executor: Finished task 191.0 in stage 9.0 (TID 1040). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 191.0 in stage 9.0 (TID 1040) in 410 ms on localhost (192/200)
15/08/21 21:31:42 INFO Executor: Finished task 192.0 in stage 9.0 (TID 1041). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 192.0 in stage 9.0 (TID 1041) in 394 ms on localhost (193/200)
15/08/21 21:31:42 INFO Executor: Finished task 193.0 in stage 9.0 (TID 1042). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 193.0 in stage 9.0 (TID 1042) in 368 ms on localhost (194/200)
15/08/21 21:31:42 INFO Executor: Finished task 194.0 in stage 9.0 (TID 1043). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 194.0 in stage 9.0 (TID 1043) in 361 ms on localhost (195/200)
15/08/21 21:31:42 INFO Executor: Finished task 195.0 in stage 9.0 (TID 1044). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 195.0 in stage 9.0 (TID 1044) in 360 ms on localhost (196/200)
15/08/21 21:31:42 INFO Executor: Finished task 196.0 in stage 9.0 (TID 1045). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO Executor: Finished task 197.0 in stage 9.0 (TID 1046). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 197.0 in stage 9.0 (TID 1046) in 318 ms on localhost (197/200)
15/08/21 21:31:42 INFO Executor: Finished task 199.0 in stage 9.0 (TID 1048). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO Executor: Finished task 198.0 in stage 9.0 (TID 1047). 2341 bytes result sent to driver
15/08/21 21:31:42 INFO TaskSetManager: Finished task 196.0 in stage 9.0 (TID 1045) in 328 ms on localhost (198/200)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 199.0 in stage 9.0 (TID 1048) in 293 ms on localhost (199/200)
15/08/21 21:31:42 INFO TaskSetManager: Finished task 198.0 in stage 9.0 (TID 1047) in 306 ms on localhost (200/200)
15/08/21 21:31:42 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/21 21:31:42 INFO DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:423) finished in 7.274 s
15/08/21 21:31:42 INFO DAGScheduler: looking for newly runnable stages
15/08/21 21:31:42 INFO DAGScheduler: running: Set()
15/08/21 21:31:42 INFO DAGScheduler: waiting: Set(ResultStage 10)
15/08/21 21:31:42 INFO DAGScheduler: failed: Set()
15/08/21 21:31:42 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1d4699d9
15/08/21 21:31:42 INFO StatsReportListener: task runtime:(count: 200, mean: 595.615000, stdev: 121.948336, max: 1036.000000, min: 293.000000)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	293.0 ms	444.0 ms	498.0 ms	540.0 ms	584.0 ms	621.0 ms	695.0 ms	925.0 ms	1.0 s
15/08/21 21:31:42 INFO DAGScheduler: Missing parents for ResultStage 10: List()
15/08/21 21:31:42 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 21:31:42 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 10842.835000, stdev: 378.879001, max: 11730.000000, min: 9841.000000)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	9.6 KB	10.0 KB	10.1 KB	10.3 KB	10.6 KB	10.8 KB	11.1 KB	11.3 KB	11.5 KB
15/08/21 21:31:42 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.165000, stdev: 0.384415, max: 2.000000, min: 0.000000)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/21 21:31:42 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 21:31:42 INFO StatsReportListener: task result size:(count: 200, mean: 2341.000000, stdev: 0.000000, max: 2341.000000, min: 2341.000000)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB
15/08/21 21:31:42 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 90.103093, stdev: 2.348555, max: 94.313725, min: 81.863980)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	82 %	86 %	87 %	89 %	90 %	92 %	93 %	94 %	94 %
15/08/21 21:31:42 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.026685, stdev: 0.062758, max: 0.308166, min: 0.000000)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 21:31:42 INFO StatsReportListener: other time pct: (count: 200, mean: 9.870222, stdev: 2.339591, max: 18.136020, min: 5.686275)
15/08/21 21:31:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:42 INFO StatsReportListener: 	 6 %	 6 %	 7 %	 8 %	10 %	11 %	13 %	14 %	18 %
15/08/21 21:31:42 INFO MemoryStore: ensureFreeSpace(84240) called with curMem=1920351, maxMem=22226833244
15/08/21 21:31:42 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 82.3 KB, free 20.7 GB)
15/08/21 21:31:42 INFO MemoryStore: ensureFreeSpace(33446) called with curMem=2004591, maxMem=22226833244
15/08/21 21:31:42 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 32.7 KB, free 20.7 GB)
15/08/21 21:31:42 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:52592 (size: 32.7 KB, free: 20.7 GB)
15/08/21 21:31:42 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874
15/08/21 21:31:42 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/21 21:31:42 INFO TaskSchedulerImpl: Adding task set 10.0 with 200 tasks
15/08/21 21:31:42 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1049, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 1050, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 1051, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 1052, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 1053, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 1054, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 1055, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 1056, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 1057, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 1058, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 1059, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 1060, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 1061, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 1062, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 1063, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 1064, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:42 INFO Executor: Running task 8.0 in stage 10.0 (TID 1057)
15/08/21 21:31:42 INFO Executor: Running task 12.0 in stage 10.0 (TID 1061)
15/08/21 21:31:42 INFO Executor: Running task 1.0 in stage 10.0 (TID 1050)
15/08/21 21:31:42 INFO Executor: Running task 5.0 in stage 10.0 (TID 1054)
15/08/21 21:31:42 INFO Executor: Running task 11.0 in stage 10.0 (TID 1060)
15/08/21 21:31:42 INFO Executor: Running task 15.0 in stage 10.0 (TID 1064)
15/08/21 21:31:42 INFO Executor: Running task 13.0 in stage 10.0 (TID 1062)
15/08/21 21:31:42 INFO Executor: Running task 3.0 in stage 10.0 (TID 1052)
15/08/21 21:31:42 INFO Executor: Running task 7.0 in stage 10.0 (TID 1056)
15/08/21 21:31:42 INFO Executor: Running task 2.0 in stage 10.0 (TID 1051)
15/08/21 21:31:42 INFO Executor: Running task 9.0 in stage 10.0 (TID 1058)
15/08/21 21:31:42 INFO Executor: Running task 0.0 in stage 10.0 (TID 1049)
15/08/21 21:31:42 INFO Executor: Running task 4.0 in stage 10.0 (TID 1053)
15/08/21 21:31:42 INFO Executor: Running task 14.0 in stage 10.0 (TID 1063)
15/08/21 21:31:42 INFO Executor: Running task 6.0 in stage 10.0 (TID 1055)
15/08/21 21:31:42 INFO Executor: Running task 10.0 in stage 10.0 (TID 1059)
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 21:31:43 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:52592 in memory (size: 8.3 KB, free: 20.7 GB)
15/08/21 21:31:43 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:52592 in memory (size: 5.6 KB, free: 20.7 GB)
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:52592 in memory (size: 3.8 KB, free: 20.7 GB)
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:52592 in memory (size: 1776.0 B, free: 20.7 GB)
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:52592 in memory (size: 31.0 KB, free: 20.7 GB)
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:43 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:43 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,516
15/08/21 21:31:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,716B for [ps_partkey] INT32: 462 values, 1,855B raw, 1,680B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,330B for [value] DOUBLE: 462 values, 3,703B raw, 2,286B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,996
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,036
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,629B for [ps_partkey] INT32: 436 values, 1,751B raw, 1,593B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,178B for [value] DOUBLE: 436 values, 3,495B raw, 2,134B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,802B for [ps_partkey] INT32: 488 values, 1,959B raw, 1,766B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,421B for [value] DOUBLE: 488 values, 3,911B raw, 2,377B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,896
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,156
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,603B for [ps_partkey] INT32: 431 values, 1,731B raw, 1,567B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,202B for [value] DOUBLE: 431 values, 3,455B raw, 2,158B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,876
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,416
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,310B for [ps_partkey] INT32: 344 values, 1,383B raw, 1,274B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,909B for [value] DOUBLE: 344 values, 2,759B raw, 1,865B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,356
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,736
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,606B for [ps_partkey] INT32: 430 values, 1,727B raw, 1,570B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,166B for [value] DOUBLE: 430 values, 3,447B raw, 2,122B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,876B for [ps_partkey] INT32: 507 values, 2,035B raw, 1,840B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,535B for [value] DOUBLE: 507 values, 4,063B raw, 2,491B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,076
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,016
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,690B for [ps_partkey] INT32: 454 values, 1,823B raw, 1,654B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,933B for [ps_partkey] INT32: 523 values, 2,099B raw, 1,897B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,320B for [value] DOUBLE: 454 values, 3,639B raw, 2,276B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,874B for [value] DOUBLE: 523 values, 4,191B raw, 2,830B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,468B for [ps_partkey] INT32: 390 values, 1,567B raw, 1,432B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,056B for [value] DOUBLE: 390 values, 3,127B raw, 2,012B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,985B for [ps_partkey] INT32: 537 values, 2,155B raw, 1,949B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,729B for [value] DOUBLE: 537 values, 4,303B raw, 2,685B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,676
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,156
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,749B for [ps_partkey] INT32: 470 values, 1,887B raw, 1,713B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,404B for [value] DOUBLE: 470 values, 3,767B raw, 2,360B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,655B for [ps_partkey] INT32: 444 values, 1,783B raw, 1,619B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,239B for [value] DOUBLE: 444 values, 3,559B raw, 2,195B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,316
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,536
15/08/21 21:31:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,356
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,514B for [ps_partkey] INT32: 402 values, 1,615B raw, 1,478B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,069B for [value] DOUBLE: 402 values, 3,223B raw, 2,025B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 1,554B for [ps_partkey] INT32: 413 values, 1,659B raw, 1,518B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,099B for [value] DOUBLE: 413 values, 3,311B raw, 2,055B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 2,214B for [ps_partkey] INT32: 604 values, 2,423B raw, 2,178B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO ColumnChunkPageWriteStore: written 3,128B for [value] DOUBLE: 604 values, 4,839B raw, 3,084B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000008
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000014
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000015
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000008_0: Committed
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000015_0: Committed
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000006
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000006_0: Committed
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000014_0: Committed
15/08/21 21:31:43 INFO Executor: Finished task 14.0 in stage 10.0 (TID 1063). 843 bytes result sent to driver
15/08/21 21:31:43 INFO Executor: Finished task 6.0 in stage 10.0 (TID 1055). 843 bytes result sent to driver
15/08/21 21:31:43 INFO Executor: Finished task 8.0 in stage 10.0 (TID 1057). 843 bytes result sent to driver
15/08/21 21:31:43 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 1065, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Finished task 15.0 in stage 10.0 (TID 1064). 843 bytes result sent to driver
15/08/21 21:31:43 INFO Executor: Running task 16.0 in stage 10.0 (TID 1065)
15/08/21 21:31:43 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 1066, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 17.0 in stage 10.0 (TID 1066)
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000001
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000001_0: Committed
15/08/21 21:31:43 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 1067, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 1068, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 19.0 in stage 10.0 (TID 1068)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 1063) in 1354 ms on localhost (1/200)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 1055) in 1356 ms on localhost (2/200)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 1057) in 1356 ms on localhost (3/200)
15/08/21 21:31:43 INFO Executor: Finished task 1.0 in stage 10.0 (TID 1050). 843 bytes result sent to driver
15/08/21 21:31:43 INFO Executor: Running task 18.0 in stage 10.0 (TID 1067)
15/08/21 21:31:43 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 1069, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 20.0 in stage 10.0 (TID 1069)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 1064) in 1355 ms on localhost (4/200)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 1050) in 1360 ms on localhost (5/200)
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000011
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000011_0: Committed
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000005
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000005_0: Committed
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000010
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000010_0: Committed
15/08/21 21:31:43 INFO Executor: Finished task 11.0 in stage 10.0 (TID 1060). 843 bytes result sent to driver
15/08/21 21:31:43 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 1070, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Finished task 10.0 in stage 10.0 (TID 1059). 843 bytes result sent to driver
15/08/21 21:31:43 INFO Executor: Running task 21.0 in stage 10.0 (TID 1070)
15/08/21 21:31:43 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 1071, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 22.0 in stage 10.0 (TID 1071)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 1060) in 1371 ms on localhost (6/200)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 1059) in 1372 ms on localhost (7/200)
15/08/21 21:31:43 INFO Executor: Finished task 5.0 in stage 10.0 (TID 1054). 843 bytes result sent to driver
15/08/21 21:31:43 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 1072, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 23.0 in stage 10.0 (TID 1072)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 1054) in 1377 ms on localhost (8/200)
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000012
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000012_0: Committed
15/08/21 21:31:43 INFO Executor: Finished task 12.0 in stage 10.0 (TID 1061). 843 bytes result sent to driver
15/08/21 21:31:43 INFO TaskSetManager: Starting task 24.0 in stage 10.0 (TID 1073, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 24.0 in stage 10.0 (TID 1073)
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000002
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000002_0: Committed
15/08/21 21:31:43 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 1061) in 1390 ms on localhost (9/200)
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000003
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000003_0: Committed
15/08/21 21:31:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000007
15/08/21 21:31:43 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000007_0: Committed
15/08/21 21:31:43 INFO Executor: Finished task 2.0 in stage 10.0 (TID 1051). 843 bytes result sent to driver
15/08/21 21:31:43 INFO Executor: Finished task 3.0 in stage 10.0 (TID 1052). 843 bytes result sent to driver
15/08/21 21:31:43 INFO TaskSetManager: Starting task 25.0 in stage 10.0 (TID 1074, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 25.0 in stage 10.0 (TID 1074)
15/08/21 21:31:43 INFO Executor: Finished task 7.0 in stage 10.0 (TID 1056). 843 bytes result sent to driver
15/08/21 21:31:43 INFO TaskSetManager: Starting task 26.0 in stage 10.0 (TID 1075, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO Executor: Running task 26.0 in stage 10.0 (TID 1075)
15/08/21 21:31:43 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 1076, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:43 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 1051) in 1397 ms on localhost (10/200)
15/08/21 21:31:43 INFO Executor: Running task 27.0 in stage 10.0 (TID 1076)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 1052) in 1397 ms on localhost (11/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 1056) in 1397 ms on localhost (12/200)
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,336
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,335B for [ps_partkey] INT32: 353 values, 1,419B raw, 1,299B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,799B for [value] DOUBLE: 353 values, 2,831B raw, 1,755B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,596
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,396
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,388B for [ps_partkey] INT32: 366 values, 1,471B raw, 1,352B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,839B for [value] DOUBLE: 366 values, 2,935B raw, 1,795B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,956
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,776
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,470,236
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,869B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,833B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,501B for [value] DOUBLE: 506 values, 4,055B raw, 2,457B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,941B for [ps_partkey] INT32: 525 values, 2,107B raw, 1,905B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,577B for [value] DOUBLE: 525 values, 4,207B raw, 2,533B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,616B for [ps_partkey] INT32: 434 values, 1,743B raw, 1,580B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,159B for [value] DOUBLE: 434 values, 3,479B raw, 2,115B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,656
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,357B for [ps_partkey] INT32: 648 values, 2,599B raw, 2,321B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 3,152B for [value] DOUBLE: 648 values, 5,191B raw, 3,108B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,696
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,089B for [ps_partkey] INT32: 569 values, 2,283B raw, 2,053B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,753B for [value] DOUBLE: 569 values, 4,559B raw, 2,709B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,616
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,578B for [ps_partkey] INT32: 421 values, 1,691B raw, 1,542B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,107B for [value] DOUBLE: 421 values, 3,375B raw, 2,063B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,756
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,565B for [ps_partkey] INT32: 417 values, 1,675B raw, 1,529B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,066B for [value] DOUBLE: 417 values, 3,343B raw, 2,022B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,588B for [ps_partkey] INT32: 424 values, 1,703B raw, 1,552B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,080B for [value] DOUBLE: 424 values, 3,399B raw, 2,036B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,776
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,596B for [ps_partkey] INT32: 425 values, 1,707B raw, 1,560B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,095B for [value] DOUBLE: 425 values, 3,407B raw, 2,051B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,196
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,839B for [ps_partkey] INT32: 496 values, 1,991B raw, 1,803B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,411B for [value] DOUBLE: 496 values, 3,975B raw, 2,367B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000022
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000022_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000024
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000024_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000027
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000027_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000016
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000016_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000025
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000025_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000020
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000020_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000026
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000026_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000019
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000019_0: Committed
15/08/21 21:31:44 INFO Executor: Finished task 22.0 in stage 10.0 (TID 1071). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 16.0 in stage 10.0 (TID 1065). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 27.0 in stage 10.0 (TID 1076). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 24.0 in stage 10.0 (TID 1073). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 26.0 in stage 10.0 (TID 1075). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 25.0 in stage 10.0 (TID 1074). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 20.0 in stage 10.0 (TID 1069). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 28.0 in stage 10.0 (TID 1077, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Finished task 19.0 in stage 10.0 (TID 1068). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Running task 28.0 in stage 10.0 (TID 1077)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 29.0 in stage 10.0 (TID 1078, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 30.0 in stage 10.0 (TID 1079, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 30.0 in stage 10.0 (TID 1079)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 31.0 in stage 10.0 (TID 1080, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 22.0 in stage 10.0 (TID 1071) in 314 ms on localhost (13/200)
15/08/21 21:31:44 INFO Executor: Running task 31.0 in stage 10.0 (TID 1080)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 1065) in 330 ms on localhost (14/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 1076) in 289 ms on localhost (15/200)
15/08/21 21:31:44 INFO Executor: Running task 29.0 in stage 10.0 (TID 1078)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 32.0 in stage 10.0 (TID 1081, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 32.0 in stage 10.0 (TID 1081)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 33.0 in stage 10.0 (TID 1082, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 34.0 in stage 10.0 (TID 1083, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 33.0 in stage 10.0 (TID 1082)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 35.0 in stage 10.0 (TID 1084, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 34.0 in stage 10.0 (TID 1083)
15/08/21 21:31:44 INFO Executor: Running task 35.0 in stage 10.0 (TID 1084)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 1075) in 293 ms on localhost (16/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 1073) in 299 ms on localhost (17/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 19.0 in stage 10.0 (TID 1068) in 335 ms on localhost (18/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 1074) in 297 ms on localhost (19/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 20.0 in stage 10.0 (TID 1069) in 334 ms on localhost (20/200)
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000013
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000013_0: Committed
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO Executor: Finished task 13.0 in stage 10.0 (TID 1062). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 36.0 in stage 10.0 (TID 1085, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 36.0 in stage 10.0 (TID 1085)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 1062) in 1752 ms on localhost (21/200)
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000004
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000004_0: Committed
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000000
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000000_0: Committed
15/08/21 21:31:44 INFO Executor: Finished task 4.0 in stage 10.0 (TID 1053). 843 bytes result sent to driver
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO TaskSetManager: Starting task 37.0 in stage 10.0 (TID 1086, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO Executor: Running task 37.0 in stage 10.0 (TID 1086)
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1049). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 38.0 in stage 10.0 (TID 1087, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 38.0 in stage 10.0 (TID 1087)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 1053) in 1768 ms on localhost (22/200)
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1049) in 1772 ms on localhost (23/200)
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000009
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000009_0: Committed
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO Executor: Finished task 9.0 in stage 10.0 (TID 1058). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 39.0 in stage 10.0 (TID 1088, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 39.0 in stage 10.0 (TID 1088)
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 1058) in 1809 ms on localhost (24/200)
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,856
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,856
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,125B for [ps_partkey] INT32: 579 values, 2,323B raw, 2,089B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,654B for [value] DOUBLE: 579 values, 4,639B raw, 2,610B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,772B for [ps_partkey] INT32: 479 values, 1,923B raw, 1,736B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,327B for [value] DOUBLE: 479 values, 3,839B raw, 2,283B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,936
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,954B for [ps_partkey] INT32: 533 values, 2,139B raw, 1,918B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,530B for [value] DOUBLE: 533 values, 4,271B raw, 2,486B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,316
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,276
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,685B for [ps_partkey] INT32: 452 values, 1,815B raw, 1,649B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,116B for [value] DOUBLE: 452 values, 3,623B raw, 2,072B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,036
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,843B for [ps_partkey] INT32: 500 values, 2,007B raw, 1,807B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,036
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,416B for [value] DOUBLE: 500 values, 4,007B raw, 2,372B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,467B for [ps_partkey] INT32: 388 values, 1,559B raw, 1,431B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,841B for [value] DOUBLE: 388 values, 3,111B raw, 1,797B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,161B for [ps_partkey] INT32: 588 values, 2,359B raw, 2,125B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,820B for [value] DOUBLE: 588 values, 4,711B raw, 2,776B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,636
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000017
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000017_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000031
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000031_0: Committed
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,738B for [ps_partkey] INT32: 468 values, 1,879B raw, 1,702B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,235B for [value] DOUBLE: 468 values, 3,751B raw, 2,191B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO Executor: Finished task 31.0 in stage 10.0 (TID 1080). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 17.0 in stage 10.0 (TID 1066). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 40.0 in stage 10.0 (TID 1089, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 40.0 in stage 10.0 (TID 1089)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 41.0 in stage 10.0 (TID 1090, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 41.0 in stage 10.0 (TID 1090)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 17.0 in stage 10.0 (TID 1066) in 621 ms on localhost (25/200)
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO TaskSetManager: Finished task 31.0 in stage 10.0 (TID 1080) in 294 ms on localhost (26/200)
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000030
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000030_0: Committed
15/08/21 21:31:44 INFO Executor: Finished task 30.0 in stage 10.0 (TID 1079). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 42.0 in stage 10.0 (TID 1091, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 42.0 in stage 10.0 (TID 1091)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 30.0 in stage 10.0 (TID 1079) in 313 ms on localhost (27/200)
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000018
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000018_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000033
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000023
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000033_0: Committed
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000023_0: Committed
15/08/21 21:31:44 INFO Executor: Finished task 33.0 in stage 10.0 (TID 1082). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 23.0 in stage 10.0 (TID 1072). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 18.0 in stage 10.0 (TID 1067). 843 bytes result sent to driver
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,096
15/08/21 21:31:44 INFO TaskSetManager: Starting task 43.0 in stage 10.0 (TID 1092, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 43.0 in stage 10.0 (TID 1092)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 44.0 in stage 10.0 (TID 1093, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,136
15/08/21 21:31:44 INFO Executor: Running task 44.0 in stage 10.0 (TID 1093)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 45.0 in stage 10.0 (TID 1094, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,136
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,812B for [ps_partkey] INT32: 491 values, 1,971B raw, 1,776B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,242B for [value] DOUBLE: 491 values, 3,935B raw, 2,198B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO TaskSetManager: Finished task 33.0 in stage 10.0 (TID 1082) in 342 ms on localhost (28/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 23.0 in stage 10.0 (TID 1072) in 653 ms on localhost (29/200)
15/08/21 21:31:44 INFO Executor: Running task 45.0 in stage 10.0 (TID 1094)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 18.0 in stage 10.0 (TID 1067) in 673 ms on localhost (30/200)
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,660B for [ps_partkey] INT32: 443 values, 1,779B raw, 1,624B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,059B for [value] DOUBLE: 443 values, 3,551B raw, 2,015B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,996B for [ps_partkey] INT32: 543 values, 2,179B raw, 1,960B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,471B for [value] DOUBLE: 543 values, 4,351B raw, 2,427B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,868
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000038
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000038_0: Committed
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,775B for [ps_partkey] INT32: 480 values, 1,927B raw, 1,739B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000036
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,186B for [value] DOUBLE: 480 values, 3,847B raw, 2,142B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000036_0: Committed
15/08/21 21:31:44 INFO Executor: Finished task 38.0 in stage 10.0 (TID 1087). 843 bytes result sent to driver
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000021
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000021_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000037
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000037_0: Committed
15/08/21 21:31:44 INFO TaskSetManager: Starting task 46.0 in stage 10.0 (TID 1095, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 46.0 in stage 10.0 (TID 1095)
15/08/21 21:31:44 INFO Executor: Finished task 36.0 in stage 10.0 (TID 1085). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 47.0 in stage 10.0 (TID 1096, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 47.0 in stage 10.0 (TID 1096)
15/08/21 21:31:44 INFO Executor: Finished task 21.0 in stage 10.0 (TID 1070). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Finished task 38.0 in stage 10.0 (TID 1087) in 316 ms on localhost (31/200)
15/08/21 21:31:44 INFO Executor: Finished task 37.0 in stage 10.0 (TID 1086). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 48.0 in stage 10.0 (TID 1097, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 48.0 in stage 10.0 (TID 1097)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 49.0 in stage 10.0 (TID 1098, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 49.0 in stage 10.0 (TID 1098)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 36.0 in stage 10.0 (TID 1085) in 329 ms on localhost (32/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 1070) in 715 ms on localhost (33/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 37.0 in stage 10.0 (TID 1086) in 323 ms on localhost (34/200)
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,256
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,846B for [ps_partkey] INT32: 499 values, 2,003B raw, 1,810B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,238B for [value] DOUBLE: 499 values, 3,999B raw, 2,194B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,268
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,849B for [ps_partkey] INT32: 500 values, 2,007B raw, 1,813B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,252B for [value] DOUBLE: 500 values, 4,007B raw, 2,208B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,208
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,841B for [ps_partkey] INT32: 497 values, 1,995B raw, 1,805B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,259B for [value] DOUBLE: 497 values, 3,983B raw, 2,215B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,160
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000042
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000042_0: Committed
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,827B for [ps_partkey] INT32: 495 values, 1,987B raw, 1,791B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,246B for [value] DOUBLE: 495 values, 3,967B raw, 2,202B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO Executor: Finished task 42.0 in stage 10.0 (TID 1091). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 50.0 in stage 10.0 (TID 1099, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 50.0 in stage 10.0 (TID 1099)
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000040
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000041
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000040_0: Committed
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000041_0: Committed
15/08/21 21:31:44 INFO TaskSetManager: Finished task 42.0 in stage 10.0 (TID 1091) in 240 ms on localhost (35/200)
15/08/21 21:31:44 INFO Executor: Finished task 41.0 in stage 10.0 (TID 1090). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 40.0 in stage 10.0 (TID 1089). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 51.0 in stage 10.0 (TID 1100, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 52.0 in stage 10.0 (TID 1101, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 52.0 in stage 10.0 (TID 1101)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 41.0 in stage 10.0 (TID 1090) in 266 ms on localhost (36/200)
15/08/21 21:31:44 INFO Executor: Running task 51.0 in stage 10.0 (TID 1100)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 40.0 in stage 10.0 (TID 1089) in 271 ms on localhost (37/200)
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000043
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000043_0: Committed
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,868
15/08/21 21:31:44 INFO Executor: Finished task 43.0 in stage 10.0 (TID 1092). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 53.0 in stage 10.0 (TID 1102, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 53.0 in stage 10.0 (TID 1102)
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,780B for [ps_partkey] INT32: 480 values, 1,927B raw, 1,744B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,149B for [value] DOUBLE: 480 values, 3,847B raw, 2,105B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO TaskSetManager: Finished task 43.0 in stage 10.0 (TID 1092) in 254 ms on localhost (38/200)
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,816
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,939B for [ps_partkey] INT32: 527 values, 2,115B raw, 1,903B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,353B for [value] DOUBLE: 527 values, 4,223B raw, 2,309B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,516
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,220
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000045
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000045_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000028
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000028_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000034
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000034_0: Committed
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000044
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000044_0: Committed
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,015B for [ps_partkey] INT32: 548 values, 2,199B raw, 1,979B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,883B for [ps_partkey] INT32: 512 values, 2,055B raw, 1,847B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,431B for [value] DOUBLE: 548 values, 4,391B raw, 2,387B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,279B for [value] DOUBLE: 512 values, 4,103B raw, 2,235B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO Executor: Finished task 28.0 in stage 10.0 (TID 1077). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 34.0 in stage 10.0 (TID 1083). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 44.0 in stage 10.0 (TID 1093). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 54.0 in stage 10.0 (TID 1103, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Finished task 45.0 in stage 10.0 (TID 1094). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Running task 54.0 in stage 10.0 (TID 1103)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 55.0 in stage 10.0 (TID 1104, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 55.0 in stage 10.0 (TID 1104)
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,248
15/08/21 21:31:44 INFO TaskSetManager: Starting task 56.0 in stage 10.0 (TID 1105, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 56.0 in stage 10.0 (TID 1105)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 57.0 in stage 10.0 (TID 1106, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 57.0 in stage 10.0 (TID 1106)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 28.0 in stage 10.0 (TID 1077) in 648 ms on localhost (39/200)
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 1,670B for [ps_partkey] INT32: 449 values, 1,803B raw, 1,634B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,028B for [value] DOUBLE: 449 values, 3,599B raw, 1,984B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO TaskSetManager: Finished task 34.0 in stage 10.0 (TID 1083) in 645 ms on localhost (40/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 45.0 in stage 10.0 (TID 1094) in 307 ms on localhost (41/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 44.0 in stage 10.0 (TID 1093) in 313 ms on localhost (42/200)
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,316
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,020B for [ps_partkey] INT32: 552 values, 2,215B raw, 1,984B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ColumnChunkPageWriteStore: written 2,460B for [value] DOUBLE: 552 values, 4,423B raw, 2,416B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000048
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000048_0: Committed
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO Executor: Finished task 48.0 in stage 10.0 (TID 1097). 843 bytes result sent to driver
15/08/21 21:31:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:44 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:44 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000035
15/08/21 21:31:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000035_0: Committed
15/08/21 21:31:44 INFO TaskSetManager: Starting task 58.0 in stage 10.0 (TID 1107, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 58.0 in stage 10.0 (TID 1107)
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000032
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000032_0: Committed
15/08/21 21:31:44 INFO Executor: Finished task 35.0 in stage 10.0 (TID 1084). 843 bytes result sent to driver
15/08/21 21:31:44 INFO Executor: Finished task 32.0 in stage 10.0 (TID 1081). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 59.0 in stage 10.0 (TID 1108, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000049
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000049_0: Committed
15/08/21 21:31:44 INFO TaskSetManager: Finished task 48.0 in stage 10.0 (TID 1097) in 299 ms on localhost (43/200)
15/08/21 21:31:44 INFO Executor: Running task 59.0 in stage 10.0 (TID 1108)
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:44 INFO TaskSetManager: Finished task 35.0 in stage 10.0 (TID 1084) in 695 ms on localhost (44/200)
15/08/21 21:31:44 INFO TaskSetManager: Finished task 32.0 in stage 10.0 (TID 1081) in 698 ms on localhost (45/200)
15/08/21 21:31:44 INFO Executor: Finished task 49.0 in stage 10.0 (TID 1098). 843 bytes result sent to driver
15/08/21 21:31:44 INFO TaskSetManager: Starting task 60.0 in stage 10.0 (TID 1109, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO TaskSetManager: Starting task 61.0 in stage 10.0 (TID 1110, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:44 INFO Executor: Running task 60.0 in stage 10.0 (TID 1109)
15/08/21 21:31:44 INFO Executor: Running task 61.0 in stage 10.0 (TID 1110)
15/08/21 21:31:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000046
15/08/21 21:31:44 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000046_0: Committed
15/08/21 21:31:44 INFO TaskSetManager: Finished task 49.0 in stage 10.0 (TID 1098) in 312 ms on localhost (46/200)
15/08/21 21:31:44 INFO Executor: Finished task 46.0 in stage 10.0 (TID 1095). 843 bytes result sent to driver
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO TaskSetManager: Starting task 62.0 in stage 10.0 (TID 1111, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO Executor: Running task 62.0 in stage 10.0 (TID 1111)
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO TaskSetManager: Finished task 46.0 in stage 10.0 (TID 1095) in 325 ms on localhost (47/200)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000029
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000029_0: Committed
15/08/21 21:31:45 INFO Executor: Finished task 29.0 in stage 10.0 (TID 1078). 843 bytes result sent to driver
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO TaskSetManager: Starting task 63.0 in stage 10.0 (TID 1112, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 63.0 in stage 10.0 (TID 1112)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 29.0 in stage 10.0 (TID 1078) in 746 ms on localhost (48/200)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000039
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000039_0: Committed
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,708
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,728
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO Executor: Finished task 39.0 in stage 10.0 (TID 1088). 843 bytes result sent to driver
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO TaskSetManager: Starting task 64.0 in stage 10.0 (TID 1113, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 64.0 in stage 10.0 (TID 1113)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,756B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,720B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,757B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,721B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,092B for [value] DOUBLE: 472 values, 3,783B raw, 2,048B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO TaskSetManager: Finished task 39.0 in stage 10.0 (TID 1088) in 714 ms on localhost (49/200)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,103B for [value] DOUBLE: 473 values, 3,791B raw, 2,059B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,316
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,216
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,850B for [ps_partkey] INT32: 502 values, 2,015B raw, 1,814B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,229B for [value] DOUBLE: 502 values, 4,023B raw, 2,185B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000052
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000052_0: Committed
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,841B for [ps_partkey] INT32: 497 values, 1,995B raw, 1,805B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,205B for [value] DOUBLE: 497 values, 3,983B raw, 2,161B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO Executor: Finished task 52.0 in stage 10.0 (TID 1101). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 65.0 in stage 10.0 (TID 1114, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO TaskSetManager: Finished task 52.0 in stage 10.0 (TID 1101) in 341 ms on localhost (50/200)
15/08/21 21:31:45 INFO Executor: Running task 65.0 in stage 10.0 (TID 1114)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,156
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,997B for [ps_partkey] INT32: 544 values, 2,183B raw, 1,961B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,384B for [value] DOUBLE: 544 values, 4,359B raw, 2,340B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,776
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,336
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,601B for [ps_partkey] INT32: 425 values, 1,707B raw, 1,565B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,908B for [value] DOUBLE: 425 values, 3,407B raw, 1,864B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000053
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000053_0: Committed
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,690B for [ps_partkey] INT32: 453 values, 1,819B raw, 1,654B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,005B for [value] DOUBLE: 453 values, 3,631B raw, 1,961B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 53.0 in stage 10.0 (TID 1102). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 66.0 in stage 10.0 (TID 1115, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 66.0 in stage 10.0 (TID 1115)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000056
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000056_0: Committed
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000050
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000050_0: Committed
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,076
15/08/21 21:31:45 INFO Executor: Finished task 56.0 in stage 10.0 (TID 1105). 843 bytes result sent to driver
15/08/21 21:31:45 INFO Executor: Finished task 50.0 in stage 10.0 (TID 1099). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Finished task 53.0 in stage 10.0 (TID 1102) in 376 ms on localhost (51/200)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,736
15/08/21 21:31:45 INFO TaskSetManager: Starting task 67.0 in stage 10.0 (TID 1116, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 67.0 in stage 10.0 (TID 1116)
15/08/21 21:31:45 INFO TaskSetManager: Starting task 68.0 in stage 10.0 (TID 1117, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 68.0 in stage 10.0 (TID 1117)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,488
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,696
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,153B for [ps_partkey] INT32: 590 values, 2,367B raw, 2,117B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,562B for [value] DOUBLE: 590 values, 4,727B raw, 2,518B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,416
15/08/21 21:31:45 INFO TaskSetManager: Finished task 56.0 in stage 10.0 (TID 1105) in 336 ms on localhost (52/200)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 50.0 in stage 10.0 (TID 1099) in 443 ms on localhost (53/200)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,757B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,721B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,920B for [ps_partkey] INT32: 521 values, 2,091B raw, 1,884B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,109B for [value] DOUBLE: 473 values, 3,791B raw, 2,065B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,891B for [ps_partkey] INT32: 511 values, 2,051B raw, 1,855B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,249B for [value] DOUBLE: 511 values, 4,095B raw, 2,205B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,266B for [value] DOUBLE: 521 values, 4,175B raw, 2,222B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000054
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000054_0: Committed
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,529B for [ps_partkey] INT32: 407 values, 1,635B raw, 1,493B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,828B for [value] DOUBLE: 407 values, 3,263B raw, 1,784B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 54.0 in stage 10.0 (TID 1103). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 69.0 in stage 10.0 (TID 1118, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,376
15/08/21 21:31:45 INFO Executor: Running task 69.0 in stage 10.0 (TID 1118)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 54.0 in stage 10.0 (TID 1103) in 352 ms on localhost (54/200)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,343B for [ps_partkey] INT32: 355 values, 1,427B raw, 1,307B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,619B for [value] DOUBLE: 355 values, 2,847B raw, 1,575B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000057
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000057_0: Committed
15/08/21 21:31:45 INFO Executor: Finished task 57.0 in stage 10.0 (TID 1106). 843 bytes result sent to driver
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,236
15/08/21 21:31:45 INFO TaskSetManager: Starting task 70.0 in stage 10.0 (TID 1119, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 70.0 in stage 10.0 (TID 1119)
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO TaskSetManager: Finished task 57.0 in stage 10.0 (TID 1106) in 363 ms on localhost (55/200)
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,664B for [ps_partkey] INT32: 448 values, 1,799B raw, 1,628B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,983B for [value] DOUBLE: 448 values, 3,591B raw, 1,939B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000061
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000061_0: Committed
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000059
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000059_0: Committed
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO Executor: Finished task 61.0 in stage 10.0 (TID 1110). 843 bytes result sent to driver
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000055
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000055_0: Committed
15/08/21 21:31:45 INFO TaskSetManager: Starting task 71.0 in stage 10.0 (TID 1120, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 71.0 in stage 10.0 (TID 1120)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000060
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000060_0: Committed
15/08/21 21:31:45 INFO Executor: Finished task 59.0 in stage 10.0 (TID 1108). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 72.0 in stage 10.0 (TID 1121, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 72.0 in stage 10.0 (TID 1121)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 61.0 in stage 10.0 (TID 1110) in 323 ms on localhost (56/200)
15/08/21 21:31:45 INFO Executor: Finished task 60.0 in stage 10.0 (TID 1109). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 73.0 in stage 10.0 (TID 1122, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Finished task 55.0 in stage 10.0 (TID 1104). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Finished task 59.0 in stage 10.0 (TID 1108) in 336 ms on localhost (57/200)
15/08/21 21:31:45 INFO Executor: Running task 73.0 in stage 10.0 (TID 1122)
15/08/21 21:31:45 INFO TaskSetManager: Starting task 74.0 in stage 10.0 (TID 1123, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 60.0 in stage 10.0 (TID 1109) in 330 ms on localhost (58/200)
15/08/21 21:31:45 INFO Executor: Running task 74.0 in stage 10.0 (TID 1123)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000062
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000062_0: Committed
15/08/21 21:31:45 INFO TaskSetManager: Finished task 55.0 in stage 10.0 (TID 1104) in 393 ms on localhost (59/200)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO Executor: Finished task 62.0 in stage 10.0 (TID 1111). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 75.0 in stage 10.0 (TID 1124, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 75.0 in stage 10.0 (TID 1124)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000063
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000063_0: Committed
15/08/21 21:31:45 INFO TaskSetManager: Finished task 62.0 in stage 10.0 (TID 1111) in 329 ms on localhost (60/200)
15/08/21 21:31:45 INFO Executor: Finished task 63.0 in stage 10.0 (TID 1112). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 76.0 in stage 10.0 (TID 1125, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 76.0 in stage 10.0 (TID 1125)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO TaskSetManager: Finished task 63.0 in stage 10.0 (TID 1112) in 313 ms on localhost (61/200)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,528
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,547B for [ps_partkey] INT32: 413 values, 1,659B raw, 1,511B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,849B for [value] DOUBLE: 413 values, 3,311B raw, 1,805B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000047
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000047_0: Committed
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO Executor: Finished task 47.0 in stage 10.0 (TID 1096). 843 bytes result sent to driver
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO TaskSetManager: Starting task 77.0 in stage 10.0 (TID 1126, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 77.0 in stage 10.0 (TID 1126)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 47.0 in stage 10.0 (TID 1096) in 700 ms on localhost (62/200)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000064
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000064_0: Committed
15/08/21 21:31:45 INFO Executor: Finished task 64.0 in stage 10.0 (TID 1113). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 78.0 in stage 10.0 (TID 1127, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO TaskSetManager: Finished task 64.0 in stage 10.0 (TID 1113) in 290 ms on localhost (63/200)
15/08/21 21:31:45 INFO Executor: Running task 78.0 in stage 10.0 (TID 1127)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,076
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,748
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,810B for [ps_partkey] INT32: 490 values, 1,967B raw, 1,774B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,157B for [value] DOUBLE: 490 values, 3,927B raw, 2,113B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,996
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,762B for [ps_partkey] INT32: 474 values, 1,903B raw, 1,726B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,630B for [ps_partkey] INT32: 436 values, 1,751B raw, 1,594B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,050B for [value] DOUBLE: 474 values, 3,799B raw, 2,006B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,924B for [value] DOUBLE: 436 values, 3,495B raw, 1,880B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,236
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,662B for [ps_partkey] INT32: 448 values, 1,799B raw, 1,626B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,963B for [value] DOUBLE: 448 values, 3,591B raw, 1,919B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000067
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000067_0: Committed
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000065
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000065_0: Committed
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,508
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO Executor: Finished task 67.0 in stage 10.0 (TID 1116). 843 bytes result sent to driver
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000066
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000066_0: Committed
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO Executor: Finished task 65.0 in stage 10.0 (TID 1114). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 79.0 in stage 10.0 (TID 1128, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,718B for [ps_partkey] INT32: 462 values, 1,855B raw, 1,682B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,025B for [value] DOUBLE: 462 values, 3,703B raw, 1,981B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Running task 79.0 in stage 10.0 (TID 1128)
15/08/21 21:31:45 INFO TaskSetManager: Starting task 80.0 in stage 10.0 (TID 1129, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO Executor: Running task 80.0 in stage 10.0 (TID 1129)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000068
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000068_0: Committed
15/08/21 21:31:45 INFO Executor: Finished task 66.0 in stage 10.0 (TID 1115). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 81.0 in stage 10.0 (TID 1130, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO TaskSetManager: Finished task 65.0 in stage 10.0 (TID 1114) in 323 ms on localhost (64/200)
15/08/21 21:31:45 INFO Executor: Running task 81.0 in stage 10.0 (TID 1130)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 67.0 in stage 10.0 (TID 1116) in 244 ms on localhost (65/200)
15/08/21 21:31:45 INFO Executor: Finished task 68.0 in stage 10.0 (TID 1117). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 82.0 in stage 10.0 (TID 1131, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 82.0 in stage 10.0 (TID 1131)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 66.0 in stage 10.0 (TID 1115) in 269 ms on localhost (66/200)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO TaskSetManager: Finished task 68.0 in stage 10.0 (TID 1117) in 249 ms on localhost (67/200)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,436
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,534B for [ps_partkey] INT32: 408 values, 1,639B raw, 1,498B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,796B for [value] DOUBLE: 408 values, 3,271B raw, 1,752B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,496
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,628
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,715B for [ps_partkey] INT32: 461 values, 1,851B raw, 1,679B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,025B for [value] DOUBLE: 461 values, 3,695B raw, 1,981B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000069
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000069_0: Committed
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,909B for [ps_partkey] INT32: 518 values, 2,079B raw, 1,873B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,254B for [value] DOUBLE: 518 values, 4,151B raw, 2,210B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 69.0 in stage 10.0 (TID 1118). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 83.0 in stage 10.0 (TID 1132, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 83.0 in stage 10.0 (TID 1132)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,156
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,020
15/08/21 21:31:45 INFO TaskSetManager: Finished task 69.0 in stage 10.0 (TID 1118) in 263 ms on localhost (68/200)
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,654B for [ps_partkey] INT32: 444 values, 1,783B raw, 1,618B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,961B for [value] DOUBLE: 444 values, 3,559B raw, 1,917B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,463B for [ps_partkey] INT32: 388 values, 1,559B raw, 1,427B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,724B for [value] DOUBLE: 388 values, 3,111B raw, 1,680B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,476
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000071
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000071_0: Committed
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,388
15/08/21 21:31:45 INFO Executor: Finished task 71.0 in stage 10.0 (TID 1120). 843 bytes result sent to driver
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,054B for [ps_partkey] INT32: 560 values, 2,247B raw, 2,018B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,425B for [value] DOUBLE: 560 values, 4,487B raw, 2,381B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO TaskSetManager: Starting task 84.0 in stage 10.0 (TID 1133, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 84.0 in stage 10.0 (TID 1133)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,041B for [ps_partkey] INT32: 556 values, 2,231B raw, 2,005B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,383B for [value] DOUBLE: 556 values, 4,455B raw, 2,339B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO TaskSetManager: Finished task 71.0 in stage 10.0 (TID 1120) in 265 ms on localhost (69/200)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000070
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000070_0: Committed
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000074
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000074_0: Committed
15/08/21 21:31:45 INFO Executor: Finished task 70.0 in stage 10.0 (TID 1119). 843 bytes result sent to driver
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000051
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000051_0: Committed
15/08/21 21:31:45 INFO TaskSetManager: Starting task 85.0 in stage 10.0 (TID 1134, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Finished task 74.0 in stage 10.0 (TID 1123). 843 bytes result sent to driver
15/08/21 21:31:45 INFO Executor: Running task 85.0 in stage 10.0 (TID 1134)
15/08/21 21:31:45 INFO TaskSetManager: Starting task 86.0 in stage 10.0 (TID 1135, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 86.0 in stage 10.0 (TID 1135)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 70.0 in stage 10.0 (TID 1119) in 299 ms on localhost (70/200)
15/08/21 21:31:45 INFO Executor: Finished task 51.0 in stage 10.0 (TID 1100). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 87.0 in stage 10.0 (TID 1136, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 74.0 in stage 10.0 (TID 1123) in 275 ms on localhost (71/200)
15/08/21 21:31:45 INFO Executor: Running task 87.0 in stage 10.0 (TID 1136)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 51.0 in stage 10.0 (TID 1100) in 760 ms on localhost (72/200)
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000076
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000076_0: Committed
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,388
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000075
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,456
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000075_0: Committed
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,039B for [ps_partkey] INT32: 556 values, 2,231B raw, 2,003B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 75.0 in stage 10.0 (TID 1124). 843 bytes result sent to driver
15/08/21 21:31:45 INFO Executor: Finished task 76.0 in stage 10.0 (TID 1125). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 88.0 in stage 10.0 (TID 1137, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,389B for [value] DOUBLE: 556 values, 4,455B raw, 2,345B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO Executor: Running task 88.0 in stage 10.0 (TID 1137)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,532B for [ps_partkey] INT32: 409 values, 1,643B raw, 1,496B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO TaskSetManager: Starting task 89.0 in stage 10.0 (TID 1138, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,810B for [value] DOUBLE: 409 values, 3,279B raw, 1,766B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO Executor: Running task 89.0 in stage 10.0 (TID 1138)
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO TaskSetManager: Finished task 76.0 in stage 10.0 (TID 1125) in 288 ms on localhost (73/200)
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO TaskSetManager: Finished task 75.0 in stage 10.0 (TID 1124) in 295 ms on localhost (74/200)
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,676
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000077
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000077_0: Committed
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000078
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000078_0: Committed
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,748
15/08/21 21:31:45 INFO Executor: Finished task 78.0 in stage 10.0 (TID 1127). 843 bytes result sent to driver
15/08/21 21:31:45 INFO Executor: Finished task 77.0 in stage 10.0 (TID 1126). 843 bytes result sent to driver
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,749B for [ps_partkey] INT32: 470 values, 1,887B raw, 1,713B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,051B for [value] DOUBLE: 470 values, 3,767B raw, 2,007B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO TaskSetManager: Starting task 90.0 in stage 10.0 (TID 1139, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 90.0 in stage 10.0 (TID 1139)
15/08/21 21:31:45 INFO TaskSetManager: Starting task 91.0 in stage 10.0 (TID 1140, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 91.0 in stage 10.0 (TID 1140)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 78.0 in stage 10.0 (TID 1127) in 294 ms on localhost (75/200)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 77.0 in stage 10.0 (TID 1126) in 314 ms on localhost (76/200)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,759B for [ps_partkey] INT32: 474 values, 1,903B raw, 1,723B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,063B for [value] DOUBLE: 474 values, 3,799B raw, 2,019B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,876
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000058
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000058_0: Committed
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,949B for [ps_partkey] INT32: 530 values, 2,127B raw, 1,913B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,242B for [value] DOUBLE: 530 values, 4,247B raw, 2,198B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 58.0 in stage 10.0 (TID 1107). 843 bytes result sent to driver
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO TaskSetManager: Starting task 92.0 in stage 10.0 (TID 1141, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,388
15/08/21 21:31:45 INFO Executor: Running task 92.0 in stage 10.0 (TID 1141)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000080
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000080_0: Committed
15/08/21 21:31:45 INFO TaskSetManager: Finished task 58.0 in stage 10.0 (TID 1107) in 742 ms on localhost (77/200)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,038B for [ps_partkey] INT32: 556 values, 2,231B raw, 2,002B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,379B for [value] DOUBLE: 556 values, 4,455B raw, 2,335B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 80.0 in stage 10.0 (TID 1129). 843 bytes result sent to driver
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO TaskSetManager: Starting task 93.0 in stage 10.0 (TID 1142, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000079
15/08/21 21:31:45 INFO Executor: Running task 93.0 in stage 10.0 (TID 1142)
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000079_0: Committed
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO TaskSetManager: Finished task 80.0 in stage 10.0 (TID 1129) in 244 ms on localhost (78/200)
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO Executor: Finished task 79.0 in stage 10.0 (TID 1128). 843 bytes result sent to driver
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO TaskSetManager: Starting task 94.0 in stage 10.0 (TID 1143, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 94.0 in stage 10.0 (TID 1143)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 79.0 in stage 10.0 (TID 1128) in 249 ms on localhost (79/200)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,956
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,788B for [ps_partkey] INT32: 484 values, 1,943B raw, 1,752B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,086B for [value] DOUBLE: 484 values, 3,879B raw, 2,042B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000082
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000082_0: Committed
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO Executor: Finished task 82.0 in stage 10.0 (TID 1131). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 95.0 in stage 10.0 (TID 1144, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000081
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000081_0: Committed
15/08/21 21:31:45 INFO Executor: Running task 95.0 in stage 10.0 (TID 1144)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 82.0 in stage 10.0 (TID 1131) in 280 ms on localhost (80/200)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,828
15/08/21 21:31:45 INFO Executor: Finished task 81.0 in stage 10.0 (TID 1130). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 96.0 in stage 10.0 (TID 1145, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 96.0 in stage 10.0 (TID 1145)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 81.0 in stage 10.0 (TID 1130) in 293 ms on localhost (81/200)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,606B for [ps_partkey] INT32: 428 values, 1,719B raw, 1,570B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,869B for [value] DOUBLE: 428 values, 3,431B raw, 1,825B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,996
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,836
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,630B for [ps_partkey] INT32: 436 values, 1,751B raw, 1,594B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,915B for [value] DOUBLE: 436 values, 3,495B raw, 1,871B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000083
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000083_0: Committed
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,599B for [ps_partkey] INT32: 428 values, 1,719B raw, 1,563B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,858B for [value] DOUBLE: 428 values, 3,431B raw, 1,814B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 83.0 in stage 10.0 (TID 1132). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 97.0 in stage 10.0 (TID 1146, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 97.0 in stage 10.0 (TID 1146)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO TaskSetManager: Finished task 83.0 in stage 10.0 (TID 1132) in 281 ms on localhost (82/200)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,888
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,676
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,756
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,444B for [ps_partkey] INT32: 381 values, 1,531B raw, 1,408B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,672B for [value] DOUBLE: 381 values, 3,055B raw, 1,628B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,102B for [ps_partkey] INT32: 574 values, 2,303B raw, 2,066B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,464B for [value] DOUBLE: 574 values, 4,599B raw, 2,420B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,746B for [ps_partkey] INT32: 470 values, 1,887B raw, 1,710B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,049B for [value] DOUBLE: 470 values, 3,767B raw, 2,005B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000086
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000084
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000084_0: Committed
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000086_0: Committed
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000085
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000085_0: Committed
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO Executor: Finished task 84.0 in stage 10.0 (TID 1133). 843 bytes result sent to driver
15/08/21 21:31:45 INFO Executor: Finished task 86.0 in stage 10.0 (TID 1135). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 98.0 in stage 10.0 (TID 1147, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 98.0 in stage 10.0 (TID 1147)
15/08/21 21:31:45 INFO TaskSetManager: Starting task 99.0 in stage 10.0 (TID 1148, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 99.0 in stage 10.0 (TID 1148)
15/08/21 21:31:45 INFO Executor: Finished task 85.0 in stage 10.0 (TID 1134). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 100.0 in stage 10.0 (TID 1149, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 100.0 in stage 10.0 (TID 1149)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 84.0 in stage 10.0 (TID 1133) in 283 ms on localhost (83/200)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 86.0 in stage 10.0 (TID 1135) in 265 ms on localhost (84/200)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO TaskSetManager: Finished task 85.0 in stage 10.0 (TID 1134) in 273 ms on localhost (85/200)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,416
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,248
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,697B for [ps_partkey] INT32: 457 values, 1,835B raw, 1,661B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,981B for [value] DOUBLE: 457 values, 3,663B raw, 1,937B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,673B for [ps_partkey] INT32: 449 values, 1,803B raw, 1,637B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,960B for [value] DOUBLE: 449 values, 3,599B raw, 1,916B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,276
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,501B for [ps_partkey] INT32: 400 values, 1,607B raw, 1,465B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,738B for [value] DOUBLE: 400 values, 3,207B raw, 1,694B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000090
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000090_0: Committed
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,316
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000091
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000091_0: Committed
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO Executor: Finished task 90.0 in stage 10.0 (TID 1139). 843 bytes result sent to driver
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,681B for [ps_partkey] INT32: 452 values, 1,815B raw, 1,645B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,048
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,954B for [value] DOUBLE: 452 values, 3,623B raw, 1,910B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO TaskSetManager: Starting task 101.0 in stage 10.0 (TID 1150, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Finished task 91.0 in stage 10.0 (TID 1140). 843 bytes result sent to driver
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO Executor: Running task 101.0 in stage 10.0 (TID 1150)
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:45 INFO TaskSetManager: Starting task 102.0 in stage 10.0 (TID 1151, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 102.0 in stage 10.0 (TID 1151)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 90.0 in stage 10.0 (TID 1139) in 250 ms on localhost (86/200)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 91.0 in stage 10.0 (TID 1140) in 248 ms on localhost (87/200)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,988B for [ps_partkey] INT32: 539 values, 2,163B raw, 1,952B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,297B for [value] DOUBLE: 539 values, 4,319B raw, 2,253B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000093
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000093_0: Committed
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,788
15/08/21 21:31:45 INFO Executor: Finished task 93.0 in stage 10.0 (TID 1142). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 103.0 in stage 10.0 (TID 1152, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,763B for [ps_partkey] INT32: 476 values, 1,911B raw, 1,727B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,076
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,072B for [value] DOUBLE: 476 values, 3,815B raw, 2,028B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Running task 103.0 in stage 10.0 (TID 1152)
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,648B for [ps_partkey] INT32: 440 values, 1,767B raw, 1,612B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO TaskSetManager: Finished task 93.0 in stage 10.0 (TID 1142) in 234 ms on localhost (88/200)
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,923B for [value] DOUBLE: 440 values, 3,527B raw, 1,879B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000094
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000094_0: Committed
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO Executor: Finished task 94.0 in stage 10.0 (TID 1143). 843 bytes result sent to driver
15/08/21 21:31:45 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:45 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:45 INFO TaskSetManager: Starting task 104.0 in stage 10.0 (TID 1153, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:45 INFO Executor: Running task 104.0 in stage 10.0 (TID 1153)
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO TaskSetManager: Finished task 94.0 in stage 10.0 (TID 1143) in 248 ms on localhost (89/200)
15/08/21 21:31:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,500
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000092
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000092_0: Committed
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 1,724B for [ps_partkey] INT32: 462 values, 1,855B raw, 1,688B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO ColumnChunkPageWriteStore: written 2,002B for [value] DOUBLE: 462 values, 3,703B raw, 1,958B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:45 INFO Executor: Finished task 92.0 in stage 10.0 (TID 1141). 843 bytes result sent to driver
15/08/21 21:31:45 INFO TaskSetManager: Starting task 105.0 in stage 10.0 (TID 1154, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:45 INFO Executor: Running task 105.0 in stage 10.0 (TID 1154)
15/08/21 21:31:45 INFO TaskSetManager: Finished task 92.0 in stage 10.0 (TID 1141) in 282 ms on localhost (90/200)
15/08/21 21:31:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000073
15/08/21 21:31:45 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000073_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000072
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000072_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 73.0 in stage 10.0 (TID 1122). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 72.0 in stage 10.0 (TID 1121). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 106.0 in stage 10.0 (TID 1155, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO Executor: Running task 106.0 in stage 10.0 (TID 1155)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO TaskSetManager: Starting task 107.0 in stage 10.0 (TID 1156, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:46 INFO Executor: Running task 107.0 in stage 10.0 (TID 1156)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 73.0 in stage 10.0 (TID 1122) in 695 ms on localhost (91/200)
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000095
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000095_0: Committed
15/08/21 21:31:46 INFO TaskSetManager: Finished task 72.0 in stage 10.0 (TID 1121) in 704 ms on localhost (92/200)
15/08/21 21:31:46 INFO Executor: Finished task 95.0 in stage 10.0 (TID 1144). 843 bytes result sent to driver
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000096
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000096_0: Committed
15/08/21 21:31:46 INFO TaskSetManager: Starting task 108.0 in stage 10.0 (TID 1157, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 108.0 in stage 10.0 (TID 1157)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,988
15/08/21 21:31:46 INFO TaskSetManager: Finished task 95.0 in stage 10.0 (TID 1144) in 253 ms on localhost (93/200)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,924
15/08/21 21:31:46 INFO Executor: Finished task 96.0 in stage 10.0 (TID 1145). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 109.0 in stage 10.0 (TID 1158, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 109.0 in stage 10.0 (TID 1158)
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,630B for [ps_partkey] INT32: 436 values, 1,751B raw, 1,594B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,897B for [value] DOUBLE: 436 values, 3,495B raw, 1,853B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO TaskSetManager: Finished task 96.0 in stage 10.0 (TID 1145) in 245 ms on localhost (94/200)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000097
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000097_0: Committed
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,656
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,625B for [ps_partkey] INT32: 434 values, 1,743B raw, 1,589B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,886B for [value] DOUBLE: 434 values, 3,479B raw, 1,842B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO Executor: Finished task 97.0 in stage 10.0 (TID 1146). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 110.0 in stage 10.0 (TID 1159, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 110.0 in stage 10.0 (TID 1159)
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,742B for [ps_partkey] INT32: 469 values, 1,883B raw, 1,706B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,034B for [value] DOUBLE: 469 values, 3,759B raw, 1,990B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO TaskSetManager: Finished task 97.0 in stage 10.0 (TID 1146) in 232 ms on localhost (95/200)
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000098
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000098_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 98.0 in stage 10.0 (TID 1147). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 111.0 in stage 10.0 (TID 1160, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 111.0 in stage 10.0 (TID 1160)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 98.0 in stage 10.0 (TID 1147) in 221 ms on localhost (96/200)
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000100
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000100_0: Committed
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000099
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000099_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 100.0 in stage 10.0 (TID 1149). 843 bytes result sent to driver
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO TaskSetManager: Starting task 112.0 in stage 10.0 (TID 1161, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 112.0 in stage 10.0 (TID 1161)
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO TaskSetManager: Finished task 100.0 in stage 10.0 (TID 1149) in 237 ms on localhost (97/200)
15/08/21 21:31:46 INFO Executor: Finished task 99.0 in stage 10.0 (TID 1148). 843 bytes result sent to driver
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO TaskSetManager: Starting task 113.0 in stage 10.0 (TID 1162, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO Executor: Running task 113.0 in stage 10.0 (TID 1162)
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO TaskSetManager: Finished task 99.0 in stage 10.0 (TID 1148) in 244 ms on localhost (98/200)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,968
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,316
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,622B for [ps_partkey] INT32: 435 values, 1,747B raw, 1,586B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,887B for [value] DOUBLE: 435 values, 3,487B raw, 1,843B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,024B for [ps_partkey] INT32: 552 values, 2,215B raw, 1,988B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,366B for [value] DOUBLE: 552 values, 4,423B raw, 2,322B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000102
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000102_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000101
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000101_0: Committed
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,720
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO Executor: Finished task 101.0 in stage 10.0 (TID 1150). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 102.0 in stage 10.0 (TID 1151). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 114.0 in stage 10.0 (TID 1163, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 114.0 in stage 10.0 (TID 1163)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 115.0 in stage 10.0 (TID 1164, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,750B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,714B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,768
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,006B for [value] DOUBLE: 473 values, 3,791B raw, 1,962B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO TaskSetManager: Finished task 101.0 in stage 10.0 (TID 1150) in 229 ms on localhost (99/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 102.0 in stage 10.0 (TID 1151) in 227 ms on localhost (100/200)
15/08/21 21:31:46 INFO Executor: Running task 115.0 in stage 10.0 (TID 1164)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,581B for [ps_partkey] INT32: 425 values, 1,707B raw, 1,545B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,823B for [value] DOUBLE: 425 values, 3,407B raw, 1,779B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000104
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000104_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000103
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000103_0: Committed
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO Executor: Finished task 104.0 in stage 10.0 (TID 1153). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 103.0 in stage 10.0 (TID 1152). 843 bytes result sent to driver
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO TaskSetManager: Starting task 116.0 in stage 10.0 (TID 1165, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 116.0 in stage 10.0 (TID 1165)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 117.0 in stage 10.0 (TID 1166, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 117.0 in stage 10.0 (TID 1166)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 104.0 in stage 10.0 (TID 1153) in 242 ms on localhost (101/200)
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,716
15/08/21 21:31:46 INFO TaskSetManager: Finished task 103.0 in stage 10.0 (TID 1152) in 266 ms on localhost (102/200)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,968
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,749B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,713B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,028B for [value] DOUBLE: 472 values, 3,783B raw, 1,984B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,716
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,799B for [ps_partkey] INT32: 485 values, 1,947B raw, 1,763B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,076B for [value] DOUBLE: 485 values, 3,887B raw, 2,032B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,388
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,747B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,711B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,025B for [value] DOUBLE: 472 values, 3,783B raw, 1,981B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,860
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,568
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,698B for [ps_partkey] INT32: 456 values, 1,831B raw, 1,662B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,962B for [value] DOUBLE: 456 values, 3,655B raw, 1,918B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,562B for [ps_partkey] INT32: 415 values, 1,667B raw, 1,526B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,779B for [value] DOUBLE: 415 values, 3,327B raw, 1,735B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,776B for [ps_partkey] INT32: 480 values, 1,927B raw, 1,740B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,041B for [value] DOUBLE: 480 values, 3,847B raw, 1,997B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,848
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,716
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,781B for [ps_partkey] INT32: 479 values, 1,923B raw, 1,745B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,747B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,711B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,071B for [value] DOUBLE: 479 values, 3,839B raw, 2,027B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,025B for [value] DOUBLE: 472 values, 3,783B raw, 1,981B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,048
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,636B for [ps_partkey] INT32: 439 values, 1,763B raw, 1,600B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,867B for [value] DOUBLE: 439 values, 3,519B raw, 1,823B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000089
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000087
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000087_0: Committed
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000089_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000088
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000088_0: Committed
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000110
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000110_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 87.0 in stage 10.0 (TID 1136). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 89.0 in stage 10.0 (TID 1138). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 110.0 in stage 10.0 (TID 1159). 843 bytes result sent to driver
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000109
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000109_0: Committed
15/08/21 21:31:46 INFO TaskSetManager: Starting task 118.0 in stage 10.0 (TID 1167, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 118.0 in stage 10.0 (TID 1167)
15/08/21 21:31:46 INFO Executor: Finished task 88.0 in stage 10.0 (TID 1137). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 119.0 in stage 10.0 (TID 1168, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 119.0 in stage 10.0 (TID 1168)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 120.0 in stage 10.0 (TID 1169, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 120.0 in stage 10.0 (TID 1169)
15/08/21 21:31:46 INFO Executor: Finished task 109.0 in stage 10.0 (TID 1158). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 121.0 in stage 10.0 (TID 1170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 121.0 in stage 10.0 (TID 1170)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 122.0 in stage 10.0 (TID 1171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 122.0 in stage 10.0 (TID 1171)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 110.0 in stage 10.0 (TID 1159) in 265 ms on localhost (103/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 87.0 in stage 10.0 (TID 1136) in 710 ms on localhost (104/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 89.0 in stage 10.0 (TID 1138) in 680 ms on localhost (105/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 88.0 in stage 10.0 (TID 1137) in 681 ms on localhost (106/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 109.0 in stage 10.0 (TID 1158) in 276 ms on localhost (107/200)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000108
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000108_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000105
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000106
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000105_0: Committed
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000106_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 105.0 in stage 10.0 (TID 1154). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 106.0 in stage 10.0 (TID 1155). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 108.0 in stage 10.0 (TID 1157). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 123.0 in stage 10.0 (TID 1172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 124.0 in stage 10.0 (TID 1173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 124.0 in stage 10.0 (TID 1173)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 125.0 in stage 10.0 (TID 1174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 125.0 in stage 10.0 (TID 1174)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 106.0 in stage 10.0 (TID 1155) in 308 ms on localhost (108/200)
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO Executor: Running task 123.0 in stage 10.0 (TID 1172)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 108.0 in stage 10.0 (TID 1157) in 300 ms on localhost (109/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 105.0 in stage 10.0 (TID 1154) in 326 ms on localhost (110/200)
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000111
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000113
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000113_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000112
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000112_0: Committed
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000111_0: Committed
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO Executor: Finished task 113.0 in stage 10.0 (TID 1162). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 112.0 in stage 10.0 (TID 1161). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 111.0 in stage 10.0 (TID 1160). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 126.0 in stage 10.0 (TID 1175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 127.0 in stage 10.0 (TID 1176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 128.0 in stage 10.0 (TID 1177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 126.0 in stage 10.0 (TID 1175)
15/08/21 21:31:46 INFO Executor: Running task 127.0 in stage 10.0 (TID 1176)
15/08/21 21:31:46 INFO Executor: Running task 128.0 in stage 10.0 (TID 1177)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 113.0 in stage 10.0 (TID 1162) in 245 ms on localhost (111/200)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,476
15/08/21 21:31:46 INFO TaskSetManager: Finished task 111.0 in stage 10.0 (TID 1160) in 275 ms on localhost (112/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 112.0 in stage 10.0 (TID 1161) in 254 ms on localhost (113/200)
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,715B for [ps_partkey] INT32: 460 values, 1,847B raw, 1,679B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,981B for [value] DOUBLE: 460 values, 3,687B raw, 1,937B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,376
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,776
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,042B for [ps_partkey] INT32: 555 values, 2,227B raw, 2,006B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,348B for [value] DOUBLE: 555 values, 4,447B raw, 2,304B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,762B for [ps_partkey] INT32: 475 values, 1,907B raw, 1,726B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,046B for [value] DOUBLE: 475 values, 3,807B raw, 2,002B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000114
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000114_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 114.0 in stage 10.0 (TID 1163). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 129.0 in stage 10.0 (TID 1178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 129.0 in stage 10.0 (TID 1178)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 114.0 in stage 10.0 (TID 1163) in 238 ms on localhost (114/200)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,036
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,644B for [ps_partkey] INT32: 438 values, 1,759B raw, 1,608B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,917B for [value] DOUBLE: 438 values, 3,511B raw, 1,873B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000116
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000116_0: Committed
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO Executor: Finished task 116.0 in stage 10.0 (TID 1165). 843 bytes result sent to driver
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 21:31:46 INFO TaskSetManager: Starting task 130.0 in stage 10.0 (TID 1179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 130.0 in stage 10.0 (TID 1179)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 116.0 in stage 10.0 (TID 1165) in 203 ms on localhost (115/200)
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000117
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000117_0: Committed
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO Executor: Finished task 117.0 in stage 10.0 (TID 1166). 843 bytes result sent to driver
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO TaskSetManager: Starting task 131.0 in stage 10.0 (TID 1180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO Executor: Running task 131.0 in stage 10.0 (TID 1180)
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO TaskSetManager: Finished task 117.0 in stage 10.0 (TID 1166) in 213 ms on localhost (116/200)
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,956
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,876
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,788
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,216
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,656
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,605B for [ps_partkey] INT32: 430 values, 1,727B raw, 1,569B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,972B for [ps_partkey] INT32: 534 values, 2,143B raw, 1,936B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,241B for [value] DOUBLE: 534 values, 4,279B raw, 2,197B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,818B for [value] DOUBLE: 430 values, 3,447B raw, 1,774B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,593B for [ps_partkey] INT32: 426 values, 1,711B raw, 1,557B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,828B for [value] DOUBLE: 426 values, 3,415B raw, 1,784B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,840B for [ps_partkey] INT32: 497 values, 1,995B raw, 1,804B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,394B for [ps_partkey] INT32: 369 values, 1,483B raw, 1,358B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,120B for [value] DOUBLE: 497 values, 3,983B raw, 2,076B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,633B for [value] DOUBLE: 369 values, 2,959B raw, 1,589B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,548
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,559B for [ps_partkey] INT32: 414 values, 1,663B raw, 1,523B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,777B for [value] DOUBLE: 414 values, 3,319B raw, 1,733B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,428
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,532B for [ps_partkey] INT32: 408 values, 1,639B raw, 1,496B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,780B for [value] DOUBLE: 408 values, 3,271B raw, 1,736B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,168
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,396
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,496
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,013B for [ps_partkey] INT32: 545 values, 2,187B raw, 1,977B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,676
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,276B for [value] DOUBLE: 545 values, 4,367B raw, 2,232B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,702B for [ps_partkey] INT32: 456 values, 1,831B raw, 1,666B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,953B for [value] DOUBLE: 456 values, 3,655B raw, 1,909B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,370B for [ps_partkey] INT32: 361 values, 1,451B raw, 1,334B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,586B for [value] DOUBLE: 361 values, 2,895B raw, 1,542B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,571B for [ps_partkey] INT32: 420 values, 1,687B raw, 1,535B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,826B for [value] DOUBLE: 420 values, 3,367B raw, 1,782B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,608
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,729B for [ps_partkey] INT32: 467 values, 1,875B raw, 1,693B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,001B for [value] DOUBLE: 467 values, 3,743B raw, 1,957B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000122
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000122_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000124
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000124_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000128
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000128_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 122.0 in stage 10.0 (TID 1171). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 124.0 in stage 10.0 (TID 1173). 843 bytes result sent to driver
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000120
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000120_0: Committed
15/08/21 21:31:46 INFO TaskSetManager: Starting task 132.0 in stage 10.0 (TID 1181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 132.0 in stage 10.0 (TID 1181)
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000123
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000123_0: Committed
15/08/21 21:31:46 INFO TaskSetManager: Starting task 133.0 in stage 10.0 (TID 1182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 133.0 in stage 10.0 (TID 1182)
15/08/21 21:31:46 INFO Executor: Finished task 120.0 in stage 10.0 (TID 1169). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Finished task 122.0 in stage 10.0 (TID 1171) in 299 ms on localhost (117/200)
15/08/21 21:31:46 INFO Executor: Finished task 123.0 in stage 10.0 (TID 1172). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 134.0 in stage 10.0 (TID 1183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 134.0 in stage 10.0 (TID 1183)
15/08/21 21:31:46 INFO Executor: Finished task 128.0 in stage 10.0 (TID 1177). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 135.0 in stage 10.0 (TID 1184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 135.0 in stage 10.0 (TID 1184)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 124.0 in stage 10.0 (TID 1173) in 289 ms on localhost (118/200)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 136.0 in stage 10.0 (TID 1185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 136.0 in stage 10.0 (TID 1185)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 120.0 in stage 10.0 (TID 1169) in 304 ms on localhost (119/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 123.0 in stage 10.0 (TID 1172) in 291 ms on localhost (120/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 128.0 in stage 10.0 (TID 1177) in 273 ms on localhost (121/200)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,448
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,736
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,882B for [ps_partkey] INT32: 509 values, 2,043B raw, 1,846B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,157B for [value] DOUBLE: 509 values, 4,079B raw, 2,113B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,754B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,718B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,997B for [value] DOUBLE: 473 values, 3,791B raw, 1,953B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000129
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000129_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 129.0 in stage 10.0 (TID 1178). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 137.0 in stage 10.0 (TID 1186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 137.0 in stage 10.0 (TID 1186)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 129.0 in stage 10.0 (TID 1178) in 243 ms on localhost (122/200)
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000131
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000131_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 131.0 in stage 10.0 (TID 1180). 843 bytes result sent to driver
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO TaskSetManager: Starting task 138.0 in stage 10.0 (TID 1187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 138.0 in stage 10.0 (TID 1187)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 131.0 in stage 10.0 (TID 1180) in 216 ms on localhost (123/200)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000107
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000107_0: Committed
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO Executor: Finished task 107.0 in stage 10.0 (TID 1156). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 139.0 in stage 10.0 (TID 1188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 139.0 in stage 10.0 (TID 1188)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 107.0 in stage 10.0 (TID 1156) in 697 ms on localhost (124/200)
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,776
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,468
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,766B for [ps_partkey] INT32: 475 values, 1,907B raw, 1,730B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,416
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,026B for [value] DOUBLE: 475 values, 3,807B raw, 1,982B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,736
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,885B for [ps_partkey] INT32: 510 values, 2,047B raw, 1,849B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,137B for [value] DOUBLE: 510 values, 4,087B raw, 2,093B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,701B for [ps_partkey] INT32: 457 values, 1,835B raw, 1,665B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,760B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,724B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,952B for [value] DOUBLE: 457 values, 3,663B raw, 1,908B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,002B for [value] DOUBLE: 473 values, 3,791B raw, 1,958B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,028
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,635B for [ps_partkey] INT32: 438 values, 1,759B raw, 1,599B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,892B for [value] DOUBLE: 438 values, 3,511B raw, 1,848B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000136
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000136_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000132
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000132_0: Committed
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,296
15/08/21 21:31:46 INFO Executor: Finished task 136.0 in stage 10.0 (TID 1185). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 132.0 in stage 10.0 (TID 1181). 843 bytes result sent to driver
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,670B for [ps_partkey] INT32: 451 values, 1,811B raw, 1,634B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,930B for [value] DOUBLE: 451 values, 3,615B raw, 1,886B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO TaskSetManager: Starting task 140.0 in stage 10.0 (TID 1189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 140.0 in stage 10.0 (TID 1189)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 141.0 in stage 10.0 (TID 1190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 141.0 in stage 10.0 (TID 1190)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,148
15/08/21 21:31:46 INFO TaskSetManager: Finished task 136.0 in stage 10.0 (TID 1185) in 165 ms on localhost (125/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 132.0 in stage 10.0 (TID 1181) in 170 ms on localhost (126/200)
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,173B for [ps_partkey] INT32: 594 values, 2,383B raw, 2,137B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,471B for [value] DOUBLE: 594 values, 4,759B raw, 2,427B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000137
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000137_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000138
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000115
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000138_0: Committed
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000115_0: Committed
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO Executor: Finished task 138.0 in stage 10.0 (TID 1187). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 137.0 in stage 10.0 (TID 1186). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 115.0 in stage 10.0 (TID 1164). 843 bytes result sent to driver
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO TaskSetManager: Starting task 142.0 in stage 10.0 (TID 1191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 142.0 in stage 10.0 (TID 1191)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 143.0 in stage 10.0 (TID 1192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 143.0 in stage 10.0 (TID 1192)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 144.0 in stage 10.0 (TID 1193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 144.0 in stage 10.0 (TID 1193)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 137.0 in stage 10.0 (TID 1186) in 193 ms on localhost (127/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 138.0 in stage 10.0 (TID 1187) in 180 ms on localhost (128/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 115.0 in stage 10.0 (TID 1164) in 665 ms on localhost (129/200)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,348
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,692B for [ps_partkey] INT32: 454 values, 1,823B raw, 1,656B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,911B for [value] DOUBLE: 454 values, 3,639B raw, 1,867B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000139
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000139_0: Committed
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO Executor: Finished task 139.0 in stage 10.0 (TID 1188). 843 bytes result sent to driver
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:46 INFO TaskSetManager: Starting task 145.0 in stage 10.0 (TID 1194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 145.0 in stage 10.0 (TID 1194)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 139.0 in stage 10.0 (TID 1188) in 169 ms on localhost (130/200)
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,676
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,220
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,667B for [ps_partkey] INT32: 448 values, 1,799B raw, 1,631B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,577B for [ps_partkey] INT32: 420 values, 1,687B raw, 1,541B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,893B for [value] DOUBLE: 448 values, 3,591B raw, 1,849B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,784B for [value] DOUBLE: 420 values, 3,367B raw, 1,740B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000141
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000141_0: Committed
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,020
15/08/21 21:31:46 INFO Executor: Finished task 141.0 in stage 10.0 (TID 1190). 843 bytes result sent to driver
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,048
15/08/21 21:31:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,836
15/08/21 21:31:46 INFO TaskSetManager: Starting task 146.0 in stage 10.0 (TID 1195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 146.0 in stage 10.0 (TID 1195)
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,986B for [ps_partkey] INT32: 539 values, 2,163B raw, 1,950B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,362B for [value] DOUBLE: 539 values, 4,319B raw, 2,318B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,944B for [ps_partkey] INT32: 528 values, 2,119B raw, 1,908B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,348B for [value] DOUBLE: 528 values, 4,231B raw, 2,304B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO TaskSetManager: Finished task 141.0 in stage 10.0 (TID 1190) in 169 ms on localhost (131/200)
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 1,807B for [ps_partkey] INT32: 488 values, 1,959B raw, 1,771B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO ColumnChunkPageWriteStore: written 2,068B for [value] DOUBLE: 488 values, 3,911B raw, 2,024B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:46 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:46 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000144
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000144_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000143
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000143_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 144.0 in stage 10.0 (TID 1193). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Finished task 143.0 in stage 10.0 (TID 1192). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 147.0 in stage 10.0 (TID 1196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 148.0 in stage 10.0 (TID 1197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 148.0 in stage 10.0 (TID 1197)
15/08/21 21:31:46 INFO Executor: Running task 147.0 in stage 10.0 (TID 1196)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 143.0 in stage 10.0 (TID 1192) in 152 ms on localhost (132/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 144.0 in stage 10.0 (TID 1193) in 151 ms on localhost (133/200)
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000119
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000119_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000125
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000125_0: Committed
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000121
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000121_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 119.0 in stage 10.0 (TID 1168). 843 bytes result sent to driver
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000118
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000118_0: Committed
15/08/21 21:31:46 INFO Executor: Finished task 125.0 in stage 10.0 (TID 1174). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 149.0 in stage 10.0 (TID 1198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Finished task 121.0 in stage 10.0 (TID 1170). 843 bytes result sent to driver
15/08/21 21:31:46 INFO Executor: Running task 149.0 in stage 10.0 (TID 1198)
15/08/21 21:31:46 INFO Executor: Finished task 118.0 in stage 10.0 (TID 1167). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Starting task 150.0 in stage 10.0 (TID 1199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 150.0 in stage 10.0 (TID 1199)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 151.0 in stage 10.0 (TID 1200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 151.0 in stage 10.0 (TID 1200)
15/08/21 21:31:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000127
15/08/21 21:31:46 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000127_0: Committed
15/08/21 21:31:46 INFO TaskSetManager: Starting task 152.0 in stage 10.0 (TID 1201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 119.0 in stage 10.0 (TID 1168) in 695 ms on localhost (134/200)
15/08/21 21:31:46 INFO TaskSetManager: Finished task 125.0 in stage 10.0 (TID 1174) in 680 ms on localhost (135/200)
15/08/21 21:31:46 INFO Executor: Running task 152.0 in stage 10.0 (TID 1201)
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 21:31:46 INFO TaskSetManager: Finished task 121.0 in stage 10.0 (TID 1170) in 700 ms on localhost (136/200)
15/08/21 21:31:46 INFO Executor: Finished task 127.0 in stage 10.0 (TID 1176). 843 bytes result sent to driver
15/08/21 21:31:46 INFO TaskSetManager: Finished task 118.0 in stage 10.0 (TID 1167) in 703 ms on localhost (137/200)
15/08/21 21:31:46 INFO TaskSetManager: Starting task 153.0 in stage 10.0 (TID 1202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:46 INFO Executor: Running task 153.0 in stage 10.0 (TID 1202)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000126
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000126_0: Committed
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,076
15/08/21 21:31:47 INFO Executor: Finished task 126.0 in stage 10.0 (TID 1175). 843 bytes result sent to driver
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,641B for [ps_partkey] INT32: 440 values, 1,767B raw, 1,605B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,926B for [value] DOUBLE: 440 values, 3,527B raw, 1,882B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO TaskSetManager: Starting task 154.0 in stage 10.0 (TID 1203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 127.0 in stage 10.0 (TID 1176) in 676 ms on localhost (138/200)
15/08/21 21:31:47 INFO Executor: Running task 154.0 in stage 10.0 (TID 1203)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 126.0 in stage 10.0 (TID 1175) in 686 ms on localhost (139/200)
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,596
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,718B for [ps_partkey] INT32: 466 values, 1,871B raw, 1,682B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,081B for [value] DOUBLE: 466 values, 3,735B raw, 2,037B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000130
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000130_0: Committed
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO Executor: Finished task 130.0 in stage 10.0 (TID 1179). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 155.0 in stage 10.0 (TID 1204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 155.0 in stage 10.0 (TID 1204)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 130.0 in stage 10.0 (TID 1179) in 703 ms on localhost (140/200)
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000145
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000145_0: Committed
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO Executor: Finished task 145.0 in stage 10.0 (TID 1194). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 156.0 in stage 10.0 (TID 1205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 156.0 in stage 10.0 (TID 1205)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 145.0 in stage 10.0 (TID 1194) in 308 ms on localhost (141/200)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000133
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000133_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 133.0 in stage 10.0 (TID 1182). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 157.0 in stage 10.0 (TID 1206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000146
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000146_0: Committed
15/08/21 21:31:47 INFO Executor: Running task 157.0 in stage 10.0 (TID 1206)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000134
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000134_0: Committed
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,148
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,640
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,940
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,248
15/08/21 21:31:47 INFO TaskSetManager: Finished task 133.0 in stage 10.0 (TID 1182) in 599 ms on localhost (142/200)
15/08/21 21:31:47 INFO Executor: Finished task 134.0 in stage 10.0 (TID 1183). 843 bytes result sent to driver
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000135
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000135_0: Committed
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,652B for [ps_partkey] INT32: 444 values, 1,783B raw, 1,616B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO TaskSetManager: Starting task 158.0 in stage 10.0 (TID 1207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,920B for [ps_partkey] INT32: 519 values, 2,083B raw, 1,884B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO Executor: Finished task 146.0 in stage 10.0 (TID 1195). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Running task 158.0 in stage 10.0 (TID 1207)
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,916
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,263B for [value] DOUBLE: 519 values, 4,159B raw, 2,219B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,020B for [ps_partkey] INT32: 549 values, 2,203B raw, 1,984B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,418B for [value] DOUBLE: 549 values, 4,399B raw, 2,374B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO Executor: Finished task 135.0 in stage 10.0 (TID 1184). 843 bytes result sent to driver
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,983B for [value] DOUBLE: 444 values, 3,559B raw, 1,939B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,208
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,788B for [ps_partkey] INT32: 484 values, 1,943B raw, 1,752B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,160B for [value] DOUBLE: 484 values, 3,879B raw, 2,116B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,956B for [ps_partkey] INT32: 532 values, 2,135B raw, 1,920B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO TaskSetManager: Starting task 159.0 in stage 10.0 (TID 1208, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,326B for [value] DOUBLE: 532 values, 4,263B raw, 2,282B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO TaskSetManager: Finished task 134.0 in stage 10.0 (TID 1183) in 609 ms on localhost (143/200)
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,228
15/08/21 21:31:47 INFO TaskSetManager: Starting task 160.0 in stage 10.0 (TID 1209, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 160.0 in stage 10.0 (TID 1209)
15/08/21 21:31:47 INFO Executor: Running task 159.0 in stage 10.0 (TID 1208)
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,499B for [ps_partkey] INT32: 397 values, 1,595B raw, 1,463B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO TaskSetManager: Finished task 146.0 in stage 10.0 (TID 1195) in 279 ms on localhost (144/200)
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,806B for [value] DOUBLE: 397 values, 3,183B raw, 1,762B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO TaskSetManager: Finished task 135.0 in stage 10.0 (TID 1184) in 609 ms on localhost (145/200)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,839B for [ps_partkey] INT32: 498 values, 1,999B raw, 1,803B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,181B for [value] DOUBLE: 498 values, 3,991B raw, 2,137B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,468
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,707B for [ps_partkey] INT32: 460 values, 1,847B raw, 1,671B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,072B for [value] DOUBLE: 460 values, 3,687B raw, 2,028B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000147
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000147_0: Committed
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO Executor: Finished task 147.0 in stage 10.0 (TID 1196). 843 bytes result sent to driver
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO TaskSetManager: Starting task 161.0 in stage 10.0 (TID 1210, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 161.0 in stage 10.0 (TID 1210)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO TaskSetManager: Finished task 147.0 in stage 10.0 (TID 1196) in 274 ms on localhost (146/200)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000153
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000153_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 153.0 in stage 10.0 (TID 1202). 843 bytes result sent to driver
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000148
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000148_0: Committed
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000149
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000149_0: Committed
15/08/21 21:31:47 INFO TaskSetManager: Starting task 162.0 in stage 10.0 (TID 1211, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 162.0 in stage 10.0 (TID 1211)
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO Executor: Finished task 149.0 in stage 10.0 (TID 1198). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Finished task 148.0 in stage 10.0 (TID 1197). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 163.0 in stage 10.0 (TID 1212, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 163.0 in stage 10.0 (TID 1212)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 164.0 in stage 10.0 (TID 1213, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 153.0 in stage 10.0 (TID 1202) in 244 ms on localhost (147/200)
15/08/21 21:31:47 INFO Executor: Running task 164.0 in stage 10.0 (TID 1213)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 149.0 in stage 10.0 (TID 1198) in 266 ms on localhost (148/200)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO TaskSetManager: Finished task 148.0 in stage 10.0 (TID 1197) in 280 ms on localhost (149/200)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000150
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000150_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 150.0 in stage 10.0 (TID 1199). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 165.0 in stage 10.0 (TID 1214, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 165.0 in stage 10.0 (TID 1214)
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO TaskSetManager: Finished task 150.0 in stage 10.0 (TID 1199) in 289 ms on localhost (150/200)
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000154
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000154_0: Committed
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,196
15/08/21 21:31:47 INFO Executor: Finished task 154.0 in stage 10.0 (TID 1203). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 166.0 in stage 10.0 (TID 1215, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 166.0 in stage 10.0 (TID 1215)
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO TaskSetManager: Finished task 154.0 in stage 10.0 (TID 1203) in 271 ms on localhost (151/200)
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,017B for [ps_partkey] INT32: 548 values, 2,199B raw, 1,981B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,404B for [value] DOUBLE: 548 values, 4,391B raw, 2,360B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000152
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000152_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 152.0 in stage 10.0 (TID 1201). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 167.0 in stage 10.0 (TID 1216, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 167.0 in stage 10.0 (TID 1216)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO TaskSetManager: Finished task 152.0 in stage 10.0 (TID 1201) in 329 ms on localhost (152/200)
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000140
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000140_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 140.0 in stage 10.0 (TID 1189). 843 bytes result sent to driver
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO TaskSetManager: Starting task 168.0 in stage 10.0 (TID 1217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 168.0 in stage 10.0 (TID 1217)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 140.0 in stage 10.0 (TID 1189) in 575 ms on localhost (153/200)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,640
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,156
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,913B for [ps_partkey] INT32: 519 values, 2,083B raw, 1,877B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,292B for [value] DOUBLE: 519 values, 4,159B raw, 2,248B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,653B for [ps_partkey] INT32: 444 values, 1,783B raw, 1,617B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,973B for [value] DOUBLE: 444 values, 3,559B raw, 1,929B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,388
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,520B for [ps_partkey] INT32: 406 values, 1,631B raw, 1,484B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,821B for [value] DOUBLE: 406 values, 3,255B raw, 1,777B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,036
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,116
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,469B for [ps_partkey] INT32: 388 values, 1,559B raw, 1,433B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,754B for [value] DOUBLE: 388 values, 3,111B raw, 1,710B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,809B for [ps_partkey] INT32: 492 values, 1,975B raw, 1,773B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,175B for [value] DOUBLE: 492 values, 3,943B raw, 2,131B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000142
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000142_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 142.0 in stage 10.0 (TID 1191). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 169.0 in stage 10.0 (TID 1218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 169.0 in stage 10.0 (TID 1218)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 142.0 in stage 10.0 (TID 1191) in 609 ms on localhost (154/200)
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,196
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,490B for [ps_partkey] INT32: 396 values, 1,591B raw, 1,454B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,809B for [value] DOUBLE: 396 values, 3,175B raw, 1,765B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,196
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,396
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,004B for [ps_partkey] INT32: 546 values, 2,191B raw, 1,968B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,396B for [value] DOUBLE: 546 values, 4,375B raw, 2,352B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,708
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,008
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,867B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,831B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,223B for [value] DOUBLE: 506 values, 4,055B raw, 2,179B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,971B for [ps_partkey] INT32: 537 values, 2,155B raw, 1,935B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,357B for [value] DOUBLE: 537 values, 4,303B raw, 2,313B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,923B for [ps_partkey] INT32: 522 values, 2,095B raw, 1,887B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,271B for [value] DOUBLE: 522 values, 4,183B raw, 2,227B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,080
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,647B for [ps_partkey] INT32: 441 values, 1,771B raw, 1,611B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,933B for [value] DOUBLE: 441 values, 3,535B raw, 1,889B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,736
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,096
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,751B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,715B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,112B for [value] DOUBLE: 473 values, 3,791B raw, 2,068B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,641B for [ps_partkey] INT32: 441 values, 1,771B raw, 1,605B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,951B for [value] DOUBLE: 441 values, 3,535B raw, 1,907B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,248
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,670B for [ps_partkey] INT32: 449 values, 1,803B raw, 1,634B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,995B for [value] DOUBLE: 449 values, 3,599B raw, 1,951B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000151
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000151_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 151.0 in stage 10.0 (TID 1200). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 170.0 in stage 10.0 (TID 1219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 170.0 in stage 10.0 (TID 1219)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 151.0 in stage 10.0 (TID 1200) in 656 ms on localhost (155/200)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,268
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,510B for [ps_partkey] INT32: 400 values, 1,607B raw, 1,474B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,801B for [value] DOUBLE: 400 values, 3,207B raw, 1,757B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000158
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000158_0: Committed
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000155
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000155_0: Committed
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000157
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000162
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000157_0: Committed
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000162_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 158.0 in stage 10.0 (TID 1207). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Finished task 157.0 in stage 10.0 (TID 1206). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Finished task 155.0 in stage 10.0 (TID 1204). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 171.0 in stage 10.0 (TID 1220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 171.0 in stage 10.0 (TID 1220)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 172.0 in stage 10.0 (TID 1221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 172.0 in stage 10.0 (TID 1221)
15/08/21 21:31:47 INFO Executor: Finished task 162.0 in stage 10.0 (TID 1211). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 173.0 in stage 10.0 (TID 1222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 174.0 in stage 10.0 (TID 1223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 158.0 in stage 10.0 (TID 1207) in 554 ms on localhost (156/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 157.0 in stage 10.0 (TID 1206) in 574 ms on localhost (157/200)
15/08/21 21:31:47 INFO Executor: Running task 173.0 in stage 10.0 (TID 1222)
15/08/21 21:31:47 INFO Executor: Running task 174.0 in stage 10.0 (TID 1223)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 155.0 in stage 10.0 (TID 1204) in 639 ms on localhost (158/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 162.0 in stage 10.0 (TID 1211) in 514 ms on localhost (159/200)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000161
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000161_0: Committed
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000168
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000168_0: Committed
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000156
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000156_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 161.0 in stage 10.0 (TID 1210). 843 bytes result sent to driver
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000164
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000164_0: Committed
15/08/21 21:31:47 INFO TaskSetManager: Starting task 175.0 in stage 10.0 (TID 1224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Finished task 168.0 in stage 10.0 (TID 1217). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Running task 175.0 in stage 10.0 (TID 1224)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000163
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000163_0: Committed
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000159
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000159_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 164.0 in stage 10.0 (TID 1213). 843 bytes result sent to driver
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000166
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000166_0: Committed
15/08/21 21:31:47 INFO TaskSetManager: Starting task 176.0 in stage 10.0 (TID 1225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 176.0 in stage 10.0 (TID 1225)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 161.0 in stage 10.0 (TID 1210) in 523 ms on localhost (160/200)
15/08/21 21:31:47 INFO Executor: Finished task 163.0 in stage 10.0 (TID 1212). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Finished task 156.0 in stage 10.0 (TID 1205). 843 bytes result sent to driver
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000167
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000167_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 159.0 in stage 10.0 (TID 1208). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Finished task 166.0 in stage 10.0 (TID 1215). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Finished task 168.0 in stage 10.0 (TID 1217) in 435 ms on localhost (161/200)
15/08/21 21:31:47 INFO Executor: Finished task 167.0 in stage 10.0 (TID 1216). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 177.0 in stage 10.0 (TID 1226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 177.0 in stage 10.0 (TID 1226)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000160
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000169
15/08/21 21:31:47 INFO TaskSetManager: Starting task 178.0 in stage 10.0 (TID 1227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000169_0: Committed
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000160_0: Committed
15/08/21 21:31:47 INFO Executor: Running task 178.0 in stage 10.0 (TID 1227)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 179.0 in stage 10.0 (TID 1228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 180.0 in stage 10.0 (TID 1229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 180.0 in stage 10.0 (TID 1229)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 181.0 in stage 10.0 (TID 1230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Finished task 169.0 in stage 10.0 (TID 1218). 843 bytes result sent to driver
15/08/21 21:31:47 INFO Executor: Running task 181.0 in stage 10.0 (TID 1230)
15/08/21 21:31:47 INFO Executor: Finished task 160.0 in stage 10.0 (TID 1209). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 182.0 in stage 10.0 (TID 1231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 182.0 in stage 10.0 (TID 1231)
15/08/21 21:31:47 INFO Executor: Running task 179.0 in stage 10.0 (TID 1228)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 183.0 in stage 10.0 (TID 1232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 183.0 in stage 10.0 (TID 1232)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 164.0 in stage 10.0 (TID 1213) in 528 ms on localhost (162/200)
15/08/21 21:31:47 INFO TaskSetManager: Starting task 184.0 in stage 10.0 (TID 1233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 184.0 in stage 10.0 (TID 1233)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 159.0 in stage 10.0 (TID 1208) in 569 ms on localhost (163/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 156.0 in stage 10.0 (TID 1205) in 612 ms on localhost (164/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 163.0 in stage 10.0 (TID 1212) in 529 ms on localhost (165/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 167.0 in stage 10.0 (TID 1216) in 466 ms on localhost (166/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 166.0 in stage 10.0 (TID 1215) in 501 ms on localhost (167/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 169.0 in stage 10.0 (TID 1218) in 360 ms on localhost (168/200)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 160.0 in stage 10.0 (TID 1209) in 570 ms on localhost (169/200)
15/08/21 21:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000170
15/08/21 21:31:47 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000170_0: Committed
15/08/21 21:31:47 INFO Executor: Finished task 170.0 in stage 10.0 (TID 1219). 843 bytes result sent to driver
15/08/21 21:31:47 INFO TaskSetManager: Starting task 185.0 in stage 10.0 (TID 1234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:47 INFO Executor: Running task 185.0 in stage 10.0 (TID 1234)
15/08/21 21:31:47 INFO TaskSetManager: Finished task 170.0 in stage 10.0 (TID 1219) in 152 ms on localhost (170/200)
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:47 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:47 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,088
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,648
15/08/21 21:31:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,818B for [ps_partkey] INT32: 491 values, 1,971B raw, 1,782B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,194B for [value] DOUBLE: 491 values, 3,935B raw, 2,150B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,917B for [ps_partkey] INT32: 519 values, 2,083B raw, 1,881B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,243B for [value] DOUBLE: 519 values, 4,159B raw, 2,199B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,264
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 1,856B for [ps_partkey] INT32: 501 values, 2,011B raw, 1,820B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO ColumnChunkPageWriteStore: written 2,166B for [value] DOUBLE: 501 values, 4,015B raw, 2,122B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,568
15/08/21 21:31:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,112
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,824B for [ps_partkey] INT32: 493 values, 1,979B raw, 1,788B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,182B for [value] DOUBLE: 493 values, 3,951B raw, 2,138B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,728B for [ps_partkey] INT32: 465 values, 1,867B raw, 1,692B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,476
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,070B for [value] DOUBLE: 465 values, 3,727B raw, 2,026B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,508
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,488
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,200
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,556
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,088
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,388
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,542B for [ps_partkey] INT32: 412 values, 1,655B raw, 1,506B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,820B for [value] DOUBLE: 412 values, 3,303B raw, 1,776B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,788
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,358B for [ps_partkey] INT32: 360 values, 1,447B raw, 1,322B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,656B for [value] DOUBLE: 360 values, 2,887B raw, 1,612B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,843B for [ps_partkey] INT32: 497 values, 1,995B raw, 1,807B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,890B for [ps_partkey] INT32: 514 values, 2,063B raw, 1,854B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,258B for [value] DOUBLE: 514 values, 4,119B raw, 2,214B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,192B for [value] DOUBLE: 497 values, 3,983B raw, 2,148B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,992B for [ps_partkey] INT32: 541 values, 2,171B raw, 1,956B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,712B for [ps_partkey] INT32: 461 values, 1,851B raw, 1,676B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,374B for [value] DOUBLE: 541 values, 4,335B raw, 2,330B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,039B for [value] DOUBLE: 461 values, 3,695B raw, 1,995B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,840
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,872B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,836B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,595B for [ps_partkey] INT32: 426 values, 1,711B raw, 1,559B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,213B for [value] DOUBLE: 506 values, 4,055B raw, 2,169B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,892B for [value] DOUBLE: 426 values, 3,415B raw, 1,848B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,435B for [ps_partkey] INT32: 379 values, 1,523B raw, 1,399B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,727B for [value] DOUBLE: 379 values, 3,039B raw, 1,683B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,528
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,058B for [ps_partkey] INT32: 563 values, 2,259B raw, 2,022B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,452B for [value] DOUBLE: 563 values, 4,511B raw, 2,408B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000171
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000171_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 171.0 in stage 10.0 (TID 1220). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Starting task 186.0 in stage 10.0 (TID 1235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 186.0 in stage 10.0 (TID 1235)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 171.0 in stage 10.0 (TID 1220) in 914 ms on localhost (171/200)
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000165
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000165_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000172
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000183
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000183_0: Committed
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000172_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 183.0 in stage 10.0 (TID 1232). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Finished task 165.0 in stage 10.0 (TID 1214). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Starting task 187.0 in stage 10.0 (TID 1236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO TaskSetManager: Starting task 188.0 in stage 10.0 (TID 1237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Finished task 172.0 in stage 10.0 (TID 1221). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Running task 188.0 in stage 10.0 (TID 1237)
15/08/21 21:31:48 INFO TaskSetManager: Starting task 189.0 in stage 10.0 (TID 1238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 189.0 in stage 10.0 (TID 1238)
15/08/21 21:31:48 INFO Executor: Running task 187.0 in stage 10.0 (TID 1236)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 183.0 in stage 10.0 (TID 1232) in 908 ms on localhost (172/200)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 165.0 in stage 10.0 (TID 1214) in 1424 ms on localhost (173/200)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 172.0 in stage 10.0 (TID 1221) in 926 ms on localhost (174/200)
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000185
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000185_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000182
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000182_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000184
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000184_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000179
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000179_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 185.0 in stage 10.0 (TID 1234). 843 bytes result sent to driver
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000178
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000178_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 182.0 in stage 10.0 (TID 1231). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Finished task 179.0 in stage 10.0 (TID 1228). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Finished task 184.0 in stage 10.0 (TID 1233). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Starting task 190.0 in stage 10.0 (TID 1239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO TaskSetManager: Starting task 191.0 in stage 10.0 (TID 1240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO TaskSetManager: Starting task 192.0 in stage 10.0 (TID 1241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 190.0 in stage 10.0 (TID 1239)
15/08/21 21:31:48 INFO Executor: Running task 191.0 in stage 10.0 (TID 1240)
15/08/21 21:31:48 INFO TaskSetManager: Starting task 193.0 in stage 10.0 (TID 1242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Finished task 178.0 in stage 10.0 (TID 1227). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Running task 193.0 in stage 10.0 (TID 1242)
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000175
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000175_0: Committed
15/08/21 21:31:48 INFO TaskSetManager: Finished task 185.0 in stage 10.0 (TID 1234) in 918 ms on localhost (175/200)
15/08/21 21:31:48 INFO Executor: Running task 192.0 in stage 10.0 (TID 1241)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 182.0 in stage 10.0 (TID 1231) in 932 ms on localhost (176/200)
15/08/21 21:31:48 INFO TaskSetManager: Starting task 194.0 in stage 10.0 (TID 1243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 194.0 in stage 10.0 (TID 1243)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 179.0 in stage 10.0 (TID 1228) in 934 ms on localhost (177/200)
15/08/21 21:31:48 INFO Executor: Finished task 175.0 in stage 10.0 (TID 1224). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Starting task 195.0 in stage 10.0 (TID 1244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 178.0 in stage 10.0 (TID 1227) in 937 ms on localhost (178/200)
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000176
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000176_0: Committed
15/08/21 21:31:48 INFO TaskSetManager: Finished task 184.0 in stage 10.0 (TID 1233) in 932 ms on localhost (179/200)
15/08/21 21:31:48 INFO Executor: Running task 195.0 in stage 10.0 (TID 1244)
15/08/21 21:31:48 INFO Executor: Finished task 176.0 in stage 10.0 (TID 1225). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Finished task 175.0 in stage 10.0 (TID 1224) in 947 ms on localhost (180/200)
15/08/21 21:31:48 INFO TaskSetManager: Starting task 196.0 in stage 10.0 (TID 1245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 196.0 in stage 10.0 (TID 1245)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 176.0 in stage 10.0 (TID 1225) in 948 ms on localhost (181/200)
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,168
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,860
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,656B for [ps_partkey] INT32: 445 values, 1,787B raw, 1,620B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,983B for [value] DOUBLE: 445 values, 3,567B raw, 1,939B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,434B for [ps_partkey] INT32: 380 values, 1,527B raw, 1,398B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,724B for [value] DOUBLE: 380 values, 3,047B raw, 1,680B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,328
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,508
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,856
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,684B for [ps_partkey] INT32: 453 values, 1,819B raw, 1,648B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,987B for [value] DOUBLE: 453 values, 3,631B raw, 1,943B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,880B for [ps_partkey] INT32: 512 values, 2,055B raw, 1,844B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,950B for [ps_partkey] INT32: 529 values, 2,123B raw, 1,914B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,232B for [value] DOUBLE: 512 values, 4,103B raw, 2,188B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,321B for [value] DOUBLE: 529 values, 4,239B raw, 2,277B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,700
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,921B for [ps_partkey] INT32: 522 values, 2,095B raw, 1,885B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,255B for [value] DOUBLE: 522 values, 4,183B raw, 2,211B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,616
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,436
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,096
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,535B for [ps_partkey] INT32: 408 values, 1,639B raw, 1,499B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,904B for [ps_partkey] INT32: 517 values, 2,075B raw, 1,868B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,787B for [value] DOUBLE: 408 values, 3,271B raw, 1,743B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,237B for [value] DOUBLE: 517 values, 4,143B raw, 2,193B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,294B for [ps_partkey] INT32: 341 values, 1,371B raw, 1,258B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,555B for [value] DOUBLE: 341 values, 2,735B raw, 1,511B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,328
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,857B for [ps_partkey] INT32: 503 values, 2,019B raw, 1,821B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,176B for [value] DOUBLE: 503 values, 4,031B raw, 2,132B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000188
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000188_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000186
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000186_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 186.0 in stage 10.0 (TID 1235). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Finished task 188.0 in stage 10.0 (TID 1237). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Starting task 197.0 in stage 10.0 (TID 1246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 197.0 in stage 10.0 (TID 1246)
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,756
15/08/21 21:31:48 INFO TaskSetManager: Starting task 198.0 in stage 10.0 (TID 1247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 198.0 in stage 10.0 (TID 1247)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 188.0 in stage 10.0 (TID 1237) in 204 ms on localhost (182/200)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 186.0 in stage 10.0 (TID 1235) in 217 ms on localhost (183/200)
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,755B for [ps_partkey] INT32: 474 values, 1,903B raw, 1,719B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,107B for [value] DOUBLE: 474 values, 3,799B raw, 2,063B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000187
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000187_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 187.0 in stage 10.0 (TID 1236). 843 bytes result sent to driver
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000192
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000192_0: Committed
15/08/21 21:31:48 INFO TaskSetManager: Starting task 199.0 in stage 10.0 (TID 1248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 21:31:48 INFO Executor: Running task 199.0 in stage 10.0 (TID 1248)
15/08/21 21:31:48 INFO Executor: Finished task 192.0 in stage 10.0 (TID 1241). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Finished task 187.0 in stage 10.0 (TID 1236) in 213 ms on localhost (184/200)
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000190
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000190_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000194
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000194_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 190.0 in stage 10.0 (TID 1239). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Finished task 192.0 in stage 10.0 (TID 1241) in 195 ms on localhost (185/200)
15/08/21 21:31:48 INFO Executor: Finished task 194.0 in stage 10.0 (TID 1243). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Finished task 190.0 in stage 10.0 (TID 1239) in 197 ms on localhost (186/200)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 194.0 in stage 10.0 (TID 1243) in 192 ms on localhost (187/200)
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000195
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000195_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 195.0 in stage 10.0 (TID 1244). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Finished task 195.0 in stage 10.0 (TID 1244) in 199 ms on localhost (188/200)
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 21:31:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO CodecConfig: Compression: GZIP
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 21:31:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 21:31:48 INFO ParquetOutputFormat: Validation is off
15/08/21 21:31:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,508
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,628
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,718B for [ps_partkey] INT32: 462 values, 1,855B raw, 1,682B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,028B for [value] DOUBLE: 462 values, 3,703B raw, 1,984B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,083B for [ps_partkey] INT32: 568 values, 2,279B raw, 2,047B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 2,476B for [value] DOUBLE: 568 values, 4,551B raw, 2,432B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,896
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,442B for [ps_partkey] INT32: 381 values, 1,531B raw, 1,406B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO ColumnChunkPageWriteStore: written 1,706B for [value] DOUBLE: 381 values, 3,055B raw, 1,662B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000199
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000199_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000198
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000198_0: Committed
15/08/21 21:31:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000197
15/08/21 21:31:48 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000197_0: Committed
15/08/21 21:31:48 INFO Executor: Finished task 199.0 in stage 10.0 (TID 1248). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Finished task 198.0 in stage 10.0 (TID 1247). 843 bytes result sent to driver
15/08/21 21:31:48 INFO Executor: Finished task 197.0 in stage 10.0 (TID 1246). 843 bytes result sent to driver
15/08/21 21:31:48 INFO TaskSetManager: Finished task 199.0 in stage 10.0 (TID 1248) in 112 ms on localhost (189/200)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 198.0 in stage 10.0 (TID 1247) in 121 ms on localhost (190/200)
15/08/21 21:31:48 INFO TaskSetManager: Finished task 197.0 in stage 10.0 (TID 1246) in 122 ms on localhost (191/200)
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000173
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000173_0: Committed
15/08/21 21:31:49 INFO Executor: Finished task 173.0 in stage 10.0 (TID 1222). 843 bytes result sent to driver
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000177
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000177_0: Committed
15/08/21 21:31:49 INFO TaskSetManager: Finished task 173.0 in stage 10.0 (TID 1222) in 1328 ms on localhost (192/200)
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000181
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000180
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000174
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000180_0: Committed
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000181_0: Committed
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000174_0: Committed
15/08/21 21:31:49 INFO Executor: Finished task 177.0 in stage 10.0 (TID 1226). 843 bytes result sent to driver
15/08/21 21:31:49 INFO Executor: Finished task 174.0 in stage 10.0 (TID 1223). 843 bytes result sent to driver
15/08/21 21:31:49 INFO Executor: Finished task 180.0 in stage 10.0 (TID 1229). 843 bytes result sent to driver
15/08/21 21:31:49 INFO Executor: Finished task 181.0 in stage 10.0 (TID 1230). 843 bytes result sent to driver
15/08/21 21:31:49 INFO TaskSetManager: Finished task 177.0 in stage 10.0 (TID 1226) in 1321 ms on localhost (193/200)
15/08/21 21:31:49 INFO TaskSetManager: Finished task 174.0 in stage 10.0 (TID 1223) in 1334 ms on localhost (194/200)
15/08/21 21:31:49 INFO TaskSetManager: Finished task 180.0 in stage 10.0 (TID 1229) in 1320 ms on localhost (195/200)
15/08/21 21:31:49 INFO TaskSetManager: Finished task 181.0 in stage 10.0 (TID 1230) in 1321 ms on localhost (196/200)
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000193
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000193_0: Committed
15/08/21 21:31:49 INFO Executor: Finished task 193.0 in stage 10.0 (TID 1242). 843 bytes result sent to driver
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000189
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000189_0: Committed
15/08/21 21:31:49 INFO TaskSetManager: Finished task 193.0 in stage 10.0 (TID 1242) in 587 ms on localhost (197/200)
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000196
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000196_0: Committed
15/08/21 21:31:49 INFO Executor: Finished task 189.0 in stage 10.0 (TID 1238). 843 bytes result sent to driver
15/08/21 21:31:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508212131_0010_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212131_0010_m_000191
15/08/21 21:31:49 INFO SparkHadoopMapRedUtil: attempt_201508212131_0010_m_000191_0: Committed
15/08/21 21:31:49 INFO Executor: Finished task 196.0 in stage 10.0 (TID 1245). 843 bytes result sent to driver
15/08/21 21:31:49 INFO TaskSetManager: Finished task 189.0 in stage 10.0 (TID 1238) in 610 ms on localhost (198/200)
15/08/21 21:31:49 INFO Executor: Finished task 191.0 in stage 10.0 (TID 1240). 843 bytes result sent to driver
15/08/21 21:31:49 INFO TaskSetManager: Finished task 196.0 in stage 10.0 (TID 1245) in 582 ms on localhost (199/200)
15/08/21 21:31:49 INFO TaskSetManager: Finished task 191.0 in stage 10.0 (TID 1240) in 594 ms on localhost (200/200)
15/08/21 21:31:49 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/21 21:31:49 INFO DAGScheduler: ResultStage 10 (processCmd at CliDriver.java:423) finished in 6.686 s
15/08/21 21:31:49 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@40a7d41c
15/08/21 21:31:49 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 14.141339 s
15/08/21 21:31:49 INFO StatsReportListener: task runtime:(count: 200, mean: 521.855000, stdev: 384.510161, max: 1809.000000, min: 112.000000)
15/08/21 21:31:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:49 INFO StatsReportListener: 	112.0 ms	180.0 ms	216.0 ms	266.0 ms	326.0 ms	681.0 ms	1.3 s	1.4 s	1.8 s
15/08/21 21:31:49 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.250000, stdev: 0.507445, max: 3.000000, min: 0.000000)
15/08/21 21:31:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:49 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	3.0 ms
15/08/21 21:31:49 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 21:31:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:49 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 21:31:49 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 21:31:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:49 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 21:31:49 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 70.886849, stdev: 17.374735, max: 95.836488, min: 21.491876)
15/08/21 21:31:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:49 INFO StatsReportListener: 	21 %	24 %	56 %	63 %	70 %	86 %	91 %	93 %	96 %
15/08/21 21:31:49 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.075005, stdev: 0.181724, max: 1.554404, min: 0.000000)
15/08/21 21:31:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:49 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 2 %
15/08/21 21:31:49 INFO StatsReportListener: other time pct: (count: 200, mean: 29.038146, stdev: 17.365396, max: 78.450185, min: 4.087812)
15/08/21 21:31:49 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:49 INFO StatsReportListener: 	 4 %	 7 %	 9 %	15 %	30 %	37 %	45 %	76 %	78 %
15/08/21 21:31:50 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 21:31:50 INFO DefaultWriterContainer: Job job_201508212131_0000 committed.
15/08/21 21:31:50 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 21:31:50 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_common_metadata
15/08/21 21:31:50 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 21:31:50 INFO DAGScheduler: Got job 5 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 21:31:50 INFO DAGScheduler: Final stage: ResultStage 11(processCmd at CliDriver.java:423)
15/08/21 21:31:50 INFO DAGScheduler: Parents of final stage: List()
15/08/21 21:31:50 INFO DAGScheduler: Missing parents: List()
15/08/21 21:31:50 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 21:31:50 INFO MemoryStore: ensureFreeSpace(2968) called with curMem=1865083, maxMem=22226833244
15/08/21 21:31:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 21:31:50 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=1868051, maxMem=22226833244
15/08/21 21:31:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 1776.0 B, free 20.7 GB)
15/08/21 21:31:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:52592 (size: 1776.0 B, free: 20.7 GB)
15/08/21 21:31:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:874
15/08/21 21:31:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at processCmd at CliDriver.java:423)
15/08/21 21:31:50 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
15/08/21 21:31:50 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 1249, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 21:31:50 INFO Executor: Running task 0.0 in stage 11.0 (TID 1249)
15/08/21 21:31:50 INFO Executor: Finished task 0.0 in stage 11.0 (TID 1249). 606 bytes result sent to driver
15/08/21 21:31:50 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 1249) in 10 ms on localhost (1/1)
15/08/21 21:31:50 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/21 21:31:50 INFO DAGScheduler: ResultStage 11 (processCmd at CliDriver.java:423) finished in 0.010 s
15/08/21 21:31:50 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@159120
15/08/21 21:31:50 INFO DAGScheduler: Job 5 finished: processCmd at CliDriver.java:423, took 0.022876 s
15/08/21 21:31:50 INFO StatsReportListener: task runtime:(count: 1, mean: 10.000000, stdev: 0.000000, max: 10.000000, min: 10.000000)
15/08/21 21:31:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:50 INFO StatsReportListener: 	10.0 ms	10.0 ms	10.0 ms	10.0 ms	10.0 ms	10.0 ms	10.0 ms	10.0 ms	10.0 ms
Time taken: 20.255 seconds
15/08/21 21:31:50 INFO CliDriver: Time taken: 20.255 seconds
15/08/21 21:31:50 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 21:31:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:50 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 21:31:50 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 21:31:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:50 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 21:31:50 INFO StatsReportListener: other time pct: (count: 1, mean: 100.000000, stdev: 0.000000, max: 100.000000, min: 100.000000)
15/08/21 21:31:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 21:31:50 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/21 21:31:50 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/21 21:31:50 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/21 21:31:50 INFO DAGScheduler: Stopping DAGScheduler
15/08/21 21:31:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/21 21:31:51 INFO Utils: path = /tmp/spark-dd5e7f8a-78c8-4ac9-bff7-401cd2464af3/blockmgr-738bdfcf-d074-4fd8-ae5d-2cf640dc07a9, already present as root for deletion.
15/08/21 21:31:51 INFO MemoryStore: MemoryStore cleared
15/08/21 21:31:51 INFO BlockManager: BlockManager stopped
15/08/21 21:31:51 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/21 21:31:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/21 21:31:51 INFO SparkContext: Successfully stopped SparkContext
15/08/21 21:31:51 INFO Utils: Shutdown hook called
15/08/21 21:31:51 INFO Utils: Deleting directory /tmp/spark-126d0a8e-e8da-4ca3-b73a-702a2cb49bd8
15/08/21 21:31:51 INFO Utils: Deleting directory /tmp/spark-b3af7e32-b27e-4c74-bc49-81f50516b4e8
15/08/21 21:31:51 INFO Utils: Deleting directory /tmp/spark-dd5e7f8a-78c8-4ac9-bff7-401cd2464af3
15/08/21 21:31:51 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/21 21:31:51 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/21 21:31:51 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
