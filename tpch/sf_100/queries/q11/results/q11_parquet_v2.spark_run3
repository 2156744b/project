 -- the query
insert into table q11_part_tmp_par
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
    join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
    join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par) sum_tmp
    join q11_part_tmp_par
where part_value > total_value * 0.000001
order by value desc; 
15/08/21 23:13:08 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 23:13:08 INFO metastore: Connected to metastore.
15/08/21 23:13:09 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/21 23:13:09 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:09 INFO SparkContext: Running Spark version 1.4.1
15/08/21 23:13:09 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:09 INFO SecurityManager: Changing view acls to: hive
15/08/21 23:13:09 INFO SecurityManager: Changing modify acls to: hive
15/08/21 23:13:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/21 23:13:10 INFO Slf4jLogger: Slf4jLogger started
15/08/21 23:13:10 INFO Remoting: Starting remoting
15/08/21 23:13:10 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:51142]
15/08/21 23:13:10 INFO Utils: Successfully started service 'sparkDriver' on port 51142.
15/08/21 23:13:11 INFO SparkEnv: Registering MapOutputTracker
15/08/21 23:13:11 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:11 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:11 INFO SparkEnv: Registering BlockManagerMaster
15/08/21 23:13:11 INFO DiskBlockManager: Created local directory at /tmp/spark-69894f8f-1d49-46d4-a6ab-62fe441221ad/blockmgr-db311b79-dee5-46cd-aff8-06992e9dd85d
15/08/21 23:13:11 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/21 23:13:11 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:11 INFO HttpFileServer: HTTP File server directory is /tmp/spark-69894f8f-1d49-46d4-a6ab-62fe441221ad/httpd-5cb217b5-7ad2-488c-9393-82b1921d8ccd
15/08/21 23:13:11 INFO HttpServer: Starting HTTP Server
15/08/21 23:13:11 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 23:13:11 INFO AbstractConnector: Started SocketConnector@0.0.0.0:57974
15/08/21 23:13:11 INFO Utils: Successfully started service 'HTTP file server' on port 57974.
15/08/21 23:13:11 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/21 23:13:11 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 23:13:11 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/21 23:13:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/21 23:13:11 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/21 23:13:11 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:11 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:11 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:11 INFO Executor: Starting executor ID driver on host localhost
15/08/21 23:13:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40586.
15/08/21 23:13:11 INFO NettyBlockTransferService: Server created on 40586
15/08/21 23:13:11 INFO BlockManagerMaster: Trying to register BlockManager
15/08/21 23:13:11 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40586 with 20.7 GB RAM, BlockManagerId(driver, localhost, 40586)
15/08/21 23:13:11 INFO BlockManagerMaster: Registered BlockManager
15/08/21 23:13:12 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 23:13:12 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/21 23:13:12 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/21 23:13:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/21 23:13:13 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 23:13:13 INFO metastore: Connected to metastore.
15/08/21 23:13:14 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/21 23:13:14 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/21 23:13:15 INFO ParseDriver: Parsing command: -- the query
insert into table q11_part_tmp_par
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
    join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
    join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey
15/08/21 23:13:15 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/21 23:13:18 INFO MemoryStore: ensureFreeSpace(326528) called with curMem=0, maxMem=22226833244
15/08/21 23:13:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 23:13:18 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=326528, maxMem=22226833244
15/08/21 23:13:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 23:13:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40586 (size: 22.3 KB, free: 20.7 GB)
15/08/21 23:13:18 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/21 23:13:19 INFO MemoryStore: ensureFreeSpace(326584) called with curMem=349321, maxMem=22226833244
15/08/21 23:13:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 23:13:19 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=675905, maxMem=22226833244
15/08/21 23:13:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 23:13:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:40586 (size: 22.3 KB, free: 20.7 GB)
15/08/21 23:13:19 INFO SparkContext: Created broadcast 1 from processCmd at CliDriver.java:423
15/08/21 23:13:19 INFO MemoryStore: ensureFreeSpace(326584) called with curMem=698698, maxMem=22226833244
15/08/21 23:13:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 23:13:19 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1025282, maxMem=22226833244
15/08/21 23:13:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 23:13:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:40586 (size: 22.3 KB, free: 20.7 GB)
15/08/21 23:13:19 INFO SparkContext: Created broadcast 2 from processCmd at CliDriver.java:423
15/08/21 23:13:19 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/21 23:13:19 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/21 23:13:19 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 23:13:19 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 23:13:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 23:13:19 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 23:13:19 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/21 23:13:19 INFO DAGScheduler: Got job 0 (run at ThreadPoolExecutor.java:1145) with 1 output partitions (allowLocal=false)
15/08/21 23:13:19 INFO DAGScheduler: Final stage: ResultStage 0(run at ThreadPoolExecutor.java:1145)
15/08/21 23:13:19 INFO DAGScheduler: Parents of final stage: List()
15/08/21 23:13:19 INFO DAGScheduler: Missing parents: List()
15/08/21 23:13:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/21 23:13:19 INFO MemoryStore: ensureFreeSpace(6144) called with curMem=1048075, maxMem=22226833244
15/08/21 23:13:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.0 KB, free 20.7 GB)
15/08/21 23:13:19 INFO MemoryStore: ensureFreeSpace(3283) called with curMem=1054219, maxMem=22226833244
15/08/21 23:13:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.2 KB, free 20.7 GB)
15/08/21 23:13:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:40586 (size: 3.2 KB, free: 20.7 GB)
15/08/21 23:13:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/21 23:13:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at run at ThreadPoolExecutor.java:1145)
15/08/21 23:13:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/08/21 23:13:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1682 bytes)
15/08/21 23:13:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/21 23:13:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 2330 length: 2330 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1856 bytes result sent to driver
15/08/21 23:13:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 415 ms on localhost (1/1)
15/08/21 23:13:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/21 23:13:20 INFO DAGScheduler: ResultStage 0 (run at ThreadPoolExecutor.java:1145) finished in 0.434 s
15/08/21 23:13:20 INFO DAGScheduler: Job 0 finished: run at ThreadPoolExecutor.java:1145, took 0.563461 s
15/08/21 23:13:20 INFO MemoryStore: ensureFreeSpace(344) called with curMem=1057502, maxMem=22226833244
15/08/21 23:13:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 344.0 B, free 20.7 GB)
15/08/21 23:13:20 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@208c3289
15/08/21 23:13:20 INFO MemoryStore: ensureFreeSpace(176) called with curMem=1057846, maxMem=22226833244
15/08/21 23:13:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 176.0 B, free 20.7 GB)
15/08/21 23:13:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:40586 (size: 176.0 B, free: 20.7 GB)
15/08/21 23:13:20 INFO SparkContext: Created broadcast 4 from run at ThreadPoolExecutor.java:1145
15/08/21 23:13:20 INFO StatsReportListener: task runtime:(count: 1, mean: 415.000000, stdev: 0.000000, max: 415.000000, min: 415.000000)
15/08/21 23:13:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:20 INFO StatsReportListener: 	415.0 ms	415.0 ms	415.0 ms	415.0 ms	415.0 ms	415.0 ms	415.0 ms	415.0 ms	415.0 ms
15/08/21 23:13:20 INFO StatsReportListener: task result size:(count: 1, mean: 1856.000000, stdev: 0.000000, max: 1856.000000, min: 1856.000000)
15/08/21 23:13:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:20 INFO StatsReportListener: 	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B	1856.0 B
15/08/21 23:13:20 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 67.710843, stdev: 0.000000, max: 67.710843, min: 67.710843)
15/08/21 23:13:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:20 INFO StatsReportListener: 	68 %	68 %	68 %	68 %	68 %	68 %	68 %	68 %	68 %
15/08/21 23:13:20 INFO StatsReportListener: other time pct: (count: 1, mean: 32.289157, stdev: 0.000000, max: 32.289157, min: 32.289157)
15/08/21 23:13:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:20 INFO StatsReportListener: 	32 %	32 %	32 %	32 %	32 %	32 %	32 %	32 %	32 %
15/08/21 23:13:20 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/21 23:13:20 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/21 23:13:20 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/21 23:13:20 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/21 23:13:20 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/21 23:13:20 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 23:13:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:13:20 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 23:13:20 INFO DAGScheduler: Registering RDD 9 (processCmd at CliDriver.java:423)
15/08/21 23:13:20 INFO DAGScheduler: Registering RDD 14 (processCmd at CliDriver.java:423)
15/08/21 23:13:20 INFO DAGScheduler: Registering RDD 20 (processCmd at CliDriver.java:423)
15/08/21 23:13:20 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 23:13:20 INFO DAGScheduler: Final stage: ResultStage 4(processCmd at CliDriver.java:423)
15/08/21 23:13:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
15/08/21 23:13:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
15/08/21 23:13:20 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 23:13:20 INFO MemoryStore: ensureFreeSpace(6536) called with curMem=1058022, maxMem=22226833244
15/08/21 23:13:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.4 KB, free 20.7 GB)
15/08/21 23:13:20 INFO MemoryStore: ensureFreeSpace(3554) called with curMem=1064558, maxMem=22226833244
15/08/21 23:13:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KB, free 20.7 GB)
15/08/21 23:13:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:40586 (size: 3.5 KB, free: 20.7 GB)
15/08/21 23:13:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:874
15/08/21 23:13:20 INFO DAGScheduler: Submitting 39 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[9] at processCmd at CliDriver.java:423)
15/08/21 23:13:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 39 tasks
15/08/21 23:13:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, ANY, 1694 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, ANY, 1699 bytes)
15/08/21 23:13:20 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 23:13:20 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, ANY, 1708 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, localhost, ANY, 1694 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, localhost, ANY, 1699 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, localhost, ANY, 1707 bytes)
15/08/21 23:13:20 INFO MemoryStore: ensureFreeSpace(9520) called with curMem=1068112, maxMem=22226833244
15/08/21 23:13:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.3 KB, free 20.7 GB)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, localhost, ANY, 1693 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, localhost, ANY, 1699 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, localhost, ANY, 1709 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, localhost, ANY, 1693 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, localhost, ANY, 1699 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, localhost, ANY, 1708 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, localhost, ANY, 1694 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, localhost, ANY, 1699 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, localhost, ANY, 1707 bytes)
15/08/21 23:13:20 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, localhost, ANY, 1694 bytes)
15/08/21 23:13:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/08/21 23:13:20 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
15/08/21 23:13:20 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
15/08/21 23:13:20 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
15/08/21 23:13:20 INFO MemoryStore: ensureFreeSpace(4935) called with curMem=1077632, maxMem=22226833244
15/08/21 23:13:20 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
15/08/21 23:13:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.8 KB, free 20.7 GB)
15/08/21 23:13:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:40586 (size: 4.8 KB, free: 20.7 GB)
15/08/21 23:13:20 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
15/08/21 23:13:20 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
15/08/21 23:13:20 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
15/08/21 23:13:20 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
15/08/21 23:13:20 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
15/08/21 23:13:20 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
15/08/21 23:13:20 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:874
15/08/21 23:13:20 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
15/08/21 23:13:20 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
15/08/21 23:13:20 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
15/08/21 23:13:20 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
15/08/21 23:13:20 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
15/08/21 23:13:20 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at processCmd at CliDriver.java:423)
15/08/21 23:13:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 268435456 end: 334658433 length: 66222977 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 268435456 end: 336808364 length: 68372908 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 268435456 end: 336837881 length: 68402425 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000010_0 start: 268435456 end: 334660800 length: 66225344 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 268435456 end: 335698071 length: 67262615 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
21-Aug-2015 23:13:16 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 23:13:16 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 23:13:16 INFO: parquet.hadoop.ParquetFileReader: reading another 1 footers
21-Aug-2015 23:13:16 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 23:13:17 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 23:13:17 INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
21-Aug-2015 23:13:17 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 23:13:17 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 23:13:17 INFO: parquet.hadoop.ParquetFileReader: reading another 13 footers
21-Aug-2015 23:13:17 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 23:13:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
21-Aug-2015 23:13:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 31 ms. row count = 25
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203347 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243920 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465498 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464663 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464967 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465101 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465498 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464958 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243981 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203579 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465108 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465186 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1222637 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464813 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465634 records.
21-Aug-2015 23:13:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465546 records.
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at r15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:21 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 23:13:27 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2125 bytes result sent to driver
15/08/21 23:13:27 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, localhost, ANY, 1699 bytes)
15/08/21 23:13:27 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
15/08/21 23:13:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:27 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 6339 ms on localhost (1/39)
15/08/21 23:13:27 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2125 bytes result sent to driver
15/08/21 23:13:27 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, localhost, ANY, 1707 bytes)
15/08/21 23:13:27 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
15/08/21 23:13:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 268435456 end: 336808298 length: 68372842 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:27 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 6725 ms on localhost (2/39)
15/08/21 23:13:28 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2125 bytes result sent to driver
15/08/21 23:13:28 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, localhost, ANY, 1694 bytes)
15/08/21 23:13:28 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
15/08/21 23:13:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:28 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 7365 ms on localhost (3/39)
15/08/21 23:13:28 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2125 bytes result sent to driver
15/08/21 23:13:28 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, localhost, ANY, 1699 bytes)
15/08/21 23:13:28 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
15/08/21 23:13:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:28 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 7449 ms on localhost (4/39)
15/08/21 23:13:28 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2125 bytes result sent to driver
15/08/21 23:13:28 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, localhost, ANY, 1710 bytes)
15/08/21 23:13:28 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
15/08/21 23:13:28 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 7804 ms on localhost (5/39)
15/08/21 23:13:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000012_0 start: 268435456 end: 333041820 length: 64606364 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:31 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2125 bytes result sent to driver
15/08/21 23:13:31 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, localhost, ANY, 1694 bytes)
15/08/21 23:13:31 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
15/08/21 23:13:31 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 11134 ms on localhost (6/39)
15/08/21 23:13:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:32 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2125 bytes result sent to driver
15/08/21 23:13:32 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23, localhost, ANY, 1699 bytes)
15/08/21 23:13:32 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
15/08/21 23:13:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:32 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 11476 ms on localhost (7/39)
15/08/21 23:13:32 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2125 bytes result sent to driver
15/08/21 23:13:32 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 24, localhost, ANY, 1710 bytes)
15/08/21 23:13:32 INFO Executor: Running task 23.0 in stage 1.0 (TID 24)
15/08/21 23:13:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 268435456 end: 334667494 length: 66232038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:32 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 11874 ms on localhost (8/39)
15/08/21 23:13:33 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2125 bytes result sent to driver
15/08/21 23:13:33 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 25, localhost, ANY, 1693 bytes)
15/08/21 23:13:33 INFO Executor: Running task 24.0 in stage 1.0 (TID 25)
15/08/21 23:13:33 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2125 bytes result sent to driver
15/08/21 23:13:33 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 26, localhost, ANY, 1698 bytes)
15/08/21 23:13:33 INFO Executor: Running task 25.0 in stage 1.0 (TID 26)
15/08/21 23:13:33 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 12654 ms on localhost (9/39)
15/08/21 23:13:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:33 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 12701 ms on localhost (10/39)
15/08/21 23:13:33 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2125 bytes result sent to driver
15/08/21 23:13:33 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 27, localhost, ANY, 1706 bytes)
15/08/21 23:13:33 INFO Executor: Running task 26.0 in stage 1.0 (TID 27)
15/08/21 23:13:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 268435456 end: 338400334 length: 69964878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:33 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 6214 ms on localhost (11/39)
ow 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 248 ms. row count = 1203579
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 272 ms. row count = 1203347
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 278 ms. row count = 1243920
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 304 ms. row count = 1243981
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 306 ms. row count = 1222637
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 442 ms. row count = 2465108
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 438 ms. row count = 2465634
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 456 ms. row count = 2464813
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 456 ms. row count = 2465498
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 446 ms. row count = 2465186
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 460 ms. row count = 2465498
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 485 ms. row count = 2464663
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 473 ms. row count = 2464958
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 495 ms. row count = 2465101
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 527 ms. row count = 2465546
21-Aug-2015 23:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 589 ms. row count = 2464967
21-Aug-2015 23:13:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2463721 records.
21-Aug-2015 23:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 131 ms. row count = 2463721
21-Aug-2015 23:13:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1243832 records.
21-Aug-2015 23:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 102 ms. row count = 1243832
21-Aug-2015 23:13:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464953 records.
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464761 records.
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 179 ms. row count = 2464953
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 272 ms. row count = 2464761
21-Aug-2015 23:13:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1174247 records.
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 1174247
21-Aug-2015 23:13:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2466033 records.
21-Aug-2015 23:13:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 217 ms. row count = 2466033
21-Aug-2015 23:13:32 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:32 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464261 records.
21-Aug-2015 23:13:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 247 ms. row count = 2464261
21-Aug-2015 23:13:32 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:32 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203394 records.
21-Aug-2015 23:13:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 153 ms. row count = 1203394
21-Aug-2015 23:13:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464540 records.
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465074 records.
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl15/08/21 23:13:33 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2125 bytes result sent to driver
15/08/21 23:13:33 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 28, localhost, ANY, 1693 bytes)
15/08/21 23:13:33 INFO Executor: Running task 27.0 in stage 1.0 (TID 28)
15/08/21 23:13:33 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 13222 ms on localhost (12/39)
15/08/21 23:13:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:40586 in memory (size: 3.2 KB, free: 20.7 GB)
15/08/21 23:13:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2125 bytes result sent to driver
15/08/21 23:13:34 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 29, localhost, ANY, 1699 bytes)
15/08/21 23:13:34 INFO Executor: Running task 28.0 in stage 1.0 (TID 29)
15/08/21 23:13:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 14247 ms on localhost (13/39)
15/08/21 23:13:35 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2125 bytes result sent to driver
15/08/21 23:13:35 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 30, localhost, ANY, 1707 bytes)
15/08/21 23:13:35 INFO Executor: Running task 29.0 in stage 1.0 (TID 30)
15/08/21 23:13:35 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 14558 ms on localhost (14/39)
15/08/21 23:13:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 268435456 end: 336803495 length: 68368039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:35 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2125 bytes result sent to driver
15/08/21 23:13:35 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 31, localhost, ANY, 1693 bytes)
15/08/21 23:13:35 INFO Executor: Running task 30.0 in stage 1.0 (TID 31)
15/08/21 23:13:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:35 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 6820 ms on localhost (15/39)
15/08/21 23:13:35 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2125 bytes result sent to driver
15/08/21 23:13:35 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 32, localhost, ANY, 1699 bytes)
15/08/21 23:13:35 INFO Executor: Running task 31.0 in stage 1.0 (TID 32)
15/08/21 23:13:35 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 14820 ms on localhost (16/39)
15/08/21 23:13:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:36 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2125 bytes result sent to driver
15/08/21 23:13:36 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 33, localhost, ANY, 1707 bytes)
15/08/21 23:13:36 INFO Executor: Running task 32.0 in stage 1.0 (TID 33)
15/08/21 23:13:36 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 15533 ms on localhost (17/39)
15/08/21 23:13:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 268435456 end: 334633553 length: 66198097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:36 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2125 bytes result sent to driver
15/08/21 23:13:36 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 34, localhost, ANY, 1694 bytes)
15/08/21 23:13:36 INFO Executor: Running task 33.0 in stage 1.0 (TID 34)
15/08/21 23:13:36 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 15685 ms on localhost (18/39)
15/08/21 23:13:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:39 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2125 bytes result sent to driver
15/08/21 23:13:39 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 35, localhost, ANY, 1699 bytes)
15/08/21 23:13:39 INFO Executor: Running task 34.0 in stage 1.0 (TID 35)
15/08/21 23:13:39 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 12038 ms on localhost (19/39)
15/08/21 23:13:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:39 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2125 bytes result sent to driver
15/08/21 23:13:39 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 36, localhost, ANY, 1710 bytes)
15/08/21 23:13:39 INFO Executor: Running task 35.0 in stage 1.0 (TID 36)
15/08/21 23:13:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000011_0 start: 268435456 end: 334670294 length: 66234838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:39 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 11062 ms on localhost (20/39)
15/08/21 23:13:39 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2125 bytes result sent to driver
15/08/21 23:13:39 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 37, localhost, ANY, 1693 bytes)
15/08/21 23:13:39 INFO Executor: Running task 36.0 in stage 1.0 (TID 37)
15/08/21 23:13:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:39 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 11714 ms on localhost (21/39)
15/08/21 23:13:39 INFO Executor: Finished task 23.0 in stage 1.0 (TID 24). 2125 bytes result sent to driver
15/08/21 23:13:39 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 38, localhost, ANY, 1699 bytes)
15/08/21 23:13:39 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
15/08/21 23:13:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:39 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 24) in 7285 ms on localhost (22/39)
15/08/21 23:13:40 INFO Executor: Finished task 26.0 in stage 1.0 (TID 27). 2125 bytes result sent to driver
15/08/21 23:13:40 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 39, localhost, ANY, 1707 bytes)
15/08/21 23:13:40 INFO Executor: Running task 38.0 in stage 1.0 (TID 39)
15/08/21 23:13:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 268435456 end: 336840779 length: 68405323 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional int32 ps_suppkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:40 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 27) in 6455 ms on localhost (23/39)
15/08/21 23:13:40 INFO Executor: Finished task 32.0 in stage 1.0 (TID 33). 2125 bytes result sent to driver
15/08/21 23:13:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 40, localhost, ANY, 1695 bytes)
15/08/21 23:13:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 40)
15/08/21 23:13:40 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 33) in 3936 ms on localhost (24/39)
15/08/21 23:13:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000001_0 start: 0 end: 10091464 length: 10091464 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:40 INFO Executor: Finished task 29.0 in stage 1.0 (TID 30). 2125 bytes result sent to driver
15/08/21 23:13:40 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 41, localhost, ANY, 1696 bytes)
15/08/21 23:13:40 INFO Executor: Running task 1.0 in stage 2.0 (TID 41)
15/08/21 23:13:40 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 30) in 4952 ms on localhost (25/39)
15/08/21 23:13:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000006_0 start: 0 end: 10091200 length: 10091200 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}

21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 194 ms. row count = 2465074
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1273547 records.
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 172 ms. row count = 2464540
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 174 ms. row count = 1273547
21-Aug-2015 23:13:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465823 records.
21-Aug-2015 23:13:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464354 records.
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 183 ms. row count = 2465823
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 298 ms. row count = 2464354
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1244217 records.
21-Aug-2015 23:13:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2464440 records.
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 86 ms. row count = 1244217
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 153 ms. row count = 2464440
21-Aug-2015 23:13:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465446 records.
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 209 ms. row count = 2465446
21-Aug-2015 23:13:36 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:36 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203324 records.
21-Aug-2015 23:13:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 75 ms. row count = 1203324
21-Aug-2015 23:13:36 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:36 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465130 records.
21-Aug-2015 23:13:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 171 ms. row count = 2465130
21-Aug-2015 23:13:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465185 records.
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1203179 records.
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 141 ms. row count = 2465185
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 59 ms. row count = 1203179
21-Aug-2015 23:13:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2466709 records.
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2465174 records.
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 122 ms. row count = 2466709
21-Aug-2015 23:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1244220 records.
21-Aug-2015 23:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 240 ms. row count = 2465174
21-Aug-2015 23:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 1244220
21-Aug-2015 23:13:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 124899 records.
21-Aug-2015 23:13:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 23:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 23:13:40 INFO: parquet.hadoop.InternalP15/08/21 23:13:40 INFO Executor: Finished task 1.0 in stage 2.0 (TID 41). 2125 bytes result sent to driver
15/08/21 23:13:40 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 42, localhost, ANY, 1696 bytes)
15/08/21 23:13:40 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 41) in 489 ms on localhost (1/8)
15/08/21 23:13:40 INFO Executor: Running task 2.0 in stage 2.0 (TID 42)
15/08/21 23:13:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000007_0 start: 0 end: 10090201 length: 10090201 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 40). 2125 bytes result sent to driver
15/08/21 23:13:40 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 43, localhost, ANY, 1694 bytes)
15/08/21 23:13:40 INFO Executor: Running task 3.0 in stage 2.0 (TID 43)
15/08/21 23:13:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 40) in 582 ms on localhost (2/8)
15/08/21 23:13:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000005_0 start: 0 end: 10091941 length: 10091941 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:41 INFO Executor: Finished task 3.0 in stage 2.0 (TID 43). 2125 bytes result sent to driver
15/08/21 23:13:41 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 44, localhost, ANY, 1694 bytes)
15/08/21 23:13:41 INFO Executor: Running task 4.0 in stage 2.0 (TID 44)
15/08/21 23:13:41 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 43) in 396 ms on localhost (3/8)
15/08/21 23:13:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000004_0 start: 0 end: 10091082 length: 10091082 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:41 INFO Executor: Finished task 2.0 in stage 2.0 (TID 42). 2125 bytes result sent to driver
15/08/21 23:13:41 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 45, localhost, ANY, 1694 bytes)
15/08/21 23:13:41 INFO Executor: Running task 5.0 in stage 2.0 (TID 45)
15/08/21 23:13:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 10153723 length: 10153723 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:41 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 42) in 490 ms on localhost (4/8)
15/08/21 23:13:41 INFO Executor: Finished task 4.0 in stage 2.0 (TID 44). 2125 bytes result sent to driver
15/08/21 23:13:41 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 46, localhost, ANY, 1695 bytes)
15/08/21 23:13:41 INFO Executor: Running task 6.0 in stage 2.0 (TID 46)
15/08/21 23:13:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000002_0 start: 0 end: 10086725 length: 10086725 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:41 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 44) in 399 ms on localhost (5/8)
15/08/21 23:13:41 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2125 bytes result sent to driver
15/08/21 23:13:41 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 47, localhost, ANY, 1695 bytes)
15/08/21 23:13:41 INFO Executor: Running task 7.0 in stage 2.0 (TID 47)
15/08/21 23:13:41 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 9483 ms on localhost (26/39)
15/08/21 23:13:41 INFO Executor: Finished task 5.0 in stage 2.0 (TID 45). 2125 bytes result sent to driver
15/08/21 23:13:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000003_0 start: 0 end: 10090247 length: 10090247 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 23:13:41 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 45) in 509 ms on localhost (6/8)
15/08/21 23:13:41 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2125 bytes result sent to driver
15/08/21 23:13:41 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 10033 ms on localhost (27/39)
15/08/21 23:13:41 INFO Executor: Finished task 25.0 in stage 1.0 (TID 26). 2125 bytes result sent to driver
15/08/21 23:13:41 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 26) in 8584 ms on localhost (28/39)
15/08/21 23:13:41 INFO Executor: Finished task 6.0 in stage 2.0 (TID 46). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 46) in 500 ms on localhost (7/8)
15/08/21 23:13:42 INFO Executor: Finished task 7.0 in stage 2.0 (TID 47). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 47) in 410 ms on localhost (8/8)
15/08/21 23:13:42 INFO DAGScheduler: ShuffleMapStage 2 (processCmd at CliDriver.java:423) finished in 21.285 s
15/08/21 23:13:42 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@52836f44
15/08/21 23:13:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/21 23:13:42 INFO DAGScheduler: looking for newly runnable stages
15/08/21 23:13:42 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
15/08/21 23:13:42 INFO StatsReportListener: task runtime:(count: 36, mean: 7942.694444, stdev: 4972.560735, max: 15685.000000, min: 396.000000)
15/08/21 23:13:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:42 INFO StatsReportListener: 	396.0 ms	399.0 ms	489.0 ms	5.0 s	7.8 s	12.0 s	14.6 s	15.5 s	15.7 s
15/08/21 23:13:42 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
15/08/21 23:13:42 INFO DAGScheduler: failed: Set()
15/08/21 23:13:42 INFO StatsReportListener: shuffle bytes written:(count: 36, mean: 34255906.861111, stdev: 21807795.795976, max: 55122523.000000, min: 39409.000000)
15/08/21 23:13:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:42 INFO StatsReportListener: 	38.5 KB	38.6 KB	38.7 KB	25.5 MB	27.1 MB	52.5 MB	52.6 MB	52.6 MB	52.6 MB
15/08/21 23:13:42 INFO StatsReportListener: task result size:(count: 36, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 23:13:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:42 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 23:13:42 INFO StatsReportListener: executor (non-fetch) time pct: (count: 36, mean: 96.817452, stdev: 4.091534, max: 99.471743, min: 85.510204)
15/08/21 23:13:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:42 INFO StatsReportListener: 	86 %	86 %	88 %	98 %	99 %	99 %	99 %	99 %	99 %
15/08/21 23:13:42 INFO StatsReportListener: other time pct: (count: 36, mean: 3.182548, stdev: 4.091534, max: 14.489796, min: 0.528257)
15/08/21 23:13:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:42 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 1 %	 2 %	12 %	14 %	14 %
15/08/21 23:13:42 INFO DAGScheduler: Missing parents for ShuffleMapStage 3: List(ShuffleMapStage 1)
15/08/21 23:13:42 INFO DAGScheduler: Missing parents for ResultStage 4: List(ShuffleMapStage 3)
15/08/21 23:13:42 INFO Executor: Finished task 24.0 in stage 1.0 (TID 25). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 25) in 8844 ms on localhost (29/39)
15/08/21 23:13:42 INFO Executor: Finished task 27.0 in stage 1.0 (TID 28). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 28) in 8346 ms on localhost (30/39)
15/08/21 23:13:42 INFO Executor: Finished task 30.0 in stage 1.0 (TID 31). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 31) in 7043 ms on localhost (31/39)
15/08/21 23:13:42 INFO Executor: Finished task 28.0 in stage 1.0 (TID 29). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO Executor: Finished task 35.0 in stage 1.0 (TID 36). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 29) in 7641 ms on localhost (32/39)
15/08/21 23:13:42 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 36) in 3401 ms on localhost (33/39)
15/08/21 23:13:42 INFO Executor: Finished task 33.0 in stage 1.0 (TID 34). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 34) in 6450 ms on localhost (34/39)
15/08/21 23:13:42 INFO Executor: Finished task 31.0 in stage 1.0 (TID 32). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 32) in 7319 ms on localhost (35/39)
15/08/21 23:13:42 INFO Executor: Finished task 38.0 in stage 1.0 (TID 39). 2125 bytes result sent to driver
15/08/21 23:13:42 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 39) in 2880 ms on localhost (36/39)
15/08/21 23:13:44 INFO Executor: Finished task 34.0 in stage 1.0 (TID 35). 2125 bytes result sent to driver
15/08/21 23:13:44 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 35) in 5260 ms on localhost (37/39)
15/08/21 23:13:44 INFO Executor: Finished task 36.0 in stage 1.0 (TID 37). 2125 bytes result sent to driver
15/08/21 23:13:44 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 37) in 4857 ms on localhost (38/39)
15/08/21 23:13:44 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2125 bytes result sent to driver
15/08/21 23:13:44 INFO DAGScheduler: ShuffleMapStage 1 (processCmd at CliDriver.java:423) finished in 24.076 s
15/08/21 23:13:44 INFO DAGScheduler: looking for newly runnable stages
15/08/21 23:13:44 INFO DAGScheduler: running: Set()
15/08/21 23:13:44 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 38) in 4974 ms on localhost (39/39)
15/08/21 23:13:44 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
15/08/21 23:13:44 INFO DAGScheduler: failed: Set()
15/08/21 23:13:44 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@14285e50
15/08/21 23:13:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/21 23:13:44 INFO StatsReportListener: task runtime:(count: 11, mean: 6092.272727, stdev: 1873.676798, max: 8844.000000, min: 2880.000000)
15/08/21 23:13:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:44 INFO StatsReportListener: 	2.9 s	2.9 s	3.4 s	4.9 s	6.5 s	7.6 s	8.3 s	8.8 s	8.8 s
15/08/21 23:13:44 INFO StatsReportListener: shuffle bytes written:(count: 11, mean: 49557698.090909, stdev: 10578389.344852, max: 55117260.000000, min: 26900011.000000)
15/08/21 23:13:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:44 INFO StatsReportListener: 	25.7 MB	25.7 MB	26.5 MB	47.5 MB	52.5 MB	52.6 MB	52.6 MB	52.6 MB	52.6 MB
15/08/21 23:13:44 INFO StatsReportListener: task result size:(count: 11, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 23:13:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:44 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 23:13:44 INFO StatsReportListener: executor (non-fetch) time pct: (count: 11, mean: 99.314437, stdev: 0.237811, max: 99.517491, min: 98.784722)
15/08/21 23:13:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:44 INFO StatsReportListener: 	99 %	99 %	99 %	99 %	99 %	100 %	100 %	100 %	100 %
15/08/21 23:13:44 INFO StatsReportListener: other time pct: (count: 11, mean: 0.685563, stdev: 0.237811, max: 1.215278, min: 0.482509)
15/08/21 23:13:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:13:44 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %	 1 %	 1 %
15/08/21 23:13:44 INFO DAGScheduler: Missing parents for ShuffleMapStage 3: List()
15/08/21 23:13:44 INFO DAGScheduler: Missing parents for ResultStage 4: List(ShuffleMapStage 3)
15/08/21 23:13:44 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[20] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 23:13:44 INFO MemoryStore: ensureFreeSpace(14224) called with curMem=1073140, maxMem=22226833244
15/08/21 23:13:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.9 KB, free 20.7 GB)
15/08/21 23:13:44 INFO MemoryStore: ensureFreeSpace(6785) called with curMem=1087364, maxMem=22226833244
15/08/21 23:13:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.6 KB, free 20.7 GB)
15/08/21 23:13:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:40586 (size: 6.6 KB, free: 20.7 GB)
15/08/21 23:13:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874
15/08/21 23:13:44 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[20] at processCmd at CliDriver.java:423)
15/08/21 23:13:44 INFO TaskSchedulerImpl: Adding task set 3.0 with 200 tasks
15/08/21 23:13:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 48, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 49, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 50, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 51, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 52, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 53, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 54, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 55, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 56, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 57, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 58, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 59, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 60, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 61, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 62, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 63, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:44 INFO Executor: Running task 5.0 in stage 3.0 (TID 53)
15/08/21 23:13:44 INFO Executor: Running task 7.0 in stage 3.0 (TID 55)
15/08/21 23:13:44 INFO Executor: Running task 3.0 in stage 3.0 (TID 51)
15/08/21 23:13:44 INFO Executor: Running task 4.0 in stage 3.0 (TID 52)
15/08/21 23:13:44 INFO Executor: Running task 2.0 in stage 3.0 (TID 50)
15/08/21 23:13:44 INFO Executor: Running task 15.0 in stage 3.0 (TID 63)
15/08/21 23:13:44 INFO Executor: Running task 10.0 in stage 3.0 (TID 58)
15/08/21 23:13:44 INFO Executor: Running task 0.0 in stage 3.0 (TID 48)
15/08/21 23:13:44 INFO Executor: Running task 14.0 in stage 3.0 (TID 62)
15/08/21 23:13:44 INFO Executor: Running task 13.0 in stage 3.0 (TID 61)
15/08/21 23:13:44 INFO Executor: Running task 12.0 in stage 3.0 (TID 60)
15/08/21 23:13:44 INFO Executor: Running task 8.0 in stage 3.0 (TID 56)
15/08/21 23:13:44 INFO Executor: Running task 11.0 in stage 3.0 (TID 59)
15/08/21 23:13:44 INFO Executor: Running task 9.0 in stage 3.0 (TID 57)
15/08/21 23:13:44 INFO Executor: Running task 6.0 in stage 3.0 (TID 54)
15/08/21 23:13:44 INFO Executor: Running task 1.0 in stage 3.0 (TID 49)
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:13:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:13:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:40586 in memory (size: 4.8 KB, free: 20.7 GB)
15/08/21 23:13:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:40586 in memory (size: 3.5 KB, free: 20.7 GB)
15/08/21 23:13:47 INFO Executor: Finished task 0.0 in stage 3.0 (TID 48). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 64, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 16.0 in stage 3.0 (TID 64)
15/08/21 23:13:48 INFO Executor: Finished task 15.0 in stage 3.0 (TID 63). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 65, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 17.0 in stage 3.0 (TID 65)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 63) in 3114 ms on localhost (1/200)
15/08/21 23:13:48 INFO Executor: Finished task 11.0 in stage 3.0 (TID 59). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 66, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO Executor: Running task 18.0 in stage 3.0 (TID 66)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 59) in 3127 ms on localhost (2/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 48) in 3150 ms on localhost (3/200)
15/08/21 23:13:48 INFO Executor: Finished task 3.0 in stage 3.0 (TID 51). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 67, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 19.0 in stage 3.0 (TID 67)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 51) in 3154 ms on localhost (4/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO Executor: Finished task 10.0 in stage 3.0 (TID 58). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 68, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 20.0 in stage 3.0 (TID 68)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 58) in 3214 ms on localhost (5/200)
15/08/21 23:13:48 INFO Executor: Finished task 6.0 in stage 3.0 (TID 54). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO Executor: Finished task 13.0 in stage 3.0 (TID 61). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 69, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 70, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 21.0 in stage 3.0 (TID 69)
15/08/21 23:13:48 INFO Executor: Running task 22.0 in stage 3.0 (TID 70)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 54) in 3224 ms on localhost (6/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO Executor: Finished task 7.0 in stage 3.0 (TID 55). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO Executor: Finished task 14.0 in stage 3.0 (TID 62). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 71, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 23.0 in stage 3.0 (TID 71)
15/08/21 23:13:48 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 72, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 24.0 in stage 3.0 (TID 72)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 55) in 3252 ms on localhost (7/200)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 62) in 3250 ms on localhost (8/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 61) in 3254 ms on localhost (9/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO Executor: Finished task 12.0 in stage 3.0 (TID 60). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 73, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 25.0 in stage 3.0 (TID 73)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 60) in 3312 ms on localhost (10/200)
15/08/21 23:13:48 INFO Executor: Finished task 2.0 in stage 3.0 (TID 50). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 74, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 26.0 in stage 3.0 (TID 74)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 50) in 3337 ms on localhost (11/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO Executor: Finished task 8.0 in stage 3.0 (TID 56). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 75, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 27.0 in stage 3.0 (TID 75)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 56) in 3533 ms on localhost (12/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:13:48 INFO Executor: Finished task 9.0 in stage 3.0 (TID 57). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO Executor: Finished task 5.0 in stage 3.0 (TID 53). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 76, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 28.0 in stage 3.0 (TID 76)
15/08/21 23:13:48 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 77, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 57) in 3610 ms on localhost (13/200)
15/08/21 23:13:48 INFO Executor: Running task 29.0 in stage 3.0 (TID 77)
15/08/21 23:13:48 INFO Executor: Finished task 1.0 in stage 3.0 (TID 49). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 78, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO Executor: Running task 30.0 in stage 3.0 (TID 78)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 53) in 3618 ms on localhost (14/200)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 49) in 3625 ms on localhost (15/200)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO Executor: Finished task 4.0 in stage 3.0 (TID 52). 1219 bytes result sent to driver
15/08/21 23:13:48 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 79, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:48 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 52) in 3653 ms on localhost (16/200)
15/08/21 23:13:48 INFO Executor: Running task 31.0 in stage 3.0 (TID 79)
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 17.0 in stage 3.0 (TID 65). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 80, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 32.0 in stage 3.0 (TID 80)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 65) in 1192 ms on localhost (17/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:49 INFO Executor: Finished task 20.0 in stage 3.0 (TID 68). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 81, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 33.0 in stage 3.0 (TID 81)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 68) in 1231 ms on localhost (18/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 22.0 in stage 3.0 (TID 70). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 82, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 34.0 in stage 3.0 (TID 82)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 70) in 1277 ms on localhost (19/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 16.0 in stage 3.0 (TID 64). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 83, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 35.0 in stage 3.0 (TID 83)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 64) in 1459 ms on localhost (20/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 19.0 in stage 3.0 (TID 67). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO Executor: Finished task 18.0 in stage 3.0 (TID 66). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 84, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 36.0 in stage 3.0 (TID 84)
15/08/21 23:13:49 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 85, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 37.0 in stage 3.0 (TID 85)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 66) in 1507 ms on localhost (21/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 24.0 in stage 3.0 (TID 72). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 86, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 38.0 in stage 3.0 (TID 86)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 67) in 1514 ms on localhost (22/200)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 72) in 1429 ms on localhost (23/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:49 INFO Executor: Finished task 26.0 in stage 3.0 (TID 74). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 87, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 39.0 in stage 3.0 (TID 87)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 74) in 1350 ms on localhost (24/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 21.0 in stage 3.0 (TID 69). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 88, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 40.0 in stage 3.0 (TID 88)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 69) in 1533 ms on localhost (25/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 25.0 in stage 3.0 (TID 73). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO Executor: Finished task 23.0 in stage 3.0 (TID 71). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 89, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 41.0 in stage 3.0 (TID 89)
15/08/21 23:13:49 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 90, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 42.0 in stage 3.0 (TID 90)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 73) in 1471 ms on localhost (26/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 71) in 1551 ms on localhost (27/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 29.0 in stage 3.0 (TID 77). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 91, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 43.0 in stage 3.0 (TID 91)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 77) in 1226 ms on localhost (28/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:49 INFO Executor: Finished task 27.0 in stage 3.0 (TID 75). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 92, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 44.0 in stage 3.0 (TID 92)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 75) in 1354 ms on localhost (29/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 30.0 in stage 3.0 (TID 78). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 93, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 45.0 in stage 3.0 (TID 93)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:49 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 78) in 1324 ms on localhost (30/200)
15/08/21 23:13:49 INFO Executor: Finished task 28.0 in stage 3.0 (TID 76). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 94, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 46.0 in stage 3.0 (TID 94)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 76) in 1345 ms on localhost (31/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO Executor: Finished task 31.0 in stage 3.0 (TID 79). 1219 bytes result sent to driver
15/08/21 23:13:49 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 95, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:49 INFO Executor: Running task 47.0 in stage 3.0 (TID 95)
15/08/21 23:13:49 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 79) in 1344 ms on localhost (32/200)
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO Executor: Finished task 32.0 in stage 3.0 (TID 80). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 96, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 48.0 in stage 3.0 (TID 96)
15/08/21 23:13:50 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 80) in 834 ms on localhost (33/200)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO Executor: Finished task 34.0 in stage 3.0 (TID 82). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 97, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 82) in 904 ms on localhost (34/200)
15/08/21 23:13:50 INFO Executor: Running task 49.0 in stage 3.0 (TID 97)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:50 INFO Executor: Finished task 38.0 in stage 3.0 (TID 86). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 98, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 50.0 in stage 3.0 (TID 98)
15/08/21 23:13:50 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 86) in 923 ms on localhost (35/200)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO Executor: Finished task 37.0 in stage 3.0 (TID 85). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 99, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 51.0 in stage 3.0 (TID 99)
15/08/21 23:13:50 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 85) in 994 ms on localhost (36/200)
15/08/21 23:13:50 INFO Executor: Finished task 33.0 in stage 3.0 (TID 81). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 100, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 52.0 in stage 3.0 (TID 100)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 81) in 1191 ms on localhost (37/200)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:50 INFO Executor: Finished task 35.0 in stage 3.0 (TID 83). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 101, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 53.0 in stage 3.0 (TID 101)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:50 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 83) in 1317 ms on localhost (38/200)
15/08/21 23:13:50 INFO Executor: Finished task 43.0 in stage 3.0 (TID 91). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 102, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 54.0 in stage 3.0 (TID 102)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 91) in 1177 ms on localhost (39/200)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:50 INFO Executor: Finished task 36.0 in stage 3.0 (TID 84). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 103, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 55.0 in stage 3.0 (TID 103)
15/08/21 23:13:50 INFO Executor: Finished task 47.0 in stage 3.0 (TID 95). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 104, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 56.0 in stage 3.0 (TID 104)
15/08/21 23:13:50 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 84) in 1461 ms on localhost (40/200)
15/08/21 23:13:50 INFO Executor: Finished task 46.0 in stage 3.0 (TID 94). 1219 bytes result sent to driver
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 105, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:50 INFO Executor: Running task 57.0 in stage 3.0 (TID 105)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 95) in 1108 ms on localhost (41/200)
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 94) in 1184 ms on localhost (42/200)
15/08/21 23:13:51 INFO Executor: Finished task 44.0 in stage 3.0 (TID 92). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 106, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 58.0 in stage 3.0 (TID 106)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 92) in 1326 ms on localhost (43/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO Executor: Finished task 42.0 in stage 3.0 (TID 90). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 107, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 59.0 in stage 3.0 (TID 107)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:51 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 90) in 1453 ms on localhost (44/200)
15/08/21 23:13:51 INFO Executor: Finished task 39.0 in stage 3.0 (TID 87). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 108, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 60.0 in stage 3.0 (TID 108)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 87) in 1597 ms on localhost (45/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO Executor: Finished task 41.0 in stage 3.0 (TID 89). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 109, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 61.0 in stage 3.0 (TID 109)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 89) in 1521 ms on localhost (46/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO Executor: Finished task 48.0 in stage 3.0 (TID 96). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 110, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 62.0 in stage 3.0 (TID 110)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 96) in 1235 ms on localhost (47/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO Executor: Finished task 40.0 in stage 3.0 (TID 88). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 111, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 63.0 in stage 3.0 (TID 111)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 88) in 1643 ms on localhost (48/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:51 INFO Executor: Finished task 45.0 in stage 3.0 (TID 93). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 112, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 64.0 in stage 3.0 (TID 112)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 93) in 1528 ms on localhost (49/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO Executor: Finished task 52.0 in stage 3.0 (TID 100). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 113, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 65.0 in stage 3.0 (TID 113)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 100) in 1042 ms on localhost (50/200)
15/08/21 23:13:51 INFO Executor: Finished task 50.0 in stage 3.0 (TID 98). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 114, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 66.0 in stage 3.0 (TID 114)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 98) in 1160 ms on localhost (51/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:51 INFO Executor: Finished task 51.0 in stage 3.0 (TID 99). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 115, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 67.0 in stage 3.0 (TID 115)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 99) in 1168 ms on localhost (52/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO Executor: Finished task 53.0 in stage 3.0 (TID 101). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 116, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 68.0 in stage 3.0 (TID 116)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 101) in 991 ms on localhost (53/200)
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:51 INFO Executor: Finished task 56.0 in stage 3.0 (TID 104). 1219 bytes result sent to driver
15/08/21 23:13:51 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 117, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:51 INFO Executor: Running task 69.0 in stage 3.0 (TID 117)
15/08/21 23:13:51 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 104) in 825 ms on localhost (54/200)
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 662 ms
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:52 INFO Executor: Finished task 49.0 in stage 3.0 (TID 97). 1219 bytes result sent to driver
15/08/21 23:13:52 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 118, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:52 INFO Executor: Running task 70.0 in stage 3.0 (TID 118)
15/08/21 23:13:52 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 97) in 2518 ms on localhost (55/200)
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:52 INFO Executor: Finished task 59.0 in stage 3.0 (TID 107). 1219 bytes result sent to driver
15/08/21 23:13:52 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 119, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:52 INFO Executor: Running task 71.0 in stage 3.0 (TID 119)
15/08/21 23:13:52 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 107) in 1792 ms on localhost (56/200)
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:52 INFO Executor: Finished task 55.0 in stage 3.0 (TID 103). 1219 bytes result sent to driver
15/08/21 23:13:52 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 120, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:52 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 103) in 1973 ms on localhost (57/200)
15/08/21 23:13:52 INFO Executor: Running task 72.0 in stage 3.0 (TID 120)
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO Executor: Finished task 60.0 in stage 3.0 (TID 108). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 121, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 73.0 in stage 3.0 (TID 121)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 108) in 1966 ms on localhost (58/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO Executor: Finished task 64.0 in stage 3.0 (TID 112). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 122, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 74.0 in stage 3.0 (TID 122)
15/08/21 23:13:53 INFO Executor: Finished task 57.0 in stage 3.0 (TID 105). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 123, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO Executor: Running task 75.0 in stage 3.0 (TID 123)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 112) in 1805 ms on localhost (59/200)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 105) in 2162 ms on localhost (60/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO Executor: Finished task 58.0 in stage 3.0 (TID 106). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 124, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 76.0 in stage 3.0 (TID 124)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 106) in 2130 ms on localhost (61/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO Executor: Finished task 54.0 in stage 3.0 (TID 102). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 125, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 77.0 in stage 3.0 (TID 125)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 102) in 2358 ms on localhost (62/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO Executor: Finished task 63.0 in stage 3.0 (TID 111). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 126, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 78.0 in stage 3.0 (TID 126)
15/08/21 23:13:53 INFO Executor: Finished task 62.0 in stage 3.0 (TID 110). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 127, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 79.0 in stage 3.0 (TID 127)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 111) in 2024 ms on localhost (63/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 110) in 2060 ms on localhost (64/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO Executor: Finished task 61.0 in stage 3.0 (TID 109). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 128, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 80.0 in stage 3.0 (TID 128)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 109) in 2169 ms on localhost (65/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO Executor: Finished task 66.0 in stage 3.0 (TID 114). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 129, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 81.0 in stage 3.0 (TID 129)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 114) in 1862 ms on localhost (66/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO Executor: Finished task 69.0 in stage 3.0 (TID 117). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 130, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 82.0 in stage 3.0 (TID 130)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 117) in 1720 ms on localhost (67/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO Executor: Finished task 65.0 in stage 3.0 (TID 113). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 131, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 83.0 in stage 3.0 (TID 131)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 113) in 2033 ms on localhost (68/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO Executor: Finished task 68.0 in stage 3.0 (TID 116). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 132, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 84.0 in stage 3.0 (TID 132)
15/08/21 23:13:53 INFO Executor: Finished task 72.0 in stage 3.0 (TID 120). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 116) in 2192 ms on localhost (69/200)
15/08/21 23:13:53 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 133, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 85.0 in stage 3.0 (TID 133)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 120) in 981 ms on localhost (70/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:53 INFO Executor: Finished task 75.0 in stage 3.0 (TID 123). 1219 bytes result sent to driver
15/08/21 23:13:53 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 134, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:53 INFO Executor: Running task 86.0 in stage 3.0 (TID 134)
15/08/21 23:13:53 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 123) in 852 ms on localhost (71/200)
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 67.0 in stage 3.0 (TID 115). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 135, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 87.0 in stage 3.0 (TID 135)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 115) in 2378 ms on localhost (72/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO Executor: Finished task 77.0 in stage 3.0 (TID 125). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 136, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 88.0 in stage 3.0 (TID 136)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 125) in 869 ms on localhost (73/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 76.0 in stage 3.0 (TID 124). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 137, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 124) in 1006 ms on localhost (74/200)
15/08/21 23:13:54 INFO Executor: Running task 89.0 in stage 3.0 (TID 137)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 78.0 in stage 3.0 (TID 126). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 138, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 90.0 in stage 3.0 (TID 138)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 126) in 939 ms on localhost (75/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO Executor: Finished task 71.0 in stage 3.0 (TID 119). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 139, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 91.0 in stage 3.0 (TID 139)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 119) in 1406 ms on localhost (76/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 73.0 in stage 3.0 (TID 121). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 140, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 92.0 in stage 3.0 (TID 140)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 121) in 1205 ms on localhost (77/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 79.0 in stage 3.0 (TID 127). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 141, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 93.0 in stage 3.0 (TID 141)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 127) in 1047 ms on localhost (78/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 80.0 in stage 3.0 (TID 128). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 142, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 94.0 in stage 3.0 (TID 142)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 128) in 1014 ms on localhost (79/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO Executor: Finished task 70.0 in stage 3.0 (TID 118). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 143, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 95.0 in stage 3.0 (TID 143)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 118) in 1640 ms on localhost (80/200)
15/08/21 23:13:54 INFO Executor: Finished task 81.0 in stage 3.0 (TID 129). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 144, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO Executor: Running task 96.0 in stage 3.0 (TID 144)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 129) in 963 ms on localhost (81/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 74.0 in stage 3.0 (TID 122). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO Executor: Finished task 82.0 in stage 3.0 (TID 130). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 145, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 97.0 in stage 3.0 (TID 145)
15/08/21 23:13:54 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 146, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 98.0 in stage 3.0 (TID 146)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 122) in 1406 ms on localhost (82/200)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 130) in 1031 ms on localhost (83/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 83.0 in stage 3.0 (TID 131). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 147, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 131) in 977 ms on localhost (84/200)
15/08/21 23:13:54 INFO Executor: Running task 99.0 in stage 3.0 (TID 147)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO Executor: Finished task 84.0 in stage 3.0 (TID 132). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 148, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 132) in 778 ms on localhost (85/200)
15/08/21 23:13:54 INFO Executor: Running task 100.0 in stage 3.0 (TID 148)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO Executor: Finished task 85.0 in stage 3.0 (TID 133). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 149, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 101.0 in stage 3.0 (TID 149)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 133) in 900 ms on localhost (86/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 86.0 in stage 3.0 (TID 134). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 150, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 102.0 in stage 3.0 (TID 150)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 134) in 940 ms on localhost (87/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO Executor: Finished task 88.0 in stage 3.0 (TID 136). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 151, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 103.0 in stage 3.0 (TID 151)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 136) in 836 ms on localhost (88/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:54 INFO Executor: Finished task 87.0 in stage 3.0 (TID 135). 1219 bytes result sent to driver
15/08/21 23:13:54 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 152, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:54 INFO Executor: Running task 104.0 in stage 3.0 (TID 152)
15/08/21 23:13:54 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 135) in 919 ms on localhost (89/200)
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO Executor: Finished task 89.0 in stage 3.0 (TID 137). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 153, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 105.0 in stage 3.0 (TID 153)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 137) in 838 ms on localhost (90/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO Executor: Finished task 94.0 in stage 3.0 (TID 142). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO Executor: Finished task 93.0 in stage 3.0 (TID 141). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 154, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 155, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 106.0 in stage 3.0 (TID 154)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 141) in 760 ms on localhost (91/200)
15/08/21 23:13:55 INFO Executor: Running task 107.0 in stage 3.0 (TID 155)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 142) in 749 ms on localhost (92/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 90.0 in stage 3.0 (TID 138). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 156, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 108.0 in stage 3.0 (TID 156)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 138) in 912 ms on localhost (93/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 91.0 in stage 3.0 (TID 139). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 157, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 109.0 in stage 3.0 (TID 157)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 139) in 873 ms on localhost (94/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO Executor: Finished task 92.0 in stage 3.0 (TID 140). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 158, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 110.0 in stage 3.0 (TID 158)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 140) in 903 ms on localhost (95/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO Executor: Finished task 95.0 in stage 3.0 (TID 143). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 159, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 111.0 in stage 3.0 (TID 159)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 143) in 935 ms on localhost (96/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 96.0 in stage 3.0 (TID 144). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 160, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 112.0 in stage 3.0 (TID 160)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 144) in 960 ms on localhost (97/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 100.0 in stage 3.0 (TID 148). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 161, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 113.0 in stage 3.0 (TID 161)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 148) in 739 ms on localhost (98/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 98.0 in stage 3.0 (TID 146). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 162, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 114.0 in stage 3.0 (TID 162)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 146) in 896 ms on localhost (99/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 97.0 in stage 3.0 (TID 145). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 163, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 115.0 in stage 3.0 (TID 163)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 145) in 919 ms on localhost (100/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO Executor: Finished task 99.0 in stage 3.0 (TID 147). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 164, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 116.0 in stage 3.0 (TID 164)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 147) in 958 ms on localhost (101/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 101.0 in stage 3.0 (TID 149). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 165, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 117.0 in stage 3.0 (TID 165)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 149) in 737 ms on localhost (102/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 103.0 in stage 3.0 (TID 151). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 166, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 118.0 in stage 3.0 (TID 166)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 151) in 767 ms on localhost (103/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 102.0 in stage 3.0 (TID 150). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO Executor: Finished task 104.0 in stage 3.0 (TID 152). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 167, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 168, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 120.0 in stage 3.0 (TID 168)
15/08/21 23:13:55 INFO Executor: Running task 119.0 in stage 3.0 (TID 167)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 152) in 852 ms on localhost (104/200)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 150) in 896 ms on localhost (105/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 105.0 in stage 3.0 (TID 153). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 169, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 121.0 in stage 3.0 (TID 169)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 153) in 822 ms on localhost (106/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 107.0 in stage 3.0 (TID 155). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 170, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 122.0 in stage 3.0 (TID 170)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 155) in 826 ms on localhost (107/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 108.0 in stage 3.0 (TID 156). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 171, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 123.0 in stage 3.0 (TID 171)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 156) in 800 ms on localhost (108/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:55 INFO Executor: Finished task 106.0 in stage 3.0 (TID 154). 1219 bytes result sent to driver
15/08/21 23:13:55 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 172, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:55 INFO Executor: Running task 124.0 in stage 3.0 (TID 172)
15/08/21 23:13:55 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 154) in 869 ms on localhost (109/200)
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 110.0 in stage 3.0 (TID 158). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 173, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 125.0 in stage 3.0 (TID 173)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 158) in 1294 ms on localhost (110/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 109.0 in stage 3.0 (TID 157). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 174, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 126.0 in stage 3.0 (TID 174)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 157) in 1361 ms on localhost (111/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 112.0 in stage 3.0 (TID 160). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 175, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 127.0 in stage 3.0 (TID 175)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 160) in 1155 ms on localhost (112/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 111.0 in stage 3.0 (TID 159). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 176, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 128.0 in stage 3.0 (TID 176)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 159) in 1201 ms on localhost (113/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 23:13:56 INFO Executor: Finished task 113.0 in stage 3.0 (TID 161). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 177, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 129.0 in stage 3.0 (TID 177)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 161) in 1239 ms on localhost (114/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 117.0 in stage 3.0 (TID 165). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 178, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 130.0 in stage 3.0 (TID 178)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 165) in 1128 ms on localhost (115/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 114.0 in stage 3.0 (TID 162). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 179, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 131.0 in stage 3.0 (TID 179)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 162) in 1257 ms on localhost (116/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 115.0 in stage 3.0 (TID 163). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 180, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 132.0 in stage 3.0 (TID 180)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 163) in 1250 ms on localhost (117/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:56 INFO Executor: Finished task 116.0 in stage 3.0 (TID 164). 1219 bytes result sent to driver
15/08/21 23:13:56 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 181, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:56 INFO Executor: Running task 133.0 in stage 3.0 (TID 181)
15/08/21 23:13:56 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 164) in 1259 ms on localhost (118/200)
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO Executor: Finished task 118.0 in stage 3.0 (TID 166). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 182, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 134.0 in stage 3.0 (TID 182)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 166) in 1383 ms on localhost (119/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO Executor: Finished task 125.0 in stage 3.0 (TID 173). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 183, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 135.0 in stage 3.0 (TID 183)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 173) in 660 ms on localhost (120/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 120.0 in stage 3.0 (TID 168). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 184, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 136.0 in stage 3.0 (TID 184)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 168) in 1455 ms on localhost (121/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 119.0 in stage 3.0 (TID 167). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 185, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 137.0 in stage 3.0 (TID 185)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 167) in 1498 ms on localhost (122/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO Executor: Finished task 124.0 in stage 3.0 (TID 172). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 186, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 138.0 in stage 3.0 (TID 186)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 172) in 1376 ms on localhost (123/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO Executor: Finished task 130.0 in stage 3.0 (TID 178). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 187, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 139.0 in stage 3.0 (TID 187)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 178) in 766 ms on localhost (124/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 122.0 in stage 3.0 (TID 170). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 188, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 140.0 in stage 3.0 (TID 188)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 170) in 1529 ms on localhost (125/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 121.0 in stage 3.0 (TID 169). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 189, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 141.0 in stage 3.0 (TID 189)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 169) in 1609 ms on localhost (126/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 123.0 in stage 3.0 (TID 171). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 190, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 142.0 in stage 3.0 (TID 190)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 171) in 1584 ms on localhost (127/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 126.0 in stage 3.0 (TID 174). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO Executor: Finished task 128.0 in stage 3.0 (TID 176). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO Executor: Finished task 127.0 in stage 3.0 (TID 175). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 191, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 143.0 in stage 3.0 (TID 191)
15/08/21 23:13:57 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 192, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 193, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 145.0 in stage 3.0 (TID 193)
15/08/21 23:13:57 INFO Executor: Running task 144.0 in stage 3.0 (TID 192)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 174) in 1047 ms on localhost (128/200)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 176) in 1006 ms on localhost (129/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 175) in 1024 ms on localhost (130/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 131.0 in stage 3.0 (TID 179). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 194, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 146.0 in stage 3.0 (TID 194)
15/08/21 23:13:57 INFO Executor: Finished task 129.0 in stage 3.0 (TID 177). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 195, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 177) in 985 ms on localhost (131/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 179) in 957 ms on localhost (132/200)
15/08/21 23:13:57 INFO Executor: Running task 147.0 in stage 3.0 (TID 195)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO Executor: Finished task 133.0 in stage 3.0 (TID 181). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO Executor: Finished task 132.0 in stage 3.0 (TID 180). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 196, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 148.0 in stage 3.0 (TID 196)
15/08/21 23:13:57 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 197, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 149.0 in stage 3.0 (TID 197)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 181) in 893 ms on localhost (133/200)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 180) in 962 ms on localhost (134/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 134.0 in stage 3.0 (TID 182). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 198, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 150.0 in stage 3.0 (TID 198)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 182) in 621 ms on localhost (135/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:57 INFO Executor: Finished task 135.0 in stage 3.0 (TID 183). 1219 bytes result sent to driver
15/08/21 23:13:57 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 199, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:57 INFO Executor: Running task 151.0 in stage 3.0 (TID 199)
15/08/21 23:13:57 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 183) in 807 ms on localhost (136/200)
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 136.0 in stage 3.0 (TID 184). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO Executor: Finished task 139.0 in stage 3.0 (TID 187). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 200, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 152.0 in stage 3.0 (TID 200)
15/08/21 23:13:58 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 201, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 153.0 in stage 3.0 (TID 201)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 184) in 869 ms on localhost (137/200)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 187) in 704 ms on localhost (138/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO Executor: Finished task 141.0 in stage 3.0 (TID 189). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 202, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 154.0 in stage 3.0 (TID 202)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 189) in 705 ms on localhost (139/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 138.0 in stage 3.0 (TID 186). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 203, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 155.0 in stage 3.0 (TID 203)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 186) in 854 ms on localhost (140/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO Executor: Finished task 143.0 in stage 3.0 (TID 191). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 204, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 156.0 in stage 3.0 (TID 204)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 191) in 658 ms on localhost (141/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 137.0 in stage 3.0 (TID 185). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 205, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 157.0 in stage 3.0 (TID 205)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 185) in 965 ms on localhost (142/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 140.0 in stage 3.0 (TID 188). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 206, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 158.0 in stage 3.0 (TID 206)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 188) in 886 ms on localhost (143/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 142.0 in stage 3.0 (TID 190). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 207, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 159.0 in stage 3.0 (TID 207)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 190) in 931 ms on localhost (144/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 144.0 in stage 3.0 (TID 192). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 208, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 160.0 in stage 3.0 (TID 208)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 192) in 904 ms on localhost (145/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 150.0 in stage 3.0 (TID 198). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 209, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 161.0 in stage 3.0 (TID 209)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 198) in 777 ms on localhost (146/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 145.0 in stage 3.0 (TID 193). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 210, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 162.0 in stage 3.0 (TID 210)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 193) in 986 ms on localhost (147/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 146.0 in stage 3.0 (TID 194). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 211, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 163.0 in stage 3.0 (TID 211)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 194) in 948 ms on localhost (148/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 148.0 in stage 3.0 (TID 196). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 212, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 164.0 in stage 3.0 (TID 212)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 196) in 958 ms on localhost (149/200)
15/08/21 23:13:58 INFO Executor: Finished task 149.0 in stage 3.0 (TID 197). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO Executor: Finished task 147.0 in stage 3.0 (TID 195). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 213, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 214, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Running task 166.0 in stage 3.0 (TID 214)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 195) in 988 ms on localhost (150/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Running task 165.0 in stage 3.0 (TID 213)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 197) in 970 ms on localhost (151/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 151.0 in stage 3.0 (TID 199). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 215, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 167.0 in stage 3.0 (TID 215)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 199) in 816 ms on localhost (152/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 154.0 in stage 3.0 (TID 202). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 216, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 202) in 627 ms on localhost (153/200)
15/08/21 23:13:58 INFO Executor: Running task 168.0 in stage 3.0 (TID 216)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO Executor: Finished task 157.0 in stage 3.0 (TID 205). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 217, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 169.0 in stage 3.0 (TID 217)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 205) in 633 ms on localhost (154/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 153.0 in stage 3.0 (TID 201). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO Executor: Finished task 155.0 in stage 3.0 (TID 203). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 218, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 219, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 171.0 in stage 3.0 (TID 219)
15/08/21 23:13:58 INFO Executor: Running task 170.0 in stage 3.0 (TID 218)
15/08/21 23:13:58 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 201) in 801 ms on localhost (155/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:58 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 203) in 748 ms on localhost (156/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO Executor: Finished task 152.0 in stage 3.0 (TID 200). 1219 bytes result sent to driver
15/08/21 23:13:58 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 220, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:58 INFO Executor: Running task 172.0 in stage 3.0 (TID 220)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:58 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 200) in 850 ms on localhost (157/200)
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO Executor: Finished task 156.0 in stage 3.0 (TID 204). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 221, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 204) in 812 ms on localhost (158/200)
15/08/21 23:13:59 INFO Executor: Running task 173.0 in stage 3.0 (TID 221)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:59 INFO Executor: Finished task 161.0 in stage 3.0 (TID 209). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 222, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 209) in 622 ms on localhost (159/200)
15/08/21 23:13:59 INFO Executor: Running task 174.0 in stage 3.0 (TID 222)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:59 INFO Executor: Finished task 162.0 in stage 3.0 (TID 210). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 223, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO Executor: Running task 175.0 in stage 3.0 (TID 223)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:59 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 210) in 649 ms on localhost (160/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO Executor: Finished task 158.0 in stage 3.0 (TID 206). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO Executor: Finished task 160.0 in stage 3.0 (TID 208). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 224, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 225, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 206) in 906 ms on localhost (161/200)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 208) in 778 ms on localhost (162/200)
15/08/21 23:13:59 INFO Executor: Running task 177.0 in stage 3.0 (TID 225)
15/08/21 23:13:59 INFO Executor: Running task 176.0 in stage 3.0 (TID 224)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO Executor: Finished task 159.0 in stage 3.0 (TID 207). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 226, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO Executor: Running task 178.0 in stage 3.0 (TID 226)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 207) in 815 ms on localhost (163/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:59 INFO Executor: Finished task 164.0 in stage 3.0 (TID 212). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 227, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO Executor: Running task 179.0 in stage 3.0 (TID 227)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 212) in 1286 ms on localhost (164/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO Executor: Finished task 166.0 in stage 3.0 (TID 214). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 228, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO Executor: Running task 180.0 in stage 3.0 (TID 228)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 214) in 1303 ms on localhost (165/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO Executor: Finished task 163.0 in stage 3.0 (TID 211). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 229, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO Executor: Running task 181.0 in stage 3.0 (TID 229)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 211) in 1357 ms on localhost (166/200)
15/08/21 23:13:59 INFO Executor: Finished task 165.0 in stage 3.0 (TID 213). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 230, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO Executor: Running task 182.0 in stage 3.0 (TID 230)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 213) in 1333 ms on localhost (167/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:13:59 INFO Executor: Finished task 170.0 in stage 3.0 (TID 218). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 231, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO Executor: Finished task 167.0 in stage 3.0 (TID 215). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO Executor: Running task 183.0 in stage 3.0 (TID 231)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 218) in 1036 ms on localhost (168/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
15/08/21 23:13:59 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 232, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 215) in 1210 ms on localhost (169/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO Executor: Running task 184.0 in stage 3.0 (TID 232)
15/08/21 23:13:59 INFO Executor: Finished task 168.0 in stage 3.0 (TID 216). 1219 bytes result sent to driver
15/08/21 23:13:59 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 233, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:13:59 INFO Executor: Running task 185.0 in stage 3.0 (TID 233)
15/08/21 23:13:59 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 216) in 1208 ms on localhost (170/200)
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:13:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO Executor: Finished task 171.0 in stage 3.0 (TID 219). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 234, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 186.0 in stage 3.0 (TID 234)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 219) in 1234 ms on localhost (171/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO Executor: Finished task 173.0 in stage 3.0 (TID 221). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO Executor: Finished task 169.0 in stage 3.0 (TID 217). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 187.0 in stage 3.0 (TID 235, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 187.0 in stage 3.0 (TID 235)
15/08/21 23:14:00 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 236, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 188.0 in stage 3.0 (TID 236)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 173.0 in stage 3.0 (TID 221) in 1257 ms on localhost (172/200)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 217) in 1383 ms on localhost (173/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:00 INFO Executor: Finished task 172.0 in stage 3.0 (TID 220). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 189.0 in stage 3.0 (TID 237, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 189.0 in stage 3.0 (TID 237)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 220) in 1323 ms on localhost (174/200)
15/08/21 23:14:00 INFO Executor: Finished task 174.0 in stage 3.0 (TID 222). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 238, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO Executor: Running task 190.0 in stage 3.0 (TID 238)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 222) in 1208 ms on localhost (175/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO Executor: Finished task 176.0 in stage 3.0 (TID 224). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 191.0 in stage 3.0 (TID 239, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 191.0 in stage 3.0 (TID 239)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 224) in 1198 ms on localhost (176/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO Executor: Finished task 175.0 in stage 3.0 (TID 223). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 240, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 192.0 in stage 3.0 (TID 240)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 223) in 1260 ms on localhost (177/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO Executor: Finished task 178.0 in stage 3.0 (TID 226). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO Executor: Finished task 177.0 in stage 3.0 (TID 225). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 241, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 193.0 in stage 3.0 (TID 241)
15/08/21 23:14:00 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 242, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 194.0 in stage 3.0 (TID 242)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 226) in 1402 ms on localhost (178/200)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 225) in 1428 ms on localhost (179/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:00 INFO Executor: Finished task 185.0 in stage 3.0 (TID 233). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 243, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 195.0 in stage 3.0 (TID 243)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 233) in 721 ms on localhost (180/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO Executor: Finished task 179.0 in stage 3.0 (TID 227). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO Executor: Finished task 180.0 in stage 3.0 (TID 228). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 196.0 in stage 3.0 (TID 244, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 196.0 in stage 3.0 (TID 244)
15/08/21 23:14:00 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 245, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 180.0 in stage 3.0 (TID 228) in 895 ms on localhost (181/200)
15/08/21 23:14:00 INFO Executor: Running task 197.0 in stage 3.0 (TID 245)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 227) in 932 ms on localhost (182/200)
15/08/21 23:14:00 INFO Executor: Finished task 182.0 in stage 3.0 (TID 230). 1219 bytes result sent to driver
15/08/21 23:14:00 INFO TaskSetManager: Starting task 198.0 in stage 3.0 (TID 246, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:00 INFO Executor: Running task 198.0 in stage 3.0 (TID 246)
15/08/21 23:14:00 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 230) in 954 ms on localhost (183/200)
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:01 INFO Executor: Finished task 184.0 in stage 3.0 (TID 232). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Starting task 199.0 in stage 3.0 (TID 247, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 23:14:01 INFO Executor: Running task 199.0 in stage 3.0 (TID 247)
15/08/21 23:14:01 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 232) in 1086 ms on localhost (184/200)
15/08/21 23:14:01 INFO ShuffleBlockFetcherIterator: Getting 39 non-empty blocks out of 39 blocks
15/08/21 23:14:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:01 INFO Executor: Finished task 183.0 in stage 3.0 (TID 231). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/21 23:14:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:01 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 231) in 1122 ms on localhost (185/200)
15/08/21 23:14:01 INFO Executor: Finished task 181.0 in stage 3.0 (TID 229). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 229) in 1174 ms on localhost (186/200)
15/08/21 23:14:01 INFO Executor: Finished task 190.0 in stage 3.0 (TID 238). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO Executor: Finished task 188.0 in stage 3.0 (TID 236). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 238) in 838 ms on localhost (187/200)
15/08/21 23:14:01 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 236) in 865 ms on localhost (188/200)
15/08/21 23:14:01 INFO Executor: Finished task 186.0 in stage 3.0 (TID 234). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 234) in 997 ms on localhost (189/200)
15/08/21 23:14:01 INFO Executor: Finished task 187.0 in stage 3.0 (TID 235). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 187.0 in stage 3.0 (TID 235) in 918 ms on localhost (190/200)
15/08/21 23:14:01 INFO Executor: Finished task 191.0 in stage 3.0 (TID 239). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO Executor: Finished task 189.0 in stage 3.0 (TID 237). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 191.0 in stage 3.0 (TID 239) in 812 ms on localhost (191/200)
15/08/21 23:14:01 INFO TaskSetManager: Finished task 189.0 in stage 3.0 (TID 237) in 945 ms on localhost (192/200)
15/08/21 23:14:01 INFO Executor: Finished task 192.0 in stage 3.0 (TID 240). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO Executor: Finished task 193.0 in stage 3.0 (TID 241). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 240) in 940 ms on localhost (193/200)
15/08/21 23:14:01 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 241) in 725 ms on localhost (194/200)
15/08/21 23:14:01 INFO Executor: Finished task 194.0 in stage 3.0 (TID 242). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 242) in 812 ms on localhost (195/200)
15/08/21 23:14:01 INFO Executor: Finished task 195.0 in stage 3.0 (TID 243). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 243) in 1027 ms on localhost (196/200)
15/08/21 23:14:01 INFO Executor: Finished task 197.0 in stage 3.0 (TID 245). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 245) in 936 ms on localhost (197/200)
15/08/21 23:14:01 INFO Executor: Finished task 198.0 in stage 3.0 (TID 246). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO Executor: Finished task 199.0 in stage 3.0 (TID 247). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 198.0 in stage 3.0 (TID 246) in 954 ms on localhost (198/200)
15/08/21 23:14:01 INFO TaskSetManager: Finished task 199.0 in stage 3.0 (TID 247) in 769 ms on localhost (199/200)
15/08/21 23:14:01 INFO Executor: Finished task 196.0 in stage 3.0 (TID 244). 1219 bytes result sent to driver
15/08/21 23:14:01 INFO TaskSetManager: Finished task 196.0 in stage 3.0 (TID 244) in 1040 ms on localhost (200/200)
15/08/21 23:14:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/21 23:14:01 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 16.950 s
15/08/21 23:14:01 INFO DAGScheduler: looking for newly runnable stages
15/08/21 23:14:01 INFO DAGScheduler: running: Set()
15/08/21 23:14:01 INFO DAGScheduler: waiting: Set(ResultStage 4)
15/08/21 23:14:01 INFO DAGScheduler: failed: Set()
15/08/21 23:14:01 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5df3d529
15/08/21 23:14:01 INFO StatsReportListener: task runtime:(count: 200, mean: 1326.660000, stdev: 699.425796, max: 3653.000000, min: 621.000000)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	621.0 ms	725.0 ms	778.0 ms	896.0 ms	1.1 s	1.5 s	2.2 s	3.3 s	3.7 s
15/08/21 23:14:01 INFO DAGScheduler: Missing parents for ResultStage 4: List()
15/08/21 23:14:01 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 23:14:01 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 243498.685000, stdev: 16472.291857, max: 288923.000000, min: 202930.000000)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	198.2 KB	213.0 KB	218.2 KB	225.2 KB	237.2 KB	247.6 KB	260.7 KB	264.8 KB	282.2 KB
15/08/21 23:14:01 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.535000, stdev: 0.760773, max: 4.000000, min: 0.000000)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	2.0 ms	4.0 ms
15/08/21 23:14:01 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 23:14:01 INFO StatsReportListener: task result size:(count: 200, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/21 23:14:01 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.922472, stdev: 0.821744, max: 99.116162, min: 94.763514)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	95 %	96 %	97 %	98 %	98 %	99 %	99 %	99 %	99 %
15/08/21 23:14:01 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.043705, stdev: 0.069007, max: 0.427350, min: 0.000000)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 23:14:01 INFO StatsReportListener: other time pct: (count: 200, mean: 2.033823, stdev: 0.813777, max: 5.152027, min: 0.820707)
15/08/21 23:14:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:01 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 2 %	 2 %	 3 %	 4 %	 5 %
15/08/21 23:14:01 INFO MemoryStore: ensureFreeSpace(82504) called with curMem=1069604, maxMem=22226833244
15/08/21 23:14:01 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 80.6 KB, free 20.7 GB)
15/08/21 23:14:01 INFO MemoryStore: ensureFreeSpace(31761) called with curMem=1152108, maxMem=22226833244
15/08/21 23:14:01 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.0 KB, free 20.7 GB)
15/08/21 23:14:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:40586 (size: 31.0 KB, free: 20.7 GB)
15/08/21 23:14:01 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/21 23:14:01 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at processCmd at CliDriver.java:423)
15/08/21 23:14:01 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks
15/08/21 23:14:01 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:01 INFO Executor: Running task 0.0 in stage 4.0 (TID 248)
15/08/21 23:14:01 INFO Executor: Running task 2.0 in stage 4.0 (TID 250)
15/08/21 23:14:01 INFO Executor: Running task 5.0 in stage 4.0 (TID 253)
15/08/21 23:14:01 INFO Executor: Running task 7.0 in stage 4.0 (TID 255)
15/08/21 23:14:01 INFO Executor: Running task 12.0 in stage 4.0 (TID 260)
15/08/21 23:14:01 INFO Executor: Running task 4.0 in stage 4.0 (TID 252)
15/08/21 23:14:01 INFO Executor: Running task 15.0 in stage 4.0 (TID 263)
15/08/21 23:14:01 INFO Executor: Running task 3.0 in stage 4.0 (TID 251)
15/08/21 23:14:01 INFO Executor: Running task 14.0 in stage 4.0 (TID 262)
15/08/21 23:14:01 INFO Executor: Running task 10.0 in stage 4.0 (TID 258)
15/08/21 23:14:01 INFO Executor: Running task 8.0 in stage 4.0 (TID 256)
15/08/21 23:14:01 INFO Executor: Running task 13.0 in stage 4.0 (TID 261)
15/08/21 23:14:01 INFO Executor: Running task 9.0 in stage 4.0 (TID 257)
15/08/21 23:14:01 INFO Executor: Running task 11.0 in stage 4.0 (TID 259)
15/08/21 23:14:01 INFO Executor: Running task 6.0 in stage 4.0 (TID 254)
15/08/21 23:14:01 INFO Executor: Running task 1.0 in stage 4.0 (TID 249)
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:02 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:02 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,848
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,012
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,368
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,540
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,396
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,048
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,548
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,068
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,948
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,936
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,452
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,436
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,752
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,948
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,648
15/08/21 23:14:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,076
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 50,483B for [ps_partkey] INT32: 15,106 values, 60,432B raw, 50,444B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,162B for [ps_partkey] INT32: 14,740 values, 58,968B raw, 49,123B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,596B for [ps_partkey] INT32: 14,829 values, 59,324B raw, 49,557B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 48,462B for [ps_partkey] INT32: 14,489 values, 57,964B raw, 48,423B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 48,792B for [ps_partkey] INT32: 14,590 values, 58,368B raw, 48,753B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,523B for [ps_partkey] INT32: 14,825 values, 59,308B raw, 49,484B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,044B for [ps_partkey] INT32: 14,684 values, 58,744B raw, 49,005B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 50,251B for [ps_partkey] INT32: 15,038 values, 60,160B raw, 50,212B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,340B for [ps_partkey] INT32: 14,755 values, 59,028B raw, 49,301B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,574B for [ps_partkey] INT32: 14,919 values, 59,684B raw, 49,535B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 74,523B for [part_value] DOUBLE: 14,489 values, 115,920B raw, 74,476B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 50,296B for [ps_partkey] INT32: 15,058 values, 60,240B raw, 50,257B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 75,515B for [part_value] DOUBLE: 14,684 values, 117,480B raw, 75,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 76,306B for [part_value] DOUBLE: 14,825 values, 118,608B raw, 76,259B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 75,891B for [part_value] DOUBLE: 14,755 values, 118,048B raw, 75,844B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,185B for [ps_partkey] INT32: 14,784 values, 59,144B raw, 49,146B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 48,680B for [ps_partkey] INT32: 14,560 values, 58,248B raw, 48,641B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 77,368B for [part_value] DOUBLE: 15,038 values, 120,312B raw, 77,321B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 76,702B for [part_value] DOUBLE: 14,919 values, 119,360B raw, 76,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 51,274B for [ps_partkey] INT32: 15,383 values, 61,540B raw, 51,235B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 49,481B for [ps_partkey] INT32: 14,814 values, 59,264B raw, 49,442B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 75,844B for [part_value] DOUBLE: 14,740 values, 117,928B raw, 75,797B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 75,037B for [part_value] DOUBLE: 14,590 values, 116,728B raw, 74,990B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 48,712B for [ps_partkey] INT32: 14,564 values, 58,264B raw, 48,673B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 77,727B for [part_value] DOUBLE: 15,106 values, 120,856B raw, 77,680B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 77,520B for [part_value] DOUBLE: 15,058 values, 120,472B raw, 77,473B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 74,857B for [part_value] DOUBLE: 14,560 values, 116,488B raw, 74,810B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 76,082B for [part_value] DOUBLE: 14,784 values, 118,280B raw, 76,035B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 76,230B for [part_value] DOUBLE: 14,829 values, 118,640B raw, 76,183B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 79,068B for [part_value] DOUBLE: 15,383 values, 123,072B raw, 79,021B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 76,190B for [part_value] DOUBLE: 14,814 values, 118,520B raw, 76,143B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO ColumnChunkPageWriteStore: written 74,922B for [part_value] DOUBLE: 14,564 values, 116,520B raw, 74,875B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000010
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000000
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000010_0: Committed
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000000_0: Committed
15/08/21 23:14:03 INFO Executor: Finished task 10.0 in stage 4.0 (TID 258). 843 bytes result sent to driver
15/08/21 23:14:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 248). 843 bytes result sent to driver
15/08/21 23:14:03 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 16.0 in stage 4.0 (TID 264)
15/08/21 23:14:03 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 17.0 in stage 4.0 (TID 265)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 248) in 1827 ms on localhost (1/200)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 258) in 1824 ms on localhost (2/200)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000014
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000014_0: Committed
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000015
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000015_0: Committed
15/08/21 23:14:03 INFO Executor: Finished task 14.0 in stage 4.0 (TID 262). 843 bytes result sent to driver
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000006
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000006_0: Committed
15/08/21 23:14:03 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 18.0 in stage 4.0 (TID 266)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000009
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000009_0: Committed
15/08/21 23:14:03 INFO Executor: Finished task 6.0 in stage 4.0 (TID 254). 843 bytes result sent to driver
15/08/21 23:14:03 INFO Executor: Finished task 15.0 in stage 4.0 (TID 263). 843 bytes result sent to driver
15/08/21 23:14:03 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 262) in 1845 ms on localhost (3/200)
15/08/21 23:14:03 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 19.0 in stage 4.0 (TID 267)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000001
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000001_0: Committed
15/08/21 23:14:03 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 20.0 in stage 4.0 (TID 268)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 254) in 1851 ms on localhost (4/200)
15/08/21 23:14:03 INFO Executor: Finished task 1.0 in stage 4.0 (TID 249). 843 bytes result sent to driver
15/08/21 23:14:03 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 263) in 1848 ms on localhost (5/200)
15/08/21 23:14:03 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Finished task 9.0 in stage 4.0 (TID 257). 843 bytes result sent to driver
15/08/21 23:14:03 INFO Executor: Running task 21.0 in stage 4.0 (TID 269)
15/08/21 23:14:03 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 22.0 in stage 4.0 (TID 270)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 249) in 1859 ms on localhost (6/200)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 257) in 1857 ms on localhost (7/200)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000007
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000007_0: Committed
15/08/21 23:14:03 INFO Executor: Finished task 7.0 in stage 4.0 (TID 255). 843 bytes result sent to driver
15/08/21 23:14:03 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 23.0 in stage 4.0 (TID 271)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 255) in 1863 ms on localhost (8/200)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000013
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000013_0: Committed
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO Executor: Finished task 13.0 in stage 4.0 (TID 261). 843 bytes result sent to driver
15/08/21 23:14:03 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000005
15/08/21 23:14:03 INFO Executor: Running task 24.0 in stage 4.0 (TID 272)
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000005_0: Committed
15/08/21 23:14:03 INFO Executor: Finished task 5.0 in stage 4.0 (TID 253). 843 bytes result sent to driver
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000008
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000008_0: Committed
15/08/21 23:14:03 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 25.0 in stage 4.0 (TID 273)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 261) in 1873 ms on localhost (9/200)
15/08/21 23:14:03 INFO Executor: Finished task 8.0 in stage 4.0 (TID 256). 843 bytes result sent to driver
15/08/21 23:14:03 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000004
15/08/21 23:14:03 INFO Executor: Running task 26.0 in stage 4.0 (TID 274)
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000004_0: Committed
15/08/21 23:14:03 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 253) in 1879 ms on localhost (10/200)
15/08/21 23:14:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000003
15/08/21 23:14:03 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000003_0: Committed
15/08/21 23:14:03 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 256) in 1879 ms on localhost (11/200)
15/08/21 23:14:03 INFO Executor: Finished task 3.0 in stage 4.0 (TID 251). 843 bytes result sent to driver
15/08/21 23:14:03 INFO Executor: Finished task 4.0 in stage 4.0 (TID 252). 843 bytes result sent to driver
15/08/21 23:14:03 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 27.0 in stage 4.0 (TID 275)
15/08/21 23:14:03 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:03 INFO Executor: Running task 28.0 in stage 4.0 (TID 276)
15/08/21 23:14:03 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 252) in 1887 ms on localhost (12/200)
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:03 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 251) in 1891 ms on localhost (13/200)
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:03 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:03 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,900
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000011
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000012
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000011_0: Committed
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000012_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 11.0 in stage 4.0 (TID 259). 843 bytes result sent to driver
15/08/21 23:14:04 INFO Executor: Finished task 12.0 in stage 4.0 (TID 260). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 29.0 in stage 4.0 (TID 277)
15/08/21 23:14:04 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,148
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,060
15/08/21 23:14:04 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 259) in 2166 ms on localhost (14/200)
15/08/21 23:14:04 INFO Executor: Running task 30.0 in stage 4.0 (TID 278)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 260) in 2166 ms on localhost (15/200)
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,840
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,928
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,308
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,156
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,744,160
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,128
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000002
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000002_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 2.0 in stage 4.0 (TID 250). 843 bytes result sent to driver
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,538B for [ps_partkey] INT32: 15,132 values, 60,536B raw, 50,499B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 31.0 in stage 4.0 (TID 279)
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,836
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 77,861B for [part_value] DOUBLE: 15,132 values, 121,064B raw, 77,814B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 250) in 2189 ms on localhost (16/200)
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,440
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,048
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 49,910B for [ps_partkey] INT32: 14,944 values, 59,784B raw, 49,871B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 48,539B for [ps_partkey] INT32: 14,590 values, 58,368B raw, 48,500B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 74,949B for [part_value] DOUBLE: 14,590 values, 116,728B raw, 74,902B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 76,958B for [part_value] DOUBLE: 14,944 values, 119,560B raw, 76,911B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 49,062B for [ps_partkey] INT32: 14,678 values, 58,720B raw, 49,023B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 75,433B for [part_value] DOUBLE: 14,678 values, 117,432B raw, 75,386B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 51,315B for [ps_partkey] INT32: 15,393 values, 61,580B raw, 51,276B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 47,681B for [ps_partkey] INT32: 14,345 values, 57,388B raw, 47,642B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 51,154B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 51,115B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 51,293B for [ps_partkey] INT32: 15,344 values, 61,384B raw, 51,254B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 73,847B for [part_value] DOUBLE: 14,345 values, 114,768B raw, 73,800B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 78,754B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,707B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 79,116B for [part_value] DOUBLE: 15,393 values, 123,152B raw, 79,069B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 51,270B for [ps_partkey] INT32: 15,383 values, 61,540B raw, 51,231B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,423B for [ps_partkey] INT32: 15,079 values, 60,324B raw, 50,384B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 78,932B for [part_value] DOUBLE: 15,344 values, 122,760B raw, 78,885B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 48,495B for [ps_partkey] INT32: 14,509 values, 58,044B raw, 48,456B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,219B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,180B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 77,549B for [part_value] DOUBLE: 15,079 values, 120,640B raw, 77,502B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 79,065B for [part_value] DOUBLE: 15,383 values, 123,072B raw, 79,018B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 74,606B for [part_value] DOUBLE: 14,509 values, 116,080B raw, 74,559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 77,318B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,271B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,676
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000016
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000016_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 16.0 in stage 4.0 (TID 264). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 32.0 in stage 4.0 (TID 280)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 264) in 449 ms on localhost (17/200)
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,502B for [ps_partkey] INT32: 15,120 values, 60,488B raw, 50,463B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 77,808B for [part_value] DOUBLE: 15,120 values, 120,968B raw, 77,761B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000020
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000020_0: Committed
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000025
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000025_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 20.0 in stage 4.0 (TID 268). 843 bytes result sent to driver
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000024
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000024_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 25.0 in stage 4.0 (TID 273). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 33.0 in stage 4.0 (TID 281)
15/08/21 23:14:04 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Finished task 24.0 in stage 4.0 (TID 272). 843 bytes result sent to driver
15/08/21 23:14:04 INFO Executor: Running task 34.0 in stage 4.0 (TID 282)
15/08/21 23:14:04 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 35.0 in stage 4.0 (TID 283)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 268) in 448 ms on localhost (18/200)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 273) in 421 ms on localhost (19/200)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 272) in 429 ms on localhost (20/200)
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000028
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000028_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 28.0 in stage 4.0 (TID 276). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 36.0 in stage 4.0 (TID 284)
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000018
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000018_0: Committed
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO Executor: Finished task 18.0 in stage 4.0 (TID 266). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 266) in 477 ms on localhost (21/200)
15/08/21 23:14:04 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 285, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 37.0 in stage 4.0 (TID 285)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 276) in 446 ms on localhost (22/200)
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000017
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000017_0: Committed
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000019
15/08/21 23:14:04 INFO Executor: Finished task 17.0 in stage 4.0 (TID 265). 843 bytes result sent to driver
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000019_0: Committed
15/08/21 23:14:04 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 286, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 38.0 in stage 4.0 (TID 286)
15/08/21 23:14:04 INFO Executor: Finished task 19.0 in stage 4.0 (TID 267). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 265) in 649 ms on localhost (23/200)
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 287, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO Executor: Running task 39.0 in stage 4.0 (TID 287)
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 267) in 626 ms on localhost (24/200)
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,596
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,840
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 49,541B for [ps_partkey] INT32: 14,816 values, 59,272B raw, 49,502B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 76,158B for [part_value] DOUBLE: 14,816 values, 118,536B raw, 76,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,178B for [ps_partkey] INT32: 15,029 values, 60,124B raw, 50,139B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 77,329B for [part_value] DOUBLE: 15,029 values, 120,240B raw, 77,282B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,136
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,796
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 49,362B for [ps_partkey] INT32: 14,776 values, 59,112B raw, 49,323B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 75,995B for [part_value] DOUBLE: 14,776 values, 118,216B raw, 75,948B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 49,591B for [ps_partkey] INT32: 14,843 values, 59,380B raw, 49,552B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 76,334B for [part_value] DOUBLE: 14,843 values, 118,752B raw, 76,287B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,996
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,188
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,256
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 51,365B for [ps_partkey] INT32: 15,386 values, 61,552B raw, 51,326B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 79,089B for [part_value] DOUBLE: 15,386 values, 123,096B raw, 79,042B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000030
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000030_0: Committed
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,088
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,301B for [ps_partkey] INT32: 15,046 values, 60,192B raw, 50,262B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO Executor: Finished task 30.0 in stage 4.0 (TID 278). 843 bytes result sent to driver
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 77,375B for [part_value] DOUBLE: 15,046 values, 120,376B raw, 77,328B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 288, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 40.0 in stage 4.0 (TID 288)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 278) in 491 ms on localhost (25/200)
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,173B for [ps_partkey] INT32: 15,049 values, 60,204B raw, 50,134B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 77,439B for [part_value] DOUBLE: 15,049 values, 120,400B raw, 77,392B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 49,175B for [ps_partkey] INT32: 14,791 values, 59,172B raw, 49,136B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 75,932B for [part_value] DOUBLE: 14,791 values, 118,336B raw, 75,885B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000035
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000035_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 35.0 in stage 4.0 (TID 283). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 289, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 41.0 in stage 4.0 (TID 289)
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,688
15/08/21 23:14:04 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 283) in 396 ms on localhost (26/200)
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 49,394B for [ps_partkey] INT32: 14,771 values, 59,092B raw, 49,355B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 75,957B for [part_value] DOUBLE: 14,771 values, 118,176B raw, 75,910B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,548
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000037
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000037_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 37.0 in stage 4.0 (TID 285). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 290, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 42.0 in stage 4.0 (TID 290)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 285) in 445 ms on localhost (27/200)
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,670B for [ps_partkey] INT32: 15,164 values, 60,664B raw, 50,631B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 78,004B for [part_value] DOUBLE: 15,164 values, 121,320B raw, 77,957B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,328
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,774B for [ps_partkey] INT32: 15,203 values, 60,820B raw, 50,735B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 78,172B for [part_value] DOUBLE: 15,203 values, 121,632B raw, 78,125B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000038
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000038_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 38.0 in stage 4.0 (TID 286). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 291, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 286) in 359 ms on localhost (28/200)
15/08/21 23:14:04 INFO Executor: Running task 43.0 in stage 4.0 (TID 291)
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000022
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000022_0: Committed
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000026
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000026_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 22.0 in stage 4.0 (TID 270). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 292, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 44.0 in stage 4.0 (TID 292)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 270) in 1004 ms on localhost (29/200)
15/08/21 23:14:04 INFO Executor: Finished task 26.0 in stage 4.0 (TID 274). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 293, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 45.0 in stage 4.0 (TID 293)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 274) in 983 ms on localhost (30/200)
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,700
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000023
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000023_0: Committed
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO Executor: Finished task 23.0 in stage 4.0 (TID 271). 843 bytes result sent to driver
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000021
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000021_0: Committed
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 294, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 46.0 in stage 4.0 (TID 294)
15/08/21 23:14:04 INFO Executor: Finished task 21.0 in stage 4.0 (TID 269). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 295, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 271) in 1034 ms on localhost (31/200)
15/08/21 23:14:04 INFO Executor: Running task 47.0 in stage 4.0 (TID 295)
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 269) in 1044 ms on localhost (32/200)
15/08/21 23:14:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000027
15/08/21 23:14:04 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000027_0: Committed
15/08/21 23:14:04 INFO Executor: Finished task 27.0 in stage 4.0 (TID 275). 843 bytes result sent to driver
15/08/21 23:14:04 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 296, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:04 INFO Executor: Running task 48.0 in stage 4.0 (TID 296)
15/08/21 23:14:04 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 275) in 1027 ms on localhost (33/200)
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,045B for [ps_partkey] INT32: 14,972 values, 59,896B raw, 50,006B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 76,973B for [part_value] DOUBLE: 14,972 values, 119,784B raw, 76,926B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,968
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 50,581B for [ps_partkey] INT32: 15,235 values, 60,948B raw, 50,542B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO ColumnChunkPageWriteStore: written 78,357B for [part_value] DOUBLE: 15,235 values, 121,888B raw, 78,310B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,328
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,416
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:04 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000040
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000040_0: Committed
15/08/21 23:14:05 INFO Executor: Finished task 40.0 in stage 4.0 (TID 288). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 297, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 49.0 in stage 4.0 (TID 297)
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 52,094B for [ps_partkey] INT32: 15,603 values, 62,420B raw, 52,055B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 80,396B for [part_value] DOUBLE: 15,603 values, 124,832B raw, 80,349B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 288) in 438 ms on localhost (34/200)
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,100
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,168
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 51,806B for [ps_partkey] INT32: 15,557 values, 62,236B raw, 51,767B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 79,910B for [part_value] DOUBLE: 15,557 values, 124,464B raw, 79,863B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 49,580B for [ps_partkey] INT32: 14,842 values, 59,376B raw, 49,541B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 50,600B for [ps_partkey] INT32: 15,145 values, 60,588B raw, 50,561B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 77,934B for [part_value] DOUBLE: 15,145 values, 121,168B raw, 77,887B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 76,384B for [part_value] DOUBLE: 14,842 values, 118,744B raw, 76,337B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000029
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000029_0: Committed
15/08/21 23:14:05 INFO Executor: Finished task 29.0 in stage 4.0 (TID 277). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 298, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 50.0 in stage 4.0 (TID 298)
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000032
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000032_0: Committed
15/08/21 23:14:05 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 277) in 999 ms on localhost (35/200)
15/08/21 23:14:05 INFO Executor: Finished task 32.0 in stage 4.0 (TID 280). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 299, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 51.0 in stage 4.0 (TID 299)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 280) in 907 ms on localhost (36/200)
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000041
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000041_0: Committed
15/08/21 23:14:05 INFO Executor: Finished task 41.0 in stage 4.0 (TID 289). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 300, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 52.0 in stage 4.0 (TID 300)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 289) in 503 ms on localhost (37/200)
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,956
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,920
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,396
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 51,228B for [ps_partkey] INT32: 15,333 values, 61,340B raw, 51,189B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 78,917B for [part_value] DOUBLE: 15,333 values, 122,672B raw, 78,870B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 51,391B for [ps_partkey] INT32: 15,384 values, 61,544B raw, 51,352B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 49,817B for [ps_partkey] INT32: 14,906 values, 59,632B raw, 49,778B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 79,099B for [part_value] DOUBLE: 15,384 values, 123,080B raw, 79,052B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 76,596B for [part_value] DOUBLE: 14,906 values, 119,256B raw, 76,549B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000031
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000031_0: Committed
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO Executor: Finished task 31.0 in stage 4.0 (TID 279). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 301, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 53.0 in stage 4.0 (TID 301)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 279) in 1096 ms on localhost (38/200)
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000033
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000033_0: Committed
15/08/21 23:14:05 INFO Executor: Finished task 33.0 in stage 4.0 (TID 281). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 302, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 54.0 in stage 4.0 (TID 302)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 281) in 993 ms on localhost (39/200)
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000034
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000034_0: Committed
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000036
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000036_0: Committed
15/08/21 23:14:05 INFO Executor: Finished task 34.0 in stage 4.0 (TID 282). 843 bytes result sent to driver
15/08/21 23:14:05 INFO Executor: Finished task 36.0 in stage 4.0 (TID 284). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 303, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,396
15/08/21 23:14:05 INFO Executor: Running task 55.0 in stage 4.0 (TID 303)
15/08/21 23:14:05 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 304, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 56.0 in stage 4.0 (TID 304)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 282) in 1073 ms on localhost (40/200)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 284) in 1061 ms on localhost (41/200)
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 49,751B for [ps_partkey] INT32: 14,956 values, 59,832B raw, 49,712B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 76,847B for [part_value] DOUBLE: 14,956 values, 119,656B raw, 76,800B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000039
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000039_0: Committed
15/08/21 23:14:05 INFO Executor: Finished task 39.0 in stage 4.0 (TID 287). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 305, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 57.0 in stage 4.0 (TID 305)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 287) in 939 ms on localhost (42/200)
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,116
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,020
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,280
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 49,962B for [ps_partkey] INT32: 14,992 values, 59,976B raw, 49,923B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 77,067B for [part_value] DOUBLE: 14,992 values, 119,944B raw, 77,020B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 50,882B for [ps_partkey] INT32: 15,238 values, 60,960B raw, 50,843B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000049
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 78,305B for [part_value] DOUBLE: 15,238 values, 121,912B raw, 78,258B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000049_0: Committed
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 51,758B for [ps_partkey] INT32: 15,501 values, 62,012B raw, 51,719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 79,721B for [part_value] DOUBLE: 15,501 values, 124,016B raw, 79,674B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO Executor: Finished task 49.0 in stage 4.0 (TID 297). 843 bytes result sent to driver
15/08/21 23:14:05 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 306, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 58.0 in stage 4.0 (TID 306)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 297) in 398 ms on localhost (43/200)
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,508
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 50,331B for [ps_partkey] INT32: 15,062 values, 60,256B raw, 50,292B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO ColumnChunkPageWriteStore: written 77,320B for [part_value] DOUBLE: 15,062 values, 120,504B raw, 77,273B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000051
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000051_0: Committed
15/08/21 23:14:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000052
15/08/21 23:14:05 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000052_0: Committed
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO Executor: Finished task 51.0 in stage 4.0 (TID 299). 843 bytes result sent to driver
15/08/21 23:14:05 INFO Executor: Finished task 52.0 in stage 4.0 (TID 300). 843 bytes result sent to driver
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 307, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 59.0 in stage 4.0 (TID 307)
15/08/21 23:14:05 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 308, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:05 INFO Executor: Running task 60.0 in stage 4.0 (TID 308)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 299) in 396 ms on localhost (44/200)
15/08/21 23:14:05 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 300) in 382 ms on localhost (45/200)
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:05 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:05 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,516
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 48,826B for [ps_partkey] INT32: 14,612 values, 58,456B raw, 48,787B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 75,159B for [part_value] DOUBLE: 14,612 values, 116,904B raw, 75,112B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,504
15/08/21 23:14:06 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:40586 in memory (size: 6.6 KB, free: 20.7 GB)
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,716
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,296
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,137B for [ps_partkey] INT32: 15,013 values, 60,060B raw, 50,098B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,255B for [part_value] DOUBLE: 15,013 values, 120,112B raw, 77,208B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000047
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000047_0: Committed
15/08/21 23:14:06 INFO Executor: Finished task 47.0 in stage 4.0 (TID 295). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 309, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 61.0 in stage 4.0 (TID 309)
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 49,109B for [ps_partkey] INT32: 14,772 values, 59,096B raw, 49,070B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 75,834B for [part_value] DOUBLE: 14,772 values, 118,184B raw, 75,787B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 295) in 1319 ms on localhost (46/200)
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000044
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000044_0: Committed
15/08/21 23:14:06 INFO Executor: Finished task 44.0 in stage 4.0 (TID 292). 843 bytes result sent to driver
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000046
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000046_0: Committed
15/08/21 23:14:06 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 310, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 62.0 in stage 4.0 (TID 310)
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,523B for [ps_partkey] INT32: 15,101 values, 60,412B raw, 50,484B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,680B for [part_value] DOUBLE: 15,101 values, 120,816B raw, 77,633B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO Executor: Finished task 46.0 in stage 4.0 (TID 294). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 292) in 1366 ms on localhost (47/200)
15/08/21 23:14:06 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 311, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 63.0 in stage 4.0 (TID 311)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 294) in 1333 ms on localhost (48/200)
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000050
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000042
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000048
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000042_0: Committed
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000048_0: Committed
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000045
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000045_0: Committed
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000050_0: Committed
15/08/21 23:14:06 INFO Executor: Finished task 42.0 in stage 4.0 (TID 290). 843 bytes result sent to driver
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000053
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000053_0: Committed
15/08/21 23:14:06 INFO Executor: Finished task 50.0 in stage 4.0 (TID 298). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 312, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000043
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000043_0: Committed
15/08/21 23:14:06 INFO Executor: Running task 64.0 in stage 4.0 (TID 312)
15/08/21 23:14:06 INFO Executor: Finished task 45.0 in stage 4.0 (TID 293). 843 bytes result sent to driver
15/08/21 23:14:06 INFO Executor: Finished task 48.0 in stage 4.0 (TID 296). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 313, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Finished task 53.0 in stage 4.0 (TID 301). 843 bytes result sent to driver
15/08/21 23:14:06 INFO Executor: Running task 65.0 in stage 4.0 (TID 313)
15/08/21 23:14:06 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 314, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 290) in 1474 ms on localhost (49/200)
15/08/21 23:14:06 INFO Executor: Finished task 43.0 in stage 4.0 (TID 291). 843 bytes result sent to driver
15/08/21 23:14:06 INFO Executor: Running task 66.0 in stage 4.0 (TID 314)
15/08/21 23:14:06 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 315, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 298) in 1079 ms on localhost (50/200)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 293) in 1380 ms on localhost (51/200)
15/08/21 23:14:06 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 316, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 68.0 in stage 4.0 (TID 316)
15/08/21 23:14:06 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 317, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 67.0 in stage 4.0 (TID 315)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 296) in 1334 ms on localhost (52/200)
15/08/21 23:14:06 INFO Executor: Running task 69.0 in stage 4.0 (TID 317)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 301) in 965 ms on localhost (53/200)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 291) in 1419 ms on localhost (54/200)
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,088
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000055
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000055_0: Committed
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:06 INFO Executor: Finished task 55.0 in stage 4.0 (TID 303). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 318, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 70.0 in stage 4.0 (TID 318)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 303) in 938 ms on localhost (55/200)
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000054
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000054_0: Committed
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:06 INFO Executor: Finished task 54.0 in stage 4.0 (TID 302). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 319, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 71.0 in stage 4.0 (TID 319)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 302) in 1029 ms on localhost (56/200)
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,476B for [ps_partkey] INT32: 15,141 values, 60,572B raw, 50,437B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,823B for [part_value] DOUBLE: 15,141 values, 121,136B raw, 77,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000056
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000056_0: Committed
15/08/21 23:14:06 INFO Executor: Finished task 56.0 in stage 4.0 (TID 304). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 320, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO Executor: Running task 72.0 in stage 4.0 (TID 320)
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 304) in 980 ms on localhost (57/200)
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,356
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 49,650B for [ps_partkey] INT32: 14,854 values, 59,424B raw, 49,611B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,372
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 76,442B for [part_value] DOUBLE: 14,854 values, 118,840B raw, 76,395B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 51,124B for [ps_partkey] INT32: 15,306 values, 61,232B raw, 51,085B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 78,692B for [part_value] DOUBLE: 15,306 values, 122,456B raw, 78,645B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000059
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000059_0: Committed
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO Executor: Finished task 59.0 in stage 4.0 (TID 307). 843 bytes result sent to driver
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 321, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO Executor: Running task 73.0 in stage 4.0 (TID 321)
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 307) in 889 ms on localhost (58/200)
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,656
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,048
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000060
15/08/21 23:14:06 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000060_0: Committed
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,691B for [ps_partkey] INT32: 15,189 values, 60,764B raw, 50,652B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 49,530B for [ps_partkey] INT32: 14,819 values, 59,284B raw, 49,491B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 78,111B for [part_value] DOUBLE: 15,189 values, 121,520B raw, 78,064B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 76,342B for [part_value] DOUBLE: 14,819 values, 118,560B raw, 76,295B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO Executor: Finished task 60.0 in stage 4.0 (TID 308). 843 bytes result sent to driver
15/08/21 23:14:06 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 322, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:06 INFO Executor: Running task 74.0 in stage 4.0 (TID 322)
15/08/21 23:14:06 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 308) in 978 ms on localhost (59/200)
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,196
15/08/21 23:14:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,088
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,556
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,960
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,528
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,040
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,908
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 49,476B for [ps_partkey] INT32: 14,896 values, 59,592B raw, 49,437B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 76,602B for [part_value] DOUBLE: 14,896 values, 119,176B raw, 76,555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,636
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,708
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,100B for [ps_partkey] INT32: 14,991 values, 59,972B raw, 50,061B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 49,955B for [ps_partkey] INT32: 14,985 values, 59,948B raw, 49,916B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 49,851B for [ps_partkey] INT32: 14,914 values, 59,664B raw, 49,812B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,033B for [part_value] DOUBLE: 14,985 values, 119,888B raw, 76,986B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 76,734B for [part_value] DOUBLE: 14,914 values, 119,320B raw, 76,687B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,153B for [part_value] DOUBLE: 14,991 values, 119,936B raw, 77,106B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,219B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,180B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,034B for [ps_partkey] INT32: 14,982 values, 59,936B raw, 49,995B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 48,837B for [ps_partkey] INT32: 14,613 values, 58,460B raw, 48,798B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,415B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,368B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,620
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,103B for [part_value] DOUBLE: 14,982 values, 119,864B raw, 77,056B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 75,206B for [part_value] DOUBLE: 14,613 values, 116,912B raw, 75,159B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 51,296B for [ps_partkey] INT32: 15,368 values, 61,480B raw, 51,257B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 79,001B for [part_value] DOUBLE: 15,368 values, 122,952B raw, 78,954B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 51,007B for [ps_partkey] INT32: 15,272 values, 61,096B raw, 50,968B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 78,724B for [part_value] DOUBLE: 15,272 values, 122,184B raw, 78,677B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 50,207B for [ps_partkey] INT32: 15,018 values, 60,080B raw, 50,168B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ColumnChunkPageWriteStore: written 77,277B for [part_value] DOUBLE: 15,018 values, 120,152B raw, 77,230B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:06 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:06 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000065
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000065_0: Committed
15/08/21 23:14:07 INFO Executor: Finished task 65.0 in stage 4.0 (TID 313). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 323, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 75.0 in stage 4.0 (TID 323)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 313) in 1508 ms on localhost (60/200)
15/08/21 23:14:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,892
15/08/21 23:14:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,636
15/08/21 23:14:07 INFO ColumnChunkPageWriteStore: written 48,889B for [ps_partkey] INT32: 14,718 values, 58,880B raw, 48,850B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:07 INFO ColumnChunkPageWriteStore: written 75,723B for [part_value] DOUBLE: 14,718 values, 117,752B raw, 75,676B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:07 INFO ColumnChunkPageWriteStore: written 50,888B for [ps_partkey] INT32: 15,282 values, 61,136B raw, 50,849B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ColumnChunkPageWriteStore: written 78,626B for [part_value] DOUBLE: 15,282 values, 122,264B raw, 78,579B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000057
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000057_0: Committed
15/08/21 23:14:07 INFO Executor: Finished task 57.0 in stage 4.0 (TID 305). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 324, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 76.0 in stage 4.0 (TID 324)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 305) in 2402 ms on localhost (61/200)
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000066
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000066_0: Committed
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000062
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000061
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000062_0: Committed
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000061_0: Committed
15/08/21 23:14:07 INFO Executor: Finished task 66.0 in stage 4.0 (TID 314). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 325, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 77.0 in stage 4.0 (TID 325)
15/08/21 23:14:07 INFO Executor: Finished task 61.0 in stage 4.0 (TID 309). 843 bytes result sent to driver
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000068
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000068_0: Committed
15/08/21 23:14:07 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 326, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 78.0 in stage 4.0 (TID 326)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 314) in 1585 ms on localhost (62/200)
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000058
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000058_0: Committed
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000067
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000067_0: Committed
15/08/21 23:14:07 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 309) in 1614 ms on localhost (63/200)
15/08/21 23:14:07 INFO Executor: Finished task 68.0 in stage 4.0 (TID 316). 843 bytes result sent to driver
15/08/21 23:14:07 INFO Executor: Finished task 58.0 in stage 4.0 (TID 306). 843 bytes result sent to driver
15/08/21 23:14:07 INFO Executor: Finished task 67.0 in stage 4.0 (TID 315). 843 bytes result sent to driver
15/08/21 23:14:07 INFO Executor: Finished task 62.0 in stage 4.0 (TID 310). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 327, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 79.0 in stage 4.0 (TID 327)
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000069
15/08/21 23:14:07 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 328, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000069_0: Committed
15/08/21 23:14:07 INFO Executor: Running task 80.0 in stage 4.0 (TID 328)
15/08/21 23:14:07 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 329, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 81.0 in stage 4.0 (TID 329)
15/08/21 23:14:07 INFO Executor: Finished task 69.0 in stage 4.0 (TID 317). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 330, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 82.0 in stage 4.0 (TID 330)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 316) in 1589 ms on localhost (64/200)
15/08/21 23:14:07 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 331, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 306) in 2347 ms on localhost (65/200)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 315) in 1592 ms on localhost (66/200)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 310) in 1611 ms on localhost (67/200)
15/08/21 23:14:07 INFO Executor: Running task 83.0 in stage 4.0 (TID 331)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 317) in 1593 ms on localhost (68/200)
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000064
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000064_0: Committed
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000071
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000071_0: Committed
15/08/21 23:14:07 INFO Executor: Finished task 64.0 in stage 4.0 (TID 312). 843 bytes result sent to driver
15/08/21 23:14:07 INFO Executor: Finished task 71.0 in stage 4.0 (TID 319). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 332, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 84.0 in stage 4.0 (TID 332)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 319) in 1534 ms on localhost (69/200)
15/08/21 23:14:07 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 333, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 85.0 in stage 4.0 (TID 333)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 312) in 1611 ms on localhost (70/200)
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000063
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000063_0: Committed
15/08/21 23:14:07 INFO Executor: Finished task 63.0 in stage 4.0 (TID 311). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 334, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 86.0 in stage 4.0 (TID 334)
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:07 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 311) in 1647 ms on localhost (71/200)
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000074
15/08/21 23:14:07 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000074_0: Committed
15/08/21 23:14:07 INFO Executor: Finished task 74.0 in stage 4.0 (TID 322). 843 bytes result sent to driver
15/08/21 23:14:07 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 335, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:07 INFO Executor: Running task 87.0 in stage 4.0 (TID 335)
15/08/21 23:14:07 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 322) in 1472 ms on localhost (72/200)
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:07 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:07 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,736
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,072
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,616
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,676
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 49,878B for [ps_partkey] INT32: 14,923 values, 59,700B raw, 49,839B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,116
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,771,796
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 76,795B for [part_value] DOUBLE: 14,923 values, 119,392B raw, 76,748B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,548
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,755B for [ps_partkey] INT32: 15,191 values, 60,772B raw, 50,716B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 78,053B for [part_value] DOUBLE: 15,191 values, 121,536B raw, 78,006B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,428
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,296
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,002B for [ps_partkey] INT32: 14,967 values, 59,876B raw, 49,963B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 77,120B for [part_value] DOUBLE: 14,967 values, 119,744B raw, 77,073B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 51,686B for [ps_partkey] INT32: 15,470 values, 61,888B raw, 51,647B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 52,486B for [ps_partkey] INT32: 15,726 values, 62,912B raw, 52,447B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,270B for [ps_partkey] INT32: 15,142 values, 60,576B raw, 50,231B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 80,849B for [part_value] DOUBLE: 15,726 values, 125,816B raw, 80,802B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 79,465B for [part_value] DOUBLE: 15,470 values, 123,768B raw, 79,418B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 77,924B for [part_value] DOUBLE: 15,142 values, 121,144B raw, 77,877B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 49,611B for [ps_partkey] INT32: 14,864 values, 59,464B raw, 49,572B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 76,384B for [part_value] DOUBLE: 14,864 values, 118,920B raw, 76,337B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,082B for [ps_partkey] INT32: 15,001 values, 60,012B raw, 50,043B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 77,129B for [part_value] DOUBLE: 15,001 values, 120,016B raw, 77,082B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,036
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 51,142B for [ps_partkey] INT32: 15,358 values, 61,440B raw, 51,103B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 78,917B for [part_value] DOUBLE: 15,358 values, 122,872B raw, 78,870B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,056
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,088
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,216B for [ps_partkey] INT32: 15,038 values, 60,160B raw, 50,177B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 77,257B for [part_value] DOUBLE: 15,038 values, 120,312B raw, 77,210B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 49,928B for [ps_partkey] INT32: 14,939 values, 59,764B raw, 49,889B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 49,730B for [ps_partkey] INT32: 14,891 values, 59,572B raw, 49,691B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 76,865B for [part_value] DOUBLE: 14,939 values, 119,520B raw, 76,818B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 76,638B for [part_value] DOUBLE: 14,891 values, 119,136B raw, 76,591B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000072
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000072_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 72.0 in stage 4.0 (TID 320). 843 bytes result sent to driver
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000070
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000070_0: Committed
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,188
15/08/21 23:14:08 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 336, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 88.0 in stage 4.0 (TID 336)
15/08/21 23:14:08 INFO Executor: Finished task 70.0 in stage 4.0 (TID 318). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 337, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 320) in 2006 ms on localhost (73/200)
15/08/21 23:14:08 INFO Executor: Running task 89.0 in stage 4.0 (TID 337)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 318) in 2051 ms on localhost (74/200)
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 51,121B for [ps_partkey] INT32: 15,296 values, 61,192B raw, 51,082B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 78,667B for [part_value] DOUBLE: 15,296 values, 122,376B raw, 78,620B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000073
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000073_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 73.0 in stage 4.0 (TID 321). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 338, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 90.0 in stage 4.0 (TID 338)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 321) in 2059 ms on localhost (75/200)
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000080
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000080_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 80.0 in stage 4.0 (TID 328). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 339, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 91.0 in stage 4.0 (TID 339)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 328) in 729 ms on localhost (76/200)
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000085
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000085_0: Committed
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,536
15/08/21 23:14:08 INFO Executor: Finished task 85.0 in stage 4.0 (TID 333). 843 bytes result sent to driver
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,316
15/08/21 23:14:08 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 340, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 92.0 in stage 4.0 (TID 340)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 333) in 763 ms on localhost (77/200)
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 51,622B for [ps_partkey] INT32: 15,463 values, 61,860B raw, 51,583B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 79,556B for [part_value] DOUBLE: 15,463 values, 123,712B raw, 79,509B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,836B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 50,797B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 78,688B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,641B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,548
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 52,016B for [ps_partkey] INT32: 15,614 values, 62,464B raw, 51,977B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 80,268B for [part_value] DOUBLE: 15,614 values, 124,920B raw, 80,221B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000089
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000089_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 89.0 in stage 4.0 (TID 337). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 341, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 93.0 in stage 4.0 (TID 341)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 337) in 462 ms on localhost (78/200)
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,356
15/08/21 23:14:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,360
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,413B for [ps_partkey] INT32: 15,104 values, 60,424B raw, 50,374B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 50,969B for [ps_partkey] INT32: 15,255 values, 61,028B raw, 50,930B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 77,613B for [part_value] DOUBLE: 15,104 values, 120,840B raw, 77,566B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO ColumnChunkPageWriteStore: written 78,402B for [part_value] DOUBLE: 15,255 values, 122,048B raw, 78,355B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000088
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000088_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 88.0 in stage 4.0 (TID 336). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 342, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 94.0 in stage 4.0 (TID 342)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 336) in 530 ms on localhost (79/200)
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000090
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000090_0: Committed
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:08 INFO Executor: Finished task 90.0 in stage 4.0 (TID 338). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 343, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 95.0 in stage 4.0 (TID 343)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 338) in 412 ms on localhost (80/200)
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000083
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000083_0: Committed
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000092
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000092_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 83.0 in stage 4.0 (TID 331). 843 bytes result sent to driver
15/08/21 23:14:08 INFO Executor: Finished task 92.0 in stage 4.0 (TID 340). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 344, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 96.0 in stage 4.0 (TID 344)
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:08 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 345, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 97.0 in stage 4.0 (TID 345)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 331) in 1125 ms on localhost (81/200)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 340) in 350 ms on localhost (82/200)
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000086
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000086_0: Committed
15/08/21 23:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000079
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000082
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000079_0: Committed
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000082_0: Committed
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000081
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000087
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000081_0: Committed
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000075
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000087_0: Committed
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000075_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 86.0 in stage 4.0 (TID 334). 843 bytes result sent to driver
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000084
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000084_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 79.0 in stage 4.0 (TID 327). 843 bytes result sent to driver
15/08/21 23:14:08 INFO Executor: Finished task 82.0 in stage 4.0 (TID 330). 843 bytes result sent to driver
15/08/21 23:14:08 INFO Executor: Finished task 81.0 in stage 4.0 (TID 329). 843 bytes result sent to driver
15/08/21 23:14:08 INFO Executor: Finished task 75.0 in stage 4.0 (TID 323). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 346, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 347, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 99.0 in stage 4.0 (TID 347)
15/08/21 23:14:08 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 348, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 98.0 in stage 4.0 (TID 346)
15/08/21 23:14:08 INFO Executor: Finished task 87.0 in stage 4.0 (TID 335). 843 bytes result sent to driver
15/08/21 23:14:08 INFO Executor: Finished task 84.0 in stage 4.0 (TID 332). 843 bytes result sent to driver
15/08/21 23:14:08 INFO Executor: Running task 100.0 in stage 4.0 (TID 348)
15/08/21 23:14:08 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 349, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 101.0 in stage 4.0 (TID 349)
15/08/21 23:14:08 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 350, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 327) in 1217 ms on localhost (83/200)
15/08/21 23:14:08 INFO Executor: Running task 102.0 in stage 4.0 (TID 350)
15/08/21 23:14:08 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 351, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 103.0 in stage 4.0 (TID 351)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 330) in 1218 ms on localhost (84/200)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 329) in 1220 ms on localhost (85/200)
15/08/21 23:14:08 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 352, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 334) in 1188 ms on localhost (86/200)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 335) in 1046 ms on localhost (87/200)
15/08/21 23:14:08 INFO Executor: Running task 104.0 in stage 4.0 (TID 352)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 323) in 1308 ms on localhost (88/200)
15/08/21 23:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:08 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 332) in 1213 ms on localhost (89/200)
15/08/21 23:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000076
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000076_0: Committed
15/08/21 23:14:08 INFO Executor: Finished task 76.0 in stage 4.0 (TID 324). 843 bytes result sent to driver
15/08/21 23:14:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000077
15/08/21 23:14:08 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000077_0: Committed
15/08/21 23:14:08 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 353, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 105.0 in stage 4.0 (TID 353)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 324) in 1262 ms on localhost (90/200)
15/08/21 23:14:08 INFO Executor: Finished task 77.0 in stage 4.0 (TID 325). 843 bytes result sent to driver
15/08/21 23:14:08 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 354, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:08 INFO Executor: Running task 106.0 in stage 4.0 (TID 354)
15/08/21 23:14:08 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 325) in 1254 ms on localhost (91/200)
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000091
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000091_0: Committed
15/08/21 23:14:09 INFO Executor: Finished task 91.0 in stage 4.0 (TID 339). 843 bytes result sent to driver
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,268
15/08/21 23:14:09 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 355, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 107.0 in stage 4.0 (TID 355)
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000078
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000078_0: Committed
15/08/21 23:14:09 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 339) in 540 ms on localhost (92/200)
15/08/21 23:14:09 INFO Executor: Finished task 78.0 in stage 4.0 (TID 326). 843 bytes result sent to driver
15/08/21 23:14:09 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 356, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 108.0 in stage 4.0 (TID 356)
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 326) in 1283 ms on localhost (93/200)
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 51,764B for [ps_partkey] INT32: 15,500 values, 62,008B raw, 51,725B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 79,825B for [part_value] DOUBLE: 15,500 values, 124,008B raw, 79,778B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,773,160
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 52,751B for [ps_partkey] INT32: 15,795 values, 63,188B raw, 52,712B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 81,317B for [part_value] DOUBLE: 15,795 values, 126,368B raw, 81,270B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,588
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000093
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000093_0: Committed
15/08/21 23:14:09 INFO Executor: Finished task 93.0 in stage 4.0 (TID 341). 843 bytes result sent to driver
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,296
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 357, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 109.0 in stage 4.0 (TID 357)
15/08/21 23:14:09 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 341) in 512 ms on localhost (94/200)
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 51,665B for [ps_partkey] INT32: 15,566 values, 62,272B raw, 51,626B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 79,967B for [part_value] DOUBLE: 15,566 values, 124,536B raw, 79,920B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,328
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,323B for [ps_partkey] INT32: 14,751 values, 59,012B raw, 49,284B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 75,816B for [part_value] DOUBLE: 14,751 values, 118,016B raw, 75,769B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 50,466B for [ps_partkey] INT32: 15,103 values, 60,420B raw, 50,427B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 77,626B for [part_value] DOUBLE: 15,103 values, 120,832B raw, 77,579B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,188
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,648
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,236
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,748
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 50,436B for [ps_partkey] INT32: 15,096 values, 60,392B raw, 50,397B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 77,569B for [part_value] DOUBLE: 15,096 values, 120,776B raw, 77,522B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 48,714B for [ps_partkey] INT32: 14,569 values, 58,284B raw, 48,675B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,492B for [ps_partkey] INT32: 14,848 values, 59,400B raw, 49,453B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 75,126B for [part_value] DOUBLE: 14,569 values, 116,560B raw, 75,079B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 76,418B for [part_value] DOUBLE: 14,848 values, 118,792B raw, 76,371B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,711B for [ps_partkey] INT32: 14,874 values, 59,504B raw, 49,672B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 76,490B for [part_value] DOUBLE: 14,874 values, 119,000B raw, 76,443B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,076
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,888
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,348
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,836
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,436
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,096
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,028
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 50,464B for [ps_partkey] INT32: 15,090 values, 60,368B raw, 50,425B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 77,675B for [part_value] DOUBLE: 15,090 values, 120,728B raw, 77,628B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 50,561B for [ps_partkey] INT32: 15,131 values, 60,532B raw, 50,522B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 77,778B for [part_value] DOUBLE: 15,131 values, 121,056B raw, 77,731B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 51,460B for [ps_partkey] INT32: 15,404 values, 61,624B raw, 51,421B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 79,159B for [part_value] DOUBLE: 15,404 values, 123,240B raw, 79,112B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 48,414B for [ps_partkey] INT32: 14,508 values, 58,040B raw, 48,375B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,681B for [ps_partkey] INT32: 14,878 values, 59,520B raw, 49,642B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 74,748B for [part_value] DOUBLE: 14,508 values, 116,072B raw, 74,701B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 76,614B for [part_value] DOUBLE: 14,878 values, 119,032B raw, 76,567B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,813B for [ps_partkey] INT32: 14,991 values, 59,972B raw, 49,774B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 77,080B for [part_value] DOUBLE: 14,991 values, 119,936B raw, 77,033B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,599B for [ps_partkey] INT32: 14,838 values, 59,360B raw, 49,560B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 76,341B for [part_value] DOUBLE: 14,838 values, 118,712B raw, 76,294B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000103
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000103_0: Committed
15/08/21 23:14:09 INFO Executor: Finished task 103.0 in stage 4.0 (TID 351). 843 bytes result sent to driver
15/08/21 23:14:09 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 358, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 110.0 in stage 4.0 (TID 358)
15/08/21 23:14:09 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 351) in 582 ms on localhost (95/200)
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,756
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,529B for [ps_partkey] INT32: 14,824 values, 59,304B raw, 49,490B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 76,382B for [part_value] DOUBLE: 14,824 values, 118,600B raw, 76,335B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000100
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000100_0: Committed
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000108
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000108_0: Committed
15/08/21 23:14:09 INFO Executor: Finished task 100.0 in stage 4.0 (TID 348). 843 bytes result sent to driver
15/08/21 23:14:09 INFO Executor: Finished task 108.0 in stage 4.0 (TID 356). 843 bytes result sent to driver
15/08/21 23:14:09 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 359, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 111.0 in stage 4.0 (TID 359)
15/08/21 23:14:09 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 360, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 112.0 in stage 4.0 (TID 360)
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000098
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000098_0: Committed
15/08/21 23:14:09 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 348) in 669 ms on localhost (96/200)
15/08/21 23:14:09 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 356) in 618 ms on localhost (97/200)
15/08/21 23:14:09 INFO Executor: Finished task 98.0 in stage 4.0 (TID 346). 843 bytes result sent to driver
15/08/21 23:14:09 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 361, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 113.0 in stage 4.0 (TID 361)
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000105
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000105_0: Committed
15/08/21 23:14:09 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 346) in 674 ms on localhost (98/200)
15/08/21 23:14:09 INFO Executor: Finished task 105.0 in stage 4.0 (TID 353). 843 bytes result sent to driver
15/08/21 23:14:09 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 362, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 114.0 in stage 4.0 (TID 362)
15/08/21 23:14:09 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 353) in 659 ms on localhost (99/200)
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000094
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000094_0: Committed
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO Executor: Finished task 94.0 in stage 4.0 (TID 342). 843 bytes result sent to driver
15/08/21 23:14:09 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 363, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 115.0 in stage 4.0 (TID 363)
15/08/21 23:14:09 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 342) in 908 ms on localhost (100/200)
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,749,200
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 48,805B for [ps_partkey] INT32: 14,597 values, 58,396B raw, 48,766B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000097
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000097_0: Committed
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 75,162B for [part_value] DOUBLE: 14,597 values, 116,784B raw, 75,115B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO Executor: Finished task 97.0 in stage 4.0 (TID 345). 843 bytes result sent to driver
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 364, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO Executor: Running task 116.0 in stage 4.0 (TID 364)
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 345) in 940 ms on localhost (101/200)
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000110
15/08/21 23:14:09 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000110_0: Committed
15/08/21 23:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:09 INFO Executor: Finished task 110.0 in stage 4.0 (TID 358). 843 bytes result sent to driver
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:09 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 365, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:09 INFO Executor: Running task 117.0 in stage 4.0 (TID 365)
15/08/21 23:14:09 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 358) in 355 ms on localhost (102/200)
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,860
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,116
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,744,856
15/08/21 23:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,268
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,133B for [ps_partkey] INT32: 14,780 values, 59,128B raw, 49,094B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 76,143B for [part_value] DOUBLE: 14,780 values, 118,248B raw, 76,096B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 47,957B for [ps_partkey] INT32: 14,379 values, 57,524B raw, 47,918B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 73,881B for [part_value] DOUBLE: 14,379 values, 115,040B raw, 73,834B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,965B for [ps_partkey] INT32: 14,942 values, 59,776B raw, 49,926B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 49,305B for [ps_partkey] INT32: 14,750 values, 59,008B raw, 49,266B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 76,683B for [part_value] DOUBLE: 14,942 values, 119,544B raw, 76,636B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO ColumnChunkPageWriteStore: written 75,859B for [part_value] DOUBLE: 14,750 values, 118,008B raw, 75,812B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,956
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000099
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000099_0: Committed
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO Executor: Finished task 99.0 in stage 4.0 (TID 347). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 366, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 118.0 in stage 4.0 (TID 366)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 347) in 1099 ms on localhost (103/200)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000106
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000106_0: Committed
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,902B for [ps_partkey] INT32: 14,934 values, 59,744B raw, 49,863B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,798B for [part_value] DOUBLE: 14,934 values, 119,480B raw, 76,751B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000095
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000095_0: Committed
15/08/21 23:14:10 INFO Executor: Finished task 106.0 in stage 4.0 (TID 354). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 367, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Finished task 95.0 in stage 4.0 (TID 343). 843 bytes result sent to driver
15/08/21 23:14:10 INFO Executor: Running task 119.0 in stage 4.0 (TID 367)
15/08/21 23:14:10 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 368, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 120.0 in stage 4.0 (TID 368)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 354) in 1102 ms on localhost (104/200)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 343) in 1259 ms on localhost (105/200)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000101
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000101_0: Committed
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO Executor: Finished task 101.0 in stage 4.0 (TID 349). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 369, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 121.0 in stage 4.0 (TID 369)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 349) in 1151 ms on localhost (106/200)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000096
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000096_0: Committed
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO Executor: Finished task 96.0 in stage 4.0 (TID 344). 843 bytes result sent to driver
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000104
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000104_0: Committed
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 370, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 122.0 in stage 4.0 (TID 370)
15/08/21 23:14:10 INFO Executor: Finished task 104.0 in stage 4.0 (TID 352). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 371, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 123.0 in stage 4.0 (TID 371)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 344) in 1263 ms on localhost (107/200)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000102
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000102_0: Committed
15/08/21 23:14:10 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 352) in 1169 ms on localhost (108/200)
15/08/21 23:14:10 INFO Executor: Finished task 102.0 in stage 4.0 (TID 350). 843 bytes result sent to driver
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 372, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO Executor: Running task 124.0 in stage 4.0 (TID 372)
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 350) in 1179 ms on localhost (109/200)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000107
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000114
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000107_0: Committed
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000114_0: Committed
15/08/21 23:14:10 INFO Executor: Finished task 114.0 in stage 4.0 (TID 362). 843 bytes result sent to driver
15/08/21 23:14:10 INFO Executor: Finished task 107.0 in stage 4.0 (TID 355). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 373, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 374, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 125.0 in stage 4.0 (TID 373)
15/08/21 23:14:10 INFO Executor: Running task 126.0 in stage 4.0 (TID 374)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 362) in 509 ms on localhost (110/200)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000109
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000109_0: Committed
15/08/21 23:14:10 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 355) in 1146 ms on localhost (111/200)
15/08/21 23:14:10 INFO Executor: Finished task 109.0 in stage 4.0 (TID 357). 843 bytes result sent to driver
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 375, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 357) in 923 ms on localhost (112/200)
15/08/21 23:14:10 INFO Executor: Running task 127.0 in stage 4.0 (TID 375)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000115
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000115_0: Committed
15/08/21 23:14:10 INFO Executor: Finished task 115.0 in stage 4.0 (TID 363). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 376, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 128.0 in stage 4.0 (TID 376)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 363) in 469 ms on localhost (113/200)
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,008
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,746,856
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,105B for [ps_partkey] INT32: 14,687 values, 58,756B raw, 49,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 75,641B for [part_value] DOUBLE: 14,687 values, 117,504B raw, 75,594B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 48,423B for [ps_partkey] INT32: 14,479 values, 57,924B raw, 48,384B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 74,557B for [part_value] DOUBLE: 14,479 values, 115,840B raw, 74,510B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,332
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,288
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,748,276
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,980B for [ps_partkey] INT32: 14,954 values, 59,824B raw, 49,941B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,859B for [part_value] DOUBLE: 14,954 values, 119,640B raw, 76,812B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 48,681B for [ps_partkey] INT32: 14,550 values, 58,208B raw, 48,642B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,971B for [ps_partkey] INT32: 14,951 values, 59,812B raw, 49,932B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 74,857B for [part_value] DOUBLE: 14,550 values, 116,408B raw, 74,810B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,964B for [part_value] DOUBLE: 14,951 values, 119,616B raw, 76,917B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,528
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000117
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000117_0: Committed
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO Executor: Finished task 117.0 in stage 4.0 (TID 365). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 377, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 129.0 in stage 4.0 (TID 377)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 365) in 636 ms on localhost (114/200)
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,746,052
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,801B for [ps_partkey] INT32: 14,913 values, 59,660B raw, 49,762B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,631B for [part_value] DOUBLE: 14,913 values, 119,312B raw, 76,584B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 48,143B for [ps_partkey] INT32: 14,440 values, 57,768B raw, 48,104B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 74,116B for [part_value] DOUBLE: 14,440 values, 115,528B raw, 74,069B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000111
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000111_0: Committed
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000112
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000112_0: Committed
15/08/21 23:14:10 INFO Executor: Finished task 111.0 in stage 4.0 (TID 359). 843 bytes result sent to driver
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,676
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,476
15/08/21 23:14:10 INFO Executor: Finished task 112.0 in stage 4.0 (TID 360). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 378, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 130.0 in stage 4.0 (TID 378)
15/08/21 23:14:10 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 379, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,896
15/08/21 23:14:10 INFO Executor: Running task 131.0 in stage 4.0 (TID 379)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 359) in 1010 ms on localhost (115/200)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 360) in 1009 ms on localhost (116/200)
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,368
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,416
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000113
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000113_0: Committed
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,300
15/08/21 23:14:10 INFO Executor: Finished task 113.0 in stage 4.0 (TID 361). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 380, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 132.0 in stage 4.0 (TID 380)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 361) in 1026 ms on localhost (117/200)
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,525B for [ps_partkey] INT32: 14,810 values, 59,248B raw, 49,486B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,193B for [part_value] DOUBLE: 14,810 values, 118,488B raw, 76,146B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 52,003B for [ps_partkey] INT32: 15,570 values, 62,288B raw, 51,964B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 80,039B for [part_value] DOUBLE: 15,570 values, 124,568B raw, 79,992B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 50,319B for [ps_partkey] INT32: 15,055 values, 60,228B raw, 50,280B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,574B for [ps_partkey] INT32: 14,831 values, 59,332B raw, 49,535B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 77,437B for [part_value] DOUBLE: 15,055 values, 120,448B raw, 77,390B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,286B for [part_value] DOUBLE: 14,831 values, 118,656B raw, 76,239B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 50,147B for [ps_partkey] INT32: 15,007 values, 60,036B raw, 50,108B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 77,232B for [part_value] DOUBLE: 15,007 values, 120,064B raw, 77,185B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,235B for [ps_partkey] INT32: 14,802 values, 59,216B raw, 49,196B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,170B for [part_value] DOUBLE: 14,802 values, 118,424B raw, 76,123B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000124
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000124_0: Committed
15/08/21 23:14:10 INFO Executor: Finished task 124.0 in stage 4.0 (TID 372). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 381, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 133.0 in stage 4.0 (TID 381)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 372) in 623 ms on localhost (118/200)
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,848
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000127
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000127_0: Committed
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000116
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000116_0: Committed
15/08/21 23:14:10 INFO Executor: Finished task 127.0 in stage 4.0 (TID 375). 843 bytes result sent to driver
15/08/21 23:14:10 INFO Executor: Finished task 116.0 in stage 4.0 (TID 364). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 382, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 134.0 in stage 4.0 (TID 382)
15/08/21 23:14:10 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 383, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 135.0 in stage 4.0 (TID 383)
15/08/21 23:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000125
15/08/21 23:14:10 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000125_0: Committed
15/08/21 23:14:10 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 375) in 713 ms on localhost (119/200)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 364) in 1061 ms on localhost (120/200)
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,502B for [ps_partkey] INT32: 14,879 values, 59,524B raw, 49,463B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 76,540B for [part_value] DOUBLE: 14,879 values, 119,040B raw, 76,493B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO Executor: Finished task 125.0 in stage 4.0 (TID 373). 843 bytes result sent to driver
15/08/21 23:14:10 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 384, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:10 INFO Executor: Running task 136.0 in stage 4.0 (TID 384)
15/08/21 23:14:10 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 373) in 743 ms on localhost (121/200)
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,088
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,747,300
15/08/21 23:14:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,136
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,270B for [ps_partkey] INT32: 14,741 values, 58,972B raw, 49,231B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 75,768B for [part_value] DOUBLE: 14,741 values, 117,936B raw, 75,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 48,404B for [ps_partkey] INT32: 14,502 values, 58,016B raw, 48,365B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 74,467B for [part_value] DOUBLE: 14,502 values, 116,024B raw, 74,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 49,112B for [ps_partkey] INT32: 14,693 values, 58,780B raw, 49,073B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO ColumnChunkPageWriteStore: written 75,560B for [part_value] DOUBLE: 14,693 values, 117,552B raw, 75,513B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000122
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000122_0: Committed
15/08/21 23:14:11 INFO Executor: Finished task 122.0 in stage 4.0 (TID 370). 843 bytes result sent to driver
15/08/21 23:14:11 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 385, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO Executor: Running task 137.0 in stage 4.0 (TID 385)
15/08/21 23:14:11 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 370) in 1037 ms on localhost (122/200)
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,856
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,136
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,468
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000119
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000119_0: Committed
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000123
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000123_0: Committed
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,216
15/08/21 23:14:11 INFO Executor: Finished task 119.0 in stage 4.0 (TID 367). 843 bytes result sent to driver
15/08/21 23:14:11 INFO Executor: Finished task 123.0 in stage 4.0 (TID 371). 843 bytes result sent to driver
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000120
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000120_0: Committed
15/08/21 23:14:11 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 386, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO Executor: Running task 138.0 in stage 4.0 (TID 386)
15/08/21 23:14:11 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 387, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO Executor: Running task 139.0 in stage 4.0 (TID 387)
15/08/21 23:14:11 INFO Executor: Finished task 120.0 in stage 4.0 (TID 368). 843 bytes result sent to driver
15/08/21 23:14:11 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 367) in 1092 ms on localhost (123/200)
15/08/21 23:14:11 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 388, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 371) in 1048 ms on localhost (124/200)
15/08/21 23:14:11 INFO Executor: Running task 140.0 in stage 4.0 (TID 388)
15/08/21 23:14:11 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 368) in 1092 ms on localhost (125/200)
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000118
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000118_0: Committed
15/08/21 23:14:11 INFO Executor: Finished task 118.0 in stage 4.0 (TID 366). 843 bytes result sent to driver
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 49,594B for [ps_partkey] INT32: 14,829 values, 59,324B raw, 49,555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 50,257B for [ps_partkey] INT32: 15,043 values, 60,180B raw, 50,218B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 76,110B for [part_value] DOUBLE: 14,829 values, 118,640B raw, 76,063B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 77,384B for [part_value] DOUBLE: 15,043 values, 120,352B raw, 77,337B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 389, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO Executor: Running task 141.0 in stage 4.0 (TID 389)
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 51,267B for [ps_partkey] INT32: 15,347 values, 61,396B raw, 51,228B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 78,878B for [part_value] DOUBLE: 15,347 values, 122,784B raw, 78,831B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 366) in 1154 ms on localhost (126/200)
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 50,105B for [ps_partkey] INT32: 15,010 values, 60,048B raw, 50,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 77,136B for [part_value] DOUBLE: 15,010 values, 120,088B raw, 77,089B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000129
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000129_0: Committed
15/08/21 23:14:11 INFO Executor: Finished task 129.0 in stage 4.0 (TID 377). 843 bytes result sent to driver
15/08/21 23:14:11 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 390, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO Executor: Running task 142.0 in stage 4.0 (TID 390)
15/08/21 23:14:11 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 377) in 745 ms on localhost (127/200)
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000128
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000128_0: Committed
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000121
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000121_0: Committed
15/08/21 23:14:11 INFO Executor: Finished task 121.0 in stage 4.0 (TID 369). 843 bytes result sent to driver
15/08/21 23:14:11 INFO Executor: Finished task 128.0 in stage 4.0 (TID 376). 843 bytes result sent to driver
15/08/21 23:14:11 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 391, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO Executor: Running task 143.0 in stage 4.0 (TID 391)
15/08/21 23:14:11 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 392, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO Executor: Running task 144.0 in stage 4.0 (TID 392)
15/08/21 23:14:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000126
15/08/21 23:14:11 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000126_0: Committed
15/08/21 23:14:11 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 369) in 1293 ms on localhost (128/200)
15/08/21 23:14:11 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 376) in 1238 ms on localhost (129/200)
15/08/21 23:14:11 INFO Executor: Finished task 126.0 in stage 4.0 (TID 374). 843 bytes result sent to driver
15/08/21 23:14:11 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 393, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO Executor: Running task 145.0 in stage 4.0 (TID 393)
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 374) in 1263 ms on localhost (130/200)
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,912
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,556
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,176
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,770,848
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 49,724B for [ps_partkey] INT32: 14,883 values, 59,540B raw, 49,685B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,480
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 76,569B for [part_value] DOUBLE: 14,883 values, 119,072B raw, 76,522B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 50,222B for [ps_partkey] INT32: 15,114 values, 60,464B raw, 50,183B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 49,514B for [ps_partkey] INT32: 14,845 values, 59,388B raw, 49,475B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 77,686B for [part_value] DOUBLE: 15,114 values, 120,920B raw, 77,639B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 76,410B for [part_value] DOUBLE: 14,845 values, 118,768B raw, 76,363B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 52,405B for [ps_partkey] INT32: 15,679 values, 62,724B raw, 52,366B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 80,646B for [part_value] DOUBLE: 15,679 values, 125,440B raw, 80,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 51,320B for [ps_partkey] INT32: 15,361 values, 61,452B raw, 51,281B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO ColumnChunkPageWriteStore: written 79,054B for [part_value] DOUBLE: 15,361 values, 122,896B raw, 79,007B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:11 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:11 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,908
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,168
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,750,816
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,028
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000131
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000131_0: Committed
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 51,925B for [ps_partkey] INT32: 15,532 values, 62,136B raw, 51,886B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 51,652B for [ps_partkey] INT32: 15,545 values, 62,188B raw, 51,613B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 80,023B for [part_value] DOUBLE: 15,532 values, 124,264B raw, 79,976B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 79,880B for [part_value] DOUBLE: 15,545 values, 124,368B raw, 79,833B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000130
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000130_0: Committed
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 49,094B for [ps_partkey] INT32: 14,677 values, 58,716B raw, 49,055B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000133
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000133_0: Committed
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000135
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000135_0: Committed
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 75,416B for [part_value] DOUBLE: 14,677 values, 117,424B raw, 75,369B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO Executor: Finished task 131.0 in stage 4.0 (TID 379). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 130.0 in stage 4.0 (TID 378). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 135.0 in stage 4.0 (TID 383). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 133.0 in stage 4.0 (TID 381). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 394, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 395, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 147.0 in stage 4.0 (TID 395)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 396, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 146.0 in stage 4.0 (TID 394)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 397, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 149.0 in stage 4.0 (TID 397)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 378) in 1419 ms on localhost (131/200)
15/08/21 23:14:12 INFO Executor: Running task 148.0 in stage 4.0 (TID 396)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 379) in 1418 ms on localhost (132/200)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 383) in 1179 ms on localhost (133/200)
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 49,437B for [ps_partkey] INT32: 14,788 values, 59,160B raw, 49,398B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 76,001B for [part_value] DOUBLE: 14,788 values, 118,312B raw, 75,954B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 381) in 1291 ms on localhost (134/200)
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000132
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000132_0: Committed
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000136
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000136_0: Committed
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000134
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000134_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 136.0 in stage 4.0 (TID 384). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 132.0 in stage 4.0 (TID 380). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 134.0 in stage 4.0 (TID 382). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 398, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 150.0 in stage 4.0 (TID 398)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 399, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 151.0 in stage 4.0 (TID 399)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 400, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 152.0 in stage 4.0 (TID 400)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 384) in 1209 ms on localhost (135/200)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 380) in 1437 ms on localhost (136/200)
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 382) in 1218 ms on localhost (137/200)
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000139
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000139_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 139.0 in stage 4.0 (TID 387). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 401, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 387) in 948 ms on localhost (138/200)
15/08/21 23:14:12 INFO Executor: Running task 153.0 in stage 4.0 (TID 401)
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000143
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000143_0: Committed
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000142
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000142_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 143.0 in stage 4.0 (TID 391). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 142.0 in stage 4.0 (TID 390). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 402, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 154.0 in stage 4.0 (TID 402)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 403, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 155.0 in stage 4.0 (TID 403)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 391) in 760 ms on localhost (139/200)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 390) in 890 ms on localhost (140/200)
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,308
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,755,548
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,736
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,456
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 51,096B for [ps_partkey] INT32: 15,302 values, 61,216B raw, 51,057B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 49,700B for [ps_partkey] INT32: 14,914 values, 59,664B raw, 49,661B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 78,697B for [part_value] DOUBLE: 15,302 values, 122,424B raw, 78,650B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 76,716B for [part_value] DOUBLE: 14,914 values, 119,320B raw, 76,669B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,520
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 50,196B for [ps_partkey] INT32: 15,023 values, 60,100B raw, 50,157B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 77,252B for [part_value] DOUBLE: 15,023 values, 120,192B raw, 77,205B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,996
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 49,504B for [ps_partkey] INT32: 14,809 values, 59,244B raw, 49,465B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 76,281B for [part_value] DOUBLE: 14,809 values, 118,480B raw, 76,234B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 50,311B for [ps_partkey] INT32: 15,063 values, 60,260B raw, 50,272B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 77,535B for [part_value] DOUBLE: 15,063 values, 120,512B raw, 77,488B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 52,190B for [ps_partkey] INT32: 15,636 values, 62,552B raw, 52,151B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 80,404B for [part_value] DOUBLE: 15,636 values, 125,096B raw, 80,357B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,448
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,936
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,756
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,448
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 51,023B for [ps_partkey] INT32: 15,359 values, 61,444B raw, 50,984B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 50,652B for [ps_partkey] INT32: 15,183 values, 60,740B raw, 50,613B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 78,933B for [part_value] DOUBLE: 15,359 values, 122,880B raw, 78,886B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 78,131B for [part_value] DOUBLE: 15,183 values, 121,472B raw, 78,084B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 51,005B for [ps_partkey] INT32: 15,274 values, 61,104B raw, 50,966B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 51,781B for [ps_partkey] INT32: 15,509 values, 62,044B raw, 51,742B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 78,604B for [part_value] DOUBLE: 15,274 values, 122,200B raw, 78,557B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO ColumnChunkPageWriteStore: written 79,704B for [part_value] DOUBLE: 15,509 values, 124,080B raw, 79,657B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000140
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000140_0: Committed
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000138
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000138_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 138.0 in stage 4.0 (TID 386). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 140.0 in stage 4.0 (TID 388). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 404, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000137
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000137_0: Committed
15/08/21 23:14:12 INFO Executor: Running task 156.0 in stage 4.0 (TID 404)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 405, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 157.0 in stage 4.0 (TID 405)
15/08/21 23:14:12 INFO Executor: Finished task 137.0 in stage 4.0 (TID 385). 843 bytes result sent to driver
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000141
15/08/21 23:14:12 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 386) in 1413 ms on localhost (141/200)
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000141_0: Committed
15/08/21 23:14:12 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 388) in 1411 ms on localhost (142/200)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 406, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 158.0 in stage 4.0 (TID 406)
15/08/21 23:14:12 INFO Executor: Finished task 141.0 in stage 4.0 (TID 389). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 407, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 159.0 in stage 4.0 (TID 407)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 385) in 1433 ms on localhost (143/200)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 389) in 1393 ms on localhost (144/200)
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000146
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000146_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 146.0 in stage 4.0 (TID 394). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 408, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 160.0 in stage 4.0 (TID 408)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 394) in 571 ms on localhost (145/200)
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000145
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000145_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 145.0 in stage 4.0 (TID 393). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 409, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 161.0 in stage 4.0 (TID 409)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 393) in 1258 ms on localhost (146/200)
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000144
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000144_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 144.0 in stage 4.0 (TID 392). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 410, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 162.0 in stage 4.0 (TID 410)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 392) in 1294 ms on localhost (147/200)
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000149
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000149_0: Committed
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000147
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000147_0: Committed
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,770,436
15/08/21 23:14:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:12 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:12 INFO Executor: Finished task 147.0 in stage 4.0 (TID 395). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 149.0 in stage 4.0 (TID 397). 843 bytes result sent to driver
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:12 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:12 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 411, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 163.0 in stage 4.0 (TID 411)
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,684
15/08/21 23:14:12 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 412, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 164.0 in stage 4.0 (TID 412)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 395) in 931 ms on localhost (148/200)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 397) in 931 ms on localhost (149/200)
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000152
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000152_0: Committed
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000150
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000150_0: Committed
15/08/21 23:14:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000151
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000151_0: Committed
15/08/21 23:14:12 INFO Executor: Finished task 150.0 in stage 4.0 (TID 398). 843 bytes result sent to driver
15/08/21 23:14:12 INFO Executor: Finished task 151.0 in stage 4.0 (TID 399). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 413, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,256
15/08/21 23:14:12 INFO Executor: Running task 165.0 in stage 4.0 (TID 413)
15/08/21 23:14:12 INFO Executor: Finished task 152.0 in stage 4.0 (TID 400). 843 bytes result sent to driver
15/08/21 23:14:12 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 414, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 166.0 in stage 4.0 (TID 414)
15/08/21 23:14:12 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 415, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:12 INFO Executor: Running task 167.0 in stage 4.0 (TID 415)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 398) in 905 ms on localhost (150/200)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 399) in 908 ms on localhost (151/200)
15/08/21 23:14:12 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 400) in 908 ms on localhost (152/200)
15/08/21 23:14:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,988
15/08/21 23:14:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000148
15/08/21 23:14:12 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000148_0: Committed
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 52,299B for [ps_partkey] INT32: 15,658 values, 62,640B raw, 52,260B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 80,491B for [part_value] DOUBLE: 15,658 values, 125,272B raw, 80,444B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO Executor: Finished task 148.0 in stage 4.0 (TID 396). 843 bytes result sent to driver
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,997B for [ps_partkey] INT32: 15,272 values, 61,096B raw, 50,958B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 416, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 78,449B for [part_value] DOUBLE: 15,272 values, 122,184B raw, 78,402B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO Executor: Running task 168.0 in stage 4.0 (TID 416)
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,766,756
15/08/21 23:14:13 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 396) in 967 ms on localhost (153/200)
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 51,914B for [ps_partkey] INT32: 15,549 values, 62,204B raw, 51,875B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 79,983B for [part_value] DOUBLE: 15,549 values, 124,400B raw, 79,936B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000153
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000154
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000153_0: Committed
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000154_0: Committed
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000155
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000155_0: Committed
15/08/21 23:14:13 INFO Executor: Finished task 153.0 in stage 4.0 (TID 401). 843 bytes result sent to driver
15/08/21 23:14:13 INFO Executor: Finished task 154.0 in stage 4.0 (TID 402). 843 bytes result sent to driver
15/08/21 23:14:13 INFO Executor: Finished task 155.0 in stage 4.0 (TID 403). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 417, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 418, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 170.0 in stage 4.0 (TID 418)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 402) in 885 ms on localhost (154/200)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 401) in 917 ms on localhost (155/200)
15/08/21 23:14:13 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 419, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 171.0 in stage 4.0 (TID 419)
15/08/21 23:14:13 INFO Executor: Running task 169.0 in stage 4.0 (TID 417)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 403) in 890 ms on localhost (156/200)
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 51,705B for [ps_partkey] INT32: 15,474 values, 61,904B raw, 51,666B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 79,616B for [part_value] DOUBLE: 15,474 values, 123,800B raw, 79,569B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,748
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 49,271B for [ps_partkey] INT32: 14,736 values, 58,952B raw, 49,232B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 75,793B for [part_value] DOUBLE: 14,736 values, 117,896B raw, 75,746B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000156
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000156_0: Committed
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,240
15/08/21 23:14:13 INFO Executor: Finished task 156.0 in stage 4.0 (TID 404). 843 bytes result sent to driver
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 420, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 172.0 in stage 4.0 (TID 420)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 404) in 493 ms on localhost (157/200)
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 51,721B for [ps_partkey] INT32: 15,524 values, 62,104B raw, 51,682B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 79,875B for [part_value] DOUBLE: 15,524 values, 124,200B raw, 79,828B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000157
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000157_0: Committed
15/08/21 23:14:13 INFO Executor: Finished task 157.0 in stage 4.0 (TID 405). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 421, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 173.0 in stage 4.0 (TID 421)
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 405) in 510 ms on localhost (158/200)
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000160
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000160_0: Committed
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:13 INFO Executor: Finished task 160.0 in stage 4.0 (TID 408). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 422, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 174.0 in stage 4.0 (TID 422)
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,149B for [ps_partkey] INT32: 15,099 values, 60,404B raw, 50,110B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000159
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000159_0: Committed
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 77,711B for [part_value] DOUBLE: 15,099 values, 120,800B raw, 77,664B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 408) in 509 ms on localhost (159/200)
15/08/21 23:14:13 INFO Executor: Finished task 159.0 in stage 4.0 (TID 407). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 423, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 175.0 in stage 4.0 (TID 423)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 407) in 534 ms on localhost (160/200)
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000162
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000162_0: Committed
15/08/21 23:14:13 INFO Executor: Finished task 162.0 in stage 4.0 (TID 410). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 424, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 176.0 in stage 4.0 (TID 424)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 410) in 482 ms on localhost (161/200)
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000161
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000161_0: Committed
15/08/21 23:14:13 INFO Executor: Finished task 161.0 in stage 4.0 (TID 409). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 425, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 177.0 in stage 4.0 (TID 425)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 409) in 532 ms on localhost (162/200)
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000158
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000158_0: Committed
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,751,888
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO Executor: Finished task 158.0 in stage 4.0 (TID 406). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 426, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 178.0 in stage 4.0 (TID 426)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 406) in 1170 ms on localhost (163/200)
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,788
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 49,221B for [ps_partkey] INT32: 14,731 values, 58,932B raw, 49,182B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 75,709B for [part_value] DOUBLE: 14,731 values, 117,856B raw, 75,662B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 49,558B for [ps_partkey] INT32: 14,826 values, 59,312B raw, 49,519B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 76,257B for [part_value] DOUBLE: 14,826 values, 118,616B raw, 76,210B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000165
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000165_0: Committed
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000164
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000164_0: Committed
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO Executor: Finished task 164.0 in stage 4.0 (TID 412). 843 bytes result sent to driver
15/08/21 23:14:13 INFO Executor: Finished task 165.0 in stage 4.0 (TID 413). 843 bytes result sent to driver
15/08/21 23:14:13 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 427, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 428, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO Executor: Running task 180.0 in stage 4.0 (TID 428)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 412) in 894 ms on localhost (164/200)
15/08/21 23:14:13 INFO Executor: Running task 179.0 in stage 4.0 (TID 427)
15/08/21 23:14:13 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 413) in 885 ms on localhost (165/200)
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,956
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,048
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 49,777B for [ps_partkey] INT32: 14,884 values, 59,544B raw, 49,738B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 76,597B for [part_value] DOUBLE: 14,884 values, 119,080B raw, 76,550B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:13 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:13 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,236
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,762,608
15/08/21 23:14:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,244
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,076B for [ps_partkey] INT32: 14,989 values, 59,964B raw, 50,037B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 77,123B for [part_value] DOUBLE: 14,989 values, 119,920B raw, 77,076B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,200
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,432
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,856
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,036
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,579B for [ps_partkey] INT32: 15,150 values, 60,608B raw, 50,540B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,438B for [ps_partkey] INT32: 15,098 values, 60,400B raw, 50,399B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 77,838B for [part_value] DOUBLE: 15,150 values, 121,208B raw, 77,791B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 77,641B for [part_value] DOUBLE: 15,098 values, 120,792B raw, 77,594B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,882B for [ps_partkey] INT32: 15,267 values, 61,076B raw, 50,843B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 78,518B for [part_value] DOUBLE: 15,267 values, 122,144B raw, 78,471B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,594B for [ps_partkey] INT32: 15,147 values, 60,596B raw, 50,555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 77,989B for [part_value] DOUBLE: 15,147 values, 121,184B raw, 77,942B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000167
15/08/21 23:14:13 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000167_0: Committed
15/08/21 23:14:13 INFO Executor: Finished task 167.0 in stage 4.0 (TID 415). 843 bytes result sent to driver
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 52,093B for [ps_partkey] INT32: 15,609 values, 62,444B raw, 52,054B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 429, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 80,224B for [part_value] DOUBLE: 15,609 values, 124,880B raw, 80,177B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 50,524B for [ps_partkey] INT32: 15,129 values, 60,524B raw, 50,485B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO Executor: Running task 181.0 in stage 4.0 (TID 429)
15/08/21 23:14:13 INFO ColumnChunkPageWriteStore: written 77,969B for [part_value] DOUBLE: 15,129 values, 121,040B raw, 77,922B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,758,056
15/08/21 23:14:13 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 415) in 1008 ms on localhost (166/200)
15/08/21 23:14:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,556
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,662B for [ps_partkey] INT32: 14,938 values, 59,760B raw, 49,623B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,717B for [part_value] DOUBLE: 14,938 values, 119,512B raw, 76,670B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000163
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000163_0: Committed
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,754,788
15/08/21 23:14:14 INFO Executor: Finished task 163.0 in stage 4.0 (TID 411). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 430, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 182.0 in stage 4.0 (TID 430)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000171
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000171_0: Committed
15/08/21 23:14:14 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 411) in 1045 ms on localhost (167/200)
15/08/21 23:14:14 INFO Executor: Finished task 171.0 in stage 4.0 (TID 419). 843 bytes result sent to driver
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000170
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000170_0: Committed
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000166
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000166_0: Committed
15/08/21 23:14:14 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 431, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 183.0 in stage 4.0 (TID 431)
15/08/21 23:14:14 INFO Executor: Finished task 170.0 in stage 4.0 (TID 418). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 419) in 974 ms on localhost (168/200)
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,788
15/08/21 23:14:14 INFO Executor: Finished task 166.0 in stage 4.0 (TID 414). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 432, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 184.0 in stage 4.0 (TID 432)
15/08/21 23:14:14 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 433, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,348B for [ps_partkey] INT32: 15,364 values, 61,464B raw, 51,309B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Running task 185.0 in stage 4.0 (TID 433)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 418) in 984 ms on localhost (169/200)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 79,172B for [part_value] DOUBLE: 15,364 values, 122,920B raw, 79,125B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 414) in 1039 ms on localhost (170/200)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000172
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000172_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 50,231B for [ps_partkey] INT32: 15,039 values, 60,164B raw, 50,192B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000174
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000174_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 77,408B for [part_value] DOUBLE: 15,039 values, 120,320B raw, 77,361B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Finished task 172.0 in stage 4.0 (TID 420). 843 bytes result sent to driver
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,687B for [ps_partkey] INT32: 14,876 values, 59,512B raw, 49,648B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000169
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000169_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,407B for [part_value] DOUBLE: 14,876 values, 119,016B raw, 76,360B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 434, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Finished task 174.0 in stage 4.0 (TID 422). 843 bytes result sent to driver
15/08/21 23:14:14 INFO Executor: Running task 186.0 in stage 4.0 (TID 434)
15/08/21 23:14:14 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 435, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Finished task 169.0 in stage 4.0 (TID 417). 843 bytes result sent to driver
15/08/21 23:14:14 INFO Executor: Running task 187.0 in stage 4.0 (TID 435)
15/08/21 23:14:14 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 436, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 188.0 in stage 4.0 (TID 436)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 420) in 972 ms on localhost (171/200)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 422) in 934 ms on localhost (172/200)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 417) in 1013 ms on localhost (173/200)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,299B for [ps_partkey] INT32: 14,826 values, 59,312B raw, 49,260B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,279B for [part_value] DOUBLE: 14,826 values, 118,616B raw, 76,232B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000175
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000175_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 175.0 in stage 4.0 (TID 423). 843 bytes result sent to driver
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000176
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000176_0: Committed
15/08/21 23:14:14 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 437, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 189.0 in stage 4.0 (TID 437)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 423) in 967 ms on localhost (174/200)
15/08/21 23:14:14 INFO Executor: Finished task 176.0 in stage 4.0 (TID 424). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 438, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000177
15/08/21 23:14:14 INFO Executor: Running task 190.0 in stage 4.0 (TID 438)
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000177_0: Committed
15/08/21 23:14:14 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 424) in 922 ms on localhost (175/200)
15/08/21 23:14:14 INFO Executor: Finished task 177.0 in stage 4.0 (TID 425). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 439, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 191.0 in stage 4.0 (TID 439)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 425) in 906 ms on localhost (176/200)
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,536
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,029B for [ps_partkey] INT32: 15,313 values, 61,260B raw, 50,990B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 78,700B for [part_value] DOUBLE: 15,313 values, 122,512B raw, 78,653B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000178
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000178_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 178.0 in stage 4.0 (TID 426). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 440, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 192.0 in stage 4.0 (TID 440)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 426) in 438 ms on localhost (177/200)
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,028
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,423B for [ps_partkey] INT32: 14,788 values, 59,160B raw, 49,384B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,170B for [part_value] DOUBLE: 14,788 values, 118,312B raw, 76,123B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,996
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000179
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000179_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 179.0 in stage 4.0 (TID 427). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 441, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 52,053B for [ps_partkey] INT32: 15,586 values, 62,352B raw, 52,014B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Running task 193.0 in stage 4.0 (TID 441)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 80,219B for [part_value] DOUBLE: 15,586 values, 124,696B raw, 80,172B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 427) in 406 ms on localhost (178/200)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000180
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000180_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 180.0 in stage 4.0 (TID 428). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 442, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 194.0 in stage 4.0 (TID 442)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 428) in 432 ms on localhost (179/200)
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000168
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000168_0: Committed
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO Executor: Finished task 168.0 in stage 4.0 (TID 416). 843 bytes result sent to driver
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 443, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 195.0 in stage 4.0 (TID 443)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 416) in 1377 ms on localhost (180/200)
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,600
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 50,512B for [ps_partkey] INT32: 15,117 values, 60,476B raw, 50,473B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 77,798B for [part_value] DOUBLE: 15,117 values, 120,944B raw, 77,751B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,767,048
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,757,448
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,416
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,752,948
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,756,308
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,760,008
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000182
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000182_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,428B for [ps_partkey] INT32: 14,784 values, 59,144B raw, 49,389B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,037B for [part_value] DOUBLE: 14,784 values, 118,280B raw, 75,990B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 50,186B for [ps_partkey] INT32: 15,009 values, 60,044B raw, 50,147B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 77,274B for [part_value] DOUBLE: 15,009 values, 120,080B raw, 77,227B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,761,488
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,168
15/08/21 23:14:14 INFO Executor: Finished task 182.0 in stage 4.0 (TID 430). 843 bytes result sent to driver
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 444, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO Executor: Running task 196.0 in stage 4.0 (TID 444)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,700B for [ps_partkey] INT32: 15,489 values, 61,964B raw, 51,661B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 79,563B for [part_value] DOUBLE: 15,489 values, 123,920B raw, 79,516B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 430) in 465 ms on localhost (181/200)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000173
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000173_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,666B for [ps_partkey] INT32: 14,952 values, 59,816B raw, 49,627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Finished task 173.0 in stage 4.0 (TID 421). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 445, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,926B for [part_value] DOUBLE: 14,952 values, 119,624B raw, 76,879B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Running task 197.0 in stage 4.0 (TID 445)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 421) in 1399 ms on localhost (182/200)
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,759,416
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 50,785B for [ps_partkey] INT32: 15,207 values, 60,836B raw, 50,746B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 78,386B for [part_value] DOUBLE: 15,207 values, 121,664B raw, 78,339B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 50,452B for [ps_partkey] INT32: 15,137 values, 60,556B raw, 50,413B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 77,897B for [part_value] DOUBLE: 15,137 values, 121,104B raw, 77,850B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000188
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000188_0: Committed
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000183
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000183_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 188.0 in stage 4.0 (TID 436). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 446, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 50,836B for [ps_partkey] INT32: 15,211 values, 60,852B raw, 50,797B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Finished task 183.0 in stage 4.0 (TID 431). 843 bytes result sent to driver
15/08/21 23:14:14 INFO Executor: Running task 198.0 in stage 4.0 (TID 446)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 78,247B for [part_value] DOUBLE: 15,211 values, 121,696B raw, 78,200B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 436) in 470 ms on localhost (183/200)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,470B for [ps_partkey] INT32: 14,795 values, 59,188B raw, 49,431B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 75,985B for [part_value] DOUBLE: 14,795 values, 118,368B raw, 75,938B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 447, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000181
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000181_0: Committed
15/08/21 23:14:14 INFO Executor: Running task 199.0 in stage 4.0 (TID 447)
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO Executor: Finished task 181.0 in stage 4.0 (TID 429). 843 bytes result sent to driver
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000185
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000185_0: Committed
15/08/21 23:14:14 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 431) in 510 ms on localhost (184/200)
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,769,768
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO Executor: Finished task 185.0 in stage 4.0 (TID 433). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 429) in 537 ms on localhost (185/200)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000187
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000187_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 50,514B for [ps_partkey] INT32: 15,107 values, 60,436B raw, 50,475B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 77,758B for [part_value] DOUBLE: 15,107 values, 120,864B raw, 77,711B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 433) in 513 ms on localhost (186/200)
15/08/21 23:14:14 INFO Executor: Finished task 187.0 in stage 4.0 (TID 435). 843 bytes result sent to driver
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 435) in 496 ms on localhost (187/200)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000186
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000186_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 186.0 in stage 4.0 (TID 434). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 434) in 501 ms on localhost (188/200)
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000184
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000184_0: Committed
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000189
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000189_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 184.0 in stage 4.0 (TID 432). 843 bytes result sent to driver
15/08/21 23:14:14 INFO Executor: Finished task 189.0 in stage 4.0 (TID 437). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 432) in 535 ms on localhost (189/200)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 437) in 478 ms on localhost (190/200)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 52,194B for [ps_partkey] INT32: 15,625 values, 62,508B raw, 52,155B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 80,315B for [part_value] DOUBLE: 15,625 values, 125,008B raw, 80,268B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000191
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000191_0: Committed
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:14 INFO Executor: Finished task 191.0 in stage 4.0 (TID 439). 843 bytes result sent to driver
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,764,068
15/08/21 23:14:14 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 439) in 490 ms on localhost (191/200)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000190
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000190_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 190.0 in stage 4.0 (TID 438). 843 bytes result sent to driver
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 438) in 518 ms on localhost (192/200)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,297B for [ps_partkey] INT32: 15,340 values, 61,368B raw, 51,258B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 78,833B for [part_value] DOUBLE: 15,340 values, 122,728B raw, 78,786B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,120
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,908
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000192
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000192_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,166B for [ps_partkey] INT32: 14,793 values, 59,180B raw, 49,127B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 75,993B for [part_value] DOUBLE: 14,793 values, 118,352B raw, 75,946B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Finished task 192.0 in stage 4.0 (TID 440). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 440) in 459 ms on localhost (193/200)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,427B for [ps_partkey] INT32: 15,432 values, 61,736B raw, 51,388B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 79,261B for [part_value] DOUBLE: 15,432 values, 123,464B raw, 79,214B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000193
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000193_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 193.0 in stage 4.0 (TID 441). 843 bytes result sent to driver
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,763,868
15/08/21 23:14:14 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 441) in 423 ms on localhost (194/200)
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000194
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000194_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 194.0 in stage 4.0 (TID 442). 843 bytes result sent to driver
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 442) in 417 ms on localhost (195/200)
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,188B for [ps_partkey] INT32: 15,330 values, 61,328B raw, 51,149B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 78,891B for [part_value] DOUBLE: 15,330 values, 122,648B raw, 78,844B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000195
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000195_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 195.0 in stage 4.0 (TID 443). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 443) in 387 ms on localhost (196/200)
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,768,688
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,976B for [ps_partkey] INT32: 15,571 values, 62,292B raw, 51,937B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 79,945B for [part_value] DOUBLE: 15,571 values, 124,576B raw, 79,898B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,088
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,753,996
15/08/21 23:14:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,765,196
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,578B for [ps_partkey] INT32: 14,836 values, 59,352B raw, 49,539B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,367B for [part_value] DOUBLE: 14,836 values, 118,696B raw, 76,320B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 49,411B for [ps_partkey] INT32: 14,791 values, 59,172B raw, 49,372B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000196
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 76,124B for [part_value] DOUBLE: 14,791 values, 118,336B raw, 76,077B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000196_0: Committed
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 51,393B for [ps_partkey] INT32: 15,396 values, 61,592B raw, 51,354B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO ColumnChunkPageWriteStore: written 79,186B for [part_value] DOUBLE: 15,396 values, 123,176B raw, 79,139B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:14 INFO Executor: Finished task 196.0 in stage 4.0 (TID 444). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 444) in 365 ms on localhost (197/200)
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000199
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000199_0: Committed
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000197
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000197_0: Committed
15/08/21 23:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0004_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_temporary/0/task_201508212314_0004_m_000198
15/08/21 23:14:14 INFO SparkHadoopMapRedUtil: attempt_201508212314_0004_m_000198_0: Committed
15/08/21 23:14:14 INFO Executor: Finished task 199.0 in stage 4.0 (TID 447). 843 bytes result sent to driver
15/08/21 23:14:14 INFO Executor: Finished task 197.0 in stage 4.0 (TID 445). 843 bytes result sent to driver
15/08/21 23:14:14 INFO Executor: Finished task 198.0 in stage 4.0 (TID 446). 843 bytes result sent to driver
15/08/21 23:14:14 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 447) in 335 ms on localhost (198/200)
15/08/21 23:14:14 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 445) in 365 ms on localhost (199/200)
15/08/21 23:14:14 INFO DAGScheduler: ResultStage 4 (processCmd at CliDriver.java:423) finished in 12.937 s
15/08/21 23:14:14 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2117f67f
15/08/21 23:14:14 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 446) in 346 ms on localhost (200/200)
15/08/21 23:14:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/21 23:14:14 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 54.421765 s
15/08/21 23:14:14 INFO StatsReportListener: task runtime:(count: 200, mean: 1023.145000, stdev: 479.951804, max: 2402.000000, min: 335.000000)
15/08/21 23:14:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:14 INFO StatsReportListener: 	335.0 ms	396.0 ms	438.0 ms	537.0 ms	999.0 ms	1.3 s	1.8 s	1.9 s	2.4 s
15/08/21 23:14:14 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.340000, stdev: 0.924338, max: 11.000000, min: 0.000000)
15/08/21 23:14:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:14 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	1.0 ms	11.0 ms
15/08/21 23:14:14 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 23:14:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:14 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 23:14:14 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 23:14:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:14 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 23:14:14 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 90.157618, stdev: 7.215892, max: 98.210481, min: 34.195726)
15/08/21 23:14:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:14 INFO StatsReportListener: 	34 %	82 %	83 %	87 %	92 %	95 %	96 %	96 %	98 %
15/08/21 23:14:14 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.039207, stdev: 0.089472, max: 0.594273, min: 0.000000)
15/08/21 23:14:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:14 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/21 23:14:14 INFO StatsReportListener: other time pct: (count: 200, mean: 9.803175, stdev: 7.200828, max: 65.804274, min: 1.789519)
15/08/21 23:14:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:14 INFO StatsReportListener: 	 2 %	 4 %	 4 %	 5 %	 8 %	13 %	17 %	18 %	66 %
15/08/21 23:14:16 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 23:14:16 INFO DefaultWriterContainer: Job job_201508212313_0000 committed.
15/08/21 23:14:16 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 23:14:16 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/_common_metadata
15/08/21 23:14:16 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 23:14:16 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 23:14:16 INFO DAGScheduler: Final stage: ResultStage 5(processCmd at CliDriver.java:423)
15/08/21 23:14:16 INFO DAGScheduler: Parents of final stage: List()
15/08/21 23:14:16 INFO DAGScheduler: Missing parents: List()
15/08/21 23:14:16 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 23:14:16 INFO MemoryStore: ensureFreeSpace(2968) called with curMem=1162860, maxMem=22226833244
15/08/21 23:14:16 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 23:14:16 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=1165828, maxMem=22226833244
15/08/21 23:14:16 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1776.0 B, free 20.7 GB)
15/08/21 23:14:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:40586 (size: 1776.0 B, free: 20.7 GB)
15/08/21 23:14:16 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874
15/08/21 23:14:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/21 23:14:16 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/08/21 23:14:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 448, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 23:14:16 INFO Executor: Running task 0.0 in stage 5.0 (TID 448)
15/08/21 23:14:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 448). 606 bytes result sent to driver
15/08/21 23:14:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 448) in 37 ms on localhost (1/1)
15/08/21 23:14:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/21 23:14:16 INFO DAGScheduler: ResultStage 5 (processCmd at CliDriver.java:423) finished in 0.038 s
15/08/21 23:14:16 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 0.052402 s
15/08/21 23:14:16 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@6afc6820
15/08/21 23:14:16 INFO StatsReportListener: task runtime:(count: 1, mean: 37.000000, stdev: 0.000000, max: 37.000000, min: 37.000000)
15/08/21 23:14:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:16 INFO StatsReportListener: 	37.0 ms	37.0 ms	37.0 ms	37.0 ms	37.0 ms	37.0 ms	37.0 ms	37.0 ms	37.0 ms
15/08/21 23:14:16 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 23:14:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:16 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 23:14:16 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 24.324324, stdev: 0.000000, max: 24.324324, min: 24.324324)
15/08/21 23:14:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:16 INFO StatsReportListener: 	24 %	24 %	24 %	24 %	24 %	24 %	24 %	24 %	24 %
15/08/21 23:14:16 INFO StatsReportListener: other time pct: (count: 1, mean: 75.675676, stdev: 0.000000, max: 75.675676, min: 75.675676)
15/08/21 23:14:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:16 INFO StatsReportListener: 	76 %	76 %	76 %	76 %	76 %	76 %	76 %	76 %	76 %
Time taken: 62.191 seconds
15/08/21 23:14:16 INFO CliDriver: Time taken: 62.191 seconds
15/08/21 23:14:16 INFO ParseDriver: Parsing command: insert into table q11_important_stock_par
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par) sum_tmp
    join q11_part_tmp_par
where part_value > total_value * 0.000001
order by value desc
15/08/21 23:14:16 INFO ParseDriver: Parse Completed
15/08/21 23:14:16 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 23:14:17 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1167604, maxMem=22226833244
15/08/21 23:14:17 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 23:14:17 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1494212, maxMem=22226833244
15/08/21 23:14:17 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 23:14:17 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:40586 (size: 22.3 KB, free: 20.7 GB)
15/08/21 23:14:17 INFO SparkContext: Created broadcast 10 from processCmd at CliDriver.java:423
15/08/21 23:14:17 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1517005, maxMem=22226833244
15/08/21 23:14:17 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 23:14:17 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1843613, maxMem=22226833244
15/08/21 23:14:17 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 23:14:17 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:40586 (size: 22.3 KB, free: 20.7 GB)
15/08/21 23:14:17 INFO SparkContext: Created broadcast 11 from processCmd at CliDriver.java:423
15/08/21 23:14:17 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 23:14:17 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 23:14:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 23:14:17 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 23:14:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 23:14:18 INFO DAGScheduler: Registering RDD 31 (processCmd at CliDriver.java:423)
15/08/21 23:14:18 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 23:14:18 INFO DAGScheduler: Final stage: ResultStage 7(processCmd at CliDriver.java:423)
15/08/21 23:14:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
15/08/21 23:14:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)
15/08/21 23:14:18 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[31] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 23:14:18 INFO MemoryStore: ensureFreeSpace(7688) called with curMem=1866406, maxMem=22226833244
15/08/21 23:14:18 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.5 KB, free 20.7 GB)
15/08/21 23:14:18 INFO MemoryStore: ensureFreeSpace(3917) called with curMem=1874094, maxMem=22226833244
15/08/21 23:14:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.8 KB, free 20.7 GB)
15/08/21 23:14:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:40586 (size: 3.8 KB, free: 20.7 GB)
15/08/21 23:14:18 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/21 23:14:18 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[31] at processCmd at CliDriver.java:423)
15/08/21 23:14:18 INFO TaskSchedulerImpl: Adding task set 6.0 with 200 tasks
15/08/21 23:14:18 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 449, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 450, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 451, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 452, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 453, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 454, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 455, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 456, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 457, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 458, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 459, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 460, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 461, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 462, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 463, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 464, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 0.0 in stage 6.0 (TID 449)
15/08/21 23:14:18 INFO Executor: Running task 2.0 in stage 6.0 (TID 451)
15/08/21 23:14:18 INFO Executor: Running task 4.0 in stage 6.0 (TID 453)
15/08/21 23:14:18 INFO Executor: Running task 7.0 in stage 6.0 (TID 456)
15/08/21 23:14:18 INFO Executor: Running task 9.0 in stage 6.0 (TID 458)
15/08/21 23:14:18 INFO Executor: Running task 3.0 in stage 6.0 (TID 452)
15/08/21 23:14:18 INFO Executor: Running task 11.0 in stage 6.0 (TID 460)
15/08/21 23:14:18 INFO Executor: Running task 1.0 in stage 6.0 (TID 450)
15/08/21 23:14:18 INFO Executor: Running task 14.0 in stage 6.0 (TID 463)
15/08/21 23:14:18 INFO Executor: Running task 12.0 in stage 6.0 (TID 461)
15/08/21 23:14:18 INFO Executor: Running task 10.0 in stage 6.0 (TID 459)
15/08/21 23:14:18 INFO Executor: Running task 6.0 in stage 6.0 (TID 455)
15/08/21 23:14:18 INFO Executor: Running task 8.0 in stage 6.0 (TID 457)
15/08/21 23:14:18 INFO Executor: Running task 13.0 in stage 6.0 (TID 462)
15/08/21 23:14:18 INFO Executor: Running task 5.0 in stage 6.0 (TID 454)
15/08/21 23:14:18 INFO Executor: Running task 15.0 in stage 6.0 (TID 464)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14795
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15570
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 14967
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15501
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14825
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14972
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 14826
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14989
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15359
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14379
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14876
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15340
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 15062
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15038
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 14956
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14824
15/08/21 23:14:18 INFO Executor: Finished task 1.0 in stage 6.0 (TID 450). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 4.0 in stage 6.0 (TID 453). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 465, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 5.0 in stage 6.0 (TID 454). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 16.0 in stage 6.0 (TID 465)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 466, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 2.0 in stage 6.0 (TID 451). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 17.0 in stage 6.0 (TID 466)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 467, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 3.0 in stage 6.0 (TID 452). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 468, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 19.0 in stage 6.0 (TID 468)
15/08/21 23:14:18 INFO Executor: Finished task 13.0 in stage 6.0 (TID 462). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 0.0 in stage 6.0 (TID 449). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 18.0 in stage 6.0 (TID 467)
15/08/21 23:14:18 INFO Executor: Finished task 10.0 in stage 6.0 (TID 459). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 469, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 470, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 21.0 in stage 6.0 (TID 470)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 471, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 22.0 in stage 6.0 (TID 471)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 450) in 128 ms on localhost (1/200)
15/08/21 23:14:18 INFO Executor: Running task 20.0 in stage 6.0 (TID 469)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 454) in 126 ms on localhost (2/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 472, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 23.0 in stage 6.0 (TID 472)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 451) in 132 ms on localhost (3/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 11.0 in stage 6.0 (TID 460). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 473, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 452) in 137 ms on localhost (4/200)
15/08/21 23:14:18 INFO Executor: Running task 24.0 in stage 6.0 (TID 473)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 462) in 133 ms on localhost (5/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 459) in 141 ms on localhost (6/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 23:14:18 INFO Executor: Finished task 12.0 in stage 6.0 (TID 461). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 9.0 in stage 6.0 (TID 458). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 14.0 in stage 6.0 (TID 463). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 7.0 in stage 6.0 (TID 456). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14793
15/08/21 23:14:18 INFO Executor: Finished task 15.0 in stage 6.0 (TID 464). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15132
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15404
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14826
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14939
15/08/21 23:14:18 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 449) in 149 ms on localhost (7/200)
15/08/21 23:14:18 INFO Executor: Finished task 8.0 in stage 6.0 (TID 457). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14736
15/08/21 23:14:18 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 474, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 25.0 in stage 6.0 (TID 474)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 475, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 23:14:18 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 476, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 27.0 in stage 6.0 (TID 476)
15/08/21 23:14:18 INFO Executor: Running task 26.0 in stage 6.0 (TID 475)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 477, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15055
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14985
15/08/21 23:14:18 INFO Executor: Running task 28.0 in stage 6.0 (TID 477)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 478, localhost, ANY, 1684 bytes)
15/08/21 23:14:18 INFO Executor: Running task 29.0 in stage 6.0 (TID 478)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 453) in 167 ms on localhost (8/200)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 479, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 30.0 in stage 6.0 (TID 479)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 6.0 in stage 6.0 (TID 455). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 15131
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 463) in 176 ms on localhost (9/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 21.0 in stage 6.0 (TID 470). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 458) in 189 ms on localhost (10/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 460) in 188 ms on localhost (11/200)
15/08/21 23:14:18 INFO Executor: Finished task 18.0 in stage 6.0 (TID 467). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 480, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 31.0 in stage 6.0 (TID 480)
15/08/21 23:14:18 INFO Executor: Finished task 23.0 in stage 6.0 (TID 472). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 19.0 in stage 6.0 (TID 468). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 481, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 32.0 in stage 6.0 (TID 481)
15/08/21 23:14:18 INFO Executor: Finished task 16.0 in stage 6.0 (TID 465). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 482, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 483, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 33.0 in stage 6.0 (TID 482)
15/08/21 23:14:18 INFO Executor: Running task 34.0 in stage 6.0 (TID 483)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15238
15/08/21 23:14:18 INFO Executor: Finished task 24.0 in stage 6.0 (TID 473). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 22.0 in stage 6.0 (TID 471). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 484, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 23:14:18 INFO Executor: Running task 35.0 in stage 6.0 (TID 484)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 17.0 in stage 6.0 (TID 466). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15296
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14878
15/08/21 23:14:18 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 456) in 201 ms on localhost (12/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 457) in 205 ms on localhost (13/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 464) in 200 ms on localhost (14/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14934
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 485, localhost, ANY, 1689 bytes)
15/08/21 23:14:18 INFO Executor: Running task 36.0 in stage 6.0 (TID 485)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 486, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 461) in 210 ms on localhost (15/200)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 487, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14906
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14772
15/08/21 23:14:18 INFO Executor: Running task 37.0 in stage 6.0 (TID 486)
15/08/21 23:14:18 INFO Executor: Running task 38.0 in stage 6.0 (TID 487)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 488, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14819
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15603
15/08/21 23:14:18 INFO Executor: Running task 39.0 in stage 6.0 (TID 488)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 455) in 224 ms on localhost (16/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15302
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 467) in 115 ms on localhost (17/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 468) in 113 ms on localhost (18/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14693
15/08/21 23:14:18 INFO Executor: Finished task 28.0 in stage 6.0 (TID 477). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 20.0 in stage 6.0 (TID 469). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:18 INFO Executor: Finished task 29.0 in stage 6.0 (TID 478). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14590
15/08/21 23:14:18 INFO Executor: Finished task 30.0 in stage 6.0 (TID 479). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 489, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15566
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 23:14:18 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 473) in 112 ms on localhost (19/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 470) in 126 ms on localhost (20/200)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 490, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 41.0 in stage 6.0 (TID 490)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 23:14:18 INFO Executor: Running task 40.0 in stage 6.0 (TID 489)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 491, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 42.0 in stage 6.0 (TID 491)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 492, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 472) in 132 ms on localhost (21/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 32.0 in stage 6.0 (TID 481). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 34.0 in stage 6.0 (TID 483). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 15636
15/08/21 23:14:18 INFO Executor: Running task 43.0 in stage 6.0 (TID 492)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 493, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 25.0 in stage 6.0 (TID 474). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 44.0 in stage 6.0 (TID 493)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 494, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 31.0 in stage 6.0 (TID 480). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 45.0 in stage 6.0 (TID 494)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 23:14:18 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 495, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 46.0 in stage 6.0 (TID 495)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15545
15/08/21 23:14:18 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 496, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 26.0 in stage 6.0 (TID 475). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 471) in 158 ms on localhost (22/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 23:14:18 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 466) in 167 ms on localhost (23/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 38.0 in stage 6.0 (TID 487). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 47.0 in stage 6.0 (TID 496)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 23:14:18 INFO Executor: Finished task 37.0 in stage 6.0 (TID 486). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 477) in 122 ms on localhost (24/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14831
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 33.0 in stage 6.0 (TID 482). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 497, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 469) in 165 ms on localhost (25/200)
15/08/21 23:14:18 INFO Executor: Running task 48.0 in stage 6.0 (TID 497)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 465) in 174 ms on localhost (26/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 478) in 125 ms on localhost (27/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 36.0 in stage 6.0 (TID 485). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 498, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14884
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 49.0 in stage 6.0 (TID 498)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14809
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 50.0 in stage 6.0 (TID 499, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 50.0 in stage 6.0 (TID 499)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 51.0 in stage 6.0 (TID 500, localhost, ANY, 1684 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 51.0 in stage 6.0 (TID 500)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 52.0 in stage 6.0 (TID 501, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 474) in 142 ms on localhost (28/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 52.0 in stage 6.0 (TID 501)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 27.0 in stage 6.0 (TID 476). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 481) in 106 ms on localhost (29/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 53.0 in stage 6.0 (TID 502, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 53.0 in stage 6.0 (TID 502)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 483) in 110 ms on localhost (30/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15347
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 41.0 in stage 6.0 (TID 490). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14913
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 480) in 118 ms on localhost (31/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 479) in 145 ms on localhost (32/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15129
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 23:14:18 INFO Executor: Finished task 40.0 in stage 6.0 (TID 489). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 39.0 in stage 6.0 (TID 488). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15302
15/08/21 23:14:18 INFO TaskSetManager: Starting task 54.0 in stage 6.0 (TID 503, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 23:14:18 INFO TaskSetManager: Starting task 55.0 in stage 6.0 (TID 504, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 55.0 in stage 6.0 (TID 504)
15/08/21 23:14:18 INFO Executor: Running task 54.0 in stage 6.0 (TID 503)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14992
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14814
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 56.0 in stage 6.0 (TID 505, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 56.0 in stage 6.0 (TID 505)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14944
15/08/21 23:14:18 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 487) in 117 ms on localhost (33/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 482) in 135 ms on localhost (34/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 14836
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 43.0 in stage 6.0 (TID 492). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 475) in 183 ms on localhost (35/200)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 57.0 in stage 6.0 (TID 506, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 46.0 in stage 6.0 (TID 495). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 50.0 in stage 6.0 (TID 499). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 48.0 in stage 6.0 (TID 497). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 57.0 in stage 6.0 (TID 506)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15549
15/08/21 23:14:18 INFO Executor: Finished task 52.0 in stage 6.0 (TID 501). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15164
15/08/21 23:14:18 INFO Executor: Finished task 35.0 in stage 6.0 (TID 484). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 58.0 in stage 6.0 (TID 507, localhost, ANY, 1683 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 53.0 in stage 6.0 (TID 502). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 59.0 in stage 6.0 (TID 508, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 51.0 in stage 6.0 (TID 500). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 60.0 in stage 6.0 (TID 509, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 45.0 in stage 6.0 (TID 494). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 476) in 193 ms on localhost (36/200)
15/08/21 23:14:18 INFO Executor: Running task 60.0 in stage 6.0 (TID 509)
15/08/21 23:14:18 INFO Executor: Running task 59.0 in stage 6.0 (TID 508)
15/08/21 23:14:18 INFO Executor: Running task 58.0 in stage 6.0 (TID 507)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 47.0 in stage 6.0 (TID 496). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 42.0 in stage 6.0 (TID 491). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 61.0 in stage 6.0 (TID 510, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 486) in 149 ms on localhost (37/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 490) in 113 ms on localhost (38/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 61.0 in stage 6.0 (TID 510)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14829
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 485) in 157 ms on localhost (39/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 62.0 in stage 6.0 (TID 511, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 62.0 in stage 6.0 (TID 511)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 63.0 in stage 6.0 (TID 512, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 64.0 in stage 6.0 (TID 513, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 492) in 118 ms on localhost (40/200)
15/08/21 23:14:18 INFO Executor: Running task 64.0 in stage 6.0 (TID 513)
15/08/21 23:14:18 INFO Executor: Running task 63.0 in stage 6.0 (TID 512)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 488) in 152 ms on localhost (41/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 44.0 in stage 6.0 (TID 493). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 65.0 in stage 6.0 (TID 514, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 49.0 in stage 6.0 (TID 498). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 65.0 in stage 6.0 (TID 514)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 66.0 in stage 6.0 (TID 515, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 66.0 in stage 6.0 (TID 515)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15103
15/08/21 23:14:18 INFO TaskSetManager: Starting task 67.0 in stage 6.0 (TID 516, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 23:14:18 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 489) in 141 ms on localhost (42/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 497) in 98 ms on localhost (43/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 495) in 115 ms on localhost (44/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 23:14:18 INFO Executor: Running task 67.0 in stage 6.0 (TID 516)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14883
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 50.0 in stage 6.0 (TID 499) in 94 ms on localhost (45/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15500
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 484) in 195 ms on localhost (46/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15489
15/08/21 23:14:18 INFO TaskSetManager: Finished task 52.0 in stage 6.0 (TID 501) in 102 ms on localhost (47/200)
15/08/21 23:14:18 INFO Executor: Finished task 55.0 in stage 6.0 (TID 504). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15282
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 15107
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 68.0 in stage 6.0 (TID 517, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Running task 68.0 in stage 6.0 (TID 517)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 69.0 in stage 6.0 (TID 518, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15571
15/08/21 23:14:18 INFO Executor: Running task 69.0 in stage 6.0 (TID 518)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 70.0 in stage 6.0 (TID 519, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 70.0 in stage 6.0 (TID 519)
15/08/21 23:14:18 INFO Executor: Finished task 56.0 in stage 6.0 (TID 505). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15207
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15795
15/08/21 23:14:18 INFO TaskSetManager: Starting task 71.0 in stage 6.0 (TID 520, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 53.0 in stage 6.0 (TID 502) in 111 ms on localhost (48/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 71.0 in stage 6.0 (TID 520)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15383
15/08/21 23:14:18 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 496) in 152 ms on localhost (49/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 51.0 in stage 6.0 (TID 500) in 127 ms on localhost (50/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 491) in 173 ms on localhost (51/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15183
15/08/21 23:14:18 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 498) in 136 ms on localhost (52/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 494) in 159 ms on localhost (53/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 56.0 in stage 6.0 (TID 505) in 109 ms on localhost (54/200)
15/08/21 23:14:18 INFO Executor: Finished task 54.0 in stage 6.0 (TID 503). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 58.0 in stage 6.0 (TID 507). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 493) in 166 ms on localhost (55/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 15079
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 14854
15/08/21 23:14:18 INFO Executor: Finished task 60.0 in stage 6.0 (TID 509). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15211
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 23:14:18 INFO Executor: Finished task 59.0 in stage 6.0 (TID 508). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 72.0 in stage 6.0 (TID 521, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15018
15/08/21 23:14:18 INFO Executor: Finished task 62.0 in stage 6.0 (TID 511). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 61.0 in stage 6.0 (TID 510). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 63.0 in stage 6.0 (TID 512). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 72.0 in stage 6.0 (TID 521)
15/08/21 23:14:18 INFO Executor: Finished task 64.0 in stage 6.0 (TID 513). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 67.0 in stage 6.0 (TID 516). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 73.0 in stage 6.0 (TID 522, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 55.0 in stage 6.0 (TID 504) in 133 ms on localhost (56/200)
15/08/21 23:14:18 INFO Executor: Running task 73.0 in stage 6.0 (TID 522)
15/08/21 23:14:18 INFO Executor: Finished task 57.0 in stage 6.0 (TID 506). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 66.0 in stage 6.0 (TID 515). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 70.0 in stage 6.0 (TID 519). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 65.0 in stage 6.0 (TID 514). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 74.0 in stage 6.0 (TID 523, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 69.0 in stage 6.0 (TID 518). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 54.0 in stage 6.0 (TID 503) in 149 ms on localhost (57/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 58.0 in stage 6.0 (TID 507) in 118 ms on localhost (58/200)
15/08/21 23:14:18 INFO Executor: Finished task 68.0 in stage 6.0 (TID 517). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 74.0 in stage 6.0 (TID 523)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15058
15/08/21 23:14:18 INFO Executor: Finished task 71.0 in stage 6.0 (TID 520). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 75.0 in stage 6.0 (TID 524, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 60.0 in stage 6.0 (TID 509) in 119 ms on localhost (59/200)
15/08/21 23:14:18 INFO Executor: Running task 75.0 in stage 6.0 (TID 524)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 76.0 in stage 6.0 (TID 525, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 59.0 in stage 6.0 (TID 508) in 127 ms on localhost (60/200)
15/08/21 23:14:18 INFO Executor: Running task 76.0 in stage 6.0 (TID 525)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 77.0 in stage 6.0 (TID 526, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14864
15/08/21 23:14:18 INFO TaskSetManager: Finished task 61.0 in stage 6.0 (TID 510) in 130 ms on localhost (61/200)
15/08/21 23:14:18 INFO Executor: Running task 77.0 in stage 6.0 (TID 526)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 78.0 in stage 6.0 (TID 527, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 78.0 in stage 6.0 (TID 527)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 79.0 in stage 6.0 (TID 528, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 79.0 in stage 6.0 (TID 528)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 80.0 in stage 6.0 (TID 529, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 63.0 in stage 6.0 (TID 512) in 116 ms on localhost (62/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 62.0 in stage 6.0 (TID 511) in 121 ms on localhost (63/200)
15/08/21 23:14:18 INFO Executor: Running task 80.0 in stage 6.0 (TID 529)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14508
15/08/21 23:14:18 INFO TaskSetManager: Starting task 81.0 in stage 6.0 (TID 530, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 81.0 in stage 6.0 (TID 530)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 72.0 in stage 6.0 (TID 521). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 82.0 in stage 6.0 (TID 531, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14564
15/08/21 23:14:18 INFO Executor: Running task 82.0 in stage 6.0 (TID 531)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 64.0 in stage 6.0 (TID 513) in 129 ms on localhost (64/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 83.0 in stage 6.0 (TID 532, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Running task 83.0 in stage 6.0 (TID 532)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 67.0 in stage 6.0 (TID 516) in 124 ms on localhost (65/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 66.0 in stage 6.0 (TID 515) in 127 ms on localhost (66/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 84.0 in stage 6.0 (TID 533, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 84.0 in stage 6.0 (TID 533)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 57.0 in stage 6.0 (TID 506) in 162 ms on localhost (67/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14838
15/08/21 23:14:18 INFO TaskSetManager: Finished task 70.0 in stage 6.0 (TID 519) in 101 ms on localhost (68/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 85.0 in stage 6.0 (TID 534, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 85.0 in stage 6.0 (TID 534)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 86.0 in stage 6.0 (TID 535, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 73.0 in stage 6.0 (TID 522). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 86.0 in stage 6.0 (TID 535)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 23:14:18 INFO TaskSetManager: Starting task 87.0 in stage 6.0 (TID 536, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 74.0 in stage 6.0 (TID 523). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 87.0 in stage 6.0 (TID 536)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 65.0 in stage 6.0 (TID 514) in 140 ms on localhost (69/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 69.0 in stage 6.0 (TID 518) in 112 ms on localhost (70/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15272
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 76.0 in stage 6.0 (TID 525). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 68.0 in stage 6.0 (TID 517) in 115 ms on localhost (71/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 71.0 in stage 6.0 (TID 520) in 108 ms on localhost (72/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 88.0 in stage 6.0 (TID 537, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 75.0 in stage 6.0 (TID 524). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 88.0 in stage 6.0 (TID 537)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 89.0 in stage 6.0 (TID 538, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 89.0 in stage 6.0 (TID 538)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 90.0 in stage 6.0 (TID 539, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 72.0 in stage 6.0 (TID 521) in 88 ms on localhost (73/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14741
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14740
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 90.0 in stage 6.0 (TID 539)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15432
15/08/21 23:14:18 INFO Executor: Finished task 79.0 in stage 6.0 (TID 528). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 23:14:18 INFO TaskSetManager: Starting task 91.0 in stage 6.0 (TID 540, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 91.0 in stage 6.0 (TID 540)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15267
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 23:14:18 INFO TaskSetManager: Starting task 92.0 in stage 6.0 (TID 541, localhost, ANY, 1684 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14509
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 92.0 in stage 6.0 (TID 541)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 93.0 in stage 6.0 (TID 542, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 93.0 in stage 6.0 (TID 542)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15098
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 23:14:18 INFO TaskSetManager: Finished task 74.0 in stage 6.0 (TID 523) in 80 ms on localhost (74/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15333
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 80.0 in stage 6.0 (TID 529). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 76.0 in stage 6.0 (TID 525) in 71 ms on localhost (75/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 73.0 in stage 6.0 (TID 522) in 97 ms on localhost (76/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 79.0 in stage 6.0 (TID 528) in 58 ms on localhost (77/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14896
15/08/21 23:14:18 INFO Executor: Finished task 78.0 in stage 6.0 (TID 527). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 94.0 in stage 6.0 (TID 543, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 75.0 in stage 6.0 (TID 524) in 78 ms on localhost (78/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15532
15/08/21 23:14:18 INFO TaskSetManager: Starting task 95.0 in stage 6.0 (TID 544, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 95.0 in stage 6.0 (TID 544)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 78.0 in stage 6.0 (TID 527) in 66 ms on localhost (79/200)
15/08/21 23:14:18 INFO Executor: Running task 94.0 in stage 6.0 (TID 543)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 80.0 in stage 6.0 (TID 529) in 69 ms on localhost (80/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 85.0 in stage 6.0 (TID 534). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 82.0 in stage 6.0 (TID 531). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 86.0 in stage 6.0 (TID 535). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 96.0 in stage 6.0 (TID 545, localhost, ANY, 1684 bytes)
15/08/21 23:14:18 INFO Executor: Running task 96.0 in stage 6.0 (TID 545)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 97.0 in stage 6.0 (TID 546, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 83.0 in stage 6.0 (TID 532). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 77.0 in stage 6.0 (TID 526). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15384
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14919
15/08/21 23:14:18 INFO Executor: Finished task 88.0 in stage 6.0 (TID 537). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 98.0 in stage 6.0 (TID 547, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 81.0 in stage 6.0 (TID 530). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15272
15/08/21 23:14:18 INFO Executor: Running task 97.0 in stage 6.0 (TID 546)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 23:14:18 INFO TaskSetManager: Finished task 85.0 in stage 6.0 (TID 534) in 57 ms on localhost (81/200)
15/08/21 23:14:18 INFO Executor: Running task 98.0 in stage 6.0 (TID 547)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 23:14:18 INFO Executor: Finished task 84.0 in stage 6.0 (TID 533). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 82.0 in stage 6.0 (TID 531) in 76 ms on localhost (82/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 23:14:18 INFO TaskSetManager: Finished task 86.0 in stage 6.0 (TID 535) in 62 ms on localhost (83/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15189
15/08/21 23:14:18 INFO TaskSetManager: Finished task 83.0 in stage 6.0 (TID 532) in 74 ms on localhost (84/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 99.0 in stage 6.0 (TID 548, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 87.0 in stage 6.0 (TID 536). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 99.0 in stage 6.0 (TID 548)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15474
15/08/21 23:14:18 INFO TaskSetManager: Starting task 100.0 in stage 6.0 (TID 549, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15150
15/08/21 23:14:18 INFO TaskSetManager: Starting task 101.0 in stage 6.0 (TID 550, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 100.0 in stage 6.0 (TID 549)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 102.0 in stage 6.0 (TID 551, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 101.0 in stage 6.0 (TID 550)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 88.0 in stage 6.0 (TID 537) in 79 ms on localhost (85/200)
15/08/21 23:14:18 INFO Executor: Running task 102.0 in stage 6.0 (TID 551)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14810
15/08/21 23:14:18 INFO Executor: Finished task 90.0 in stage 6.0 (TID 539). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 103.0 in stage 6.0 (TID 552, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 92.0 in stage 6.0 (TID 541). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 91.0 in stage 6.0 (TID 540). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 103.0 in stage 6.0 (TID 552)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 77.0 in stage 6.0 (TID 526) in 120 ms on localhost (86/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14569
15/08/21 23:14:18 INFO TaskSetManager: Starting task 104.0 in stage 6.0 (TID 553, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 95.0 in stage 6.0 (TID 544). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 104.0 in stage 6.0 (TID 553)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 81.0 in stage 6.0 (TID 530) in 113 ms on localhost (87/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 89.0 in stage 6.0 (TID 538). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 23:14:18 INFO Executor: Finished task 96.0 in stage 6.0 (TID 545). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14951
15/08/21 23:14:18 INFO TaskSetManager: Starting task 105.0 in stage 6.0 (TID 554, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15393
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 84.0 in stage 6.0 (TID 533) in 107 ms on localhost (88/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 87.0 in stage 6.0 (TID 536) in 98 ms on localhost (89/200)
15/08/21 23:14:18 INFO Executor: Finished task 94.0 in stage 6.0 (TID 543). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 105.0 in stage 6.0 (TID 554)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 106.0 in stage 6.0 (TID 555, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 106.0 in stage 6.0 (TID 555)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 107.0 in stage 6.0 (TID 556, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 90.0 in stage 6.0 (TID 539) in 93 ms on localhost (90/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14502
15/08/21 23:14:18 INFO TaskSetManager: Starting task 108.0 in stage 6.0 (TID 557, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 91.0 in stage 6.0 (TID 540) in 92 ms on localhost (91/200)
15/08/21 23:14:18 INFO Executor: Running task 108.0 in stage 6.0 (TID 557)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15106
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 109.0 in stage 6.0 (TID 558, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14879
15/08/21 23:14:18 INFO Executor: Running task 109.0 in stage 6.0 (TID 558)
15/08/21 23:14:18 INFO Executor: Running task 107.0 in stage 6.0 (TID 556)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 110.0 in stage 6.0 (TID 559, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 110.0 in stage 6.0 (TID 559)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 111.0 in stage 6.0 (TID 560, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Running task 111.0 in stage 6.0 (TID 560)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 99.0 in stage 6.0 (TID 548). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 92.0 in stage 6.0 (TID 541) in 96 ms on localhost (92/200)
15/08/21 23:14:18 INFO Executor: Finished task 97.0 in stage 6.0 (TID 546). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 95.0 in stage 6.0 (TID 544) in 88 ms on localhost (93/200)
15/08/21 23:14:18 INFO Executor: Finished task 101.0 in stage 6.0 (TID 550). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 100.0 in stage 6.0 (TID 549). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14816
15/08/21 23:14:18 INFO TaskSetManager: Starting task 112.0 in stage 6.0 (TID 561, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 98.0 in stage 6.0 (TID 547). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Running task 112.0 in stage 6.0 (TID 561)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 94.0 in stage 6.0 (TID 543) in 96 ms on localhost (94/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 96.0 in stage 6.0 (TID 545) in 83 ms on localhost (95/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 89.0 in stage 6.0 (TID 538) in 121 ms on localhost (96/200)
15/08/21 23:14:18 INFO Executor: Finished task 103.0 in stage 6.0 (TID 552). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 102.0 in stage 6.0 (TID 551). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 113.0 in stage 6.0 (TID 562, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 99.0 in stage 6.0 (TID 548) in 68 ms on localhost (97/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 23:14:18 INFO Executor: Running task 113.0 in stage 6.0 (TID 562)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 104.0 in stage 6.0 (TID 553). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14776
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14954
15/08/21 23:14:18 INFO TaskSetManager: Starting task 114.0 in stage 6.0 (TID 563, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14788
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 114.0 in stage 6.0 (TID 563)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14718
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14891
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 115.0 in stage 6.0 (TID 564, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14550
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 101.0 in stage 6.0 (TID 550) in 80 ms on localhost (98/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 23:14:18 INFO Executor: Running task 115.0 in stage 6.0 (TID 564)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 97.0 in stage 6.0 (TID 546) in 100 ms on localhost (99/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14848
15/08/21 23:14:18 INFO Executor: Finished task 93.0 in stage 6.0 (TID 542). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 100.0 in stage 6.0 (TID 549) in 85 ms on localhost (100/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15009
15/08/21 23:14:18 INFO TaskSetManager: Starting task 116.0 in stage 6.0 (TID 565, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 116.0 in stage 6.0 (TID 565)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 117.0 in stage 6.0 (TID 566, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 98.0 in stage 6.0 (TID 547) in 108 ms on localhost (101/200)
15/08/21 23:14:18 INFO Executor: Finished task 105.0 in stage 6.0 (TID 554). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15361
15/08/21 23:14:18 INFO Executor: Running task 117.0 in stage 6.0 (TID 566)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 103.0 in stage 6.0 (TID 552) in 74 ms on localhost (102/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 108.0 in stage 6.0 (TID 557). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 110.0 in stage 6.0 (TID 559). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 109.0 in stage 6.0 (TID 558). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 23:14:18 INFO TaskSetManager: Starting task 118.0 in stage 6.0 (TID 567, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 107.0 in stage 6.0 (TID 556). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15557
15/08/21 23:14:18 INFO Executor: Running task 118.0 in stage 6.0 (TID 567)
15/08/21 23:14:18 INFO Executor: Finished task 112.0 in stage 6.0 (TID 561). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 119.0 in stage 6.0 (TID 568, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 102.0 in stage 6.0 (TID 551) in 105 ms on localhost (103/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 119.0 in stage 6.0 (TID 568)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15099
15/08/21 23:14:18 INFO Executor: Finished task 111.0 in stage 6.0 (TID 560). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 114.0 in stage 6.0 (TID 563). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 106.0 in stage 6.0 (TID 555). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 120.0 in stage 6.0 (TID 569, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 120.0 in stage 6.0 (TID 569)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 104.0 in stage 6.0 (TID 553) in 99 ms on localhost (104/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 121.0 in stage 6.0 (TID 570, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 121.0 in stage 6.0 (TID 570)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 93.0 in stage 6.0 (TID 542) in 167 ms on localhost (105/200)
15/08/21 23:14:18 INFO Executor: Finished task 113.0 in stage 6.0 (TID 562). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 105.0 in stage 6.0 (TID 554) in 97 ms on localhost (106/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 122.0 in stage 6.0 (TID 571, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 108.0 in stage 6.0 (TID 557) in 90 ms on localhost (107/200)
15/08/21 23:14:18 INFO Executor: Running task 122.0 in stage 6.0 (TID 571)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 123.0 in stage 6.0 (TID 572, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15029
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 123.0 in stage 6.0 (TID 572)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15306
15/08/21 23:14:18 INFO TaskSetManager: Finished task 110.0 in stage 6.0 (TID 559) in 90 ms on localhost (108/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14842
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14791
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Finished task 115.0 in stage 6.0 (TID 564). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 124.0 in stage 6.0 (TID 573, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO Executor: Running task 124.0 in stage 6.0 (TID 573)
15/08/21 23:14:18 INFO Executor: Finished task 116.0 in stage 6.0 (TID 565). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 23:14:18 INFO TaskSetManager: Finished task 109.0 in stage 6.0 (TID 558) in 100 ms on localhost (109/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 107.0 in stage 6.0 (TID 556) in 106 ms on localhost (110/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15274
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 125.0 in stage 6.0 (TID 574, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 125.0 in stage 6.0 (TID 574)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 126.0 in stage 6.0 (TID 575, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 126.0 in stage 6.0 (TID 575)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15120
15/08/21 23:14:18 INFO TaskSetManager: Starting task 127.0 in stage 6.0 (TID 576, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Running task 127.0 in stage 6.0 (TID 576)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14784
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 120.0 in stage 6.0 (TID 569). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 111.0 in stage 6.0 (TID 560) in 109 ms on localhost (111/200)
15/08/21 23:14:18 INFO Executor: Finished task 121.0 in stage 6.0 (TID 570). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 119.0 in stage 6.0 (TID 568). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 112.0 in stage 6.0 (TID 561) in 103 ms on localhost (112/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 128.0 in stage 6.0 (TID 577, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 128.0 in stage 6.0 (TID 577)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14938
15/08/21 23:14:18 INFO TaskSetManager: Starting task 129.0 in stage 6.0 (TID 578, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 23:14:18 INFO Executor: Running task 129.0 in stage 6.0 (TID 578)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 114.0 in stage 6.0 (TID 563) in 100 ms on localhost (113/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15049
15/08/21 23:14:18 INFO TaskSetManager: Starting task 130.0 in stage 6.0 (TID 579, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 131.0 in stage 6.0 (TID 580, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 131.0 in stage 6.0 (TID 580)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15145
15/08/21 23:14:18 INFO TaskSetManager: Starting task 132.0 in stage 6.0 (TID 581, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 132.0 in stage 6.0 (TID 581)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 106.0 in stage 6.0 (TID 555) in 138 ms on localhost (114/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14942
15/08/21 23:14:18 INFO Executor: Finished task 123.0 in stage 6.0 (TID 572). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 133.0 in stage 6.0 (TID 582, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Finished task 118.0 in stage 6.0 (TID 567). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 134.0 in stage 6.0 (TID 583, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 116.0 in stage 6.0 (TID 565) in 85 ms on localhost (115/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 115.0 in stage 6.0 (TID 564) in 105 ms on localhost (116/200)
15/08/21 23:14:18 INFO Executor: Running task 133.0 in stage 6.0 (TID 582)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Finished task 113.0 in stage 6.0 (TID 562) in 113 ms on localhost (117/200)
15/08/21 23:14:18 INFO Executor: Running task 130.0 in stage 6.0 (TID 579)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 135.0 in stage 6.0 (TID 584, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 135.0 in stage 6.0 (TID 584)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 134.0 in stage 6.0 (TID 583)
15/08/21 23:14:18 INFO Executor: Finished task 122.0 in stage 6.0 (TID 571). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 124.0 in stage 6.0 (TID 573). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO TaskSetManager: Starting task 136.0 in stage 6.0 (TID 585, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 120.0 in stage 6.0 (TID 569) in 76 ms on localhost (118/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 23:14:18 INFO Executor: Running task 136.0 in stage 6.0 (TID 585)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14780
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14923
15/08/21 23:14:18 INFO TaskSetManager: Finished task 121.0 in stage 6.0 (TID 570) in 65 ms on localhost (119/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15368
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 137.0 in stage 6.0 (TID 586, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15010
15/08/21 23:14:18 INFO TaskSetManager: Finished task 119.0 in stage 6.0 (TID 568) in 86 ms on localhost (120/200)
15/08/21 23:14:18 INFO Executor: Finished task 126.0 in stage 6.0 (TID 575). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 23:14:18 INFO Executor: Finished task 117.0 in stage 6.0 (TID 566). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14489
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Finished task 118.0 in stage 6.0 (TID 567) in 94 ms on localhost (121/200)
15/08/21 23:14:18 INFO Executor: Running task 137.0 in stage 6.0 (TID 586)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14687
15/08/21 23:14:18 INFO TaskSetManager: Finished task 123.0 in stage 6.0 (TID 572) in 68 ms on localhost (122/200)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 138.0 in stage 6.0 (TID 587, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO Executor: Running task 138.0 in stage 6.0 (TID 587)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Starting task 139.0 in stage 6.0 (TID 588, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14612
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15609
15/08/21 23:14:18 INFO Executor: Finished task 125.0 in stage 6.0 (TID 574). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14750
15/08/21 23:14:18 INFO Executor: Running task 139.0 in stage 6.0 (TID 588)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 122.0 in stage 6.0 (TID 571) in 79 ms on localhost (123/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 124.0 in stage 6.0 (TID 573) in 76 ms on localhost (124/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 128.0 in stage 6.0 (TID 577). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 127.0 in stage 6.0 (TID 576). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 131.0 in stage 6.0 (TID 580). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 126.0 in stage 6.0 (TID 575) in 70 ms on localhost (125/200)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 140.0 in stage 6.0 (TID 589, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 140.0 in stage 6.0 (TID 589)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 141.0 in stage 6.0 (TID 590, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 142.0 in stage 6.0 (TID 591, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Running task 142.0 in stage 6.0 (TID 591)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 141.0 in stage 6.0 (TID 590)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 135.0 in stage 6.0 (TID 584). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Starting task 143.0 in stage 6.0 (TID 592, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 143.0 in stage 6.0 (TID 592)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 23:14:18 INFO TaskSetManager: Starting task 144.0 in stage 6.0 (TID 593, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO Executor: Running task 144.0 in stage 6.0 (TID 593)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 117.0 in stage 6.0 (TID 566) in 134 ms on localhost (126/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Finished task 132.0 in stage 6.0 (TID 581). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO Executor: Finished task 133.0 in stage 6.0 (TID 582). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 125.0 in stage 6.0 (TID 574) in 84 ms on localhost (127/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15141
15/08/21 23:14:18 INFO Executor: Finished task 136.0 in stage 6.0 (TID 585). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO TaskSetManager: Finished task 127.0 in stage 6.0 (TID 576) in 84 ms on localhost (128/200)
15/08/21 23:14:18 INFO Executor: Finished task 129.0 in stage 6.0 (TID 578). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO TaskSetManager: Finished task 128.0 in stage 6.0 (TID 577) in 70 ms on localhost (129/200)
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Finished task 130.0 in stage 6.0 (TID 579). 1925 bytes result sent to driver
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 14771
15/08/21 23:14:18 INFO TaskSetManager: Starting task 145.0 in stage 6.0 (TID 594, localhost, ANY, 1687 bytes)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO Executor: Running task 145.0 in stage 6.0 (TID 594)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 131.0 in stage 6.0 (TID 580) in 65 ms on localhost (130/200)
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15330
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14479
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO TaskSetManager: Starting task 146.0 in stage 6.0 (TID 595, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14751
15/08/21 23:14:18 INFO Executor: Finished task 134.0 in stage 6.0 (TID 583). 1925 bytes result sent to driver
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14678
15/08/21 23:14:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:18 INFO Executor: Running task 146.0 in stage 6.0 (TID 595)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 147.0 in stage 6.0 (TID 596, localhost, ANY, 1688 bytes)
15/08/21 23:14:18 INFO Executor: Running task 147.0 in stage 6.0 (TID 596)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 148.0 in stage 6.0 (TID 597, localhost, ANY, 1686 bytes)
15/08/21 23:14:18 INFO Executor: Running task 148.0 in stage 6.0 (TID 597)
15/08/21 23:14:18 INFO TaskSetManager: Starting task 149.0 in stage 6.0 (TID 598, localhost, ANY, 1685 bytes)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 135.0 in stage 6.0 (TID 584) in 75 ms on localhost (131/200)
15/08/21 23:14:18 INFO Executor: Running task 149.0 in stage 6.0 (TID 598)
15/08/21 23:14:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 23:14:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:18 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15090
15/08/21 23:14:18 INFO TaskSetManager: Finished task 132.0 in stage 6.0 (TID 581) in 83 ms on localhost (132/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 136.0 in stage 6.0 (TID 585) in 72 ms on localhost (133/200)
15/08/21 23:14:18 INFO TaskSetManager: Finished task 133.0 in stage 6.0 (TID 582) in 82 ms on localhost (134/200)
15/08/21 23:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 150.0 in stage 6.0 (TID 599, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 129.0 in stage 6.0 (TID 578) in 95 ms on localhost (135/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 138.0 in stage 6.0 (TID 587). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Running task 150.0 in stage 6.0 (TID 599)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 151.0 in stage 6.0 (TID 600, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 INFO Executor: Running task 151.0 in stage 6.0 (TID 600)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 152.0 in stage 6.0 (TID 601, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO Executor: Running task 152.0 in stage 6.0 (TID 601)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 153.0 in stage 6.0 (TID 602, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO Executor: Running task 153.0 in stage 6.0 (TID 602)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 130.0 in stage 6.0 (TID 579) in 98 ms on localhost (136/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 140.0 in stage 6.0 (TID 589). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 137.0 in stage 6.0 (TID 586). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15063
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 154.0 in stage 6.0 (TID 603, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Running task 154.0 in stage 6.0 (TID 603)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 155.0 in stage 6.0 (TID 604, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Running task 155.0 in stage 6.0 (TID 604)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15046
15/08/21 23:14:19 INFO TaskSetManager: Finished task 138.0 in stage 6.0 (TID 587) in 74 ms on localhost (137/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 142.0 in stage 6.0 (TID 591). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 139.0 in stage 6.0 (TID 588). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 134.0 in stage 6.0 (TID 583) in 101 ms on localhost (138/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15524
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 156.0 in stage 6.0 (TID 605, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Starting task 157.0 in stage 6.0 (TID 606, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15114
15/08/21 23:14:19 INFO TaskSetManager: Finished task 140.0 in stage 6.0 (TID 589) in 69 ms on localhost (139/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14802
15/08/21 23:14:19 INFO TaskSetManager: Finished task 137.0 in stage 6.0 (TID 586) in 94 ms on localhost (140/200)
15/08/21 23:14:19 INFO Executor: Running task 156.0 in stage 6.0 (TID 605)
15/08/21 23:14:19 INFO Executor: Running task 157.0 in stage 6.0 (TID 606)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 142.0 in stage 6.0 (TID 591) in 71 ms on localhost (141/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 139.0 in stage 6.0 (TID 588) in 88 ms on localhost (142/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14731
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15679
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 144.0 in stage 6.0 (TID 593). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15096
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Starting task 158.0 in stage 6.0 (TID 607, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 143.0 in stage 6.0 (TID 592). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 141.0 in stage 6.0 (TID 590). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14952
15/08/21 23:14:19 INFO TaskSetManager: Finished task 144.0 in stage 6.0 (TID 593) in 75 ms on localhost (143/200)
15/08/21 23:14:19 INFO Executor: Running task 158.0 in stage 6.0 (TID 607)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 143.0 in stage 6.0 (TID 592) in 78 ms on localhost (144/200)
15/08/21 23:14:19 INFO Executor: Finished task 145.0 in stage 6.0 (TID 594). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Starting task 159.0 in stage 6.0 (TID 608, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15142
15/08/21 23:14:19 INFO Executor: Running task 159.0 in stage 6.0 (TID 608)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 160.0 in stage 6.0 (TID 609, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 161.0 in stage 6.0 (TID 610, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Running task 161.0 in stage 6.0 (TID 610)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 141.0 in stage 6.0 (TID 590) in 91 ms on localhost (145/200)
15/08/21 23:14:19 INFO Executor: Finished task 153.0 in stage 6.0 (TID 602). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Running task 160.0 in stage 6.0 (TID 609)
15/08/21 23:14:19 INFO Executor: Finished task 149.0 in stage 6.0 (TID 598). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14991
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 145.0 in stage 6.0 (TID 594) in 74 ms on localhost (146/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15726
15/08/21 23:14:19 INFO Executor: Finished task 148.0 in stage 6.0 (TID 597). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 150.0 in stage 6.0 (TID 599). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 151.0 in stage 6.0 (TID 600). 1925 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 152.0 in stage 6.0 (TID 601). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 155.0 in stage 6.0 (TID 604). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15463
15/08/21 23:14:19 INFO TaskSetManager: Starting task 162.0 in stage 6.0 (TID 611, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO Executor: Running task 162.0 in stage 6.0 (TID 611)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 163.0 in stage 6.0 (TID 612, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO Executor: Finished task 147.0 in stage 6.0 (TID 596). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 153.0 in stage 6.0 (TID 602) in 59 ms on localhost (147/200)
15/08/21 23:14:19 INFO Executor: Running task 163.0 in stage 6.0 (TID 612)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 164.0 in stage 6.0 (TID 613, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 146.0 in stage 6.0 (TID 595). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Running task 164.0 in stage 6.0 (TID 613)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14755
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 165.0 in stage 6.0 (TID 614, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 148.0 in stage 6.0 (TID 597) in 89 ms on localhost (148/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 149.0 in stage 6.0 (TID 598) in 88 ms on localhost (149/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 23:14:19 INFO Executor: Running task 165.0 in stage 6.0 (TID 614)
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15007
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 154.0 in stage 6.0 (TID 603). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15396
15/08/21 23:14:19 INFO Executor: Finished task 157.0 in stage 6.0 (TID 606). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15203
15/08/21 23:14:19 INFO TaskSetManager: Starting task 166.0 in stage 6.0 (TID 615, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15313
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 23:14:19 INFO Executor: Running task 166.0 in stage 6.0 (TID 615)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 156.0 in stage 6.0 (TID 605). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 150.0 in stage 6.0 (TID 599) in 87 ms on localhost (150/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15191
15/08/21 23:14:19 INFO TaskSetManager: Starting task 167.0 in stage 6.0 (TID 616, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO Executor: Running task 167.0 in stage 6.0 (TID 616)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 168.0 in stage 6.0 (TID 617, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 151.0 in stage 6.0 (TID 600) in 97 ms on localhost (151/200)
15/08/21 23:14:19 INFO Executor: Running task 168.0 in stage 6.0 (TID 617)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 152.0 in stage 6.0 (TID 601) in 98 ms on localhost (152/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 161.0 in stage 6.0 (TID 610). 1925 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 159.0 in stage 6.0 (TID 608). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14560
15/08/21 23:14:19 INFO Executor: Finished task 158.0 in stage 6.0 (TID 607). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 169.0 in stage 6.0 (TID 618, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO Executor: Running task 169.0 in stage 6.0 (TID 618)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 147.0 in stage 6.0 (TID 596) in 130 ms on localhost (153/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 155.0 in stage 6.0 (TID 604) in 104 ms on localhost (154/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 160.0 in stage 6.0 (TID 609). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 163.0 in stage 6.0 (TID 612). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14845
15/08/21 23:14:19 INFO TaskSetManager: Starting task 170.0 in stage 6.0 (TID 619, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14440
15/08/21 23:14:19 INFO Executor: Running task 170.0 in stage 6.0 (TID 619)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15364
15/08/21 23:14:19 INFO Executor: Finished task 162.0 in stage 6.0 (TID 611). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 171.0 in stage 6.0 (TID 620, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Running task 171.0 in stage 6.0 (TID 620)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 154.0 in stage 6.0 (TID 603) in 114 ms on localhost (155/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 146.0 in stage 6.0 (TID 595) in 146 ms on localhost (156/200)
15/08/21 23:14:19 INFO Executor: Finished task 164.0 in stage 6.0 (TID 613). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 172.0 in stage 6.0 (TID 621, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 INFO Executor: Running task 172.0 in stage 6.0 (TID 621)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 173.0 in stage 6.0 (TID 622, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 157.0 in stage 6.0 (TID 606) in 107 ms on localhost (157/200)
15/08/21 23:14:19 INFO Executor: Running task 173.0 in stage 6.0 (TID 622)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 156.0 in stage 6.0 (TID 605) in 113 ms on localhost (158/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15358
15/08/21 23:14:19 INFO TaskSetManager: Starting task 174.0 in stage 6.0 (TID 623, localhost, ANY, 1685 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Running task 174.0 in stage 6.0 (TID 623)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 23:14:19 INFO TaskSetManager: Starting task 175.0 in stage 6.0 (TID 624, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 161.0 in stage 6.0 (TID 610) in 91 ms on localhost (159/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Running task 175.0 in stage 6.0 (TID 624)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15137
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 176.0 in stage 6.0 (TID 625, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO Executor: Running task 176.0 in stage 6.0 (TID 625)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 177.0 in stage 6.0 (TID 626, localhost, ANY, 1685 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Running task 177.0 in stage 6.0 (TID 626)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 167.0 in stage 6.0 (TID 616). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 165.0 in stage 6.0 (TID 614). 1925 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 178.0 in stage 6.0 (TID 627, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15101
15/08/21 23:14:19 INFO Executor: Finished task 168.0 in stage 6.0 (TID 617). 1925 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 166.0 in stage 6.0 (TID 615). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15586
15/08/21 23:14:19 INFO Executor: Running task 178.0 in stage 6.0 (TID 627)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 159.0 in stage 6.0 (TID 608) in 108 ms on localhost (160/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 158.0 in stage 6.0 (TID 607) in 118 ms on localhost (161/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 163.0 in stage 6.0 (TID 612) in 95 ms on localhost (162/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 160.0 in stage 6.0 (TID 609) in 114 ms on localhost (163/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Starting task 179.0 in stage 6.0 (TID 628, localhost, ANY, 1685 bytes)
15/08/21 23:14:19 INFO Executor: Finished task 169.0 in stage 6.0 (TID 618). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 162.0 in stage 6.0 (TID 611) in 109 ms on localhost (164/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15013
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15117
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15344
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15235
15/08/21 23:14:19 INFO Executor: Running task 179.0 in stage 6.0 (TID 628)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 180.0 in stage 6.0 (TID 629, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO Executor: Running task 180.0 in stage 6.0 (TID 629)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 181.0 in stage 6.0 (TID 630, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 164.0 in stage 6.0 (TID 613) in 100 ms on localhost (165/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14613
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 167.0 in stage 6.0 (TID 616) in 69 ms on localhost (166/200)
15/08/21 23:14:19 INFO Executor: Running task 181.0 in stage 6.0 (TID 630)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 182.0 in stage 6.0 (TID 631, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO Executor: Running task 182.0 in stage 6.0 (TID 631)
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 183.0 in stage 6.0 (TID 632, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 INFO Executor: Running task 183.0 in stage 6.0 (TID 632)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 184.0 in stage 6.0 (TID 633, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 170.0 in stage 6.0 (TID 619). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15104
15/08/21 23:14:19 INFO Executor: Running task 184.0 in stage 6.0 (TID 633)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 185.0 in stage 6.0 (TID 634, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 165.0 in stage 6.0 (TID 614) in 105 ms on localhost (167/200)
15/08/21 23:14:19 INFO Executor: Finished task 171.0 in stage 6.0 (TID 620). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 168.0 in stage 6.0 (TID 617) in 78 ms on localhost (168/200)
15/08/21 23:14:19 INFO Executor: Running task 185.0 in stage 6.0 (TID 634)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 166.0 in stage 6.0 (TID 615) in 102 ms on localhost (169/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 186.0 in stage 6.0 (TID 635, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO Executor: Running task 186.0 in stage 6.0 (TID 635)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 169.0 in stage 6.0 (TID 618) in 75 ms on localhost (170/200)
15/08/21 23:14:19 INFO Executor: Finished task 173.0 in stage 6.0 (TID 622). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 187.0 in stage 6.0 (TID 636, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO Executor: Finished task 176.0 in stage 6.0 (TID 625). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 188.0 in stage 6.0 (TID 637, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO Executor: Running task 187.0 in stage 6.0 (TID 636)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Running task 188.0 in stage 6.0 (TID 637)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 170.0 in stage 6.0 (TID 619) in 78 ms on localhost (171/200)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 189.0 in stage 6.0 (TID 638, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Running task 189.0 in stage 6.0 (TID 638)
15/08/21 23:14:19 INFO Executor: Finished task 174.0 in stage 6.0 (TID 623). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 172.0 in stage 6.0 (TID 621). 1925 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 177.0 in stage 6.0 (TID 626). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 171.0 in stage 6.0 (TID 620) in 78 ms on localhost (172/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 175.0 in stage 6.0 (TID 624). 1925 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15147
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14677
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15614
15/08/21 23:14:19 INFO TaskSetManager: Finished task 176.0 in stage 6.0 (TID 625) in 63 ms on localhost (173/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15386
15/08/21 23:14:19 INFO TaskSetManager: Starting task 190.0 in stage 6.0 (TID 639, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO Executor: Running task 190.0 in stage 6.0 (TID 639)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 173.0 in stage 6.0 (TID 622) in 76 ms on localhost (174/200)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 191.0 in stage 6.0 (TID 640, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO Executor: Running task 191.0 in stage 6.0 (TID 640)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Starting task 192.0 in stage 6.0 (TID 641, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14597
15/08/21 23:14:19 INFO TaskSetManager: Starting task 193.0 in stage 6.0 (TID 642, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 174.0 in stage 6.0 (TID 623) in 77 ms on localhost (175/200)
15/08/21 23:14:19 INFO Executor: Running task 193.0 in stage 6.0 (TID 642)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 177.0 in stage 6.0 (TID 626) in 72 ms on localhost (176/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15658
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Running task 192.0 in stage 6.0 (TID 641)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 175.0 in stage 6.0 (TID 624) in 77 ms on localhost (177/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 172.0 in stage 6.0 (TID 621) in 86 ms on localhost (178/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15470
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 178.0 in stage 6.0 (TID 627). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Starting task 194.0 in stage 6.0 (TID 643, localhost, ANY, 1684 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14788
15/08/21 23:14:19 INFO Executor: Running task 194.0 in stage 6.0 (TID 643)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 178.0 in stage 6.0 (TID 627) in 77 ms on localhost (179/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14982
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 23:14:19 INFO Executor: Finished task 183.0 in stage 6.0 (TID 632). 1925 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 23:14:19 INFO Executor: Finished task 180.0 in stage 6.0 (TID 629). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 183.0 in stage 6.0 (TID 632) in 59 ms on localhost (180/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 23:14:19 INFO Executor: Finished task 182.0 in stage 6.0 (TID 631). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 184.0 in stage 6.0 (TID 633). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:19 INFO TaskSetManager: Starting task 195.0 in stage 6.0 (TID 644, localhost, ANY, 1686 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Starting task 196.0 in stage 6.0 (TID 645, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15509
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15039
15/08/21 23:14:19 INFO TaskSetManager: Finished task 180.0 in stage 6.0 (TID 629) in 74 ms on localhost (181/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 182.0 in stage 6.0 (TID 631) in 69 ms on localhost (182/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15043
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 23:14:19 INFO Executor: Running task 196.0 in stage 6.0 (TID 645)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15038
15/08/21 23:14:19 INFO Executor: Finished task 186.0 in stage 6.0 (TID 635). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 181.0 in stage 6.0 (TID 630). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Running task 195.0 in stage 6.0 (TID 644)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 197.0 in stage 6.0 (TID 646, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO Executor: Running task 197.0 in stage 6.0 (TID 646)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 198.0 in stage 6.0 (TID 647, localhost, ANY, 1687 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Running task 198.0 in stage 6.0 (TID 647)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15625
15/08/21 23:14:19 INFO Executor: Finished task 188.0 in stage 6.0 (TID 637). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 185.0 in stage 6.0 (TID 634). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 184.0 in stage 6.0 (TID 633) in 69 ms on localhost (183/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 186.0 in stage 6.0 (TID 635) in 65 ms on localhost (184/200)
15/08/21 23:14:19 INFO Executor: Finished task 187.0 in stage 6.0 (TID 636). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 199.0 in stage 6.0 (TID 648, localhost, ANY, 1688 bytes)
15/08/21 23:14:19 INFO Executor: Running task 199.0 in stage 6.0 (TID 648)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 181.0 in stage 6.0 (TID 630) in 87 ms on localhost (185/200)
15/08/21 23:14:19 INFO Executor: Finished task 179.0 in stage 6.0 (TID 628). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 188.0 in stage 6.0 (TID 637) in 64 ms on localhost (186/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 187.0 in stage 6.0 (TID 636) in 76 ms on localhost (187/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 185.0 in stage 6.0 (TID 634) in 80 ms on localhost (188/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15001
15/08/21 23:14:19 INFO TaskSetManager: Finished task 179.0 in stage 6.0 (TID 628) in 101 ms on localhost (189/200)
15/08/21 23:14:19 INFO Executor: Finished task 192.0 in stage 6.0 (TID 641). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 190.0 in stage 6.0 (TID 639). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 191.0 in stage 6.0 (TID 640). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 23:14:19 INFO Executor: Finished task 189.0 in stage 6.0 (TID 638). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 192.0 in stage 6.0 (TID 641) in 54 ms on localhost (190/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 194.0 in stage 6.0 (TID 643). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14843
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 191.0 in stage 6.0 (TID 640) in 59 ms on localhost (191/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 190.0 in stage 6.0 (TID 639) in 61 ms on localhost (192/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15023
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 189.0 in stage 6.0 (TID 638) in 69 ms on localhost (193/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 15302
15/08/21 23:14:19 INFO Executor: Finished task 193.0 in stage 6.0 (TID 642). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 194.0 in stage 6.0 (TID 643) in 57 ms on localhost (194/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 193.0 in stage 6.0 (TID 642) in 67 ms on localhost (195/200)
15/08/21 23:14:19 INFO Executor: Finished task 197.0 in stage 6.0 (TID 646). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 199.0 in stage 6.0 (TID 648). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 196.0 in stage 6.0 (TID 645). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 198.0 in stage 6.0 (TID 647). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 197.0 in stage 6.0 (TID 646) in 61 ms on localhost (196/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 199.0 in stage 6.0 (TID 648) in 52 ms on localhost (197/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 198.0 in stage 6.0 (TID 647) in 62 ms on localhost (198/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 196.0 in stage 6.0 (TID 645) in 72 ms on localhost (199/200)
15/08/21 23:14:19 INFO Executor: Finished task 195.0 in stage 6.0 (TID 644). 1925 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 195.0 in stage 6.0 (TID 644) in 77 ms on localhost (200/200)
15/08/21 23:14:19 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/21 23:14:19 INFO DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:423) finished in 1.152 s
15/08/21 23:14:19 INFO DAGScheduler: looking for newly runnable stages
15/08/21 23:14:19 INFO DAGScheduler: running: Set()
15/08/21 23:14:19 INFO DAGScheduler: waiting: Set(ResultStage 7)
15/08/21 23:14:19 INFO DAGScheduler: failed: Set()
15/08/21 23:14:19 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7a47cef5
15/08/21 23:14:19 INFO StatsReportListener: task runtime:(count: 200, mean: 106.160000, stdev: 35.722044, max: 224.000000, min: 52.000000)
15/08/21 23:14:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:19 INFO StatsReportListener: 	52.0 ms	62.0 ms	68.0 ms	77.0 ms	100.0 ms	126.0 ms	159.0 ms	183.0 ms	224.0 ms
15/08/21 23:14:19 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 31.000000, stdev: 0.000000, max: 31.000000, min: 31.000000)
15/08/21 23:14:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:19 INFO StatsReportListener: 	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B
15/08/21 23:14:19 INFO StatsReportListener: task result size:(count: 200, mean: 1925.000000, stdev: 0.000000, max: 1925.000000, min: 1925.000000)
15/08/21 23:14:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:19 INFO StatsReportListener: 	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B
15/08/21 23:14:19 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 54.157693, stdev: 12.120061, max: 85.507246, min: 26.174497)
15/08/21 23:14:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:19 INFO StatsReportListener: 	26 %	34 %	39 %	46 %	53 %	61 %	72 %	76 %	86 %
15/08/21 23:14:19 INFO DAGScheduler: Missing parents for ResultStage 7: List()
15/08/21 23:14:19 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[43] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 23:14:19 INFO StatsReportListener: other time pct: (count: 200, mean: 45.842307, stdev: 12.120061, max: 73.825503, min: 14.492754)
15/08/21 23:14:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:19 INFO StatsReportListener: 	14 %	24 %	28 %	40 %	47 %	54 %	61 %	67 %	74 %
15/08/21 23:14:19 INFO MemoryStore: ensureFreeSpace(12056) called with curMem=1878011, maxMem=22226833244
15/08/21 23:14:19 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 11.8 KB, free 20.7 GB)
15/08/21 23:14:19 INFO MemoryStore: ensureFreeSpace(5718) called with curMem=1890067, maxMem=22226833244
15/08/21 23:14:19 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.6 KB, free 20.7 GB)
15/08/21 23:14:19 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:40586 (size: 5.6 KB, free: 20.7 GB)
15/08/21 23:14:19 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/21 23:14:19 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 7 (MapPartitionsRDD[43] at processCmd at CliDriver.java:423)
15/08/21 23:14:19 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/21 23:14:19 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 649, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 650, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 651, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 652, localhost, ANY, 1980 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 653, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 654, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 655, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 656, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 657, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 658, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 659, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 660, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 661, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 662, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 663, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 664, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 0.0 in stage 7.0 (TID 649)
15/08/21 23:14:19 INFO Executor: Running task 4.0 in stage 7.0 (TID 653)
15/08/21 23:14:19 INFO Executor: Running task 1.0 in stage 7.0 (TID 650)
15/08/21 23:14:19 INFO Executor: Running task 5.0 in stage 7.0 (TID 654)
15/08/21 23:14:19 INFO Executor: Running task 2.0 in stage 7.0 (TID 651)
15/08/21 23:14:19 INFO Executor: Running task 7.0 in stage 7.0 (TID 656)
15/08/21 23:14:19 INFO Executor: Running task 3.0 in stage 7.0 (TID 652)
15/08/21 23:14:19 INFO Executor: Running task 9.0 in stage 7.0 (TID 658)
15/08/21 23:14:19 INFO Executor: Running task 8.0 in stage 7.0 (TID 657)
15/08/21 23:14:19 INFO Executor: Running task 6.0 in stage 7.0 (TID 655)
15/08/21 23:14:19 INFO Executor: Running task 10.0 in stage 7.0 (TID 659)
15/08/21 23:14:19 INFO Executor: Running task 11.0 in stage 7.0 (TID 660)
15/08/21 23:14:19 INFO Executor: Running task 12.0 in stage 7.0 (TID 661)
15/08/21 23:14:19 INFO Executor: Running task 13.0 in stage 7.0 (TID 662)
15/08/21 23:14:19 INFO Executor: Running task 15.0 in stage 7.0 (TID 664)
15/08/21 23:14:19 INFO Executor: Running task 14.0 in stage 7.0 (TID 663)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15570
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15062
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14824
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15359
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14967
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15501
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14795
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15038
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14876
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14379
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14826
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14825
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14972
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14989
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15340
15/08/21 23:14:19 INFO Executor: Finished task 8.0 in stage 7.0 (TID 657). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 14.0 in stage 7.0 (TID 663). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 6.0 in stage 7.0 (TID 655). 3232 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 665, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 666, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 17.0 in stage 7.0 (TID 666)
15/08/21 23:14:19 INFO Executor: Running task 16.0 in stage 7.0 (TID 665)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 667, localhost, ANY, 1980 bytes)
15/08/21 23:14:19 INFO Executor: Running task 18.0 in stage 7.0 (TID 667)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 657) in 214 ms on localhost (1/200)
15/08/21 23:14:19 INFO Executor: Finished task 15.0 in stage 7.0 (TID 664). 3234 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 12.0 in stage 7.0 (TID 661). 3229 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 668, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO Executor: Running task 19.0 in stage 7.0 (TID 668)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 669, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 655) in 219 ms on localhost (2/200)
15/08/21 23:14:19 INFO Executor: Running task 20.0 in stage 7.0 (TID 669)
15/08/21 23:14:19 INFO Executor: Finished task 5.0 in stage 7.0 (TID 654). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO Executor: Finished task 4.0 in stage 7.0 (TID 653). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO Executor: Finished task 10.0 in stage 7.0 (TID 659). 3227 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 663) in 218 ms on localhost (3/200)
15/08/21 23:14:19 INFO Executor: Finished task 9.0 in stage 7.0 (TID 658). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 664) in 218 ms on localhost (4/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 661) in 221 ms on localhost (5/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO Executor: Finished task 13.0 in stage 7.0 (TID 662). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 670, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Finished task 0.0 in stage 7.0 (TID 649). 3228 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 3.0 in stage 7.0 (TID 652). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 7.0 in stage 7.0 (TID 656). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 2.0 in stage 7.0 (TID 651). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Running task 21.0 in stage 7.0 (TID 670)
15/08/21 23:14:19 INFO Executor: Finished task 1.0 in stage 7.0 (TID 650). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 671, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO Executor: Running task 22.0 in stage 7.0 (TID 671)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 672, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Running task 23.0 in stage 7.0 (TID 672)
15/08/21 23:14:19 INFO Executor: Finished task 11.0 in stage 7.0 (TID 660). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 673, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO Executor: Running task 24.0 in stage 7.0 (TID 673)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 674, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 675, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 26.0 in stage 7.0 (TID 675)
15/08/21 23:14:19 INFO Executor: Running task 25.0 in stage 7.0 (TID 674)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 654) in 242 ms on localhost (6/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 658) in 240 ms on localhost (7/200)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 676, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO Executor: Running task 27.0 in stage 7.0 (TID 676)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 653) in 245 ms on localhost (8/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 659) in 245 ms on localhost (9/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 652) in 253 ms on localhost (10/200)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 677, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 28.0 in stage 7.0 (TID 677)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 678, localhost, ANY, 1980 bytes)
15/08/21 23:14:19 INFO Executor: Running task 29.0 in stage 7.0 (TID 678)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 649) in 269 ms on localhost (11/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 662) in 254 ms on localhost (12/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 679, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Running task 30.0 in stage 7.0 (TID 679)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 656) in 264 ms on localhost (13/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14985
15/08/21 23:14:19 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 680, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO Executor: Running task 31.0 in stage 7.0 (TID 680)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 651) in 270 ms on localhost (14/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 660) in 269 ms on localhost (15/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 650) in 275 ms on localhost (16/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15131
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14939
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15132
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14793
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 17.0 in stage 7.0 (TID 666). 3233 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 681, localhost, ANY, 1980 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Running task 32.0 in stage 7.0 (TID 681)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14736
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15055
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 666) in 98 ms on localhost (17/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Finished task 20.0 in stage 7.0 (TID 669). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 682, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Running task 33.0 in stage 7.0 (TID 682)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 669) in 95 ms on localhost (18/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 15238
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14878
15/08/21 23:14:19 INFO Executor: Finished task 18.0 in stage 7.0 (TID 667). 3229 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO Executor: Finished task 19.0 in stage 7.0 (TID 668). 3229 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14934
15/08/21 23:14:19 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 683, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 34.0 in stage 7.0 (TID 683)
15/08/21 23:14:19 INFO Executor: Finished task 21.0 in stage 7.0 (TID 670). 3233 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14906
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 23:14:19 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 684, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15296
15/08/21 23:14:19 INFO Executor: Running task 35.0 in stage 7.0 (TID 684)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 685, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO Executor: Running task 36.0 in stage 7.0 (TID 685)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 667) in 130 ms on localhost (19/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 668) in 118 ms on localhost (20/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 670) in 108 ms on localhost (21/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO Executor: Finished task 24.0 in stage 7.0 (TID 673). 3233 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 22.0 in stage 7.0 (TID 671). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 23.0 in stage 7.0 (TID 672). 3232 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 686, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO Executor: Running task 37.0 in stage 7.0 (TID 686)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 687, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO Executor: Running task 38.0 in stage 7.0 (TID 687)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 688, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 673) in 109 ms on localhost (22/200)
15/08/21 23:14:19 INFO Executor: Finished task 29.0 in stage 7.0 (TID 678). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 671) in 117 ms on localhost (23/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 672) in 115 ms on localhost (24/200)
15/08/21 23:14:19 INFO Executor: Finished task 28.0 in stage 7.0 (TID 677). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 30.0 in stage 7.0 (TID 679). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Running task 39.0 in stage 7.0 (TID 688)
15/08/21 23:14:19 INFO Executor: Finished task 26.0 in stage 7.0 (TID 675). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 689, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Running task 40.0 in stage 7.0 (TID 689)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 690, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Running task 41.0 in stage 7.0 (TID 690)
15/08/21 23:14:19 INFO Executor: Finished task 25.0 in stage 7.0 (TID 674). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 27.0 in stage 7.0 (TID 676). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 678) in 98 ms on localhost (25/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 677) in 102 ms on localhost (26/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 679) in 97 ms on localhost (27/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 691, localhost, ANY, 1980 bytes)
15/08/21 23:14:19 INFO Executor: Running task 42.0 in stage 7.0 (TID 691)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 692, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Running task 43.0 in stage 7.0 (TID 692)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 693, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 694, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 44.0 in stage 7.0 (TID 693)
15/08/21 23:14:19 INFO Executor: Running task 45.0 in stage 7.0 (TID 694)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 675) in 130 ms on localhost (28/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 674) in 133 ms on localhost (29/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 676) in 129 ms on localhost (30/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15603
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15404
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14590
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15302
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 15039
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15566
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15545
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14884
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14693
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14809
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15636
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14772
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14819
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14944
15/08/21 23:14:19 INFO Executor: Finished task 31.0 in stage 7.0 (TID 680). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14831
15/08/21 23:14:19 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 695, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Finished task 16.0 in stage 7.0 (TID 665). 3231 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Running task 46.0 in stage 7.0 (TID 695)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 696, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Finished task 39.0 in stage 7.0 (TID 688). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO Executor: Finished task 36.0 in stage 7.0 (TID 685). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 697, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 665) in 284 ms on localhost (31/200)
15/08/21 23:14:19 INFO Executor: Running task 47.0 in stage 7.0 (TID 696)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 680) in 219 ms on localhost (32/200)
15/08/21 23:14:19 INFO Executor: Finished task 41.0 in stage 7.0 (TID 690). 3233 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 43.0 in stage 7.0 (TID 692). 3233 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 32.0 in stage 7.0 (TID 681). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 698, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 49.0 in stage 7.0 (TID 698)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 699, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 688) in 148 ms on localhost (33/200)
15/08/21 23:14:19 INFO Executor: Running task 50.0 in stage 7.0 (TID 699)
15/08/21 23:14:19 INFO Executor: Running task 48.0 in stage 7.0 (TID 697)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 685) in 163 ms on localhost (34/200)
15/08/21 23:14:19 INFO Executor: Finished task 34.0 in stage 7.0 (TID 683). 3230 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 700, localhost, ANY, 1980 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 690) in 142 ms on localhost (35/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO Executor: Finished task 42.0 in stage 7.0 (TID 691). 3229 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO Executor: Finished task 38.0 in stage 7.0 (TID 687). 3233 bytes result sent to driver
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 692) in 135 ms on localhost (36/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 681) in 203 ms on localhost (37/200)
15/08/21 23:14:19 INFO Executor: Running task 51.0 in stage 7.0 (TID 700)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 701, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO Executor: Running task 52.0 in stage 7.0 (TID 701)
15/08/21 23:14:19 INFO Executor: Finished task 45.0 in stage 7.0 (TID 694). 3229 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 702, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO Executor: Running task 53.0 in stage 7.0 (TID 702)
15/08/21 23:14:19 INFO Executor: Finished task 40.0 in stage 7.0 (TID 689). 3228 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 703, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO Executor: Running task 54.0 in stage 7.0 (TID 703)
15/08/21 23:14:19 INFO Executor: Finished task 44.0 in stage 7.0 (TID 693). 3229 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 704, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 683) in 188 ms on localhost (38/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 691) in 147 ms on localhost (39/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:19 INFO Executor: Running task 55.0 in stage 7.0 (TID 704)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 705, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Running task 56.0 in stage 7.0 (TID 705)
15/08/21 23:14:19 INFO Executor: Finished task 37.0 in stage 7.0 (TID 686). 3232 bytes result sent to driver
15/08/21 23:14:19 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 694) in 157 ms on localhost (40/200)
15/08/21 23:14:19 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 687) in 181 ms on localhost (41/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 689) in 178 ms on localhost (42/200)
15/08/21 23:14:19 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 706, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 INFO Executor: Running task 57.0 in stage 7.0 (TID 706)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 707, localhost, ANY, 1979 bytes)
15/08/21 23:14:19 INFO Executor: Running task 58.0 in stage 7.0 (TID 707)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 708, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 693) in 176 ms on localhost (43/200)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 686) in 204 ms on localhost (44/200)
15/08/21 23:14:19 INFO Executor: Running task 59.0 in stage 7.0 (TID 708)
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO Executor: Finished task 33.0 in stage 7.0 (TID 682). 3233 bytes result sent to driver
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15129
15/08/21 23:14:19 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 709, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15347
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 682) in 249 ms on localhost (45/200)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Running task 60.0 in stage 7.0 (TID 709)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Finished task 35.0 in stage 7.0 (TID 684). 3228 bytes result sent to driver
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 710, localhost, ANY, 1981 bytes)
15/08/21 23:14:19 INFO Executor: Running task 61.0 in stage 7.0 (TID 710)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 684) in 237 ms on localhost (46/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14836
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14829
15/08/21 23:14:19 INFO Executor: Finished task 46.0 in stage 7.0 (TID 695). 3233 bytes result sent to driver
15/08/21 23:14:19 INFO Executor: Finished task 50.0 in stage 7.0 (TID 699). 3232 bytes result sent to driver
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 711, localhost, ANY, 1983 bytes)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO Executor: Running task 62.0 in stage 7.0 (TID 711)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 712, localhost, ANY, 1982 bytes)
15/08/21 23:14:19 INFO Executor: Running task 63.0 in stage 7.0 (TID 712)
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 695) in 102 ms on localhost (47/200)
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14814
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:19 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 699) in 89 ms on localhost (48/200)
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15302
15/08/21 23:14:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 23:14:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14913
15/08/21 23:14:19 INFO Executor: Finished task 49.0 in stage 7.0 (TID 698). 3229 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 713, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Running task 64.0 in stage 7.0 (TID 713)
15/08/21 23:14:20 INFO Executor: Finished task 53.0 in stage 7.0 (TID 702). 3229 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 714, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Running task 65.0 in stage 7.0 (TID 714)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 698) in 106 ms on localhost (49/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 702) in 94 ms on localhost (50/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15489
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15164
15/08/21 23:14:20 INFO Executor: Finished task 51.0 in stage 7.0 (TID 700). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 14992
15/08/21 23:14:20 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 715, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 66.0 in stage 7.0 (TID 715)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 48.0 in stage 7.0 (TID 697). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15549
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 700) in 126 ms on localhost (51/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14883
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 47.0 in stage 7.0 (TID 696). 3230 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 716, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 67.0 in stage 7.0 (TID 716)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 697) in 146 ms on localhost (52/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 58.0 in stage 7.0 (TID 707). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15282
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 717, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 68.0 in stage 7.0 (TID 717)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 718, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15500
15/08/21 23:14:20 INFO Executor: Finished task 55.0 in stage 7.0 (TID 704). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 696) in 160 ms on localhost (53/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15107
15/08/21 23:14:20 INFO Executor: Running task 69.0 in stage 7.0 (TID 718)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 56.0 in stage 7.0 (TID 705). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 54.0 in stage 7.0 (TID 703). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 719, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 707) in 110 ms on localhost (54/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 23:14:20 INFO Executor: Finished task 57.0 in stage 7.0 (TID 706). 3228 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 70.0 in stage 7.0 (TID 719)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 720, localhost, ANY, 1984 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Running task 71.0 in stage 7.0 (TID 720)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 23:14:20 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 721, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 704) in 144 ms on localhost (55/200)
15/08/21 23:14:20 INFO Executor: Running task 72.0 in stage 7.0 (TID 721)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 705) in 140 ms on localhost (56/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 15103
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 62.0 in stage 7.0 (TID 711). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 703) in 148 ms on localhost (57/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 52.0 in stage 7.0 (TID 701). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 722, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15795
15/08/21 23:14:20 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 723, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 74.0 in stage 7.0 (TID 723)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 724, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 59.0 in stage 7.0 (TID 708). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 706) in 132 ms on localhost (58/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 711) in 91 ms on localhost (59/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 75.0 in stage 7.0 (TID 724)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 725, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 76.0 in stage 7.0 (TID 725)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 701) in 167 ms on localhost (60/200)
15/08/21 23:14:20 INFO Executor: Running task 73.0 in stage 7.0 (TID 722)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 708) in 137 ms on localhost (61/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15571
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Finished task 60.0 in stage 7.0 (TID 709). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 65.0 in stage 7.0 (TID 714). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 63.0 in stage 7.0 (TID 712). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 726, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15079
15/08/21 23:14:20 INFO Executor: Running task 77.0 in stage 7.0 (TID 726)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 727, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 78.0 in stage 7.0 (TID 727)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 709) in 135 ms on localhost (62/200)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 728, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 714) in 100 ms on localhost (63/200)
15/08/21 23:14:20 INFO Executor: Finished task 61.0 in stage 7.0 (TID 710). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 79.0 in stage 7.0 (TID 728)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15211
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 729, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15018
15/08/21 23:14:20 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 712) in 123 ms on localhost (64/200)
15/08/21 23:14:20 INFO Executor: Running task 80.0 in stage 7.0 (TID 729)
15/08/21 23:14:20 INFO Executor: Finished task 64.0 in stage 7.0 (TID 713). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 730, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Running task 81.0 in stage 7.0 (TID 730)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 710) in 145 ms on localhost (65/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 713) in 115 ms on localhost (66/200)
15/08/21 23:14:20 INFO Executor: Finished task 66.0 in stage 7.0 (TID 715). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14854
15/08/21 23:14:20 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 731, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 715) in 110 ms on localhost (67/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 82.0 in stage 7.0 (TID 731)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14564
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15207
15/08/21 23:14:20 INFO Executor: Finished task 71.0 in stage 7.0 (TID 720). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 732, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Running task 83.0 in stage 7.0 (TID 732)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14864
15/08/21 23:14:20 INFO Executor: Finished task 68.0 in stage 7.0 (TID 717). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 720) in 89 ms on localhost (68/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15183
15/08/21 23:14:20 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 733, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Running task 84.0 in stage 7.0 (TID 733)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14508
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15058
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 67.0 in stage 7.0 (TID 716). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 734, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Running task 85.0 in stage 7.0 (TID 734)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 74.0 in stage 7.0 (TID 723). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15333
15/08/21 23:14:20 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 735, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 716) in 128 ms on localhost (69/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Running task 86.0 in stage 7.0 (TID 735)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 717) in 121 ms on localhost (70/200)
15/08/21 23:14:20 INFO Executor: Finished task 76.0 in stage 7.0 (TID 725). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14838
15/08/21 23:14:20 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 736, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 723) in 99 ms on localhost (71/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 87.0 in stage 7.0 (TID 736)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 725) in 98 ms on localhost (72/200)
15/08/21 23:14:20 INFO Executor: Finished task 75.0 in stage 7.0 (TID 724). 3233 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 737, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 69.0 in stage 7.0 (TID 718). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 88.0 in stage 7.0 (TID 737)
15/08/21 23:14:20 INFO Executor: Finished task 70.0 in stage 7.0 (TID 719). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 73.0 in stage 7.0 (TID 722). 3229 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 738, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 89.0 in stage 7.0 (TID 738)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 739, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 77.0 in stage 7.0 (TID 726). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Running task 90.0 in stage 7.0 (TID 739)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 740, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 91.0 in stage 7.0 (TID 740)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15267
15/08/21 23:14:20 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 741, localhost, ANY, 1979 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 92.0 in stage 7.0 (TID 741)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14741
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 724) in 120 ms on localhost (73/200)
15/08/21 23:14:20 INFO Executor: Finished task 72.0 in stage 7.0 (TID 721). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 79.0 in stage 7.0 (TID 728). 3233 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 718) in 148 ms on localhost (74/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 742, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 743, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO Executor: Running task 93.0 in stage 7.0 (TID 742)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 23:14:20 INFO Executor: Running task 94.0 in stage 7.0 (TID 743)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14740
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15272
15/08/21 23:14:20 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 722) in 136 ms on localhost (75/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15532
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 83.0 in stage 7.0 (TID 732). 3228 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 726) in 119 ms on localhost (76/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15432
15/08/21 23:14:20 INFO Executor: Finished task 81.0 in stage 7.0 (TID 730). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14509
15/08/21 23:14:20 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 728) in 128 ms on localhost (77/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 719) in 176 ms on localhost (78/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15098
15/08/21 23:14:20 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 744, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 80.0 in stage 7.0 (TID 729). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 95.0 in stage 7.0 (TID 744)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 745, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 721) in 181 ms on localhost (79/200)
15/08/21 23:14:20 INFO Executor: Running task 96.0 in stage 7.0 (TID 745)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 732) in 100 ms on localhost (80/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 730) in 132 ms on localhost (81/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 87.0 in stage 7.0 (TID 736). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 746, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 97.0 in stage 7.0 (TID 746)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 85.0 in stage 7.0 (TID 734). 3231 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14896
15/08/21 23:14:20 INFO Executor: Finished task 84.0 in stage 7.0 (TID 733). 3230 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 747, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 729) in 152 ms on localhost (82/200)
15/08/21 23:14:20 INFO Executor: Running task 98.0 in stage 7.0 (TID 747)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 748, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 99.0 in stage 7.0 (TID 748)
15/08/21 23:14:20 INFO Executor: Finished task 78.0 in stage 7.0 (TID 727). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 86.0 in stage 7.0 (TID 735). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15189
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 15384
15/08/21 23:14:20 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 749, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Finished task 89.0 in stage 7.0 (TID 738). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15150
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15272
15/08/21 23:14:20 INFO Executor: Finished task 82.0 in stage 7.0 (TID 731). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 100.0 in stage 7.0 (TID 749)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 736) in 97 ms on localhost (83/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 23:14:20 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 750, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 101.0 in stage 7.0 (TID 750)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 751, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 102.0 in stage 7.0 (TID 751)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 752, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Running task 103.0 in stage 7.0 (TID 752)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 753, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 104.0 in stage 7.0 (TID 753)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 734) in 128 ms on localhost (84/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 14919
15/08/21 23:14:20 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 733) in 146 ms on localhost (85/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 727) in 198 ms on localhost (86/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 90.0 in stage 7.0 (TID 739). 3228 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 94.0 in stage 7.0 (TID 743). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 91.0 in stage 7.0 (TID 740). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 93.0 in stage 7.0 (TID 742). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 754, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 105.0 in stage 7.0 (TID 754)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 735) in 138 ms on localhost (87/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 731) in 186 ms on localhost (88/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14569
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 23:14:20 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 738) in 123 ms on localhost (89/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 15474
15/08/21 23:14:20 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 755, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 739) in 122 ms on localhost (90/200)
15/08/21 23:14:20 INFO Executor: Running task 106.0 in stage 7.0 (TID 755)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Finished task 92.0 in stage 7.0 (TID 741). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 756, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 757, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 108.0 in stage 7.0 (TID 757)
15/08/21 23:14:20 INFO Executor: Running task 107.0 in stage 7.0 (TID 756)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 97.0 in stage 7.0 (TID 746). 3233 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 758, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 740) in 144 ms on localhost (91/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 742) in 135 ms on localhost (92/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 109.0 in stage 7.0 (TID 758)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 743) in 136 ms on localhost (93/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 95.0 in stage 7.0 (TID 744). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 759, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 760, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 741) in 150 ms on localhost (94/200)
15/08/21 23:14:20 INFO Executor: Running task 111.0 in stage 7.0 (TID 760)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 746) in 95 ms on localhost (95/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 96.0 in stage 7.0 (TID 745). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 110.0 in stage 7.0 (TID 759)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15106
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14951
15/08/21 23:14:20 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 761, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 88.0 in stage 7.0 (TID 737). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 744) in 114 ms on localhost (96/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Running task 112.0 in stage 7.0 (TID 761)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 762, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 113.0 in stage 7.0 (TID 762)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 737) in 179 ms on localhost (97/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14816
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14914
15/08/21 23:14:20 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 745) in 121 ms on localhost (98/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14810
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 99.0 in stage 7.0 (TID 748). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15393
15/08/21 23:14:20 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 763, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14502
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Running task 114.0 in stage 7.0 (TID 763)
15/08/21 23:14:20 INFO Executor: Finished task 102.0 in stage 7.0 (TID 751). 3226 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 748) in 112 ms on localhost (99/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 23:14:20 INFO Executor: Finished task 105.0 in stage 7.0 (TID 754). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 764, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 115.0 in stage 7.0 (TID 764)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 765, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14879
15/08/21 23:14:20 INFO Executor: Running task 116.0 in stage 7.0 (TID 765)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 754) in 85 ms on localhost (100/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 751) in 113 ms on localhost (101/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 98.0 in stage 7.0 (TID 747). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 101.0 in stage 7.0 (TID 750). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 103.0 in stage 7.0 (TID 752). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14550
15/08/21 23:14:20 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 766, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 117.0 in stage 7.0 (TID 766)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 767, localhost, ANY, 1984 bytes)
15/08/21 23:14:20 INFO Executor: Running task 118.0 in stage 7.0 (TID 767)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 768, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 119.0 in stage 7.0 (TID 768)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14954
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 747) in 144 ms on localhost (102/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14718
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 750) in 127 ms on localhost (103/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 752) in 125 ms on localhost (104/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14776
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Finished task 100.0 in stage 7.0 (TID 749). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 769, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 120.0 in stage 7.0 (TID 769)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 104.0 in stage 7.0 (TID 753). 3235 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 770, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 106.0 in stage 7.0 (TID 755). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14848
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14891
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 111.0 in stage 7.0 (TID 760). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 121.0 in stage 7.0 (TID 770)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 749) in 152 ms on localhost (105/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 109.0 in stage 7.0 (TID 758). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 753) in 154 ms on localhost (106/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15009
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 771, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 122.0 in stage 7.0 (TID 771)
15/08/21 23:14:20 INFO Executor: Finished task 107.0 in stage 7.0 (TID 756). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 772, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 123.0 in stage 7.0 (TID 772)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 773, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 108.0 in stage 7.0 (TID 757). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 124.0 in stage 7.0 (TID 773)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 23:14:20 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 760) in 105 ms on localhost (107/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15099
15/08/21 23:14:20 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 755) in 140 ms on localhost (108/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15557
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14784
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 774, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 758) in 123 ms on localhost (109/200)
15/08/21 23:14:20 INFO Executor: Running task 125.0 in stage 7.0 (TID 774)
15/08/21 23:14:20 INFO Executor: Finished task 114.0 in stage 7.0 (TID 763). 3229 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 775, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 126.0 in stage 7.0 (TID 775)
15/08/21 23:14:20 INFO Executor: Finished task 110.0 in stage 7.0 (TID 759). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 776, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 127.0 in stage 7.0 (TID 776)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 756) in 147 ms on localhost (110/200)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 777, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 757) in 144 ms on localhost (111/200)
15/08/21 23:14:20 INFO Executor: Running task 128.0 in stage 7.0 (TID 777)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 23:14:20 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 763) in 98 ms on localhost (112/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 116.0 in stage 7.0 (TID 765). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 112.0 in stage 7.0 (TID 761). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Finished task 117.0 in stage 7.0 (TID 766). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 778, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 129.0 in stage 7.0 (TID 778)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 759) in 133 ms on localhost (113/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15029
15/08/21 23:14:20 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 779, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 130.0 in stage 7.0 (TID 779)
15/08/21 23:14:20 INFO Executor: Finished task 115.0 in stage 7.0 (TID 764). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 780, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 23:14:20 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 781, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 761) in 134 ms on localhost (114/200)
15/08/21 23:14:20 INFO Executor: Running task 132.0 in stage 7.0 (TID 781)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 765) in 103 ms on localhost (115/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 766) in 93 ms on localhost (116/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14842
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 131.0 in stage 7.0 (TID 780)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 764) in 109 ms on localhost (117/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14938
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15274
15/08/21 23:14:20 INFO Executor: Finished task 118.0 in stage 7.0 (TID 767). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 782, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14791
15/08/21 23:14:20 INFO Executor: Running task 133.0 in stage 7.0 (TID 782)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 767) in 113 ms on localhost (118/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15361
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 119.0 in stage 7.0 (TID 768). 3235 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 783, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Running task 134.0 in stage 7.0 (TID 783)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15120
15/08/21 23:14:20 INFO Executor: Finished task 120.0 in stage 7.0 (TID 769). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14942
15/08/21 23:14:20 INFO Executor: Finished task 124.0 in stage 7.0 (TID 773). 3235 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 122.0 in stage 7.0 (TID 771). 3231 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 784, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 121.0 in stage 7.0 (TID 770). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 135.0 in stage 7.0 (TID 784)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 768) in 126 ms on localhost (119/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14780
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 785, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 136.0 in stage 7.0 (TID 785)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 769) in 128 ms on localhost (120/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 23:14:20 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 786, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15010
15/08/21 23:14:20 INFO Executor: Running task 137.0 in stage 7.0 (TID 786)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 787, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO Executor: Running task 138.0 in stage 7.0 (TID 787)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 773) in 100 ms on localhost (121/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15145
15/08/21 23:14:20 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 771) in 103 ms on localhost (122/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 770) in 128 ms on localhost (123/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 123.0 in stage 7.0 (TID 772). 3233 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 113.0 in stage 7.0 (TID 762). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 788, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 139.0 in stage 7.0 (TID 788)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 789, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15049
15/08/21 23:14:20 INFO Executor: Running task 140.0 in stage 7.0 (TID 789)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 772) in 121 ms on localhost (124/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 127.0 in stage 7.0 (TID 776). 3228 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 132.0 in stage 7.0 (TID 781). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 125.0 in stage 7.0 (TID 774). 3227 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 762) in 213 ms on localhost (125/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14687
15/08/21 23:14:20 INFO Executor: Finished task 128.0 in stage 7.0 (TID 777). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 790, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14923
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14750
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 776) in 109 ms on localhost (126/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15368
15/08/21 23:14:20 INFO Executor: Running task 141.0 in stage 7.0 (TID 790)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 791, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 142.0 in stage 7.0 (TID 791)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 792, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 143.0 in stage 7.0 (TID 792)
15/08/21 23:14:20 INFO Executor: Finished task 126.0 in stage 7.0 (TID 775). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 793, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15609
15/08/21 23:14:20 INFO Executor: Running task 144.0 in stage 7.0 (TID 793)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 794, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 145.0 in stage 7.0 (TID 794)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 781) in 106 ms on localhost (127/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 23:14:20 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 774) in 135 ms on localhost (128/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 777) in 126 ms on localhost (129/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14489
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14612
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 23:14:20 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 775) in 134 ms on localhost (130/200)
15/08/21 23:14:20 INFO Executor: Finished task 133.0 in stage 7.0 (TID 782). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 129.0 in stage 7.0 (TID 778). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 795, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 130.0 in stage 7.0 (TID 779). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 796, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 146.0 in stage 7.0 (TID 795)
15/08/21 23:14:20 INFO Executor: Finished task 131.0 in stage 7.0 (TID 780). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 147.0 in stage 7.0 (TID 796)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 136.0 in stage 7.0 (TID 785). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 797, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 148.0 in stage 7.0 (TID 797)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 798, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Running task 149.0 in stage 7.0 (TID 798)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 799, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 135.0 in stage 7.0 (TID 784). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 138.0 in stage 7.0 (TID 787). 3232 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 782) in 111 ms on localhost (131/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 150.0 in stage 7.0 (TID 799)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 15330
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 780) in 135 ms on localhost (132/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 778) in 146 ms on localhost (133/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 134.0 in stage 7.0 (TID 783). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 800, localhost, ANY, 1979 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 801, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 152.0 in stage 7.0 (TID 801)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 802, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 153.0 in stage 7.0 (TID 802)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15090
15/08/21 23:14:20 INFO Executor: Running task 151.0 in stage 7.0 (TID 800)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 779) in 159 ms on localhost (134/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 15141
15/08/21 23:14:20 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 785) in 103 ms on localhost (135/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14678
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 787) in 103 ms on localhost (136/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 784) in 123 ms on localhost (137/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14771
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15063
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 23:14:20 INFO Executor: Finished task 144.0 in stage 7.0 (TID 793). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 140.0 in stage 7.0 (TID 789). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 803, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 154.0 in stage 7.0 (TID 803)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 804, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 155.0 in stage 7.0 (TID 804)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 783) in 148 ms on localhost (138/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14751
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 789) in 110 ms on localhost (139/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 793) in 85 ms on localhost (140/200)
15/08/21 23:14:20 INFO Executor: Finished task 139.0 in stage 7.0 (TID 788). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 145.0 in stage 7.0 (TID 794). 3233 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 805, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 143.0 in stage 7.0 (TID 792). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 156.0 in stage 7.0 (TID 805)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 806, localhost, ANY, 1979 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15524
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15046
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14952
15/08/21 23:14:20 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 807, localhost, ANY, 1979 bytes)
15/08/21 23:14:20 INFO Executor: Running task 157.0 in stage 7.0 (TID 806)
15/08/21 23:14:20 INFO Executor: Running task 158.0 in stage 7.0 (TID 807)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 788) in 118 ms on localhost (141/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 794) in 91 ms on localhost (142/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14731
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 23:14:20 INFO Executor: Finished task 141.0 in stage 7.0 (TID 790). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 792) in 96 ms on localhost (143/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 808, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 14479
15/08/21 23:14:20 INFO Executor: Finished task 137.0 in stage 7.0 (TID 786). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 809, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15114
15/08/21 23:14:20 INFO Executor: Running task 159.0 in stage 7.0 (TID 808)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 23:14:20 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 790) in 117 ms on localhost (144/200)
15/08/21 23:14:20 INFO Executor: Running task 160.0 in stage 7.0 (TID 809)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15679
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 149.0 in stage 7.0 (TID 798). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15096
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 810, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 150.0 in stage 7.0 (TID 799). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14802
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 146.0 in stage 7.0 (TID 795). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 798) in 87 ms on localhost (145/200)
15/08/21 23:14:20 INFO Executor: Running task 161.0 in stage 7.0 (TID 810)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 786) in 162 ms on localhost (146/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14590
15/08/21 23:14:20 INFO Executor: Finished task 148.0 in stage 7.0 (TID 797). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Finished task 152.0 in stage 7.0 (TID 801). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 147.0 in stage 7.0 (TID 796). 3231 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 811, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 162.0 in stage 7.0 (TID 811)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15142
15/08/21 23:14:20 INFO Executor: Finished task 151.0 in stage 7.0 (TID 800). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 812, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 163.0 in stage 7.0 (TID 812)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 813, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 799) in 101 ms on localhost (147/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 795) in 115 ms on localhost (148/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 797) in 108 ms on localhost (149/200)
15/08/21 23:14:20 INFO Executor: Running task 164.0 in stage 7.0 (TID 813)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 814, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Running task 165.0 in stage 7.0 (TID 814)
15/08/21 23:14:20 INFO Executor: Finished task 153.0 in stage 7.0 (TID 802). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 155.0 in stage 7.0 (TID 804). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 815, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 801) in 92 ms on localhost (150/200)
15/08/21 23:14:20 INFO Executor: Running task 166.0 in stage 7.0 (TID 815)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 796) in 121 ms on localhost (151/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 816, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 167.0 in stage 7.0 (TID 816)
15/08/21 23:14:20 INFO Executor: Finished task 154.0 in stage 7.0 (TID 803). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 817, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15463
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 800) in 101 ms on localhost (152/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14991
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 142.0 in stage 7.0 (TID 791). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 168.0 in stage 7.0 (TID 817)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 818, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 23:14:20 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 802) in 98 ms on localhost (153/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 169.0 in stage 7.0 (TID 818)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15726
15/08/21 23:14:20 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 804) in 68 ms on localhost (154/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 819, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 170.0 in stage 7.0 (TID 819)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15203
15/08/21 23:14:20 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 820, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 171.0 in stage 7.0 (TID 820)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15007
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 791) in 167 ms on localhost (155/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14755
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 803) in 91 ms on localhost (156/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 156.0 in stage 7.0 (TID 805). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 157.0 in stage 7.0 (TID 806). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 158.0 in stage 7.0 (TID 807). 3229 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 23:14:20 INFO Executor: Finished task 160.0 in stage 7.0 (TID 809). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15191
15/08/21 23:14:20 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 821, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO Executor: Running task 172.0 in stage 7.0 (TID 821)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 822, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 805) in 95 ms on localhost (157/200)
15/08/21 23:14:20 INFO Executor: Finished task 159.0 in stage 7.0 (TID 808). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 823, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 161.0 in stage 7.0 (TID 810). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 173.0 in stage 7.0 (TID 822)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 807) in 100 ms on localhost (158/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 824, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 175.0 in stage 7.0 (TID 824)
15/08/21 23:14:20 INFO Executor: Running task 174.0 in stage 7.0 (TID 823)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 806) in 106 ms on localhost (159/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15396
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15313
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 164.0 in stage 7.0 (TID 813). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 809) in 88 ms on localhost (160/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14560
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15101
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 808) in 101 ms on localhost (161/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15137
15/08/21 23:14:20 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 825, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 176.0 in stage 7.0 (TID 825)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 826, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 177.0 in stage 7.0 (TID 826)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15358
15/08/21 23:14:20 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 827, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 810) in 98 ms on localhost (162/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14440
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 813) in 82 ms on localhost (163/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO Executor: Running task 178.0 in stage 7.0 (TID 827)
15/08/21 23:14:20 INFO Executor: Finished task 163.0 in stage 7.0 (TID 812). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 165.0 in stage 7.0 (TID 814). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 812) in 89 ms on localhost (164/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 162.0 in stage 7.0 (TID 811). 3232 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 828, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Running task 179.0 in stage 7.0 (TID 828)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15364
15/08/21 23:14:20 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 829, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 23:14:20 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 830, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 171.0 in stage 7.0 (TID 820). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 180.0 in stage 7.0 (TID 829)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15586
15/08/21 23:14:20 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 814) in 92 ms on localhost (165/200)
15/08/21 23:14:20 INFO Executor: Running task 181.0 in stage 7.0 (TID 830)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 811) in 102 ms on localhost (166/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Finished task 169.0 in stage 7.0 (TID 818). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 166.0 in stage 7.0 (TID 815). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 831, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 182.0 in stage 7.0 (TID 831)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 832, localhost, ANY, 1980 bytes)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 820) in 75 ms on localhost (167/200)
15/08/21 23:14:20 INFO Executor: Finished task 170.0 in stage 7.0 (TID 819). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 183.0 in stage 7.0 (TID 832)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 833, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 184.0 in stage 7.0 (TID 833)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 834, localhost, ANY, 1979 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 815) in 100 ms on localhost (168/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 818) in 92 ms on localhost (169/200)
15/08/21 23:14:20 INFO Executor: Running task 185.0 in stage 7.0 (TID 834)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15013
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 14845
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 819) in 91 ms on localhost (170/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14613
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15235
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15344
15/08/21 23:14:20 INFO Executor: Finished task 172.0 in stage 7.0 (TID 821). 3231 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15117
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 173.0 in stage 7.0 (TID 822). 3231 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 835, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 167.0 in stage 7.0 (TID 816). 3233 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 186.0 in stage 7.0 (TID 835)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15104
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 836, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 187.0 in stage 7.0 (TID 836)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 837, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 821) in 95 ms on localhost (171/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 816) in 133 ms on localhost (172/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 822) in 97 ms on localhost (173/200)
15/08/21 23:14:20 INFO Executor: Running task 188.0 in stage 7.0 (TID 837)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO Executor: Finished task 175.0 in stage 7.0 (TID 824). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15470
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14597
15/08/21 23:14:20 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 838, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 189.0 in stage 7.0 (TID 838)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 824) in 92 ms on localhost (174/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO Executor: Finished task 178.0 in stage 7.0 (TID 827). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15147
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15386
15/08/21 23:14:20 INFO Executor: Finished task 176.0 in stage 7.0 (TID 825). 3229 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Finished task 174.0 in stage 7.0 (TID 823). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 839, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14677
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Running task 190.0 in stage 7.0 (TID 839)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 840, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 191.0 in stage 7.0 (TID 840)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 841, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 827) in 82 ms on localhost (175/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 825) in 89 ms on localhost (176/200)
15/08/21 23:14:20 INFO Executor: Running task 192.0 in stage 7.0 (TID 841)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15614
15/08/21 23:14:20 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 823) in 110 ms on localhost (177/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15658
15/08/21 23:14:20 INFO Executor: Finished task 177.0 in stage 7.0 (TID 826). 3229 bytes result sent to driver
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 842, localhost, ANY, 1979 bytes)
15/08/21 23:14:20 INFO Executor: Finished task 179.0 in stage 7.0 (TID 828). 3232 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 168.0 in stage 7.0 (TID 817). 3230 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 193.0 in stage 7.0 (TID 842)
15/08/21 23:14:20 INFO Executor: Finished task 181.0 in stage 7.0 (TID 830). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 843, localhost, ANY, 1979 bytes)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 826) in 101 ms on localhost (178/200)
15/08/21 23:14:20 INFO Executor: Finished task 180.0 in stage 7.0 (TID 829). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Running task 194.0 in stage 7.0 (TID 843)
15/08/21 23:14:20 INFO Executor: Finished task 182.0 in stage 7.0 (TID 831). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 184.0 in stage 7.0 (TID 833). 3233 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 828) in 90 ms on localhost (179/200)
15/08/21 23:14:20 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 844, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 845, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO Executor: Running task 195.0 in stage 7.0 (TID 844)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 817) in 170 ms on localhost (180/200)
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 830) in 89 ms on localhost (181/200)
15/08/21 23:14:20 INFO Executor: Running task 196.0 in stage 7.0 (TID 845)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14784
15/08/21 23:14:20 INFO Executor: Finished task 183.0 in stage 7.0 (TID 832). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 23:14:20 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 829) in 95 ms on localhost (182/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 23:14:20 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 846, localhost, ANY, 1983 bytes)
15/08/21 23:14:20 INFO Executor: Running task 197.0 in stage 7.0 (TID 846)
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 185.0 in stage 7.0 (TID 834). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 847, localhost, ANY, 1982 bytes)
15/08/21 23:14:20 INFO Executor: Running task 198.0 in stage 7.0 (TID 847)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 831) in 96 ms on localhost (183/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 833) in 91 ms on localhost (184/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14982
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15038
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 23:14:20 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 848, localhost, ANY, 1981 bytes)
15/08/21 23:14:20 INFO Executor: Running task 199.0 in stage 7.0 (TID 848)
15/08/21 23:14:20 INFO Executor: Finished task 187.0 in stage 7.0 (TID 836). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 186.0 in stage 7.0 (TID 835). 3235 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 834) in 101 ms on localhost (185/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:20 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 836) in 70 ms on localhost (186/200)
15/08/21 23:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:20 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 832) in 108 ms on localhost (187/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 835) in 74 ms on localhost (188/200)
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15043
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 188.0 in stage 7.0 (TID 837). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 190.0 in stage 7.0 (TID 839). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15023
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15625
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 837) in 90 ms on localhost (189/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 839) in 75 ms on localhost (190/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15509
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15039
15/08/21 23:14:20 INFO Executor: Finished task 189.0 in stage 7.0 (TID 838). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 838) in 87 ms on localhost (191/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15001
15/08/21 23:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 191.0 in stage 7.0 (TID 840). 3231 bytes result sent to driver
15/08/21 23:14:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:20 INFO Executor: Finished task 196.0 in stage 7.0 (TID 845). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 840) in 90 ms on localhost (192/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 845) in 72 ms on localhost (193/200)
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO Executor: Finished task 192.0 in stage 7.0 (TID 841). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14843
15/08/21 23:14:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:20 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 23:14:20 INFO Executor: Finished task 195.0 in stage 7.0 (TID 844). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 194.0 in stage 7.0 (TID 843). 3231 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 841) in 96 ms on localhost (194/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 844) in 79 ms on localhost (195/200)
15/08/21 23:14:20 INFO Executor: Finished task 193.0 in stage 7.0 (TID 842). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 843) in 87 ms on localhost (196/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 842) in 93 ms on localhost (197/200)
15/08/21 23:14:20 INFO Executor: Finished task 197.0 in stage 7.0 (TID 846). 3234 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 846) in 74 ms on localhost (198/200)
15/08/21 23:14:20 INFO Executor: Finished task 199.0 in stage 7.0 (TID 848). 3232 bytes result sent to driver
15/08/21 23:14:20 INFO Executor: Finished task 198.0 in stage 7.0 (TID 847). 3229 bytes result sent to driver
15/08/21 23:14:20 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 848) in 75 ms on localhost (199/200)
15/08/21 23:14:20 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 847) in 83 ms on localhost (200/200)
15/08/21 23:14:20 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/21 23:14:20 INFO DAGScheduler: ResultStage 7 (processCmd at CliDriver.java:423) finished in 1.577 s
15/08/21 23:14:20 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3a317470
15/08/21 23:14:20 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 3.361825 s
15/08/21 23:14:20 INFO StatsReportListener: task runtime:(count: 200, mean: 131.495000, stdev: 47.671270, max: 284.000000, min: 68.000000)
15/08/21 23:14:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:20 INFO StatsReportListener: 	68.0 ms	82.0 ms	89.0 ms	98.0 ms	120.0 ms	147.0 ms	214.0 ms	245.0 ms	284.0 ms
15/08/21 23:14:20 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.340000, stdev: 0.874300, max: 10.000000, min: 0.000000)
15/08/21 23:14:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:20 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	1.0 ms	10.0 ms
15/08/21 23:14:20 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 23:14:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:20 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 23:14:20 INFO StatsReportListener: task result size:(count: 200, mean: 3231.480000, stdev: 1.705755, max: 3235.000000, min: 3226.000000)
15/08/21 23:14:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:20 INFO StatsReportListener: 	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB
15/08/21 23:14:20 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 75.305313, stdev: 7.443821, max: 89.473684, min: 55.468750)
15/08/21 23:14:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:20 INFO StatsReportListener: 	55 %	62 %	64 %	70 %	77 %	81 %	84 %	86 %	89 %
15/08/21 23:14:20 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.265298, stdev: 0.700013, max: 8.130081, min: 0.000000)
15/08/21 23:14:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:20 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 8 %
15/08/21 23:14:20 INFO StatsReportListener: other time pct: (count: 200, mean: 24.429389, stdev: 7.466654, max: 44.531250, min: 10.344828)
15/08/21 23:14:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:20 INFO StatsReportListener: 	10 %	14 %	16 %	19 %	23 %	30 %	36 %	39 %	45 %
15/08/21 23:14:21 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 23:14:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:21 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 23:14:21 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 200 bytes
15/08/21 23:14:21 INFO DAGScheduler: Registering RDD 44 (processCmd at CliDriver.java:423)
15/08/21 23:14:21 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 23:14:21 INFO DAGScheduler: Final stage: ResultStage 10(processCmd at CliDriver.java:423)
15/08/21 23:14:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
15/08/21 23:14:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
15/08/21 23:14:21 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 23:14:21 INFO MemoryStore: ensureFreeSpace(16112) called with curMem=1895785, maxMem=22226833244
15/08/21 23:14:21 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.7 KB, free 20.7 GB)
15/08/21 23:14:21 INFO MemoryStore: ensureFreeSpace(8458) called with curMem=1911897, maxMem=22226833244
15/08/21 23:14:21 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.3 KB, free 20.7 GB)
15/08/21 23:14:21 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:40586 (size: 8.3 KB, free: 20.7 GB)
15/08/21 23:14:21 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/21 23:14:21 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423)
15/08/21 23:14:21 INFO TaskSchedulerImpl: Adding task set 9.0 with 200 tasks
15/08/21 23:14:21 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 849, localhost, ANY, 1971 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 850, localhost, ANY, 1972 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 851, localhost, ANY, 1972 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 852, localhost, ANY, 1969 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 853, localhost, ANY, 1972 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 854, localhost, ANY, 1972 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 855, localhost, ANY, 1970 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 856, localhost, ANY, 1971 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 857, localhost, ANY, 1970 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 858, localhost, ANY, 1970 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 859, localhost, ANY, 1970 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 860, localhost, ANY, 1972 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 861, localhost, ANY, 1971 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 862, localhost, ANY, 1972 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 863, localhost, ANY, 1971 bytes)
15/08/21 23:14:21 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 864, localhost, ANY, 1972 bytes)
15/08/21 23:14:21 INFO Executor: Running task 4.0 in stage 9.0 (TID 853)
15/08/21 23:14:21 INFO Executor: Running task 1.0 in stage 9.0 (TID 850)
15/08/21 23:14:21 INFO Executor: Running task 6.0 in stage 9.0 (TID 855)
15/08/21 23:14:21 INFO Executor: Running task 7.0 in stage 9.0 (TID 856)
15/08/21 23:14:21 INFO Executor: Running task 2.0 in stage 9.0 (TID 851)
15/08/21 23:14:21 INFO Executor: Running task 9.0 in stage 9.0 (TID 858)
15/08/21 23:14:21 INFO Executor: Running task 10.0 in stage 9.0 (TID 859)
15/08/21 23:14:21 INFO Executor: Running task 0.0 in stage 9.0 (TID 849)
15/08/21 23:14:21 INFO Executor: Running task 12.0 in stage 9.0 (TID 861)
15/08/21 23:14:21 INFO Executor: Running task 3.0 in stage 9.0 (TID 852)
15/08/21 23:14:21 INFO Executor: Running task 15.0 in stage 9.0 (TID 864)
15/08/21 23:14:21 INFO Executor: Running task 5.0 in stage 9.0 (TID 854)
15/08/21 23:14:21 INFO Executor: Running task 14.0 in stage 9.0 (TID 863)
15/08/21 23:14:21 INFO Executor: Running task 13.0 in stage 9.0 (TID 862)
15/08/21 23:14:21 INFO Executor: Running task 11.0 in stage 9.0 (TID 860)
15/08/21 23:14:21 INFO Executor: Running task 8.0 in stage 9.0 (TID 857)
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00049-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127044 length: 127044 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00176-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126540 length: 126540 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00128-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132488 length: 132488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00189-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125901 length: 125901 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00079-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127568 length: 127568 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00153-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130402 length: 130402 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00053-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128097 length: 128097 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14876 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14876
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15570 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14956 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14795 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00004-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128065 length: 128065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14795
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00109-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126357 length: 126357 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14967 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15570
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14956
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14967
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15359 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15359
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00192-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130576 length: 130576 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00011-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126275 length: 126275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00167-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127645 length: 127645 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00177-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126024 length: 126024 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00052-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131925 length: 131925 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14824 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15038
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14824
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00114-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 122284 length: 122284 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15062 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15062
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14825 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15340 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14825
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15501 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14989 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14826
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15340
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15501
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14989
15/08/21 23:14:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14379 records.
15/08/21 23:14:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14379
15/08/21 23:14:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00040-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127464 length: 127464 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14972 records.
15/08/21 23:14:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:22 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14972
15/08/21 23:14:22 INFO Executor: Finished task 1.0 in stage 9.0 (TID 850). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 865, localhost, ANY, 1970 bytes)
15/08/21 23:14:22 INFO Executor: Running task 16.0 in stage 9.0 (TID 865)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 850) in 755 ms on localhost (1/200)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00100-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131065 length: 131065 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15404 records.
15/08/21 23:14:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:22 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15404
15/08/21 23:14:22 INFO Executor: Finished task 6.0 in stage 9.0 (TID 855). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 866, localhost, ANY, 1972 bytes)
15/08/21 23:14:22 INFO Executor: Running task 17.0 in stage 9.0 (TID 866)
15/08/21 23:14:22 INFO Executor: Finished task 8.0 in stage 9.0 (TID 857). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO Executor: Finished task 11.0 in stage 9.0 (TID 860). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 867, localhost, ANY, 1969 bytes)
15/08/21 23:14:22 INFO Executor: Running task 18.0 in stage 9.0 (TID 867)
15/08/21 23:14:22 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 868, localhost, ANY, 1971 bytes)
15/08/21 23:14:22 INFO Executor: Running task 19.0 in stage 9.0 (TID 868)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 855) in 1099 ms on localhost (2/200)
15/08/21 23:14:22 INFO Executor: Finished task 5.0 in stage 9.0 (TID 854). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 869, localhost, ANY, 1971 bytes)
15/08/21 23:14:22 INFO Executor: Running task 20.0 in stage 9.0 (TID 869)
15/08/21 23:14:22 INFO Executor: Finished task 3.0 in stage 9.0 (TID 852). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 870, localhost, ANY, 1970 bytes)
15/08/21 23:14:22 INFO Executor: Running task 21.0 in stage 9.0 (TID 870)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 860) in 1099 ms on localhost (3/200)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 857) in 1103 ms on localhost (4/200)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 854) in 1105 ms on localhost (5/200)
15/08/21 23:14:22 INFO Executor: Finished task 13.0 in stage 9.0 (TID 862). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 871, localhost, ANY, 1971 bytes)
15/08/21 23:14:22 INFO Executor: Running task 22.0 in stage 9.0 (TID 871)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 852) in 1108 ms on localhost (6/200)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO Executor: Finished task 14.0 in stage 9.0 (TID 863). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO Executor: Finished task 15.0 in stage 9.0 (TID 864). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 872, localhost, ANY, 1970 bytes)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:22 INFO Executor: Running task 23.0 in stage 9.0 (TID 872)
15/08/21 23:14:22 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 873, localhost, ANY, 1972 bytes)
15/08/21 23:14:22 INFO Executor: Running task 24.0 in stage 9.0 (TID 873)
15/08/21 23:14:22 INFO Executor: Finished task 10.0 in stage 9.0 (TID 859). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:22 INFO Executor: Finished task 9.0 in stage 9.0 (TID 858). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 874, localhost, ANY, 1971 bytes)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 875, localhost, ANY, 1972 bytes)
15/08/21 23:14:22 INFO Executor: Finished task 12.0 in stage 9.0 (TID 861). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO Executor: Running task 26.0 in stage 9.0 (TID 875)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 862) in 1113 ms on localhost (7/200)
15/08/21 23:14:22 INFO Executor: Running task 25.0 in stage 9.0 (TID 874)
15/08/21 23:14:22 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 876, localhost, ANY, 1972 bytes)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:22 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 863) in 1114 ms on localhost (8/200)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 859) in 1117 ms on localhost (9/200)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 858) in 1124 ms on localhost (10/200)
15/08/21 23:14:22 INFO Executor: Running task 27.0 in stage 9.0 (TID 876)
15/08/21 23:14:22 INFO Executor: Finished task 2.0 in stage 9.0 (TID 851). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 877, localhost, ANY, 1972 bytes)
15/08/21 23:14:22 INFO Executor: Running task 28.0 in stage 9.0 (TID 877)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 851) in 1133 ms on localhost (11/200)
15/08/21 23:14:22 INFO Executor: Finished task 7.0 in stage 9.0 (TID 856). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 861) in 1128 ms on localhost (12/200)
15/08/21 23:14:22 INFO Executor: Finished task 4.0 in stage 9.0 (TID 853). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO Executor: Finished task 16.0 in stage 9.0 (TID 865). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 878, localhost, ANY, 1969 bytes)
15/08/21 23:14:22 INFO Executor: Finished task 0.0 in stage 9.0 (TID 849). 2341 bytes result sent to driver
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO Executor: Running task 29.0 in stage 9.0 (TID 878)
15/08/21 23:14:22 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 879, localhost, ANY, 1970 bytes)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 856) in 1135 ms on localhost (13/200)
15/08/21 23:14:22 INFO Executor: Running task 30.0 in stage 9.0 (TID 879)
15/08/21 23:14:22 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 880, localhost, ANY, 1970 bytes)
15/08/21 23:14:22 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 881, localhost, ANY, 1969 bytes)
15/08/21 23:14:22 INFO Executor: Running task 31.0 in stage 9.0 (TID 880)
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:22 INFO Executor: Running task 32.0 in stage 9.0 (TID 881)
15/08/21 23:14:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00066-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127434 length: 127434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00080-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127239 length: 127239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00107-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128785 length: 128785 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:22 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 864) in 1143 ms on localhost (14/200)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 853) in 1153 ms on localhost (15/200)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 865) in 406 ms on localhost (16/200)
15/08/21 23:14:22 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 849) in 1159 ms on localhost (17/200)
15/08/21 23:14:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00159-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125510 length: 125510 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00127-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128202 length: 128202 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14939 records.
15/08/21 23:14:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14985 records.
15/08/21 23:14:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15131 records.
15/08/21 23:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:22 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14939
15/08/21 23:14:22 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14985
15/08/21 23:14:22 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15131
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14736 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14736
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00193-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125605 length: 125605 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15055 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15055
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00016-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128845 length: 128845 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14793 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14793
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15132 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15132
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00087-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130234 length: 130234 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00047-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126859 length: 126859 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00164-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126261 length: 126261 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00051-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129633 length: 129633 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00102-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126741 length: 126741 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14826 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14826
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14906 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15296 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15296
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14906
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14878 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14878
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15238 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15238
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00043-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132936 length: 132936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00115-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127146 length: 127146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00061-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126318 length: 126318 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14934 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15603 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14819 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14934
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15603
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14819
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00064-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127031 length: 127031 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14914
15/08/21 23:14:23 INFO Executor: Finished task 19.0 in stage 9.0 (TID 868). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 882, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO Executor: Running task 33.0 in stage 9.0 (TID 882)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 868) in 402 ms on localhost (18/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00057-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125389 length: 125389 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14772 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14772
15/08/21 23:14:23 INFO Executor: Finished task 17.0 in stage 9.0 (TID 866). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 883, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO Executor: Running task 34.0 in stage 9.0 (TID 883)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 866) in 498 ms on localhost (19/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO Executor: Finished task 22.0 in stage 9.0 (TID 871). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 884, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO Executor: Running task 35.0 in stage 9.0 (TID 884)
15/08/21 23:14:23 INFO Executor: Finished task 20.0 in stage 9.0 (TID 869). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00147-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130239 length: 130239 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 885, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 871) in 532 ms on localhost (20/200)
15/08/21 23:14:23 INFO Executor: Running task 36.0 in stage 9.0 (TID 885)
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 869) in 540 ms on localhost (21/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15302
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00151-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133040 length: 133040 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00132-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125118 length: 125118 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14693 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15636 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14693
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15636
15/08/21 23:14:23 INFO Executor: Finished task 23.0 in stage 9.0 (TID 872). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO Executor: Finished task 24.0 in stage 9.0 (TID 873). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 886, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO Executor: Finished task 21.0 in stage 9.0 (TID 870). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO Executor: Running task 37.0 in stage 9.0 (TID 886)
15/08/21 23:14:23 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 887, localhost, ANY, 1971 bytes)
15/08/21 23:14:23 INFO Executor: Running task 38.0 in stage 9.0 (TID 887)
15/08/21 23:14:23 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 888, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 872) in 594 ms on localhost (22/200)
15/08/21 23:14:23 INFO Executor: Running task 39.0 in stage 9.0 (TID 888)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 870) in 605 ms on localhost (23/200)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 873) in 597 ms on localhost (24/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00063-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128080 length: 128080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00097-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132078 length: 132078 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00025-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123934 length: 123934 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15566 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15039
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15566
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14590
15/08/21 23:14:23 INFO Executor: Finished task 26.0 in stage 9.0 (TID 875). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO Executor: Finished task 27.0 in stage 9.0 (TID 876). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 889, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 890, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO Executor: Running task 40.0 in stage 9.0 (TID 889)
15/08/21 23:14:23 INFO Executor: Finished task 18.0 in stage 9.0 (TID 867). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 875) in 663 ms on localhost (25/200)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 876) in 666 ms on localhost (26/200)
15/08/21 23:14:23 INFO Executor: Running task 41.0 in stage 9.0 (TID 890)
15/08/21 23:14:23 INFO Executor: Finished task 32.0 in stage 9.0 (TID 881). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 891, localhost, ANY, 1969 bytes)
15/08/21 23:14:23 INFO Executor: Running task 42.0 in stage 9.0 (TID 891)
15/08/21 23:14:23 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 892, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO Executor: Running task 43.0 in stage 9.0 (TID 892)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 867) in 692 ms on localhost (27/200)
15/08/21 23:14:23 INFO Executor: Finished task 25.0 in stage 9.0 (TID 874). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 893, localhost, ANY, 1971 bytes)
15/08/21 23:14:23 INFO Executor: Running task 44.0 in stage 9.0 (TID 893)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 881) in 654 ms on localhost (28/200)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 874) in 677 ms on localhost (29/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO Executor: Finished task 30.0 in stage 9.0 (TID 879). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO Executor: Finished task 29.0 in stage 9.0 (TID 878). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO Executor: Finished task 28.0 in stage 9.0 (TID 877). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO Executor: Finished task 31.0 in stage 9.0 (TID 880). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 894, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO Executor: Running task 45.0 in stage 9.0 (TID 894)
15/08/21 23:14:23 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 895, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO Executor: Running task 46.0 in stage 9.0 (TID 895)
15/08/21 23:14:23 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 896, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO Executor: Running task 47.0 in stage 9.0 (TID 896)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 879) in 671 ms on localhost (30/200)
15/08/21 23:14:23 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 897, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO Executor: Running task 48.0 in stage 9.0 (TID 897)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 878) in 679 ms on localhost (31/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 877) in 686 ms on localhost (32/200)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 880) in 680 ms on localhost (33/200)
15/08/21 23:14:23 INFO Executor: Finished task 33.0 in stage 9.0 (TID 882). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 898, localhost, ANY, 1972 bytes)
15/08/21 23:14:23 INFO Executor: Running task 49.0 in stage 9.0 (TID 898)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 882) in 335 ms on localhost (34/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00168-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126820 length: 126820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00105-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127339 length: 127339 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00126-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126306 length: 126306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00019-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127314 length: 127314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14884 records.
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14884
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14831 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14944 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO Executor: Finished task 34.0 in stage 9.0 (TID 883). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14991
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14831
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14944
15/08/21 23:14:23 INFO TaskSetManager: Starting task 50.0 in stage 9.0 (TID 899, localhost, ANY, 1971 bytes)
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00134-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00145-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131978 length: 131978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO Executor: Running task 50.0 in stage 9.0 (TID 899)
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00152-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126231 length: 126231 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 883) in 286 ms on localhost (35/200)
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00199-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126391 length: 126391 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15545 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15545
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15347 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15347
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14809 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14809
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14836 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14836
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00089-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129970 length: 129970 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00123-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126878 length: 126878 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO Executor: Finished task 36.0 in stage 9.0 (TID 885). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 51.0 in stage 9.0 (TID 900, localhost, ANY, 1969 bytes)
15/08/21 23:14:23 INFO Executor: Running task 51.0 in stage 9.0 (TID 900)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 885) in 278 ms on localhost (36/200)
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15302
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14913 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14913
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00174-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128939 length: 128939 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 INFO Executor: Finished task 35.0 in stage 9.0 (TID 884). 2341 bytes result sent to driver
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO TaskSetManager: Starting task 52.0 in stage 9.0 (TID 901, localhost, ANY, 1971 bytes)
15/08/21 23:14:23 INFO Executor: Running task 52.0 in stage 9.0 (TID 901)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 884) in 334 ms on localhost (37/200)
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15129 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15129
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00000-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126117 length: 126117 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO Executor: Finished task 38.0 in stage 9.0 (TID 887). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 53.0 in stage 9.0 (TID 902, localhost, ANY, 1971 bytes)
15/08/21 23:14:23 INFO Executor: Running task 53.0 in stage 9.0 (TID 902)
15/08/21 23:14:23 INFO Executor: Finished task 37.0 in stage 9.0 (TID 886). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 54.0 in stage 9.0 (TID 903, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO Executor: Running task 54.0 in stage 9.0 (TID 903)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 887) in 303 ms on localhost (38/200)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 886) in 307 ms on localhost (39/200)
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14814 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14814
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO Executor: Finished task 39.0 in stage 9.0 (TID 888). 2341 bytes result sent to driver
15/08/21 23:14:23 INFO TaskSetManager: Starting task 55.0 in stage 9.0 (TID 904, localhost, ANY, 1970 bytes)
15/08/21 23:14:23 INFO Executor: Running task 55.0 in stage 9.0 (TID 904)
15/08/21 23:14:23 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 888) in 318 ms on localhost (40/200)
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00050-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127475 length: 127475 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00136-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126150 length: 126150 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00157-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132343 length: 132343 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14992 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14992
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15549 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15549
15/08/21 23:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00038-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129120 length: 129120 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15164 records.
15/08/21 23:14:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15164
15/08/21 23:14:24 INFO Executor: Finished task 43.0 in stage 9.0 (TID 892). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 56.0 in stage 9.0 (TID 905, localhost, ANY, 1970 bytes)
15/08/21 23:14:24 INFO Executor: Running task 56.0 in stage 9.0 (TID 905)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 892) in 474 ms on localhost (41/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO Executor: Finished task 44.0 in stage 9.0 (TID 893). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 57.0 in stage 9.0 (TID 906, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO Executor: Running task 57.0 in stage 9.0 (TID 906)
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00007-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126272 length: 126272 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 893) in 506 ms on localhost (42/200)
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO Executor: Finished task 42.0 in stage 9.0 (TID 891). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 58.0 in stage 9.0 (TID 907, localhost, ANY, 1968 bytes)
15/08/21 23:14:24 INFO Executor: Running task 58.0 in stage 9.0 (TID 907)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 891) in 516 ms on localhost (43/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14829 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14829
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO Executor: Finished task 41.0 in stage 9.0 (TID 890). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 59.0 in stage 9.0 (TID 908, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO Executor: Running task 59.0 in stage 9.0 (TID 908)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 890) in 541 ms on localhost (44/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00140-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126739 length: 126739 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00181-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131709 length: 131709 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO Executor: Finished task 49.0 in stage 9.0 (TID 898). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 60.0 in stage 9.0 (TID 909, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 60.0 in stage 9.0 (TID 909)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 898) in 529 ms on localhost (45/200)
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14883 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15489 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14883
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15489
15/08/21 23:14:24 INFO Executor: Finished task 46.0 in stage 9.0 (TID 895). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 61.0 in stage 9.0 (TID 910, localhost, ANY, 1970 bytes)
15/08/21 23:14:24 INFO Executor: Running task 61.0 in stage 9.0 (TID 910)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 895) in 553 ms on localhost (46/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO Executor: Finished task 45.0 in stage 9.0 (TID 894). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 62.0 in stage 9.0 (TID 911, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO Executor: Running task 62.0 in stage 9.0 (TID 911)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00191-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128718 length: 128718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 894) in 573 ms on localhost (47/200)
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO Executor: Finished task 40.0 in stage 9.0 (TID 889). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 63.0 in stage 9.0 (TID 912, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 63.0 in stage 9.0 (TID 912)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 889) in 606 ms on localhost (48/200)
15/08/21 23:14:24 INFO Executor: Finished task 48.0 in stage 9.0 (TID 897). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 64.0 in stage 9.0 (TID 913, localhost, ANY, 1969 bytes)
15/08/21 23:14:24 INFO Executor: Running task 64.0 in stage 9.0 (TID 913)
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15107 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15107
15/08/21 23:14:24 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 897) in 588 ms on localhost (49/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00093-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132035 length: 132035 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00096-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128538 length: 128538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00002-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130788 length: 130788 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00074-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129960 length: 129960 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15500 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15103 records.
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15500
15/08/21 23:14:24 INFO Executor: Finished task 47.0 in stage 9.0 (TID 896). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15103
15/08/21 23:14:24 INFO TaskSetManager: Starting task 65.0 in stage 9.0 (TID 914, localhost, ANY, 1969 bytes)
15/08/21 23:14:24 INFO Executor: Running task 65.0 in stage 9.0 (TID 914)
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00196-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132367 length: 132367 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15383
15/08/21 23:14:24 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 896) in 629 ms on localhost (50/200)
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15282 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15282
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15571 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO Executor: Finished task 50.0 in stage 9.0 (TID 899). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO Executor: Finished task 51.0 in stage 9.0 (TID 900). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15571
15/08/21 23:14:24 INFO TaskSetManager: Starting task 66.0 in stage 9.0 (TID 915, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO Executor: Running task 66.0 in stage 9.0 (TID 915)
15/08/21 23:14:24 INFO TaskSetManager: Starting task 67.0 in stage 9.0 (TID 916, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 67.0 in stage 9.0 (TID 916)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 50.0 in stage 9.0 (TID 899) in 579 ms on localhost (51/200)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 51.0 in stage 9.0 (TID 900) in 538 ms on localhost (52/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00187-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129617 length: 129617 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO Executor: Finished task 54.0 in stage 9.0 (TID 903). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO Executor: Finished task 53.0 in stage 9.0 (TID 902). 2341 bytes result sent to driver
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO TaskSetManager: Starting task 68.0 in stage 9.0 (TID 917, localhost, ANY, 1970 bytes)
15/08/21 23:14:24 INFO Executor: Running task 68.0 in stage 9.0 (TID 917)
15/08/21 23:14:24 INFO TaskSetManager: Starting task 69.0 in stage 9.0 (TID 918, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Finished task 52.0 in stage 9.0 (TID 901). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO Executor: Finished task 55.0 in stage 9.0 (TID 904). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Finished task 54.0 in stage 9.0 (TID 903) in 507 ms on localhost (53/200)
15/08/21 23:14:24 INFO Executor: Running task 69.0 in stage 9.0 (TID 918)
15/08/21 23:14:24 INFO TaskSetManager: Starting task 70.0 in stage 9.0 (TID 919, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15207 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO TaskSetManager: Starting task 71.0 in stage 9.0 (TID 920, localhost, ANY, 1973 bytes)
15/08/21 23:14:24 INFO Executor: Running task 71.0 in stage 9.0 (TID 920)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 53.0 in stage 9.0 (TID 902) in 514 ms on localhost (54/200)
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00094-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 134514 length: 134514 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15207
15/08/21 23:14:24 INFO TaskSetManager: Finished task 55.0 in stage 9.0 (TID 904) in 502 ms on localhost (55/200)
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO TaskSetManager: Finished task 52.0 in stage 9.0 (TID 901) in 560 ms on localhost (56/200)
15/08/21 23:14:24 INFO Executor: Running task 70.0 in stage 9.0 (TID 919)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15795 records.
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15795
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00023-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128418 length: 128418 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15079 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15079
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00184-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129529 length: 129529 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15211 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00072-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127930 length: 127930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15211
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00059-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126538 length: 126538 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15018 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15018
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00154-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129229 length: 129229 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14854 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14854
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15183 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15183
15/08/21 23:14:24 INFO Executor: Finished task 56.0 in stage 9.0 (TID 905). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 72.0 in stage 9.0 (TID 921, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 72.0 in stage 9.0 (TID 921)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 56.0 in stage 9.0 (TID 905) in 435 ms on localhost (57/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00003-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128262 length: 128262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO Executor: Finished task 57.0 in stage 9.0 (TID 906). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO Executor: Finished task 58.0 in stage 9.0 (TID 907). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 73.0 in stage 9.0 (TID 922, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 73.0 in stage 9.0 (TID 922)
15/08/21 23:14:24 INFO TaskSetManager: Starting task 74.0 in stage 9.0 (TID 923, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO Executor: Running task 74.0 in stage 9.0 (TID 923)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 57.0 in stage 9.0 (TID 906) in 448 ms on localhost (58/200)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 58.0 in stage 9.0 (TID 907) in 444 ms on localhost (59/200)
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15058 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15058
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00083-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126441 length: 126441 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00091-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129817 length: 129817 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO Executor: Finished task 59.0 in stage 9.0 (TID 908). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14864 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO TaskSetManager: Starting task 75.0 in stage 9.0 (TID 924, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15255 records.
15/08/21 23:14:24 INFO Executor: Running task 75.0 in stage 9.0 (TID 924)
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14864
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15255
15/08/21 23:14:24 INFO TaskSetManager: Finished task 59.0 in stage 9.0 (TID 908) in 483 ms on localhost (60/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:24 INFO Executor: Finished task 63.0 in stage 9.0 (TID 912). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 76.0 in stage 9.0 (TID 925, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO Executor: Running task 76.0 in stage 9.0 (TID 925)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 63.0 in stage 9.0 (TID 912) in 462 ms on localhost (61/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00098-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123608 length: 123608 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14508 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14508
15/08/21 23:14:24 INFO Executor: Finished task 61.0 in stage 9.0 (TID 910). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 77.0 in stage 9.0 (TID 926, localhost, ANY, 1969 bytes)
15/08/21 23:14:24 INFO Executor: Running task 77.0 in stage 9.0 (TID 926)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 61.0 in stage 9.0 (TID 910) in 525 ms on localhost (62/200)
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00012-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124080 length: 124080 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO Executor: Finished task 62.0 in stage 9.0 (TID 911). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14564 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14564
15/08/21 23:14:24 INFO TaskSetManager: Starting task 78.0 in stage 9.0 (TID 927, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 78.0 in stage 9.0 (TID 927)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 62.0 in stage 9.0 (TID 911) in 528 ms on localhost (63/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO Executor: Finished task 60.0 in stage 9.0 (TID 909). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 79.0 in stage 9.0 (TID 928, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 INFO Executor: Running task 79.0 in stage 9.0 (TID 928)
15/08/21 23:14:24 INFO Executor: Finished task 64.0 in stage 9.0 (TID 913). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Finished task 60.0 in stage 9.0 (TID 909) in 582 ms on localhost (64/200)
15/08/21 23:14:24 INFO TaskSetManager: Starting task 80.0 in stage 9.0 (TID 929, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 80.0 in stage 9.0 (TID 929)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 64.0 in stage 9.0 (TID 913) in 544 ms on localhost (65/200)
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00046-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130591 length: 130591 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15333 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15333
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00070-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130177 length: 130177 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00108-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126386 length: 126386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15272
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14838 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14838
15/08/21 23:14:24 INFO Executor: Finished task 67.0 in stage 9.0 (TID 916). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 81.0 in stage 9.0 (TID 930, localhost, ANY, 1969 bytes)
15/08/21 23:14:24 INFO Executor: Running task 81.0 in stage 9.0 (TID 930)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 67.0 in stage 9.0 (TID 916) in 537 ms on localhost (66/200)
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00018-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130781 length: 130781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO Executor: Finished task 65.0 in stage 9.0 (TID 914). 2341 bytes result sent to driver
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO TaskSetManager: Starting task 82.0 in stage 9.0 (TID 931, localhost, ANY, 1970 bytes)
15/08/21 23:14:24 INFO Executor: Running task 82.0 in stage 9.0 (TID 931)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 65.0 in stage 9.0 (TID 914) in 566 ms on localhost (67/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15383 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15383
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:24 INFO Executor: Finished task 66.0 in stage 9.0 (TID 915). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 83.0 in stage 9.0 (TID 932, localhost, ANY, 1970 bytes)
15/08/21 23:14:24 INFO Executor: Running task 83.0 in stage 9.0 (TID 932)
15/08/21 23:14:24 INFO Executor: Finished task 71.0 in stage 9.0 (TID 920). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 84.0 in stage 9.0 (TID 933, localhost, ANY, 1971 bytes)
15/08/21 23:14:24 INFO Executor: Running task 84.0 in stage 9.0 (TID 933)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 66.0 in stage 9.0 (TID 915) in 584 ms on localhost (68/200)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 71.0 in stage 9.0 (TID 920) in 514 ms on localhost (69/200)
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO Executor: Finished task 68.0 in stage 9.0 (TID 917). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO TaskSetManager: Starting task 85.0 in stage 9.0 (TID 934, localhost, ANY, 1969 bytes)
15/08/21 23:14:24 INFO Executor: Running task 85.0 in stage 9.0 (TID 934)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 68.0 in stage 9.0 (TID 917) in 545 ms on localhost (70/200)
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00010-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125452 length: 125452 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00170-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129846 length: 129846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14740 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15267 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14740
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 15267
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00171-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128525 length: 128525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00131-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125484 length: 125484 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 INFO Executor: Finished task 70.0 in stage 9.0 (TID 919). 2341 bytes result sent to driver
15/08/21 23:14:24 INFO Executor: Finished task 69.0 in stage 9.0 (TID 918). 2341 bytes result sent to driver
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO TaskSetManager: Starting task 86.0 in stage 9.0 (TID 935, localhost, ANY, 1972 bytes)
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO TaskSetManager: Starting task 87.0 in stage 9.0 (TID 936, localhost, ANY, 1969 bytes)
15/08/21 23:14:24 INFO Executor: Running task 87.0 in stage 9.0 (TID 936)
15/08/21 23:14:24 INFO Executor: Running task 86.0 in stage 9.0 (TID 935)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 70.0 in stage 9.0 (TID 919) in 568 ms on localhost (71/200)
15/08/21 23:14:24 INFO TaskSetManager: Finished task 69.0 in stage 9.0 (TID 918) in 582 ms on localhost (72/200)
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14741 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15098 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14741
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15098
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00194-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131134 length: 131134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15432 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15432
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00144-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132394 length: 132394 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00021-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123547 length: 123547 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15532 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15532
15/08/21 23:14:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14509 records.
15/08/21 23:14:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:24 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14509
15/08/21 23:14:25 INFO Executor: Finished task 72.0 in stage 9.0 (TID 921). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 88.0 in stage 9.0 (TID 937, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 88.0 in stage 9.0 (TID 937)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 72.0 in stage 9.0 (TID 921) in 492 ms on localhost (73/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00065-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126524 length: 126524 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO Executor: Finished task 74.0 in stage 9.0 (TID 923). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO Executor: Finished task 73.0 in stage 9.0 (TID 922). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 89.0 in stage 9.0 (TID 938, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 89.0 in stage 9.0 (TID 938)
15/08/21 23:14:25 INFO TaskSetManager: Starting task 90.0 in stage 9.0 (TID 939, localhost, ANY, 1969 bytes)
15/08/21 23:14:25 INFO Executor: Running task 90.0 in stage 9.0 (TID 939)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 74.0 in stage 9.0 (TID 923) in 503 ms on localhost (74/200)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 73.0 in stage 9.0 (TID 922) in 506 ms on localhost (75/200)
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14896 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14896
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00017-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 121974 length: 121974 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00048-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130936 length: 130936 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15384 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14345 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15384
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14345
15/08/21 23:14:25 INFO Executor: Finished task 75.0 in stage 9.0 (TID 924). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 91.0 in stage 9.0 (TID 940, localhost, ANY, 1969 bytes)
15/08/21 23:14:25 INFO Executor: Running task 91.0 in stage 9.0 (TID 940)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 75.0 in stage 9.0 (TID 924) in 522 ms on localhost (76/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO Executor: Finished task 76.0 in stage 9.0 (TID 925). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 92.0 in stage 9.0 (TID 941, localhost, ANY, 1968 bytes)
15/08/21 23:14:25 INFO Executor: Running task 92.0 in stage 9.0 (TID 941)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 76.0 in stage 9.0 (TID 925) in 520 ms on localhost (77/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00156-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129892 length: 129892 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15272 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15272
15/08/21 23:14:25 INFO Executor: Finished task 77.0 in stage 9.0 (TID 926). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 93.0 in stage 9.0 (TID 942, localhost, ANY, 1969 bytes)
15/08/21 23:14:25 INFO Executor: Running task 93.0 in stage 9.0 (TID 942)
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00009-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126722 length: 126722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO TaskSetManager: Finished task 77.0 in stage 9.0 (TID 926) in 525 ms on localhost (78/200)
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14919 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14919
15/08/21 23:14:25 INFO Executor: Finished task 79.0 in stage 9.0 (TID 928). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 94.0 in stage 9.0 (TID 943, localhost, ANY, 1971 bytes)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 79.0 in stage 9.0 (TID 928) in 524 ms on localhost (79/200)
15/08/21 23:14:25 INFO Executor: Running task 94.0 in stage 9.0 (TID 943)
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00163-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128863 length: 128863 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO Executor: Finished task 78.0 in stage 9.0 (TID 927). 2341 bytes result sent to driver
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO TaskSetManager: Starting task 95.0 in stage 9.0 (TID 944, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Running task 95.0 in stage 9.0 (TID 944)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 78.0 in stage 9.0 (TID 927) in 563 ms on localhost (80/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15150 records.
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15150
15/08/21 23:14:25 INFO Executor: Finished task 80.0 in stage 9.0 (TID 929). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 96.0 in stage 9.0 (TID 945, localhost, ANY, 1969 bytes)
15/08/21 23:14:25 INFO Executor: Running task 96.0 in stage 9.0 (TID 945)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 80.0 in stage 9.0 (TID 929) in 548 ms on localhost (81/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00101-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126647 length: 126647 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00062-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129248 length: 129248 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15189 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14874 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15189
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14874
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00160-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131767 length: 131767 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15474 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15474
15/08/21 23:14:25 INFO Executor: Finished task 82.0 in stage 9.0 (TID 931). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 97.0 in stage 9.0 (TID 946, localhost, ANY, 1971 bytes)
15/08/21 23:14:25 INFO Executor: Running task 97.0 in stage 9.0 (TID 946)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 82.0 in stage 9.0 (TID 931) in 560 ms on localhost (82/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO Executor: Finished task 81.0 in stage 9.0 (TID 930). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 98.0 in stage 9.0 (TID 947, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Running task 98.0 in stage 9.0 (TID 947)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 81.0 in stage 9.0 (TID 930) in 586 ms on localhost (83/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO Executor: Finished task 84.0 in stage 9.0 (TID 933). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 99.0 in stage 9.0 (TID 948, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Running task 99.0 in stage 9.0 (TID 948)
15/08/21 23:14:25 INFO Executor: Finished task 83.0 in stage 9.0 (TID 932). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Finished task 84.0 in stage 9.0 (TID 933) in 558 ms on localhost (84/200)
15/08/21 23:14:25 INFO TaskSetManager: Starting task 100.0 in stage 9.0 (TID 949, localhost, ANY, 1969 bytes)
15/08/21 23:14:25 INFO Executor: Running task 100.0 in stage 9.0 (TID 949)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 83.0 in stage 9.0 (TID 932) in 566 ms on localhost (85/200)
15/08/21 23:14:25 INFO Executor: Finished task 85.0 in stage 9.0 (TID 934). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00104-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124286 length: 124286 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO TaskSetManager: Starting task 101.0 in stage 9.0 (TID 950, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 101.0 in stage 9.0 (TID 950)
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO TaskSetManager: Finished task 85.0 in stage 9.0 (TID 934) in 555 ms on localhost (86/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14569 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14569
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00124-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126164 length: 126164 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14810 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14810
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00146-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126862 length: 126862 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00120-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127381 length: 127381 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO Executor: Finished task 87.0 in stage 9.0 (TID 936). 2341 bytes result sent to driver
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO TaskSetManager: Starting task 102.0 in stage 9.0 (TID 951, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00026-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130877 length: 130877 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO Executor: Running task 102.0 in stage 9.0 (TID 951)
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO TaskSetManager: Finished task 87.0 in stage 9.0 (TID 936) in 570 ms on localhost (87/200)
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14914 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14914
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14951 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14951
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15393 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO Executor: Finished task 86.0 in stage 9.0 (TID 935). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15393
15/08/21 23:14:25 INFO TaskSetManager: Starting task 103.0 in stage 9.0 (TID 952, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Running task 103.0 in stage 9.0 (TID 952)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 86.0 in stage 9.0 (TID 935) in 583 ms on localhost (88/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO Executor: Finished task 88.0 in stage 9.0 (TID 937). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 104.0 in stage 9.0 (TID 953, localhost, ANY, 1971 bytes)
15/08/21 23:14:25 INFO Executor: Running task 104.0 in stage 9.0 (TID 953)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO TaskSetManager: Finished task 88.0 in stage 9.0 (TID 937) in 513 ms on localhost (89/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00005-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128656 length: 128656 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15106 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15106
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00129-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126488 length: 126488 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO Executor: Finished task 89.0 in stage 9.0 (TID 938). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14879 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14879
15/08/21 23:14:25 INFO TaskSetManager: Starting task 105.0 in stage 9.0 (TID 954, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Running task 105.0 in stage 9.0 (TID 954)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 89.0 in stage 9.0 (TID 938) in 499 ms on localhost (90/200)
15/08/21 23:14:25 INFO Executor: Finished task 90.0 in stage 9.0 (TID 939). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 106.0 in stage 9.0 (TID 955, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 106.0 in stage 9.0 (TID 955)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 90.0 in stage 9.0 (TID 939) in 505 ms on localhost (91/200)
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00130-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123317 length: 123317 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14502 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14502
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00030-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126145 length: 126145 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00118-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123984 length: 123984 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14816 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14550 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14816
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14550
15/08/21 23:14:25 INFO Executor: Finished task 91.0 in stage 9.0 (TID 940). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 107.0 in stage 9.0 (TID 956, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Finished task 92.0 in stage 9.0 (TID 941). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO Executor: Running task 107.0 in stage 9.0 (TID 956)
15/08/21 23:14:25 INFO TaskSetManager: Starting task 108.0 in stage 9.0 (TID 957, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 108.0 in stage 9.0 (TID 957)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 91.0 in stage 9.0 (TID 940) in 491 ms on localhost (92/200)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 92.0 in stage 9.0 (TID 941) in 456 ms on localhost (93/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00073-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125058 length: 125058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00032-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125803 length: 125803 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO Executor: Finished task 93.0 in stage 9.0 (TID 942). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 109.0 in stage 9.0 (TID 958, localhost, ANY, 1971 bytes)
15/08/21 23:14:25 INFO Executor: Running task 109.0 in stage 9.0 (TID 958)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 93.0 in stage 9.0 (TID 942) in 457 ms on localhost (94/200)
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14718 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14776 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14718
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14776
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO Executor: Finished task 94.0 in stage 9.0 (TID 943). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 110.0 in stage 9.0 (TID 959, localhost, ANY, 1971 bytes)
15/08/21 23:14:25 INFO Executor: Running task 110.0 in stage 9.0 (TID 959)
15/08/21 23:14:25 INFO Executor: Finished task 95.0 in stage 9.0 (TID 944). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 111.0 in stage 9.0 (TID 960, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 111.0 in stage 9.0 (TID 960)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 94.0 in stage 9.0 (TID 943) in 444 ms on localhost (95/200)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 95.0 in stage 9.0 (TID 944) in 436 ms on localhost (96/200)
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00179-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126039 length: 126039 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO Executor: Finished task 96.0 in stage 9.0 (TID 945). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 112.0 in stage 9.0 (TID 961, localhost, ANY, 1971 bytes)
15/08/21 23:14:25 INFO Executor: Running task 112.0 in stage 9.0 (TID 961)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 96.0 in stage 9.0 (TID 945) in 445 ms on localhost (97/200)
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00086-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126814 length: 126814 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00119-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127285 length: 127285 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00106-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126356 length: 126356 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14954 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14891 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14954
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14891
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14848 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14848
15/08/21 23:14:25 INFO Executor: Finished task 97.0 in stage 9.0 (TID 946). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 113.0 in stage 9.0 (TID 962, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 113.0 in stage 9.0 (TID 962)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 97.0 in stage 9.0 (TID 946) in 445 ms on localhost (98/200)
15/08/21 23:14:25 INFO Executor: Finished task 98.0 in stage 9.0 (TID 947). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 114.0 in stage 9.0 (TID 963, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Running task 114.0 in stage 9.0 (TID 963)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO TaskSetManager: Finished task 98.0 in stage 9.0 (TID 947) in 441 ms on localhost (99/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO Executor: Finished task 101.0 in stage 9.0 (TID 950). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 115.0 in stage 9.0 (TID 964, localhost, ANY, 1970 bytes)
15/08/21 23:14:25 INFO Executor: Running task 115.0 in stage 9.0 (TID 964)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 101.0 in stage 9.0 (TID 950) in 436 ms on localhost (100/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00139-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130820 length: 130820 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00188-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127906 length: 127906 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15009 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15361 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15009
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15361
15/08/21 23:14:25 INFO Executor: Finished task 99.0 in stage 9.0 (TID 948). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 116.0 in stage 9.0 (TID 965, localhost, ANY, 1969 bytes)
15/08/21 23:14:25 INFO Executor: Running task 116.0 in stage 9.0 (TID 965)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 99.0 in stage 9.0 (TID 948) in 484 ms on localhost (101/200)
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00042-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132162 length: 132162 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15557 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15557
15/08/21 23:14:25 INFO Executor: Finished task 100.0 in stage 9.0 (TID 949). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 117.0 in stage 9.0 (TID 966, localhost, ANY, 1972 bytes)
15/08/21 23:14:25 INFO Executor: Running task 117.0 in stage 9.0 (TID 966)
15/08/21 23:14:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00161-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128306 length: 128306 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:25 INFO TaskSetManager: Finished task 100.0 in stage 9.0 (TID 949) in 532 ms on localhost (102/200)
15/08/21 23:14:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15099 records.
15/08/21 23:14:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15099
15/08/21 23:14:25 INFO Executor: Finished task 102.0 in stage 9.0 (TID 951). 2341 bytes result sent to driver
15/08/21 23:14:25 INFO TaskSetManager: Starting task 118.0 in stage 9.0 (TID 967, localhost, ANY, 1973 bytes)
15/08/21 23:14:25 INFO Executor: Running task 118.0 in stage 9.0 (TID 967)
15/08/21 23:14:25 INFO TaskSetManager: Finished task 102.0 in stage 9.0 (TID 951) in 492 ms on localhost (103/200)
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00183-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125911 length: 125911 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO Executor: Finished task 104.0 in stage 9.0 (TID 953). 2341 bytes result sent to driver
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO TaskSetManager: Starting task 119.0 in stage 9.0 (TID 968, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 119.0 in stage 9.0 (TID 968)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 104.0 in stage 9.0 (TID 953) in 504 ms on localhost (104/200)
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00045-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126410 length: 126410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14842 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14842
15/08/21 23:14:26 INFO Executor: Finished task 103.0 in stage 9.0 (TID 952). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 120.0 in stage 9.0 (TID 969, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 120.0 in stage 9.0 (TID 969)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 103.0 in stage 9.0 (TID 952) in 560 ms on localhost (105/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 105.0 in stage 9.0 (TID 954). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 121.0 in stage 9.0 (TID 970, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 121.0 in stage 9.0 (TID 970)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 105.0 in stage 9.0 (TID 954) in 499 ms on localhost (106/200)
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00029-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127953 length: 127953 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15029 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15029
15/08/21 23:14:26 INFO Executor: Finished task 106.0 in stage 9.0 (TID 955). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 122.0 in stage 9.0 (TID 971, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 122.0 in stage 9.0 (TID 971)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 106.0 in stage 9.0 (TID 955) in 519 ms on localhost (107/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00197-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125981 length: 125981 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00060-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130262 length: 130262 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15306 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15306
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00155-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130055 length: 130055 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15274 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15274
15/08/21 23:14:26 INFO Executor: Finished task 108.0 in stage 9.0 (TID 957). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO Executor: Finished task 107.0 in stage 9.0 (TID 956). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 123.0 in stage 9.0 (TID 972, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 123.0 in stage 9.0 (TID 972)
15/08/21 23:14:26 INFO TaskSetManager: Starting task 124.0 in stage 9.0 (TID 973, localhost, ANY, 1969 bytes)
15/08/21 23:14:26 INFO Executor: Running task 124.0 in stage 9.0 (TID 973)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 108.0 in stage 9.0 (TID 957) in 520 ms on localhost (108/200)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 107.0 in stage 9.0 (TID 956) in 523 ms on localhost (109/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00027-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00169-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126825 length: 126825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO Executor: Finished task 109.0 in stage 9.0 (TID 958). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 125.0 in stage 9.0 (TID 974, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 125.0 in stage 9.0 (TID 974)
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO TaskSetManager: Finished task 109.0 in stage 9.0 (TID 958) in 520 ms on localhost (110/200)
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14938 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15120 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14938
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15120
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 111.0 in stage 9.0 (TID 960). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 126.0 in stage 9.0 (TID 975, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 126.0 in stage 9.0 (TID 975)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 111.0 in stage 9.0 (TID 960) in 499 ms on localhost (111/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 112.0 in stage 9.0 (TID 961). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 127.0 in stage 9.0 (TID 976, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 127.0 in stage 9.0 (TID 976)
15/08/21 23:14:26 INFO Executor: Finished task 110.0 in stage 9.0 (TID 959). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 128.0 in stage 9.0 (TID 977, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 128.0 in stage 9.0 (TID 977)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 112.0 in stage 9.0 (TID 961) in 498 ms on localhost (112/200)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 110.0 in stage 9.0 (TID 959) in 521 ms on localhost (113/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00044-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128980 length: 128980 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15145 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15145
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00034-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128058 length: 128058 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15049 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15049
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00112-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127094 length: 127094 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14942 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14942
15/08/21 23:14:26 INFO Executor: Finished task 114.0 in stage 9.0 (TID 963). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 129.0 in stage 9.0 (TID 978, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 129.0 in stage 9.0 (TID 978)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 114.0 in stage 9.0 (TID 963) in 469 ms on localhost (114/200)
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00113-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125722 length: 125722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO Executor: Finished task 113.0 in stage 9.0 (TID 962). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 130.0 in stage 9.0 (TID 979, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 130.0 in stage 9.0 (TID 979)
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14780 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14780
15/08/21 23:14:26 INFO TaskSetManager: Finished task 113.0 in stage 9.0 (TID 962) in 499 ms on localhost (115/200)
15/08/21 23:14:26 INFO Executor: Finished task 115.0 in stage 9.0 (TID 964). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 131.0 in stage 9.0 (TID 980, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 131.0 in stage 9.0 (TID 980)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO TaskSetManager: Finished task 115.0 in stage 9.0 (TID 964) in 472 ms on localhost (116/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00071-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130743 length: 130743 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15368 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15368
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00111-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125610 length: 125610 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO Executor: Finished task 116.0 in stage 9.0 (TID 965). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 132.0 in stage 9.0 (TID 981, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 132.0 in stage 9.0 (TID 981)
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14750 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14750
15/08/21 23:14:26 INFO TaskSetManager: Finished task 116.0 in stage 9.0 (TID 965) in 472 ms on localhost (117/200)
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00075-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127119 length: 127119 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14923 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14923
15/08/21 23:14:26 INFO Executor: Finished task 117.0 in stage 9.0 (TID 966). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 133.0 in stage 9.0 (TID 982, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 133.0 in stage 9.0 (TID 982)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 117.0 in stage 9.0 (TID 966) in 450 ms on localhost (118/200)
15/08/21 23:14:26 INFO Executor: Finished task 118.0 in stage 9.0 (TID 967). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO TaskSetManager: Starting task 134.0 in stage 9.0 (TID 983, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 134.0 in stage 9.0 (TID 983)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 118.0 in stage 9.0 (TID 967) in 442 ms on localhost (119/200)
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00135-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127687 length: 127687 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15010 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15010
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00116-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125192 length: 125192 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14687 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14687
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00054-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124431 length: 124431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14612 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14612
15/08/21 23:14:26 INFO Executor: Finished task 119.0 in stage 9.0 (TID 968). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 135.0 in stage 9.0 (TID 984, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 135.0 in stage 9.0 (TID 984)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 119.0 in stage 9.0 (TID 968) in 512 ms on localhost (120/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 122.0 in stage 9.0 (TID 971). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO Executor: Finished task 120.0 in stage 9.0 (TID 969). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO Executor: Finished task 121.0 in stage 9.0 (TID 970). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 136.0 in stage 9.0 (TID 985, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 136.0 in stage 9.0 (TID 985)
15/08/21 23:14:26 INFO TaskSetManager: Starting task 137.0 in stage 9.0 (TID 986, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 137.0 in stage 9.0 (TID 986)
15/08/21 23:14:26 INFO TaskSetManager: Starting task 138.0 in stage 9.0 (TID 987, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 122.0 in stage 9.0 (TID 971) in 439 ms on localhost (121/200)
15/08/21 23:14:26 INFO Executor: Running task 138.0 in stage 9.0 (TID 987)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 121.0 in stage 9.0 (TID 970) in 468 ms on localhost (122/200)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 120.0 in stage 9.0 (TID 969) in 489 ms on localhost (123/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00015-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123431 length: 123431 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00037-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125797 length: 125797 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14489 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14489
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00033-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125553 length: 125553 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14771 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14771
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14791 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14791
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00172-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132763 length: 132763 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO Executor: Finished task 124.0 in stage 9.0 (TID 973). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 139.0 in stage 9.0 (TID 988, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 139.0 in stage 9.0 (TID 988)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 124.0 in stage 9.0 (TID 973) in 463 ms on localhost (124/200)
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15609 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15609
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 123.0 in stage 9.0 (TID 972). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 140.0 in stage 9.0 (TID 989, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 140.0 in stage 9.0 (TID 989)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 123.0 in stage 9.0 (TID 972) in 482 ms on localhost (125/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 125.0 in stage 9.0 (TID 974). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 141.0 in stage 9.0 (TID 990, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 141.0 in stage 9.0 (TID 990)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 125.0 in stage 9.0 (TID 974) in 469 ms on localhost (126/200)
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00195-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130525 length: 130525 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00058-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128745 length: 128745 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15330 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15141 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15330
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15141
15/08/21 23:14:26 INFO Executor: Finished task 126.0 in stage 9.0 (TID 975). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 142.0 in stage 9.0 (TID 991, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 142.0 in stage 9.0 (TID 991)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 126.0 in stage 9.0 (TID 975) in 493 ms on localhost (127/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00095-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125585 length: 125585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14751 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14751
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00117-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123426 length: 123426 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO Executor: Finished task 127.0 in stage 9.0 (TID 976). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14479 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO TaskSetManager: Starting task 143.0 in stage 9.0 (TID 992, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 143.0 in stage 9.0 (TID 992)
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 14479
15/08/21 23:14:26 INFO TaskSetManager: Finished task 127.0 in stage 9.0 (TID 976) in 535 ms on localhost (128/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 128.0 in stage 9.0 (TID 977). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 144.0 in stage 9.0 (TID 993, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 144.0 in stage 9.0 (TID 993)
15/08/21 23:14:26 INFO Executor: Finished task 129.0 in stage 9.0 (TID 978). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 145.0 in stage 9.0 (TID 994, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 128.0 in stage 9.0 (TID 977) in 562 ms on localhost (129/200)
15/08/21 23:14:26 INFO Executor: Running task 145.0 in stage 9.0 (TID 994)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 129.0 in stage 9.0 (TID 978) in 503 ms on localhost (130/200)
15/08/21 23:14:26 INFO Executor: Finished task 130.0 in stage 9.0 (TID 979). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 146.0 in stage 9.0 (TID 995, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 146.0 in stage 9.0 (TID 995)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 130.0 in stage 9.0 (TID 979) in 496 ms on localhost (131/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00020-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124941 length: 124941 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14678 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO Executor: Finished task 131.0 in stage 9.0 (TID 980). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 14678
15/08/21 23:14:26 INFO TaskSetManager: Starting task 147.0 in stage 9.0 (TID 996, localhost, ANY, 1972 bytes)
15/08/21 23:14:26 INFO Executor: Running task 147.0 in stage 9.0 (TID 996)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 131.0 in stage 9.0 (TID 980) in 501 ms on localhost (132/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00103-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128585 length: 128585 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00185-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127038 length: 127038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00150-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128292 length: 128292 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15063 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14952 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15090 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15063
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15090
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14952
15/08/21 23:14:26 INFO Executor: Finished task 132.0 in stage 9.0 (TID 981). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00099-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128451 length: 128451 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 INFO TaskSetManager: Starting task 148.0 in stage 9.0 (TID 997, localhost, ANY, 1970 bytes)
15/08/21 23:14:26 INFO Executor: Running task 148.0 in stage 9.0 (TID 997)
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO TaskSetManager: Finished task 132.0 in stage 9.0 (TID 981) in 509 ms on localhost (133/200)
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15096 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15096
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO Executor: Finished task 133.0 in stage 9.0 (TID 982). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 149.0 in stage 9.0 (TID 998, localhost, ANY, 1969 bytes)
15/08/21 23:14:26 INFO Executor: Running task 149.0 in stage 9.0 (TID 998)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 133.0 in stage 9.0 (TID 982) in 505 ms on localhost (134/200)
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00036-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128122 length: 128122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15046 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15046
15/08/21 23:14:26 INFO Executor: Finished task 134.0 in stage 9.0 (TID 983). 2341 bytes result sent to driver
15/08/21 23:14:26 INFO TaskSetManager: Starting task 150.0 in stage 9.0 (TID 999, localhost, ANY, 1971 bytes)
15/08/21 23:14:26 INFO Executor: Running task 150.0 in stage 9.0 (TID 999)
15/08/21 23:14:26 INFO TaskSetManager: Finished task 134.0 in stage 9.0 (TID 983) in 540 ms on localhost (135/200)
15/08/21 23:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00162-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132042 length: 132042 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15524 records.
15/08/21 23:14:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15524
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00165-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125376 length: 125376 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14731 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14731
15/08/21 23:14:27 INFO Executor: Finished task 135.0 in stage 9.0 (TID 984). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 151.0 in stage 9.0 (TID 1000, localhost, ANY, 1968 bytes)
15/08/21 23:14:27 INFO Executor: Running task 151.0 in stage 9.0 (TID 1000)
15/08/21 23:14:27 INFO Executor: Finished task 137.0 in stage 9.0 (TID 986). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 152.0 in stage 9.0 (TID 1001, localhost, ANY, 1970 bytes)
15/08/21 23:14:27 INFO Executor: Running task 152.0 in stage 9.0 (TID 1001)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 135.0 in stage 9.0 (TID 984) in 515 ms on localhost (136/200)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 137.0 in stage 9.0 (TID 986) in 499 ms on localhost (137/200)
15/08/21 23:14:27 INFO Executor: Finished task 138.0 in stage 9.0 (TID 987). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Finished task 138.0 in stage 9.0 (TID 987) in 504 ms on localhost (138/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO TaskSetManager: Starting task 153.0 in stage 9.0 (TID 1002, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO Executor: Running task 153.0 in stage 9.0 (TID 1002)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00141-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133497 length: 133497 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00137-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128354 length: 128354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO Executor: Finished task 136.0 in stage 9.0 (TID 985). 2341 bytes result sent to driver
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO TaskSetManager: Finished task 136.0 in stage 9.0 (TID 985) in 548 ms on localhost (139/200)
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00121-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125851 length: 125851 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO TaskSetManager: Starting task 154.0 in stage 9.0 (TID 1003, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 154.0 in stage 9.0 (TID 1003)
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15114 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14802 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15679 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15679
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14802
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15114
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO Executor: Finished task 140.0 in stage 9.0 (TID 989). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 155.0 in stage 9.0 (TID 1004, localhost, ANY, 1970 bytes)
15/08/21 23:14:27 INFO Executor: Running task 155.0 in stage 9.0 (TID 1004)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 140.0 in stage 9.0 (TID 989) in 499 ms on localhost (140/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00081-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128640 length: 128640 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO Executor: Finished task 139.0 in stage 9.0 (TID 988). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 156.0 in stage 9.0 (TID 1005, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO Executor: Running task 156.0 in stage 9.0 (TID 1005)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 139.0 in stage 9.0 (TID 988) in 545 ms on localhost (141/200)
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15142 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO Executor: Finished task 141.0 in stage 9.0 (TID 990). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15142
15/08/21 23:14:27 INFO TaskSetManager: Starting task 157.0 in stage 9.0 (TID 1006, localhost, ANY, 1968 bytes)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO Executor: Running task 157.0 in stage 9.0 (TID 1006)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 141.0 in stage 9.0 (TID 990) in 505 ms on localhost (142/200)
15/08/21 23:14:27 INFO Executor: Finished task 142.0 in stage 9.0 (TID 991). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00013-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124275 length: 124275 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO TaskSetManager: Starting task 158.0 in stage 9.0 (TID 1007, localhost, ANY, 1968 bytes)
15/08/21 23:14:27 INFO Executor: Running task 158.0 in stage 9.0 (TID 1007)
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO TaskSetManager: Finished task 142.0 in stage 9.0 (TID 991) in 495 ms on localhost (143/200)
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14590 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 14590
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00069-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127699 length: 127699 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00076-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133781 length: 133781 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14991 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14991
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15726 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15726
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00088-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131624 length: 131624 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO Executor: Finished task 143.0 in stage 9.0 (TID 992). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15463 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15463
15/08/21 23:14:27 INFO TaskSetManager: Starting task 159.0 in stage 9.0 (TID 1008, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 159.0 in stage 9.0 (TID 1008)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 143.0 in stage 9.0 (TID 992) in 507 ms on localhost (144/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO Executor: Finished task 146.0 in stage 9.0 (TID 995). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO Executor: Finished task 145.0 in stage 9.0 (TID 994). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 160.0 in stage 9.0 (TID 1009, localhost, ANY, 1970 bytes)
15/08/21 23:14:27 INFO Executor: Running task 160.0 in stage 9.0 (TID 1009)
15/08/21 23:14:27 INFO TaskSetManager: Starting task 161.0 in stage 9.0 (TID 1010, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 146.0 in stage 9.0 (TID 995) in 520 ms on localhost (145/200)
15/08/21 23:14:27 INFO Executor: Running task 161.0 in stage 9.0 (TID 1010)
15/08/21 23:14:27 INFO Executor: Finished task 144.0 in stage 9.0 (TID 993). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Finished task 145.0 in stage 9.0 (TID 994) in 531 ms on localhost (146/200)
15/08/21 23:14:27 INFO TaskSetManager: Starting task 162.0 in stage 9.0 (TID 1011, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 162.0 in stage 9.0 (TID 1011)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 144.0 in stage 9.0 (TID 993) in 538 ms on localhost (147/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00125-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127825 length: 127825 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15007 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15007
15/08/21 23:14:27 INFO Executor: Finished task 147.0 in stage 9.0 (TID 996). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 163.0 in stage 9.0 (TID 1012, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 163.0 in stage 9.0 (TID 1012)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 147.0 in stage 9.0 (TID 996) in 530 ms on localhost (148/200)
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00008-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125677 length: 125677 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00039-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129392 length: 129392 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15203 records.
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00178-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130175 length: 130175 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14755 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15203
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14755
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15313 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15313
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00198-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131025 length: 131025 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO Executor: Finished task 148.0 in stage 9.0 (TID 997). 2341 bytes result sent to driver
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO TaskSetManager: Starting task 164.0 in stage 9.0 (TID 1013, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO Executor: Running task 164.0 in stage 9.0 (TID 1013)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 148.0 in stage 9.0 (TID 997) in 551 ms on localhost (149/200)
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15396 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 15396
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO Executor: Finished task 149.0 in stage 9.0 (TID 998). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 165.0 in stage 9.0 (TID 1014, localhost, ANY, 1970 bytes)
15/08/21 23:14:27 INFO Executor: Running task 165.0 in stage 9.0 (TID 1014)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 149.0 in stage 9.0 (TID 998) in 561 ms on localhost (150/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO Executor: Finished task 150.0 in stage 9.0 (TID 999). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 166.0 in stage 9.0 (TID 1015, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 166.0 in stage 9.0 (TID 1015)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 150.0 in stage 9.0 (TID 999) in 543 ms on localhost (151/200)
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00078-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129254 length: 129254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15191 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15191
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00006-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 123983 length: 123983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14560 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 14560
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00122-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 122705 length: 122705 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14440 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14440
15/08/21 23:14:27 INFO Executor: Finished task 151.0 in stage 9.0 (TID 1000). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 167.0 in stage 9.0 (TID 1016, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO Executor: Running task 167.0 in stage 9.0 (TID 1016)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 151.0 in stage 9.0 (TID 1000) in 582 ms on localhost (152/200)
15/08/21 23:14:27 INFO Executor: Finished task 152.0 in stage 9.0 (TID 1001). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO Executor: Finished task 153.0 in stage 9.0 (TID 1002). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 168.0 in stage 9.0 (TID 1017, localhost, ANY, 1970 bytes)
15/08/21 23:14:27 INFO Executor: Running task 168.0 in stage 9.0 (TID 1017)
15/08/21 23:14:27 INFO TaskSetManager: Starting task 169.0 in stage 9.0 (TID 1018, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO Executor: Running task 169.0 in stage 9.0 (TID 1018)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 152.0 in stage 9.0 (TID 1001) in 586 ms on localhost (153/200)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 153.0 in stage 9.0 (TID 1002) in 577 ms on localhost (154/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00082-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130505 length: 130505 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15358 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15358
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00138-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126370 length: 126370 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00175-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130966 length: 130966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14845 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14845
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15364 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15364
15/08/21 23:14:27 INFO Executor: Finished task 156.0 in stage 9.0 (TID 1005). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO Executor: Finished task 154.0 in stage 9.0 (TID 1003). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO Executor: Finished task 155.0 in stage 9.0 (TID 1004). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 170.0 in stage 9.0 (TID 1019, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 170.0 in stage 9.0 (TID 1019)
15/08/21 23:14:27 INFO TaskSetManager: Starting task 171.0 in stage 9.0 (TID 1020, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 171.0 in stage 9.0 (TID 1020)
15/08/21 23:14:27 INFO TaskSetManager: Starting task 172.0 in stage 9.0 (TID 1021, localhost, ANY, 1969 bytes)
15/08/21 23:14:27 INFO Executor: Running task 172.0 in stage 9.0 (TID 1021)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 155.0 in stage 9.0 (TID 1004) in 584 ms on localhost (155/200)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 154.0 in stage 9.0 (TID 1003) in 639 ms on localhost (156/200)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 156.0 in stage 9.0 (TID 1005) in 561 ms on localhost (157/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO Executor: Finished task 157.0 in stage 9.0 (TID 1006). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 173.0 in stage 9.0 (TID 1022, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 173.0 in stage 9.0 (TID 1022)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 157.0 in stage 9.0 (TID 1006) in 573 ms on localhost (158/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO Executor: Finished task 158.0 in stage 9.0 (TID 1007). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00056-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128649 length: 128649 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00186-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128795 length: 128795 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO TaskSetManager: Starting task 174.0 in stage 9.0 (TID 1023, localhost, ANY, 1970 bytes)
15/08/21 23:14:27 INFO Executor: Running task 174.0 in stage 9.0 (TID 1023)
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO TaskSetManager: Finished task 158.0 in stage 9.0 (TID 1007) in 594 ms on localhost (159/200)
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15137 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15137
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15101 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15101
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00180-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132718 length: 132718 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00055-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127838 length: 127838 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15586 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15586
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15013 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15013
15/08/21 23:14:27 INFO Executor: Finished task 159.0 in stage 9.0 (TID 1008). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 175.0 in stage 9.0 (TID 1024, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO Executor: Running task 175.0 in stage 9.0 (TID 1024)
15/08/21 23:14:27 INFO Executor: Finished task 161.0 in stage 9.0 (TID 1010). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Finished task 159.0 in stage 9.0 (TID 1008) in 586 ms on localhost (160/200)
15/08/21 23:14:27 INFO TaskSetManager: Starting task 176.0 in stage 9.0 (TID 1025, localhost, ANY, 1971 bytes)
15/08/21 23:14:27 INFO Executor: Running task 176.0 in stage 9.0 (TID 1025)
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00041-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129384 length: 129384 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO TaskSetManager: Finished task 161.0 in stage 9.0 (TID 1010) in 526 ms on localhost (161/200)
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15235 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15235
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO Executor: Finished task 160.0 in stage 9.0 (TID 1009). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 177.0 in stage 9.0 (TID 1026, localhost, ANY, 1969 bytes)
15/08/21 23:14:27 INFO Executor: Running task 177.0 in stage 9.0 (TID 1026)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 160.0 in stage 9.0 (TID 1009) in 554 ms on localhost (162/200)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO Executor: Finished task 163.0 in stage 9.0 (TID 1012). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 178.0 in stage 9.0 (TID 1027, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00068-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124489 length: 124489 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00182-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128756 length: 128756 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO Executor: Running task 178.0 in stage 9.0 (TID 1027)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 163.0 in stage 9.0 (TID 1012) in 558 ms on localhost (163/200)
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO Executor: Finished task 162.0 in stage 9.0 (TID 1011). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 179.0 in stage 9.0 (TID 1028, localhost, ANY, 1969 bytes)
15/08/21 23:14:27 INFO Executor: Running task 179.0 in stage 9.0 (TID 1028)
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14613 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO TaskSetManager: Finished task 162.0 in stage 9.0 (TID 1011) in 595 ms on localhost (164/200)
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14613
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15117 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15117
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00022-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130671 length: 130671 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO Executor: Finished task 164.0 in stage 9.0 (TID 1013). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 180.0 in stage 9.0 (TID 1029, localhost, ANY, 1970 bytes)
15/08/21 23:14:27 INFO Executor: Running task 180.0 in stage 9.0 (TID 1029)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 164.0 in stage 9.0 (TID 1013) in 529 ms on localhost (165/200)
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15344 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15344
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00092-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128472 length: 128472 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00077-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131597 length: 131597 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15104 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15470 records.
15/08/21 23:14:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15104
15/08/21 23:14:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15470
15/08/21 23:14:27 INFO Executor: Finished task 165.0 in stage 9.0 (TID 1014). 2341 bytes result sent to driver
15/08/21 23:14:27 INFO TaskSetManager: Starting task 181.0 in stage 9.0 (TID 1030, localhost, ANY, 1972 bytes)
15/08/21 23:14:27 INFO Executor: Running task 181.0 in stage 9.0 (TID 1030)
15/08/21 23:14:27 INFO TaskSetManager: Finished task 165.0 in stage 9.0 (TID 1014) in 530 ms on localhost (166/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO Executor: Finished task 166.0 in stage 9.0 (TID 1015). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 182.0 in stage 9.0 (TID 1031, localhost, ANY, 1970 bytes)
15/08/21 23:14:28 INFO Executor: Running task 182.0 in stage 9.0 (TID 1031)
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00166-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 129029 length: 129029 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO TaskSetManager: Finished task 166.0 in stage 9.0 (TID 1015) in 526 ms on localhost (167/200)
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15147 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15147
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00110-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124413 length: 124413 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14597 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14597
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00035-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130900 length: 130900 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15386 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15386
15/08/21 23:14:28 INFO Executor: Finished task 169.0 in stage 9.0 (TID 1018). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 183.0 in stage 9.0 (TID 1032, localhost, ANY, 1969 bytes)
15/08/21 23:14:28 INFO Executor: Running task 183.0 in stage 9.0 (TID 1032)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 169.0 in stage 9.0 (TID 1018) in 534 ms on localhost (168/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO Executor: Finished task 167.0 in stage 9.0 (TID 1016). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 184.0 in stage 9.0 (TID 1033, localhost, ANY, 1972 bytes)
15/08/21 23:14:28 INFO Executor: Running task 184.0 in stage 9.0 (TID 1033)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 167.0 in stage 9.0 (TID 1016) in 561 ms on localhost (169/200)
15/08/21 23:14:28 INFO Executor: Finished task 168.0 in stage 9.0 (TID 1017). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 185.0 in stage 9.0 (TID 1034, localhost, ANY, 1968 bytes)
15/08/21 23:14:28 INFO Executor: Running task 185.0 in stage 9.0 (TID 1034)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO TaskSetManager: Finished task 168.0 in stage 9.0 (TID 1017) in 562 ms on localhost (170/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00090-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132730 length: 132730 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15614 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15614
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00143-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 124956 length: 124956 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00158-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 133236 length: 133236 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14677 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15658 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14677
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15658
15/08/21 23:14:28 INFO Executor: Finished task 170.0 in stage 9.0 (TID 1019). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 186.0 in stage 9.0 (TID 1035, localhost, ANY, 1971 bytes)
15/08/21 23:14:28 INFO Executor: Running task 186.0 in stage 9.0 (TID 1035)
15/08/21 23:14:28 INFO Executor: Finished task 171.0 in stage 9.0 (TID 1020). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Finished task 170.0 in stage 9.0 (TID 1019) in 598 ms on localhost (171/200)
15/08/21 23:14:28 INFO TaskSetManager: Starting task 187.0 in stage 9.0 (TID 1036, localhost, ANY, 1972 bytes)
15/08/21 23:14:28 INFO Executor: Running task 187.0 in stage 9.0 (TID 1036)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 171.0 in stage 9.0 (TID 1020) in 608 ms on localhost (172/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:28 INFO Executor: Finished task 172.0 in stage 9.0 (TID 1021). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 188.0 in stage 9.0 (TID 1037, localhost, ANY, 1970 bytes)
15/08/21 23:14:28 INFO Executor: Running task 188.0 in stage 9.0 (TID 1037)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 172.0 in stage 9.0 (TID 1021) in 644 ms on localhost (173/200)
15/08/21 23:14:28 INFO Executor: Finished task 173.0 in stage 9.0 (TID 1022). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 189.0 in stage 9.0 (TID 1038, localhost, ANY, 1972 bytes)
15/08/21 23:14:28 INFO Executor: Running task 189.0 in stage 9.0 (TID 1038)
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00142-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125884 length: 125884 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00001-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125713 length: 125713 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO TaskSetManager: Finished task 173.0 in stage 9.0 (TID 1022) in 624 ms on localhost (174/200)
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14784 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14784
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14788 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14788
15/08/21 23:14:28 INFO Executor: Finished task 174.0 in stage 9.0 (TID 1023). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 190.0 in stage 9.0 (TID 1039, localhost, ANY, 1971 bytes)
15/08/21 23:14:28 INFO Executor: Running task 190.0 in stage 9.0 (TID 1039)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 174.0 in stage 9.0 (TID 1023) in 613 ms on localhost (175/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00084-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127919 length: 127919 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00067-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127583 length: 127583 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15038 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14982 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15038
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 14982
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00173-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128085 length: 128085 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 15039
15/08/21 23:14:28 INFO Executor: Finished task 176.0 in stage 9.0 (TID 1025). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO Executor: Finished task 175.0 in stage 9.0 (TID 1024). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 191.0 in stage 9.0 (TID 1040, localhost, ANY, 1971 bytes)
15/08/21 23:14:28 INFO Executor: Running task 191.0 in stage 9.0 (TID 1040)
15/08/21 23:14:28 INFO TaskSetManager: Starting task 192.0 in stage 9.0 (TID 1041, localhost, ANY, 1972 bytes)
15/08/21 23:14:28 INFO Executor: Running task 192.0 in stage 9.0 (TID 1041)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 176.0 in stage 9.0 (TID 1025) in 627 ms on localhost (176/200)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 175.0 in stage 9.0 (TID 1024) in 633 ms on localhost (177/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:28 INFO Executor: Finished task 177.0 in stage 9.0 (TID 1026). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 193.0 in stage 9.0 (TID 1042, localhost, ANY, 1968 bytes)
15/08/21 23:14:28 INFO Executor: Running task 193.0 in stage 9.0 (TID 1042)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 177.0 in stage 9.0 (TID 1026) in 643 ms on localhost (178/200)
15/08/21 23:14:28 INFO Executor: Finished task 178.0 in stage 9.0 (TID 1027). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 194.0 in stage 9.0 (TID 1043, localhost, ANY, 1968 bytes)
15/08/21 23:14:28 INFO Executor: Running task 194.0 in stage 9.0 (TID 1043)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 178.0 in stage 9.0 (TID 1027) in 609 ms on localhost (179/200)
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00133-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 128087 length: 128087 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00028-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127983 length: 127983 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO Executor: Finished task 179.0 in stage 9.0 (TID 1028). 2341 bytes result sent to driver
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO TaskSetManager: Starting task 195.0 in stage 9.0 (TID 1044, localhost, ANY, 1970 bytes)
15/08/21 23:14:28 INFO Executor: Running task 195.0 in stage 9.0 (TID 1044)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 179.0 in stage 9.0 (TID 1028) in 610 ms on localhost (180/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15043 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15043
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15039 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15039
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO Executor: Finished task 180.0 in stage 9.0 (TID 1029). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 196.0 in stage 9.0 (TID 1045, localhost, ANY, 1972 bytes)
15/08/21 23:14:28 INFO Executor: Running task 196.0 in stage 9.0 (TID 1045)
15/08/21 23:14:28 INFO TaskSetManager: Finished task 180.0 in stage 9.0 (TID 1029) in 603 ms on localhost (181/200)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00148-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 131931 length: 131931 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00190-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 132955 length: 132955 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00024-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 130354 length: 130354 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15509 records.
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15625 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15509
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 15625
15/08/21 23:14:28 INFO Executor: Finished task 181.0 in stage 9.0 (TID 1030). 2341 bytes result sent to driver
15/08/21 23:14:28 INFO TaskSetManager: Starting task 197.0 in stage 9.0 (TID 1046, localhost, ANY, 1972 bytes)
15/08/21 23:14:28 INFO Executor: Running task 197.0 in stage 9.0 (TID 1046)
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15302 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO TaskSetManager: Finished task 181.0 in stage 9.0 (TID 1030) in 623 ms on localhost (182/200)
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 15302
15/08/21 23:14:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00149-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127894 length: 127894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:28 INFO Executor: Finished task 182.0 in stage 9.0 (TID 1031). 2341 bytes result sent to driver
15/08/21 23:14:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:28 INFO TaskSetManager: Starting task 198.0 in stage 9.0 (TID 1047, localhost, ANY, 1971 bytes)
15/08/21 23:14:28 INFO Executor: Running task 198.0 in stage 9.0 (TID 1047)
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:28 INFO TaskSetManager: Finished task 182.0 in stage 9.0 (TID 1031) in 611 ms on localhost (183/200)
15/08/21 23:14:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15023 records.
15/08/21 23:14:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15023
15/08/21 23:14:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00031-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 126371 length: 126371 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00085-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 127657 length: 127657 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 15001 records.
15/08/21 23:14:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14843 records.
15/08/21 23:14:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:29 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 15001
15/08/21 23:14:29 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14843
15/08/21 23:14:29 INFO Executor: Finished task 183.0 in stage 9.0 (TID 1032). 2341 bytes result sent to driver
15/08/21 23:14:29 INFO TaskSetManager: Starting task 199.0 in stage 9.0 (TID 1048, localhost, ANY, 1970 bytes)
15/08/21 23:14:29 INFO Executor: Running task 199.0 in stage 9.0 (TID 1048)
15/08/21 23:14:29 INFO TaskSetManager: Finished task 183.0 in stage 9.0 (TID 1032) in 1810 ms on localhost (184/200)
15/08/21 23:14:29 INFO Executor: Finished task 185.0 in stage 9.0 (TID 1034). 2341 bytes result sent to driver
15/08/21 23:14:29 INFO TaskSetManager: Finished task 185.0 in stage 9.0 (TID 1034) in 1790 ms on localhost (185/200)
15/08/21 23:14:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:29 INFO Executor: Finished task 184.0 in stage 9.0 (TID 1033). 2341 bytes result sent to driver
15/08/21 23:14:29 INFO TaskSetManager: Finished task 184.0 in stage 9.0 (TID 1033) in 1816 ms on localhost (186/200)
15/08/21 23:14:30 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:40586 in memory (size: 5.6 KB, free: 20.7 GB)
15/08/21 23:14:30 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:40586 in memory (size: 31.0 KB, free: 20.7 GB)
15/08/21 23:14:30 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:40586 in memory (size: 3.8 KB, free: 20.7 GB)
15/08/21 23:14:30 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:40586 in memory (size: 1776.0 B, free: 20.7 GB)
15/08/21 23:14:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par/part-r-00014-118bac29-0e6a-441e-ab07-58de1f3a7f95.gz.parquet start: 0 end: 125005 length: 125005 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 23:14:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 23:14:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 14684 records.
15/08/21 23:14:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 23:14:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 14684
15/08/21 23:14:30 INFO Executor: Finished task 186.0 in stage 9.0 (TID 1035). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 187.0 in stage 9.0 (TID 1036). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 186.0 in stage 9.0 (TID 1035) in 1758 ms on localhost (187/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 187.0 in stage 9.0 (TID 1036) in 1747 ms on localhost (188/200)
15/08/21 23:14:30 INFO Executor: Finished task 189.0 in stage 9.0 (TID 1038). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 188.0 in stage 9.0 (TID 1037). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 190.0 in stage 9.0 (TID 1039). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 189.0 in stage 9.0 (TID 1038) in 1760 ms on localhost (189/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 188.0 in stage 9.0 (TID 1037) in 1765 ms on localhost (190/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 190.0 in stage 9.0 (TID 1039) in 1728 ms on localhost (191/200)
15/08/21 23:14:30 INFO Executor: Finished task 191.0 in stage 9.0 (TID 1040). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 191.0 in stage 9.0 (TID 1040) in 1679 ms on localhost (192/200)
15/08/21 23:14:30 INFO Executor: Finished task 192.0 in stage 9.0 (TID 1041). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 192.0 in stage 9.0 (TID 1041) in 1695 ms on localhost (193/200)
15/08/21 23:14:30 INFO Executor: Finished task 193.0 in stage 9.0 (TID 1042). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 194.0 in stage 9.0 (TID 1043). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 193.0 in stage 9.0 (TID 1042) in 1688 ms on localhost (194/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 194.0 in stage 9.0 (TID 1043) in 1685 ms on localhost (195/200)
15/08/21 23:14:30 INFO Executor: Finished task 195.0 in stage 9.0 (TID 1044). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 195.0 in stage 9.0 (TID 1044) in 1677 ms on localhost (196/200)
15/08/21 23:14:30 INFO Executor: Finished task 196.0 in stage 9.0 (TID 1045). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 197.0 in stage 9.0 (TID 1046). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 196.0 in stage 9.0 (TID 1045) in 1661 ms on localhost (197/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 197.0 in stage 9.0 (TID 1046) in 1614 ms on localhost (198/200)
15/08/21 23:14:30 INFO Executor: Finished task 198.0 in stage 9.0 (TID 1047). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 199.0 in stage 9.0 (TID 1048). 2341 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 198.0 in stage 9.0 (TID 1047) in 1604 ms on localhost (199/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 199.0 in stage 9.0 (TID 1048) in 273 ms on localhost (200/200)
15/08/21 23:14:30 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/21 23:14:30 INFO DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:423) finished in 8.389 s
15/08/21 23:14:30 INFO DAGScheduler: looking for newly runnable stages
15/08/21 23:14:30 INFO DAGScheduler: running: Set()
15/08/21 23:14:30 INFO DAGScheduler: waiting: Set(ResultStage 10)
15/08/21 23:14:30 INFO DAGScheduler: failed: Set()
15/08/21 23:14:30 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@26b7563e
15/08/21 23:14:30 INFO StatsReportListener: task runtime:(count: 200, mean: 668.435000, stdev: 354.374316, max: 1816.000000, min: 273.000000)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	273.0 ms	435.0 ms	448.0 ms	501.0 ms	544.0 ms	613.0 ms	1.1 s	1.7 s	1.8 s
15/08/21 23:14:30 INFO DAGScheduler: Missing parents for ResultStage 10: List()
15/08/21 23:14:30 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 23:14:30 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 10842.040000, stdev: 378.701397, max: 11717.000000, min: 9836.000000)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	9.6 KB	10.0 KB	10.2 KB	10.3 KB	10.6 KB	10.8 KB	11.1 KB	11.3 KB	11.4 KB
15/08/21 23:14:30 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.170000, stdev: 0.437150, max: 3.000000, min: 0.000000)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	3.0 ms
15/08/21 23:14:30 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 23:14:30 INFO StatsReportListener: task result size:(count: 200, mean: 2341.000000, stdev: 0.000000, max: 2341.000000, min: 2341.000000)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB
15/08/21 23:14:30 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 95.305081, stdev: 2.046898, max: 99.198626, min: 88.811795)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	89 %	91 %	93 %	94 %	96 %	97 %	97 %	99 %	99 %
15/08/21 23:14:30 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.031092, stdev: 0.082732, max: 0.518135, min: 0.000000)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/21 23:14:30 INFO StatsReportListener: other time pct: (count: 200, mean: 4.663827, stdev: 2.038956, max: 11.188205, min: 0.801374)
15/08/21 23:14:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:30 INFO StatsReportListener: 	 1 %	 1 %	 3 %	 3 %	 4 %	 6 %	 8 %	 9 %	11 %
15/08/21 23:14:30 INFO MemoryStore: ensureFreeSpace(84248) called with curMem=1771967, maxMem=22226833244
15/08/21 23:14:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 82.3 KB, free 20.7 GB)
15/08/21 23:14:30 INFO MemoryStore: ensureFreeSpace(33421) called with curMem=1856215, maxMem=22226833244
15/08/21 23:14:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 32.6 KB, free 20.7 GB)
15/08/21 23:14:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:40586 (size: 32.6 KB, free: 20.7 GB)
15/08/21 23:14:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874
15/08/21 23:14:30 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/21 23:14:30 INFO TaskSchedulerImpl: Adding task set 10.0 with 200 tasks
15/08/21 23:14:30 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1049, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 1050, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 1051, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 1052, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 1053, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 1054, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 1055, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 1056, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 1057, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 1058, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 1059, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 1060, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 1061, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 1062, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 1063, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 1064, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 2.0 in stage 10.0 (TID 1051)
15/08/21 23:14:30 INFO Executor: Running task 1.0 in stage 10.0 (TID 1050)
15/08/21 23:14:30 INFO Executor: Running task 8.0 in stage 10.0 (TID 1057)
15/08/21 23:14:30 INFO Executor: Running task 6.0 in stage 10.0 (TID 1055)
15/08/21 23:14:30 INFO Executor: Running task 9.0 in stage 10.0 (TID 1058)
15/08/21 23:14:30 INFO Executor: Running task 3.0 in stage 10.0 (TID 1052)
15/08/21 23:14:30 INFO Executor: Running task 10.0 in stage 10.0 (TID 1059)
15/08/21 23:14:30 INFO Executor: Running task 11.0 in stage 10.0 (TID 1060)
15/08/21 23:14:30 INFO Executor: Running task 4.0 in stage 10.0 (TID 1053)
15/08/21 23:14:30 INFO Executor: Running task 5.0 in stage 10.0 (TID 1054)
15/08/21 23:14:30 INFO Executor: Running task 14.0 in stage 10.0 (TID 1063)
15/08/21 23:14:30 INFO Executor: Running task 15.0 in stage 10.0 (TID 1064)
15/08/21 23:14:30 INFO Executor: Running task 0.0 in stage 10.0 (TID 1049)
15/08/21 23:14:30 INFO Executor: Running task 13.0 in stage 10.0 (TID 1062)
15/08/21 23:14:30 INFO Executor: Running task 12.0 in stage 10.0 (TID 1061)
15/08/21 23:14:30 INFO Executor: Running task 7.0 in stage 10.0 (TID 1056)
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,596
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,564B for [ps_partkey] INT32: 416 values, 1,671B raw, 1,528B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,856
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,245B for [value] DOUBLE: 416 values, 3,335B raw, 2,201B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,772B for [ps_partkey] INT32: 479 values, 1,923B raw, 1,736B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,336
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,384B for [value] DOUBLE: 479 values, 3,839B raw, 2,340B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,683B for [ps_partkey] INT32: 453 values, 1,819B raw, 1,647B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,716
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,291B for [value] DOUBLE: 453 values, 3,631B raw, 2,247B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,584B for [ps_partkey] INT32: 422 values, 1,695B raw, 1,548B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,140B for [value] DOUBLE: 422 values, 3,383B raw, 2,096B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,576
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,905B for [ps_partkey] INT32: 515 values, 2,067B raw, 1,869B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,578B for [value] DOUBLE: 515 values, 4,127B raw, 2,534B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,616
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,596
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,276
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,732B for [ps_partkey] INT32: 467 values, 1,875B raw, 1,696B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,560B for [ps_partkey] INT32: 416 values, 1,671B raw, 1,524B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,334B for [value] DOUBLE: 467 values, 3,743B raw, 2,290B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,160B for [value] DOUBLE: 416 values, 3,335B raw, 2,116B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,656
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,743B for [ps_partkey] INT32: 469 values, 1,883B raw, 1,707B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,375B for [value] DOUBLE: 469 values, 3,759B raw, 2,331B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,196
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,673B for [ps_partkey] INT32: 450 values, 1,807B raw, 1,637B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,339B for [value] DOUBLE: 450 values, 3,607B raw, 2,295B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,616
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,296
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,496
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,756
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,666B for [ps_partkey] INT32: 446 values, 1,791B raw, 1,630B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,339B for [value] DOUBLE: 446 values, 3,575B raw, 2,295B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,739B for [ps_partkey] INT32: 467 values, 1,875B raw, 1,703B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,316
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,404B for [value] DOUBLE: 467 values, 3,743B raw, 2,360B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,881B for [ps_partkey] INT32: 511 values, 2,051B raw, 1,845B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,591B for [ps_partkey] INT32: 424 values, 1,703B raw, 1,555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,570B for [value] DOUBLE: 511 values, 4,095B raw, 2,526B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,191B for [value] DOUBLE: 424 values, 3,399B raw, 2,147B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,680B for [ps_partkey] INT32: 451 values, 1,811B raw, 1,644B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,243B for [value] DOUBLE: 451 values, 3,615B raw, 2,199B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,516B for [ps_partkey] INT32: 402 values, 1,615B raw, 1,480B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,876
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,149B for [value] DOUBLE: 402 values, 3,223B raw, 2,105B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,781B for [ps_partkey] INT32: 480 values, 1,927B raw, 1,745B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,653B for [value] DOUBLE: 480 values, 3,847B raw, 2,609B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000009
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000007
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000007_0: Committed
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000009_0: Committed
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000001
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000001_0: Committed
15/08/21 23:14:30 INFO Executor: Finished task 9.0 in stage 10.0 (TID 1058). 843 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 1.0 in stage 10.0 (TID 1050). 843 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 1065, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Finished task 7.0 in stage 10.0 (TID 1056). 843 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Running task 16.0 in stage 10.0 (TID 1065)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 1066, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 17.0 in stage 10.0 (TID 1066)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 1067, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 18.0 in stage 10.0 (TID 1067)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 1058) in 474 ms on localhost (1/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 1050) in 476 ms on localhost (2/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 1056) in 475 ms on localhost (3/200)
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000015
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000015_0: Committed
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000004
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000004_0: Committed
15/08/21 23:14:30 INFO Executor: Finished task 15.0 in stage 10.0 (TID 1064). 843 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 4.0 in stage 10.0 (TID 1053). 843 bytes result sent to driver
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000005
15/08/21 23:14:30 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 1068, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000005_0: Committed
15/08/21 23:14:30 INFO Executor: Running task 19.0 in stage 10.0 (TID 1068)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 1069, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 20.0 in stage 10.0 (TID 1069)
15/08/21 23:14:30 INFO Executor: Finished task 5.0 in stage 10.0 (TID 1054). 843 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 1064) in 490 ms on localhost (4/200)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 1070, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 1053) in 493 ms on localhost (5/200)
15/08/21 23:14:30 INFO Executor: Running task 21.0 in stage 10.0 (TID 1070)
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000011
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000011_0: Committed
15/08/21 23:14:30 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 1054) in 494 ms on localhost (6/200)
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000002
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000002_0: Committed
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000010
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000000
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000010_0: Committed
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000000_0: Committed
15/08/21 23:14:30 INFO Executor: Finished task 11.0 in stage 10.0 (TID 1060). 843 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 1071, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Finished task 2.0 in stage 10.0 (TID 1051). 843 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Running task 22.0 in stage 10.0 (TID 1071)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 1072, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 23.0 in stage 10.0 (TID 1072)
15/08/21 23:14:30 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1049). 843 bytes result sent to driver
15/08/21 23:14:30 INFO Executor: Finished task 10.0 in stage 10.0 (TID 1059). 843 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Starting task 24.0 in stage 10.0 (TID 1073, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 24.0 in stage 10.0 (TID 1073)
15/08/21 23:14:30 INFO TaskSetManager: Starting task 25.0 in stage 10.0 (TID 1074, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 25.0 in stage 10.0 (TID 1074)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 1051) in 501 ms on localhost (7/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 1060) in 499 ms on localhost (8/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1049) in 504 ms on localhost (9/200)
15/08/21 23:14:30 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 1059) in 502 ms on localhost (10/200)
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:30 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:30 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,236
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,116
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,844B for [ps_partkey] INT32: 498 values, 1,999B raw, 1,808B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,452B for [value] DOUBLE: 498 values, 3,991B raw, 2,408B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,479B for [ps_partkey] INT32: 392 values, 1,575B raw, 1,443B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,971B for [value] DOUBLE: 392 values, 3,143B raw, 1,927B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,616
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,196
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,136
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,676
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,666B for [ps_partkey] INT32: 446 values, 1,791B raw, 1,630B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,221B for [value] DOUBLE: 446 values, 3,575B raw, 2,177B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,482B for [ps_partkey] INT32: 393 values, 1,579B raw, 1,446B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,563B for [ps_partkey] INT32: 417 values, 1,675B raw, 1,527B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,967B for [value] DOUBLE: 393 values, 3,151B raw, 1,923B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,080B for [value] DOUBLE: 417 values, 3,343B raw, 2,036B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,742B for [ps_partkey] INT32: 470 values, 1,887B raw, 1,706B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,116
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,320B for [value] DOUBLE: 470 values, 3,767B raw, 2,276B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,036
15/08/21 23:14:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000017
15/08/21 23:14:30 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000017_0: Committed
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,356
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,825B for [ps_partkey] INT32: 492 values, 1,975B raw, 1,789B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,394B for [value] DOUBLE: 492 values, 3,943B raw, 2,350B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,805B for [ps_partkey] INT32: 488 values, 1,959B raw, 1,769B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,376B for [value] DOUBLE: 488 values, 3,911B raw, 2,332B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 1,691B for [ps_partkey] INT32: 454 values, 1,823B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,416
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,257B for [value] DOUBLE: 454 values, 3,639B raw, 2,213B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO Executor: Finished task 17.0 in stage 10.0 (TID 1066). 843 bytes result sent to driver
15/08/21 23:14:30 INFO TaskSetManager: Starting task 26.0 in stage 10.0 (TID 1075, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:30 INFO Executor: Running task 26.0 in stage 10.0 (TID 1075)
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,051B for [ps_partkey] INT32: 557 values, 2,235B raw, 2,015B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO ColumnChunkPageWriteStore: written 2,706B for [value] DOUBLE: 557 values, 4,463B raw, 2,662B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:30 INFO TaskSetManager: Finished task 17.0 in stage 10.0 (TID 1066) in 238 ms on localhost (11/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000019
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000019_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 19.0 in stage 10.0 (TID 1068). 843 bytes result sent to driver
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000021
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000021_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 1076, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 27.0 in stage 10.0 (TID 1076)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000025
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000025_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Finished task 19.0 in stage 10.0 (TID 1068) in 239 ms on localhost (12/200)
15/08/21 23:14:31 INFO Executor: Finished task 21.0 in stage 10.0 (TID 1070). 843 bytes result sent to driver
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000016
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000016_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Starting task 28.0 in stage 10.0 (TID 1077, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 28.0 in stage 10.0 (TID 1077)
15/08/21 23:14:31 INFO Executor: Finished task 25.0 in stage 10.0 (TID 1074). 843 bytes result sent to driver
15/08/21 23:14:31 INFO Executor: Finished task 16.0 in stage 10.0 (TID 1065). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 29.0 in stage 10.0 (TID 1078, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 30.0 in stage 10.0 (TID 1079, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 29.0 in stage 10.0 (TID 1078)
15/08/21 23:14:31 INFO Executor: Running task 30.0 in stage 10.0 (TID 1079)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 1070) in 238 ms on localhost (13/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000020
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000020_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 1074) in 233 ms on localhost (14/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 1065) in 260 ms on localhost (15/200)
15/08/21 23:14:31 INFO Executor: Finished task 20.0 in stage 10.0 (TID 1069). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 31.0 in stage 10.0 (TID 1080, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 31.0 in stage 10.0 (TID 1080)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000022
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000022_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 22.0 in stage 10.0 (TID 1071). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 32.0 in stage 10.0 (TID 1081, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 32.0 in stage 10.0 (TID 1081)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 22.0 in stage 10.0 (TID 1071) in 245 ms on localhost (16/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000023
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000023_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 23.0 in stage 10.0 (TID 1072). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 33.0 in stage 10.0 (TID 1082, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 20.0 in stage 10.0 (TID 1069) in 256 ms on localhost (17/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 23.0 in stage 10.0 (TID 1072) in 249 ms on localhost (18/200)
15/08/21 23:14:31 INFO Executor: Running task 33.0 in stage 10.0 (TID 1082)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000012
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000012_0: Committed
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000003
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000008
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000003_0: Committed
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000008_0: Committed
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000006
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000006_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 8.0 in stage 10.0 (TID 1057). 843 bytes result sent to driver
15/08/21 23:14:31 INFO Executor: Finished task 6.0 in stage 10.0 (TID 1055). 843 bytes result sent to driver
15/08/21 23:14:31 INFO Executor: Finished task 12.0 in stage 10.0 (TID 1061). 843 bytes result sent to driver
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000013
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000013_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Starting task 34.0 in stage 10.0 (TID 1083, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Finished task 3.0 in stage 10.0 (TID 1052). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 1057) in 901 ms on localhost (19/200)
15/08/21 23:14:31 INFO Executor: Finished task 13.0 in stage 10.0 (TID 1062). 843 bytes result sent to driver
15/08/21 23:14:31 INFO Executor: Running task 34.0 in stage 10.0 (TID 1083)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000014
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000014_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 14.0 in stage 10.0 (TID 1063). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 35.0 in stage 10.0 (TID 1084, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 35.0 in stage 10.0 (TID 1084)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 36.0 in stage 10.0 (TID 1085, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 36.0 in stage 10.0 (TID 1085)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 37.0 in stage 10.0 (TID 1086, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 1052) in 916 ms on localhost (20/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 1061) in 914 ms on localhost (21/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 1055) in 915 ms on localhost (22/200)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 38.0 in stage 10.0 (TID 1087, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 37.0 in stage 10.0 (TID 1086)
15/08/21 23:14:31 INFO Executor: Running task 38.0 in stage 10.0 (TID 1087)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 39.0 in stage 10.0 (TID 1088, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 1062) in 920 ms on localhost (23/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 1063) in 920 ms on localhost (24/200)
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,176
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,136
15/08/21 23:14:31 INFO Executor: Running task 39.0 in stage 10.0 (TID 1088)
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,636
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,662B for [ps_partkey] INT32: 445 values, 1,787B raw, 1,626B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,166B for [value] DOUBLE: 445 values, 3,567B raw, 2,122B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,818B for [ps_partkey] INT32: 493 values, 1,979B raw, 1,782B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,394B for [value] DOUBLE: 493 values, 3,951B raw, 2,350B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,908B for [ps_partkey] INT32: 518 values, 2,079B raw, 1,872B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,517B for [value] DOUBLE: 518 values, 4,151B raw, 2,473B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,896
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,785B for [ps_partkey] INT32: 481 values, 1,931B raw, 1,749B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,346B for [value] DOUBLE: 481 values, 3,855B raw, 2,302B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,916
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,783B for [ps_partkey] INT32: 482 values, 1,935B raw, 1,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,345B for [value] DOUBLE: 482 values, 3,863B raw, 2,301B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,476
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,881B for [ps_partkey] INT32: 510 values, 2,047B raw, 1,845B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,436B for [value] DOUBLE: 510 values, 4,087B raw, 2,392B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000027
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000027_0: Committed
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000029
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000029_0: Committed
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000026
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000026_0: Committed
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,756
15/08/21 23:14:31 INFO Executor: Finished task 29.0 in stage 10.0 (TID 1078). 843 bytes result sent to driver
15/08/21 23:14:31 INFO Executor: Finished task 26.0 in stage 10.0 (TID 1075). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 40.0 in stage 10.0 (TID 1089, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 40.0 in stage 10.0 (TID 1089)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 41.0 in stage 10.0 (TID 1090, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,762B for [ps_partkey] INT32: 474 values, 1,903B raw, 1,726B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO TaskSetManager: Finished task 29.0 in stage 10.0 (TID 1078) in 254 ms on localhost (25/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 1075) in 277 ms on localhost (26/200)
15/08/21 23:14:31 INFO Executor: Finished task 27.0 in stage 10.0 (TID 1076). 843 bytes result sent to driver
15/08/21 23:14:31 INFO Executor: Running task 41.0 in stage 10.0 (TID 1090)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 42.0 in stage 10.0 (TID 1091, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 42.0 in stage 10.0 (TID 1091)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 1076) in 267 ms on localhost (27/200)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,236B for [value] DOUBLE: 474 values, 3,799B raw, 2,192B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000030
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000030_0: Committed
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,336
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000028
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000028_0: Committed
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO Executor: Finished task 30.0 in stage 10.0 (TID 1079). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 43.0 in stage 10.0 (TID 1092, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,861B for [ps_partkey] INT32: 503 values, 2,019B raw, 1,825B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO Executor: Running task 43.0 in stage 10.0 (TID 1092)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,359B for [value] DOUBLE: 503 values, 4,031B raw, 2,315B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO Executor: Finished task 28.0 in stage 10.0 (TID 1077). 843 bytes result sent to driver
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO TaskSetManager: Starting task 44.0 in stage 10.0 (TID 1093, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 44.0 in stage 10.0 (TID 1093)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO TaskSetManager: Finished task 30.0 in stage 10.0 (TID 1079) in 281 ms on localhost (28/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 28.0 in stage 10.0 (TID 1077) in 284 ms on localhost (29/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000031
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000031_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 31.0 in stage 10.0 (TID 1080). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 45.0 in stage 10.0 (TID 1094, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 45.0 in stage 10.0 (TID 1094)
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO TaskSetManager: Finished task 31.0 in stage 10.0 (TID 1080) in 289 ms on localhost (30/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000033
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000033_0: Committed
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO Executor: Finished task 33.0 in stage 10.0 (TID 1082). 843 bytes result sent to driver
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO TaskSetManager: Starting task 46.0 in stage 10.0 (TID 1095, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 46.0 in stage 10.0 (TID 1095)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 33.0 in stage 10.0 (TID 1082) in 293 ms on localhost (31/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000032
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000032_0: Committed
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO Executor: Finished task 32.0 in stage 10.0 (TID 1081). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 47.0 in stage 10.0 (TID 1096, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 47.0 in stage 10.0 (TID 1096)
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO TaskSetManager: Finished task 32.0 in stage 10.0 (TID 1081) in 306 ms on localhost (32/200)
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000018
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000018_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 18.0 in stage 10.0 (TID 1067). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 48.0 in stage 10.0 (TID 1097, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 48.0 in stage 10.0 (TID 1097)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000024
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000024_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Finished task 18.0 in stage 10.0 (TID 1067) in 652 ms on localhost (33/200)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO Executor: Finished task 24.0 in stage 10.0 (TID 1073). 843 bytes result sent to driver
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,296
15/08/21 23:14:31 INFO TaskSetManager: Starting task 49.0 in stage 10.0 (TID 1098, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 49.0 in stage 10.0 (TID 1098)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 1073) in 637 ms on localhost (34/200)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,027B for [ps_partkey] INT32: 551 values, 2,211B raw, 1,991B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,545B for [value] DOUBLE: 551 values, 4,415B raw, 2,501B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,416
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,396
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,224B for [ps_partkey] INT32: 607 values, 2,435B raw, 2,188B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,781B for [value] DOUBLE: 607 values, 4,863B raw, 2,737B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,867B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,831B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,318B for [value] DOUBLE: 506 values, 4,055B raw, 2,274B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000035
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000035_0: Committed
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO Executor: Finished task 35.0 in stage 10.0 (TID 1084). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 50.0 in stage 10.0 (TID 1099, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 50.0 in stage 10.0 (TID 1099)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 35.0 in stage 10.0 (TID 1084) in 288 ms on localhost (35/200)
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,136
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,822B for [ps_partkey] INT32: 493 values, 1,979B raw, 1,786B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,254B for [value] DOUBLE: 493 values, 3,951B raw, 2,210B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000034
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000034_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 34.0 in stage 10.0 (TID 1083). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 51.0 in stage 10.0 (TID 1100, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 34.0 in stage 10.0 (TID 1083) in 333 ms on localhost (36/200)
15/08/21 23:14:31 INFO Executor: Running task 51.0 in stage 10.0 (TID 1100)
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000036
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000036_0: Committed
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,588
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO Executor: Finished task 36.0 in stage 10.0 (TID 1085). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 52.0 in stage 10.0 (TID 1101, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,558B for [ps_partkey] INT32: 416 values, 1,671B raw, 1,522B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,913B for [value] DOUBLE: 416 values, 3,335B raw, 1,869B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO Executor: Running task 52.0 in stage 10.0 (TID 1101)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 36.0 in stage 10.0 (TID 1085) in 335 ms on localhost (37/200)
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,876
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,964B for [ps_partkey] INT32: 530 values, 2,127B raw, 1,928B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,425B for [value] DOUBLE: 530 values, 4,247B raw, 2,381B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,056
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,068
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000038
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000038_0: Committed
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,811B for [ps_partkey] INT32: 489 values, 1,963B raw, 1,775B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,199B for [value] DOUBLE: 489 values, 3,919B raw, 2,155B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO Executor: Finished task 38.0 in stage 10.0 (TID 1087). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 53.0 in stage 10.0 (TID 1102, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 53.0 in stage 10.0 (TID 1102)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,990B for [ps_partkey] INT32: 540 values, 2,167B raw, 1,954B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,423B for [value] DOUBLE: 540 values, 4,327B raw, 2,379B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO TaskSetManager: Finished task 38.0 in stage 10.0 (TID 1087) in 373 ms on localhost (38/200)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,776
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,568
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,520
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,933B for [ps_partkey] INT32: 525 values, 2,107B raw, 1,897B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,342B for [value] DOUBLE: 525 values, 4,207B raw, 2,298B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,730B for [ps_partkey] INT32: 465 values, 1,867B raw, 1,694B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,097B for [value] DOUBLE: 465 values, 3,727B raw, 2,053B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,268
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000037
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000037_0: Committed
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,717B for [ps_partkey] INT32: 463 values, 1,859B raw, 1,681B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,110B for [value] DOUBLE: 463 values, 3,711B raw, 2,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO Executor: Finished task 37.0 in stage 10.0 (TID 1086). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 54.0 in stage 10.0 (TID 1103, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 54.0 in stage 10.0 (TID 1103)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,021B for [ps_partkey] INT32: 550 values, 2,207B raw, 1,985B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,439B for [value] DOUBLE: 550 values, 4,407B raw, 2,395B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO TaskSetManager: Finished task 37.0 in stage 10.0 (TID 1086) in 419 ms on localhost (39/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000039
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000039_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 39.0 in stage 10.0 (TID 1088). 843 bytes result sent to driver
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000042
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000042_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Starting task 55.0 in stage 10.0 (TID 1104, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 55.0 in stage 10.0 (TID 1104)
15/08/21 23:14:31 INFO Executor: Finished task 42.0 in stage 10.0 (TID 1091). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Finished task 39.0 in stage 10.0 (TID 1088) in 422 ms on localhost (40/200)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:31 INFO TaskSetManager: Starting task 56.0 in stage 10.0 (TID 1105, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 56.0 in stage 10.0 (TID 1105)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000040
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000040_0: Committed
15/08/21 23:14:31 INFO TaskSetManager: Finished task 42.0 in stage 10.0 (TID 1091) in 362 ms on localhost (41/200)
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO Executor: Finished task 40.0 in stage 10.0 (TID 1089). 843 bytes result sent to driver
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,876
15/08/21 23:14:31 INFO TaskSetManager: Starting task 57.0 in stage 10.0 (TID 1106, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 57.0 in stage 10.0 (TID 1106)
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,941B for [ps_partkey] INT32: 530 values, 2,127B raw, 1,905B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,364B for [value] DOUBLE: 530 values, 4,247B raw, 2,320B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO TaskSetManager: Finished task 40.0 in stage 10.0 (TID 1089) in 377 ms on localhost (42/200)
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000043
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000043_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 43.0 in stage 10.0 (TID 1092). 843 bytes result sent to driver
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000041
15/08/21 23:14:31 INFO TaskSetManager: Starting task 58.0 in stage 10.0 (TID 1107, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000041_0: Committed
15/08/21 23:14:31 INFO Executor: Running task 58.0 in stage 10.0 (TID 1107)
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO Executor: Finished task 41.0 in stage 10.0 (TID 1090). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Finished task 43.0 in stage 10.0 (TID 1092) in 367 ms on localhost (43/200)
15/08/21 23:14:31 INFO TaskSetManager: Starting task 59.0 in stage 10.0 (TID 1108, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 59.0 in stage 10.0 (TID 1108)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 41.0 in stage 10.0 (TID 1090) in 399 ms on localhost (44/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000045
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000045_0: Committed
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000044
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000044_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 45.0 in stage 10.0 (TID 1094). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 60.0 in stage 10.0 (TID 1109, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 60.0 in stage 10.0 (TID 1109)
15/08/21 23:14:31 INFO Executor: Finished task 44.0 in stage 10.0 (TID 1093). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 61.0 in stage 10.0 (TID 1110, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 61.0 in stage 10.0 (TID 1110)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 45.0 in stage 10.0 (TID 1094) in 367 ms on localhost (45/200)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 44.0 in stage 10.0 (TID 1093) in 380 ms on localhost (46/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000046
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000046_0: Committed
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO Executor: Finished task 46.0 in stage 10.0 (TID 1095). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 62.0 in stage 10.0 (TID 1111, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 62.0 in stage 10.0 (TID 1111)
15/08/21 23:14:31 INFO TaskSetManager: Finished task 46.0 in stage 10.0 (TID 1095) in 370 ms on localhost (47/200)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,656
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,738B for [ps_partkey] INT32: 469 values, 1,883B raw, 1,702B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,100B for [value] DOUBLE: 469 values, 3,759B raw, 2,056B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,028
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,160
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,631B for [ps_partkey] INT32: 438 values, 1,759B raw, 1,595B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,978B for [value] DOUBLE: 438 values, 3,511B raw, 1,934B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000047
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000047_0: Committed
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,908
15/08/21 23:14:31 INFO Executor: Finished task 47.0 in stage 10.0 (TID 1096). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 63.0 in stage 10.0 (TID 1112, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,829B for [ps_partkey] INT32: 495 values, 1,987B raw, 1,793B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,876
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,442B for [ps_partkey] INT32: 382 values, 1,535B raw, 1,406B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,741B for [value] DOUBLE: 382 values, 3,063B raw, 1,697B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO Executor: Running task 63.0 in stage 10.0 (TID 1112)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,221B for [value] DOUBLE: 495 values, 3,967B raw, 2,177B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO TaskSetManager: Finished task 47.0 in stage 10.0 (TID 1096) in 455 ms on localhost (48/200)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000049
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000049_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 49.0 in stage 10.0 (TID 1098). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 64.0 in stage 10.0 (TID 1113, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO Executor: Running task 64.0 in stage 10.0 (TID 1113)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,778B for [ps_partkey] INT32: 480 values, 1,927B raw, 1,742B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,142B for [value] DOUBLE: 480 values, 3,847B raw, 2,098B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO TaskSetManager: Finished task 49.0 in stage 10.0 (TID 1098) in 376 ms on localhost (49/200)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,948
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000048
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000048_0: Committed
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,145B for [ps_partkey] INT32: 584 values, 2,343B raw, 2,109B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO Executor: Finished task 48.0 in stage 10.0 (TID 1097). 843 bytes result sent to driver
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,555B for [value] DOUBLE: 584 values, 4,679B raw, 2,511B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO TaskSetManager: Starting task 65.0 in stage 10.0 (TID 1114, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 65.0 in stage 10.0 (TID 1114)
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO TaskSetManager: Finished task 48.0 in stage 10.0 (TID 1097) in 443 ms on localhost (50/200)
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000052
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000052_0: Committed
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO Executor: Finished task 52.0 in stage 10.0 (TID 1101). 843 bytes result sent to driver
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000050
15/08/21 23:14:31 INFO TaskSetManager: Starting task 66.0 in stage 10.0 (TID 1115, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000050_0: Committed
15/08/21 23:14:31 INFO Executor: Running task 66.0 in stage 10.0 (TID 1115)
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO TaskSetManager: Finished task 52.0 in stage 10.0 (TID 1101) in 335 ms on localhost (51/200)
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO Executor: Finished task 50.0 in stage 10.0 (TID 1099). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 67.0 in stage 10.0 (TID 1116, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 67.0 in stage 10.0 (TID 1116)
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO TaskSetManager: Finished task 50.0 in stage 10.0 (TID 1099) in 401 ms on localhost (52/200)
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,716
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,753B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,717B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,104B for [value] DOUBLE: 472 values, 3,783B raw, 2,060B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,396
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,703B for [ps_partkey] INT32: 456 values, 1,831B raw, 1,667B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,035B for [value] DOUBLE: 456 values, 3,655B raw, 1,991B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,716
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,156
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000051
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000051_0: Committed
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,656B for [ps_partkey] INT32: 444 values, 1,783B raw, 1,620B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,982B for [value] DOUBLE: 444 values, 3,559B raw, 1,938B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,576
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:31 INFO Executor: Finished task 51.0 in stage 10.0 (TID 1100). 843 bytes result sent to driver
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,754B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,718B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,079B for [value] DOUBLE: 472 values, 3,783B raw, 2,035B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:31 INFO TaskSetManager: Starting task 68.0 in stage 10.0 (TID 1117, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 68.0 in stage 10.0 (TID 1117)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,728B for [ps_partkey] INT32: 465 values, 1,867B raw, 1,692B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,066B for [value] DOUBLE: 465 values, 3,727B raw, 2,022B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO TaskSetManager: Finished task 51.0 in stage 10.0 (TID 1100) in 429 ms on localhost (53/200)
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000053
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000053_0: Committed
15/08/21 23:14:31 INFO Executor: Finished task 53.0 in stage 10.0 (TID 1102). 843 bytes result sent to driver
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,608
15/08/21 23:14:31 INFO TaskSetManager: Starting task 69.0 in stage 10.0 (TID 1118, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 69.0 in stage 10.0 (TID 1118)
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,456
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,568B for [ps_partkey] INT32: 417 values, 1,675B raw, 1,532B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,861B for [value] DOUBLE: 417 values, 3,343B raw, 1,817B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO TaskSetManager: Finished task 53.0 in stage 10.0 (TID 1102) in 389 ms on localhost (54/200)
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,055B for [ps_partkey] INT32: 559 values, 2,243B raw, 2,019B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,428B for [value] DOUBLE: 559 values, 4,479B raw, 2,384B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,896
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,446B for [ps_partkey] INT32: 381 values, 1,531B raw, 1,410B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,732B for [value] DOUBLE: 381 values, 3,055B raw, 1,688B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000057
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000057_0: Committed
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,756
15/08/21 23:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000054
15/08/21 23:14:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 1,935B for [ps_partkey] INT32: 524 values, 2,103B raw, 1,899B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000054_0: Committed
15/08/21 23:14:31 INFO ColumnChunkPageWriteStore: written 2,292B for [value] DOUBLE: 524 values, 4,199B raw, 2,248B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:31 INFO Executor: Finished task 57.0 in stage 10.0 (TID 1106). 843 bytes result sent to driver
15/08/21 23:14:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,716
15/08/21 23:14:31 INFO TaskSetManager: Starting task 70.0 in stage 10.0 (TID 1119, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 70.0 in stage 10.0 (TID 1119)
15/08/21 23:14:31 INFO Executor: Finished task 54.0 in stage 10.0 (TID 1103). 843 bytes result sent to driver
15/08/21 23:14:31 INFO TaskSetManager: Starting task 71.0 in stage 10.0 (TID 1120, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:31 INFO Executor: Running task 71.0 in stage 10.0 (TID 1120)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 57.0 in stage 10.0 (TID 1106) in 361 ms on localhost (55/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 54.0 in stage 10.0 (TID 1103) in 387 ms on localhost (56/200)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,412B for [ps_partkey] INT32: 372 values, 1,495B raw, 1,376B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,669B for [value] DOUBLE: 372 values, 2,983B raw, 1,625B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,716
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,102B for [ps_partkey] INT32: 572 values, 2,295B raw, 2,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,486B for [value] DOUBLE: 572 values, 4,583B raw, 2,442B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000056
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000055
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000056_0: Committed
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000055_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 56.0 in stage 10.0 (TID 1105). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 55.0 in stage 10.0 (TID 1104). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 72.0 in stage 10.0 (TID 1121, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 56.0 in stage 10.0 (TID 1105) in 394 ms on localhost (57/200)
15/08/21 23:14:32 INFO Executor: Running task 72.0 in stage 10.0 (TID 1121)
15/08/21 23:14:32 INFO TaskSetManager: Starting task 73.0 in stage 10.0 (TID 1122, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 73.0 in stage 10.0 (TID 1122)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000062
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000062_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Finished task 55.0 in stage 10.0 (TID 1104) in 405 ms on localhost (58/200)
15/08/21 23:14:32 INFO Executor: Finished task 62.0 in stage 10.0 (TID 1111). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 74.0 in stage 10.0 (TID 1123, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 74.0 in stage 10.0 (TID 1123)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 62.0 in stage 10.0 (TID 1111) in 347 ms on localhost (59/200)
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000063
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000063_0: Committed
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000059
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000059_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 63.0 in stage 10.0 (TID 1112). 843 bytes result sent to driver
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO Executor: Finished task 59.0 in stage 10.0 (TID 1108). 843 bytes result sent to driver
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO TaskSetManager: Starting task 75.0 in stage 10.0 (TID 1124, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO Executor: Running task 75.0 in stage 10.0 (TID 1124)
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO TaskSetManager: Starting task 76.0 in stage 10.0 (TID 1125, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO Executor: Running task 76.0 in stage 10.0 (TID 1125)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000061
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000061_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Finished task 63.0 in stage 10.0 (TID 1112) in 294 ms on localhost (60/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 59.0 in stage 10.0 (TID 1108) in 383 ms on localhost (61/200)
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO Executor: Finished task 61.0 in stage 10.0 (TID 1110). 843 bytes result sent to driver
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO TaskSetManager: Starting task 77.0 in stage 10.0 (TID 1126, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO Executor: Running task 77.0 in stage 10.0 (TID 1126)
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO TaskSetManager: Finished task 61.0 in stage 10.0 (TID 1110) in 377 ms on localhost (62/200)
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,388
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,866B for [ps_partkey] INT32: 506 values, 2,031B raw, 1,830B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,226B for [value] DOUBLE: 506 values, 4,055B raw, 2,182B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,716
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,748B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,712B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,087B for [value] DOUBLE: 472 values, 3,783B raw, 2,043B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,276
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,188
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,678B for [ps_partkey] INT32: 450 values, 1,807B raw, 1,642B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,979B for [value] DOUBLE: 450 values, 3,607B raw, 1,935B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000064
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000064_0: Committed
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,493B for [ps_partkey] INT32: 396 values, 1,591B raw, 1,457B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,763B for [value] DOUBLE: 396 values, 3,175B raw, 1,719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO Executor: Finished task 64.0 in stage 10.0 (TID 1113). 843 bytes result sent to driver
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO TaskSetManager: Starting task 78.0 in stage 10.0 (TID 1127, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 78.0 in stage 10.0 (TID 1127)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 64.0 in stage 10.0 (TID 1113) in 341 ms on localhost (63/200)
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,188
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,832B for [ps_partkey] INT32: 496 values, 1,991B raw, 1,796B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,158B for [value] DOUBLE: 496 values, 3,975B raw, 2,114B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,376
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,736
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000065
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000065_0: Committed
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000067
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000067_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 65.0 in stage 10.0 (TID 1114). 843 bytes result sent to driver
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,691B for [ps_partkey] INT32: 455 values, 1,827B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,578B for [ps_partkey] INT32: 423 values, 1,699B raw, 1,542B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000066
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000066_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Starting task 79.0 in stage 10.0 (TID 1128, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 79.0 in stage 10.0 (TID 1128)
15/08/21 23:14:32 INFO Executor: Finished task 67.0 in stage 10.0 (TID 1116). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 66.0 in stage 10.0 (TID 1115). 843 bytes result sent to driver
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,998B for [value] DOUBLE: 455 values, 3,647B raw, 1,954B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,852B for [value] DOUBLE: 423 values, 3,391B raw, 1,808B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO TaskSetManager: Finished task 65.0 in stage 10.0 (TID 1114) in 338 ms on localhost (64/200)
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO TaskSetManager: Starting task 80.0 in stage 10.0 (TID 1129, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO TaskSetManager: Starting task 81.0 in stage 10.0 (TID 1130, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO TaskSetManager: Finished task 67.0 in stage 10.0 (TID 1116) in 320 ms on localhost (65/200)
15/08/21 23:14:32 INFO Executor: Running task 81.0 in stage 10.0 (TID 1130)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 66.0 in stage 10.0 (TID 1115) in 325 ms on localhost (66/200)
15/08/21 23:14:32 INFO Executor: Running task 80.0 in stage 10.0 (TID 1129)
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000069
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000069_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 69.0 in stage 10.0 (TID 1118). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 82.0 in stage 10.0 (TID 1131, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 82.0 in stage 10.0 (TID 1131)
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,556
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,128
15/08/21 23:14:32 INFO TaskSetManager: Finished task 69.0 in stage 10.0 (TID 1118) in 258 ms on localhost (67/200)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,557B for [ps_partkey] INT32: 414 values, 1,663B raw, 1,521B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,828B for [value] DOUBLE: 414 values, 3,319B raw, 1,784B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,653B for [ps_partkey] INT32: 443 values, 1,779B raw, 1,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,962B for [value] DOUBLE: 443 values, 3,551B raw, 1,918B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000071
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000071_0: Committed
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000068
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000068_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 71.0 in stage 10.0 (TID 1120). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 68.0 in stage 10.0 (TID 1117). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 83.0 in stage 10.0 (TID 1132, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 83.0 in stage 10.0 (TID 1132)
15/08/21 23:14:32 INFO TaskSetManager: Starting task 84.0 in stage 10.0 (TID 1133, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 84.0 in stage 10.0 (TID 1133)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 71.0 in stage 10.0 (TID 1120) in 242 ms on localhost (68/200)
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,716
15/08/21 23:14:32 INFO TaskSetManager: Finished task 68.0 in stage 10.0 (TID 1117) in 300 ms on localhost (69/200)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,577B for [ps_partkey] INT32: 422 values, 1,695B raw, 1,541B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,879B for [value] DOUBLE: 422 values, 3,383B raw, 1,835B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,056
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,580
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,636B for [ps_partkey] INT32: 439 values, 1,763B raw, 1,600B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,927B for [value] DOUBLE: 439 values, 3,519B raw, 1,883B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000074
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000074_0: Committed
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,556
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,028
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,730B for [ps_partkey] INT32: 466 values, 1,871B raw, 1,694B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,027B for [value] DOUBLE: 466 values, 3,735B raw, 1,983B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO Executor: Finished task 74.0 in stage 10.0 (TID 1123). 843 bytes result sent to driver
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,385B for [ps_partkey] INT32: 364 values, 1,463B raw, 1,349B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,634B for [value] DOUBLE: 364 values, 2,919B raw, 1,590B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000070
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,637B for [ps_partkey] INT32: 438 values, 1,759B raw, 1,601B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000070_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,916B for [value] DOUBLE: 438 values, 3,511B raw, 1,872B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO TaskSetManager: Starting task 85.0 in stage 10.0 (TID 1134, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 85.0 in stage 10.0 (TID 1134)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 74.0 in stage 10.0 (TID 1123) in 242 ms on localhost (70/200)
15/08/21 23:14:32 INFO Executor: Finished task 70.0 in stage 10.0 (TID 1119). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 86.0 in stage 10.0 (TID 1135, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 86.0 in stage 10.0 (TID 1135)
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO TaskSetManager: Finished task 70.0 in stage 10.0 (TID 1119) in 282 ms on localhost (71/200)
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000076
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000076_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 76.0 in stage 10.0 (TID 1125). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 87.0 in stage 10.0 (TID 1136, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 87.0 in stage 10.0 (TID 1136)
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO TaskSetManager: Finished task 76.0 in stage 10.0 (TID 1125) in 249 ms on localhost (72/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000075
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000075_0: Committed
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000077
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000077_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 75.0 in stage 10.0 (TID 1124). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 77.0 in stage 10.0 (TID 1126). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 88.0 in stage 10.0 (TID 1137, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 88.0 in stage 10.0 (TID 1137)
15/08/21 23:14:32 INFO TaskSetManager: Starting task 89.0 in stage 10.0 (TID 1138, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 89.0 in stage 10.0 (TID 1138)
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO TaskSetManager: Finished task 75.0 in stage 10.0 (TID 1124) in 254 ms on localhost (73/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 77.0 in stage 10.0 (TID 1126) in 249 ms on localhost (74/200)
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,016
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,465B for [ps_partkey] INT32: 387 values, 1,555B raw, 1,429B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,719B for [value] DOUBLE: 387 values, 3,103B raw, 1,675B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,208
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,416
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,699B for [ps_partkey] INT32: 457 values, 1,835B raw, 1,663B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,986B for [value] DOUBLE: 457 values, 3,663B raw, 1,942B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,496B for [ps_partkey] INT32: 397 values, 1,595B raw, 1,460B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,752B for [value] DOUBLE: 397 values, 3,183B raw, 1,708B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,316
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,348
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,236
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,616
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,036B for [ps_partkey] INT32: 552 values, 2,215B raw, 2,000B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,345B for [value] DOUBLE: 552 values, 4,423B raw, 2,301B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,588
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,209B for [ps_partkey] INT32: 604 values, 2,423B raw, 2,173B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,549B for [value] DOUBLE: 604 values, 4,839B raw, 2,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,835B for [ps_partkey] INT32: 498 values, 1,999B raw, 1,799B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,150B for [value] DOUBLE: 498 values, 3,991B raw, 2,106B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,735B for [ps_partkey] INT32: 467 values, 1,875B raw, 1,699B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000060
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000060_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,021B for [value] DOUBLE: 467 values, 3,743B raw, 1,977B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000081
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000081_0: Committed
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000058
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000058_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,075B for [ps_partkey] INT32: 566 values, 2,271B raw, 2,039B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,396B for [value] DOUBLE: 566 values, 4,535B raw, 2,352B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO Executor: Finished task 60.0 in stage 10.0 (TID 1109). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 81.0 in stage 10.0 (TID 1130). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 58.0 in stage 10.0 (TID 1107). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 90.0 in stage 10.0 (TID 1139, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 90.0 in stage 10.0 (TID 1139)
15/08/21 23:14:32 INFO TaskSetManager: Starting task 91.0 in stage 10.0 (TID 1140, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 91.0 in stage 10.0 (TID 1140)
15/08/21 23:14:32 INFO TaskSetManager: Starting task 92.0 in stage 10.0 (TID 1141, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 92.0 in stage 10.0 (TID 1141)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 60.0 in stage 10.0 (TID 1109) in 816 ms on localhost (75/200)
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,628
15/08/21 23:14:32 INFO TaskSetManager: Finished task 81.0 in stage 10.0 (TID 1130) in 306 ms on localhost (76/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 58.0 in stage 10.0 (TID 1107) in 832 ms on localhost (77/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000080
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000080_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,734B for [ps_partkey] INT32: 468 values, 1,879B raw, 1,698B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000082
15/08/21 23:14:32 INFO Executor: Finished task 80.0 in stage 10.0 (TID 1129). 843 bytes result sent to driver
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000082_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Starting task 93.0 in stage 10.0 (TID 1142, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 93.0 in stage 10.0 (TID 1142)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,047B for [value] DOUBLE: 468 values, 3,751B raw, 2,003B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO Executor: Finished task 82.0 in stage 10.0 (TID 1131). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 94.0 in stage 10.0 (TID 1143, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 94.0 in stage 10.0 (TID 1143)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 80.0 in stage 10.0 (TID 1129) in 322 ms on localhost (78/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 82.0 in stage 10.0 (TID 1131) in 304 ms on localhost (79/200)
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,036
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,108
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,276
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000078
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,980B for [ps_partkey] INT32: 538 values, 2,159B raw, 1,944B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000078_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,825B for [ps_partkey] INT32: 492 values, 1,975B raw, 1,789B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,670B for [ps_partkey] INT32: 450 values, 1,807B raw, 1,634B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,119B for [value] DOUBLE: 492 values, 3,943B raw, 2,075B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,972B for [value] DOUBLE: 450 values, 3,607B raw, 1,928B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000084
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000084_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 78.0 in stage 10.0 (TID 1127). 843 bytes result sent to driver
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,282B for [value] DOUBLE: 538 values, 4,311B raw, 2,238B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO TaskSetManager: Starting task 95.0 in stage 10.0 (TID 1144, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 95.0 in stage 10.0 (TID 1144)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000086
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000086_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 84.0 in stage 10.0 (TID 1133). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 86.0 in stage 10.0 (TID 1135). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 96.0 in stage 10.0 (TID 1145, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 96.0 in stage 10.0 (TID 1145)
15/08/21 23:14:32 INFO TaskSetManager: Starting task 97.0 in stage 10.0 (TID 1146, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 78.0 in stage 10.0 (TID 1127) in 394 ms on localhost (80/200)
15/08/21 23:14:32 INFO Executor: Running task 97.0 in stage 10.0 (TID 1146)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 84.0 in stage 10.0 (TID 1133) in 297 ms on localhost (81/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 86.0 in stage 10.0 (TID 1135) in 256 ms on localhost (82/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000079
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000079_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 79.0 in stage 10.0 (TID 1128). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 98.0 in stage 10.0 (TID 1147, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 98.0 in stage 10.0 (TID 1147)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000088
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000088_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Finished task 79.0 in stage 10.0 (TID 1128) in 368 ms on localhost (83/200)
15/08/21 23:14:32 INFO Executor: Finished task 88.0 in stage 10.0 (TID 1137). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 99.0 in stage 10.0 (TID 1148, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000087
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000087_0: Committed
15/08/21 23:14:32 INFO Executor: Running task 99.0 in stage 10.0 (TID 1148)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 88.0 in stage 10.0 (TID 1137) in 265 ms on localhost (84/200)
15/08/21 23:14:32 INFO Executor: Finished task 87.0 in stage 10.0 (TID 1136). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 100.0 in stage 10.0 (TID 1149, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 100.0 in stage 10.0 (TID 1149)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000089
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000089_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Finished task 87.0 in stage 10.0 (TID 1136) in 272 ms on localhost (85/200)
15/08/21 23:14:32 INFO Executor: Finished task 89.0 in stage 10.0 (TID 1138). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 101.0 in stage 10.0 (TID 1150, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 101.0 in stage 10.0 (TID 1150)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 89.0 in stage 10.0 (TID 1138) in 274 ms on localhost (86/200)
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,648
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,569B for [ps_partkey] INT32: 419 values, 1,683B raw, 1,533B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,825B for [value] DOUBLE: 419 values, 3,359B raw, 1,781B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,496
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,576
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000072
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,256
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000072_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,539B for [ps_partkey] INT32: 411 values, 1,651B raw, 1,503B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,786B for [value] DOUBLE: 411 values, 3,295B raw, 1,742B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,895B for [ps_partkey] INT32: 515 values, 2,067B raw, 1,859B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000073
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000073_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,192B for [value] DOUBLE: 515 values, 4,127B raw, 2,148B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,021B for [ps_partkey] INT32: 549 values, 2,203B raw, 1,985B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,342B for [value] DOUBLE: 549 values, 4,399B raw, 2,298B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO Executor: Finished task 73.0 in stage 10.0 (TID 1122). 843 bytes result sent to driver
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO TaskSetManager: Starting task 102.0 in stage 10.0 (TID 1151, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,796
15/08/21 23:14:32 INFO Executor: Running task 102.0 in stage 10.0 (TID 1151)
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO TaskSetManager: Finished task 73.0 in stage 10.0 (TID 1122) in 685 ms on localhost (87/200)
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,208
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,440
15/08/21 23:14:32 INFO Executor: Finished task 72.0 in stage 10.0 (TID 1121). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 103.0 in stage 10.0 (TID 1152, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 103.0 in stage 10.0 (TID 1152)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 72.0 in stage 10.0 (TID 1121) in 697 ms on localhost (88/200)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,883B for [ps_partkey] INT32: 509 values, 2,043B raw, 1,847B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,834B for [ps_partkey] INT32: 497 values, 1,995B raw, 1,798B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,165B for [value] DOUBLE: 509 values, 4,079B raw, 2,121B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,140B for [value] DOUBLE: 497 values, 3,983B raw, 2,096B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,590B for [ps_partkey] INT32: 426 values, 1,711B raw, 1,554B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,870B for [value] DOUBLE: 426 values, 3,415B raw, 1,826B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,048
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,296
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000093
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000093_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,635B for [ps_partkey] INT32: 439 values, 1,763B raw, 1,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,915B for [value] DOUBLE: 439 values, 3,519B raw, 1,871B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO Executor: Finished task 93.0 in stage 10.0 (TID 1142). 843 bytes result sent to driver
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,683B for [ps_partkey] INT32: 451 values, 1,811B raw, 1,647B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO TaskSetManager: Starting task 104.0 in stage 10.0 (TID 1153, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,970B for [value] DOUBLE: 451 values, 3,615B raw, 1,926B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO Executor: Running task 104.0 in stage 10.0 (TID 1153)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 93.0 in stage 10.0 (TID 1142) in 256 ms on localhost (89/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000094
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000094_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 94.0 in stage 10.0 (TID 1143). 843 bytes result sent to driver
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,216
15/08/21 23:14:32 INFO TaskSetManager: Starting task 105.0 in stage 10.0 (TID 1154, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 105.0 in stage 10.0 (TID 1154)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000090
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,664B for [ps_partkey] INT32: 447 values, 1,795B raw, 1,628B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000090_0: Committed
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,934B for [value] DOUBLE: 447 values, 3,583B raw, 1,890B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO TaskSetManager: Finished task 94.0 in stage 10.0 (TID 1143) in 267 ms on localhost (90/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000097
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000097_0: Committed
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,828
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,024
15/08/21 23:14:32 INFO Executor: Finished task 97.0 in stage 10.0 (TID 1146). 843 bytes result sent to driver
15/08/21 23:14:32 INFO Executor: Finished task 90.0 in stage 10.0 (TID 1139). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 106.0 in stage 10.0 (TID 1155, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,782B for [ps_partkey] INT32: 478 values, 1,919B raw, 1,746B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO TaskSetManager: Starting task 107.0 in stage 10.0 (TID 1156, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 106.0 in stage 10.0 (TID 1155)
15/08/21 23:14:32 INFO Executor: Running task 107.0 in stage 10.0 (TID 1156)
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,050B for [value] DOUBLE: 478 values, 3,831B raw, 2,006B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,468B for [ps_partkey] INT32: 389 values, 1,563B raw, 1,432B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,699B for [value] DOUBLE: 389 values, 3,119B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO TaskSetManager: Finished task 97.0 in stage 10.0 (TID 1146) in 262 ms on localhost (91/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 90.0 in stage 10.0 (TID 1139) in 300 ms on localhost (92/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000092
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000092_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 92.0 in stage 10.0 (TID 1141). 843 bytes result sent to driver
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000096
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000096_0: Committed
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO TaskSetManager: Starting task 108.0 in stage 10.0 (TID 1157, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 108.0 in stage 10.0 (TID 1157)
15/08/21 23:14:32 INFO Executor: Finished task 96.0 in stage 10.0 (TID 1145). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 109.0 in stage 10.0 (TID 1158, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 109.0 in stage 10.0 (TID 1158)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 92.0 in stage 10.0 (TID 1141) in 310 ms on localhost (93/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 96.0 in stage 10.0 (TID 1145) in 279 ms on localhost (94/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000091
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000091_0: Committed
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO Executor: Finished task 91.0 in stage 10.0 (TID 1140). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 110.0 in stage 10.0 (TID 1159, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 110.0 in stage 10.0 (TID 1159)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 91.0 in stage 10.0 (TID 1140) in 318 ms on localhost (95/200)
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000098
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000098_0: Committed
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO Executor: Finished task 98.0 in stage 10.0 (TID 1147). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 111.0 in stage 10.0 (TID 1160, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 111.0 in stage 10.0 (TID 1160)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 98.0 in stage 10.0 (TID 1147) in 298 ms on localhost (96/200)
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,636
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,300
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,734B for [ps_partkey] INT32: 468 values, 1,879B raw, 1,698B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,041B for [value] DOUBLE: 468 values, 3,751B raw, 1,997B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,508B for [ps_partkey] INT32: 402 values, 1,615B raw, 1,472B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,752B for [value] DOUBLE: 402 values, 3,223B raw, 1,708B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000085
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000085_0: Committed
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000103
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000103_0: Committed
15/08/21 23:14:32 INFO Executor: Finished task 85.0 in stage 10.0 (TID 1134). 843 bytes result sent to driver
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000102
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000102_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Starting task 112.0 in stage 10.0 (TID 1161, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 112.0 in stage 10.0 (TID 1161)
15/08/21 23:14:32 INFO Executor: Finished task 103.0 in stage 10.0 (TID 1152). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Starting task 113.0 in stage 10.0 (TID 1162, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO Executor: Running task 113.0 in stage 10.0 (TID 1162)
15/08/21 23:14:32 INFO Executor: Finished task 102.0 in stage 10.0 (TID 1151). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Finished task 85.0 in stage 10.0 (TID 1134) in 661 ms on localhost (97/200)
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000083
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000083_0: Committed
15/08/21 23:14:32 INFO TaskSetManager: Starting task 114.0 in stage 10.0 (TID 1163, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 103.0 in stage 10.0 (TID 1152) in 217 ms on localhost (98/200)
15/08/21 23:14:32 INFO Executor: Running task 114.0 in stage 10.0 (TID 1163)
15/08/21 23:14:32 INFO Executor: Finished task 83.0 in stage 10.0 (TID 1132). 843 bytes result sent to driver
15/08/21 23:14:32 INFO TaskSetManager: Finished task 102.0 in stage 10.0 (TID 1151) in 227 ms on localhost (99/200)
15/08/21 23:14:32 INFO TaskSetManager: Finished task 83.0 in stage 10.0 (TID 1132) in 712 ms on localhost (100/200)
15/08/21 23:14:32 INFO TaskSetManager: Starting task 115.0 in stage 10.0 (TID 1164, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,560
15/08/21 23:14:32 INFO Executor: Running task 115.0 in stage 10.0 (TID 1164)
15/08/21 23:14:32 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:32 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,736
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,756
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,724B for [ps_partkey] INT32: 465 values, 1,867B raw, 1,688B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,700
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,927B for [ps_partkey] INT32: 524 values, 2,103B raw, 1,891B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,584B for [ps_partkey] INT32: 423 values, 1,699B raw, 1,548B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,830B for [value] DOUBLE: 423 values, 3,391B raw, 1,786B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,219B for [value] DOUBLE: 524 values, 4,199B raw, 2,175B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,989B for [value] DOUBLE: 465 values, 3,727B raw, 1,945B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,300
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,928
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,933B for [ps_partkey] INT32: 522 values, 2,095B raw, 1,897B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,197B for [value] DOUBLE: 522 values, 4,183B raw, 2,153B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,016
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,785B for [ps_partkey] INT32: 483 values, 1,939B raw, 1,749B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,063B for [value] DOUBLE: 483 values, 3,871B raw, 2,019B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,020B for [ps_partkey] INT32: 552 values, 2,215B raw, 1,984B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 2,303B for [value] DOUBLE: 552 values, 4,423B raw, 2,259B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,635B for [ps_partkey] INT32: 437 values, 1,755B raw, 1,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ColumnChunkPageWriteStore: written 1,884B for [value] DOUBLE: 437 values, 3,503B raw, 1,840B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000104
15/08/21 23:14:32 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000104_0: Committed
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000109
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000109_0: Committed
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO Executor: Finished task 104.0 in stage 10.0 (TID 1153). 843 bytes result sent to driver
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000105
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000105_0: Committed
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,476
15/08/21 23:14:33 INFO TaskSetManager: Starting task 116.0 in stage 10.0 (TID 1165, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000106
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000106_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 109.0 in stage 10.0 (TID 1158). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 117.0 in stage 10.0 (TID 1166, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Finished task 105.0 in stage 10.0 (TID 1154). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 118.0 in stage 10.0 (TID 1167, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO Executor: Running task 118.0 in stage 10.0 (TID 1167)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO Executor: Running task 116.0 in stage 10.0 (TID 1165)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,054B for [ps_partkey] INT32: 560 values, 2,247B raw, 2,018B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,367B for [value] DOUBLE: 560 values, 4,487B raw, 2,323B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Finished task 109.0 in stage 10.0 (TID 1158) in 216 ms on localhost (101/200)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 105.0 in stage 10.0 (TID 1154) in 247 ms on localhost (102/200)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 104.0 in stage 10.0 (TID 1153) in 276 ms on localhost (103/200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000108
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000108_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 106.0 in stage 10.0 (TID 1155). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Running task 117.0 in stage 10.0 (TID 1166)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000110
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000110_0: Committed
15/08/21 23:14:33 INFO TaskSetManager: Starting task 119.0 in stage 10.0 (TID 1168, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 119.0 in stage 10.0 (TID 1168)
15/08/21 23:14:33 INFO Executor: Finished task 110.0 in stage 10.0 (TID 1159). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 106.0 in stage 10.0 (TID 1155) in 244 ms on localhost (104/200)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 120.0 in stage 10.0 (TID 1169, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Finished task 108.0 in stage 10.0 (TID 1157). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Running task 120.0 in stage 10.0 (TID 1169)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 121.0 in stage 10.0 (TID 1170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 121.0 in stage 10.0 (TID 1170)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 110.0 in stage 10.0 (TID 1159) in 224 ms on localhost (105/200)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 108.0 in stage 10.0 (TID 1157) in 232 ms on localhost (106/200)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000111
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000111_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 111.0 in stage 10.0 (TID 1160). 843 bytes result sent to driver
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO TaskSetManager: Starting task 122.0 in stage 10.0 (TID 1171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO Executor: Running task 122.0 in stage 10.0 (TID 1171)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Finished task 111.0 in stage 10.0 (TID 1160) in 212 ms on localhost (107/200)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,568
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,616
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,906B for [ps_partkey] INT32: 515 values, 2,067B raw, 1,870B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,192B for [value] DOUBLE: 515 values, 4,127B raw, 2,148B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,571B for [ps_partkey] INT32: 417 values, 1,675B raw, 1,535B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,796B for [value] DOUBLE: 417 values, 3,343B raw, 1,752B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,528
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,548B for [ps_partkey] INT32: 413 values, 1,659B raw, 1,512B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,805B for [value] DOUBLE: 413 values, 3,311B raw, 1,761B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,296
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,850B for [ps_partkey] INT32: 501 values, 2,011B raw, 1,814B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,137B for [value] DOUBLE: 501 values, 4,015B raw, 2,093B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000112
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000112_0: Committed
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO Executor: Finished task 112.0 in stage 10.0 (TID 1161). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 123.0 in stage 10.0 (TID 1172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 123.0 in stage 10.0 (TID 1172)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO TaskSetManager: Finished task 112.0 in stage 10.0 (TID 1161) in 208 ms on localhost (108/200)
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000113
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000113_0: Committed
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000095
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000095_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 113.0 in stage 10.0 (TID 1162). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 124.0 in stage 10.0 (TID 1173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 124.0 in stage 10.0 (TID 1173)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 113.0 in stage 10.0 (TID 1162) in 226 ms on localhost (109/200)
15/08/21 23:14:33 INFO Executor: Finished task 95.0 in stage 10.0 (TID 1144). 843 bytes result sent to driver
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 125.0 in stage 10.0 (TID 1174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 125.0 in stage 10.0 (TID 1174)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 95.0 in stage 10.0 (TID 1144) in 646 ms on localhost (110/200)
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,676
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,743B for [ps_partkey] INT32: 470 values, 1,887B raw, 1,707B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,008B for [value] DOUBLE: 470 values, 3,767B raw, 1,964B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,788
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,496
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,536
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,887B for [ps_partkey] INT32: 511 values, 2,051B raw, 1,851B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,943B for [ps_partkey] INT32: 526 values, 2,111B raw, 1,907B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,608
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,153B for [value] DOUBLE: 511 values, 4,095B raw, 2,109B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,227B for [value] DOUBLE: 526 values, 4,215B raw, 2,183B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,892B for [ps_partkey] INT32: 513 values, 2,059B raw, 1,856B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,209B for [value] DOUBLE: 513 values, 4,111B raw, 2,165B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,738B for [ps_partkey] INT32: 467 values, 1,875B raw, 1,702B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,998B for [value] DOUBLE: 467 values, 3,743B raw, 1,954B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,256
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,916
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,847B for [ps_partkey] INT32: 499 values, 2,003B raw, 1,811B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000100
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000100_0: Committed
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,126B for [value] DOUBLE: 499 values, 3,999B raw, 2,082B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,617B for [ps_partkey] INT32: 432 values, 1,735B raw, 1,581B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,850B for [value] DOUBLE: 432 values, 3,463B raw, 1,806B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000099
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000099_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 100.0 in stage 10.0 (TID 1149). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 126.0 in stage 10.0 (TID 1175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 126.0 in stage 10.0 (TID 1175)
15/08/21 23:14:33 INFO Executor: Finished task 99.0 in stage 10.0 (TID 1148). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 100.0 in stage 10.0 (TID 1149) in 661 ms on localhost (111/200)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 127.0 in stage 10.0 (TID 1176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 127.0 in stage 10.0 (TID 1176)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000116
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000116_0: Committed
15/08/21 23:14:33 INFO TaskSetManager: Finished task 99.0 in stage 10.0 (TID 1148) in 666 ms on localhost (112/200)
15/08/21 23:14:33 INFO Executor: Finished task 116.0 in stage 10.0 (TID 1165). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 128.0 in stage 10.0 (TID 1177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 128.0 in stage 10.0 (TID 1177)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 116.0 in stage 10.0 (TID 1165) in 218 ms on localhost (113/200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000119
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000119_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000120
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000117
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000120_0: Committed
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000117_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000118
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000118_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 119.0 in stage 10.0 (TID 1168). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 129.0 in stage 10.0 (TID 1178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Finished task 120.0 in stage 10.0 (TID 1169). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Finished task 117.0 in stage 10.0 (TID 1166). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Finished task 118.0 in stage 10.0 (TID 1167). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 130.0 in stage 10.0 (TID 1179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 131.0 in stage 10.0 (TID 1180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 132.0 in stage 10.0 (TID 1181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 132.0 in stage 10.0 (TID 1181)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 119.0 in stage 10.0 (TID 1168) in 212 ms on localhost (114/200)
15/08/21 23:14:33 INFO Executor: Running task 129.0 in stage 10.0 (TID 1178)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 120.0 in stage 10.0 (TID 1169) in 211 ms on localhost (115/200)
15/08/21 23:14:33 INFO Executor: Running task 130.0 in stage 10.0 (TID 1179)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 117.0 in stage 10.0 (TID 1166) in 225 ms on localhost (116/200)
15/08/21 23:14:33 INFO Executor: Running task 131.0 in stage 10.0 (TID 1180)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO TaskSetManager: Finished task 118.0 in stage 10.0 (TID 1167) in 224 ms on localhost (117/200)
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000101
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000101_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000121
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000121_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 101.0 in stage 10.0 (TID 1150). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 133.0 in stage 10.0 (TID 1182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 133.0 in stage 10.0 (TID 1182)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000122
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000122_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 121.0 in stage 10.0 (TID 1170). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 134.0 in stage 10.0 (TID 1183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 101.0 in stage 10.0 (TID 1150) in 682 ms on localhost (118/200)
15/08/21 23:14:33 INFO Executor: Running task 134.0 in stage 10.0 (TID 1183)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO Executor: Finished task 122.0 in stage 10.0 (TID 1171). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 135.0 in stage 10.0 (TID 1184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 135.0 in stage 10.0 (TID 1184)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 121.0 in stage 10.0 (TID 1170) in 224 ms on localhost (119/200)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Finished task 122.0 in stage 10.0 (TID 1171) in 209 ms on localhost (120/200)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,528
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,722B for [ps_partkey] INT32: 463 values, 1,859B raw, 1,686B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,960B for [value] DOUBLE: 463 values, 3,711B raw, 1,916B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,416
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,872B for [ps_partkey] INT32: 507 values, 2,035B raw, 1,836B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,129B for [value] DOUBLE: 507 values, 4,063B raw, 2,085B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000123
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000123_0: Committed
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000124
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000124_0: Committed
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,836
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO Executor: Finished task 123.0 in stage 10.0 (TID 1172). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Finished task 124.0 in stage 10.0 (TID 1173). 843 bytes result sent to driver
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO TaskSetManager: Starting task 136.0 in stage 10.0 (TID 1185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,950B for [ps_partkey] INT32: 528 values, 2,119B raw, 1,914B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Running task 136.0 in stage 10.0 (TID 1185)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,234B for [value] DOUBLE: 528 values, 4,231B raw, 2,190B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 137.0 in stage 10.0 (TID 1186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 137.0 in stage 10.0 (TID 1186)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 123.0 in stage 10.0 (TID 1172) in 188 ms on localhost (121/200)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 124.0 in stage 10.0 (TID 1173) in 162 ms on localhost (122/200)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,463,716
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,236B for [ps_partkey] INT32: 322 values, 1,295B raw, 1,200B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,451B for [value] DOUBLE: 322 values, 2,583B raw, 1,407B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,516
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,808
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,891B for [ps_partkey] INT32: 512 values, 2,055B raw, 1,855B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,152B for [value] DOUBLE: 512 values, 4,103B raw, 2,108B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,761B for [ps_partkey] INT32: 477 values, 1,915B raw, 1,725B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,026B for [value] DOUBLE: 477 values, 3,823B raw, 1,982B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,996
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,228
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,978B for [ps_partkey] INT32: 536 values, 2,151B raw, 1,942B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,232B for [value] DOUBLE: 536 values, 4,295B raw, 2,188B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,017B for [ps_partkey] INT32: 548 values, 2,199B raw, 1,981B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,310B for [value] DOUBLE: 548 values, 4,391B raw, 2,266B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,676
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,236
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000107
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000107_0: Committed
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,576B for [ps_partkey] INT32: 420 values, 1,687B raw, 1,540B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,788B for [value] DOUBLE: 420 values, 3,367B raw, 1,744B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,843B for [ps_partkey] INT32: 498 values, 1,999B raw, 1,807B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,105B for [value] DOUBLE: 498 values, 3,991B raw, 2,061B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,716
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,148
15/08/21 23:14:33 INFO Executor: Finished task 107.0 in stage 10.0 (TID 1156). 843 bytes result sent to driver
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO TaskSetManager: Starting task 138.0 in stage 10.0 (TID 1187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 138.0 in stage 10.0 (TID 1187)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,660B for [ps_partkey] INT32: 444 values, 1,783B raw, 1,624B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,897B for [value] DOUBLE: 444 values, 3,559B raw, 1,853B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,746B for [ps_partkey] INT32: 472 values, 1,895B raw, 1,710B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,004B for [value] DOUBLE: 472 values, 3,783B raw, 1,960B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000127
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000127_0: Committed
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,268
15/08/21 23:14:33 INFO TaskSetManager: Finished task 107.0 in stage 10.0 (TID 1156) in 643 ms on localhost (123/200)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,326B for [ps_partkey] INT32: 350 values, 1,407B raw, 1,290B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,542B for [value] DOUBLE: 350 values, 2,807B raw, 1,498B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000126
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000126_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 127.0 in stage 10.0 (TID 1176). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Finished task 126.0 in stage 10.0 (TID 1175). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 139.0 in stage 10.0 (TID 1188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 139.0 in stage 10.0 (TID 1188)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 140.0 in stage 10.0 (TID 1189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Finished task 127.0 in stage 10.0 (TID 1176) in 221 ms on localhost (124/200)
15/08/21 23:14:33 INFO Executor: Running task 140.0 in stage 10.0 (TID 1189)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 126.0 in stage 10.0 (TID 1175) in 231 ms on localhost (125/200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000135
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000135_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 135.0 in stage 10.0 (TID 1184). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 141.0 in stage 10.0 (TID 1190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 141.0 in stage 10.0 (TID 1190)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 135.0 in stage 10.0 (TID 1184) in 200 ms on localhost (126/200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000133
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000133_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 133.0 in stage 10.0 (TID 1182). 843 bytes result sent to driver
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO TaskSetManager: Starting task 142.0 in stage 10.0 (TID 1191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO Executor: Running task 142.0 in stage 10.0 (TID 1191)
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000132
15/08/21 23:14:33 INFO TaskSetManager: Finished task 133.0 in stage 10.0 (TID 1182) in 214 ms on localhost (127/200)
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000132_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 132.0 in stage 10.0 (TID 1181). 843 bytes result sent to driver
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000134
15/08/21 23:14:33 INFO TaskSetManager: Starting task 143.0 in stage 10.0 (TID 1192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000134_0: Committed
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,948
15/08/21 23:14:33 INFO Executor: Running task 143.0 in stage 10.0 (TID 1192)
15/08/21 23:14:33 INFO Executor: Finished task 134.0 in stage 10.0 (TID 1183). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 132.0 in stage 10.0 (TID 1181) in 231 ms on localhost (128/200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000128
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000128_0: Committed
15/08/21 23:14:33 INFO TaskSetManager: Starting task 144.0 in stage 10.0 (TID 1193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 144.0 in stage 10.0 (TID 1193)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,134B for [ps_partkey] INT32: 584 values, 2,343B raw, 2,098B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,405B for [value] DOUBLE: 584 values, 4,679B raw, 2,361B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Finished task 128.0 in stage 10.0 (TID 1177). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 134.0 in stage 10.0 (TID 1183) in 219 ms on localhost (129/200)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 145.0 in stage 10.0 (TID 1194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 145.0 in stage 10.0 (TID 1194)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Finished task 128.0 in stage 10.0 (TID 1177) in 242 ms on localhost (130/200)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000137
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000137_0: Committed
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,208
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO Executor: Finished task 137.0 in stage 10.0 (TID 1186). 843 bytes result sent to driver
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,831B for [ps_partkey] INT32: 497 values, 1,995B raw, 1,795B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,095B for [value] DOUBLE: 497 values, 3,983B raw, 2,051B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 146.0 in stage 10.0 (TID 1195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 146.0 in stage 10.0 (TID 1195)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 137.0 in stage 10.0 (TID 1186) in 185 ms on localhost (131/200)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000136
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000136_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000115
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000115_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000114
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000114_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 115.0 in stage 10.0 (TID 1164). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Finished task 136.0 in stage 10.0 (TID 1185). 843 bytes result sent to driver
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 147.0 in stage 10.0 (TID 1196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Finished task 114.0 in stage 10.0 (TID 1163). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Running task 147.0 in stage 10.0 (TID 1196)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 148.0 in stage 10.0 (TID 1197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 148.0 in stage 10.0 (TID 1197)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 149.0 in stage 10.0 (TID 1198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 149.0 in stage 10.0 (TID 1198)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO TaskSetManager: Finished task 115.0 in stage 10.0 (TID 1164) in 604 ms on localhost (132/200)
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO TaskSetManager: Finished task 114.0 in stage 10.0 (TID 1163) in 615 ms on localhost (133/200)
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO TaskSetManager: Finished task 136.0 in stage 10.0 (TID 1185) in 234 ms on localhost (134/200)
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,788
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,696
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,426B for [ps_partkey] INT32: 376 values, 1,511B raw, 1,390B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,637B for [value] DOUBLE: 376 values, 3,015B raw, 1,593B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,573B for [ps_partkey] INT32: 421 values, 1,691B raw, 1,537B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,809B for [value] DOUBLE: 421 values, 3,375B raw, 1,765B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,488
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,709B for [ps_partkey] INT32: 461 values, 1,851B raw, 1,673B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,948B for [value] DOUBLE: 461 values, 3,695B raw, 1,904B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,008
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,440
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,976
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,980B for [ps_partkey] INT32: 537 values, 2,155B raw, 1,944B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,619B for [ps_partkey] INT32: 435 values, 1,747B raw, 1,583B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,334B for [value] DOUBLE: 537 values, 4,303B raw, 2,290B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,008
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,930B for [value] DOUBLE: 435 values, 3,487B raw, 1,886B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,231B for [ps_partkey] INT32: 609 values, 2,443B raw, 2,195B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,614B for [value] DOUBLE: 609 values, 4,879B raw, 2,570B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,969B for [ps_partkey] INT32: 537 values, 2,155B raw, 1,933B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,232B for [value] DOUBLE: 537 values, 4,303B raw, 2,188B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000138
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000138_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 138.0 in stage 10.0 (TID 1187). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 150.0 in stage 10.0 (TID 1199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 150.0 in stage 10.0 (TID 1199)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000139
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000139_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 139.0 in stage 10.0 (TID 1188). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 138.0 in stage 10.0 (TID 1187) in 214 ms on localhost (135/200)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 151.0 in stage 10.0 (TID 1200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 151.0 in stage 10.0 (TID 1200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000141
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000141_0: Committed
15/08/21 23:14:33 INFO TaskSetManager: Finished task 139.0 in stage 10.0 (TID 1188) in 199 ms on localhost (136/200)
15/08/21 23:14:33 INFO Executor: Finished task 141.0 in stage 10.0 (TID 1190). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 152.0 in stage 10.0 (TID 1201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 152.0 in stage 10.0 (TID 1201)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO TaskSetManager: Finished task 141.0 in stage 10.0 (TID 1190) in 199 ms on localhost (137/200)
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,056
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000144
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,468B for [ps_partkey] INT32: 389 values, 1,563B raw, 1,432B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,761B for [value] DOUBLE: 389 values, 3,119B raw, 1,717B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000144_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000140
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000140_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000145
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000145_0: Committed
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,168
15/08/21 23:14:33 INFO Executor: Finished task 140.0 in stage 10.0 (TID 1189). 843 bytes result sent to driver
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 153.0 in stage 10.0 (TID 1202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 153.0 in stage 10.0 (TID 1202)
15/08/21 23:14:33 INFO Executor: Finished task 145.0 in stage 10.0 (TID 1194). 843 bytes result sent to driver
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000142
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000142_0: Committed
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,170B for [ps_partkey] INT32: 595 values, 2,387B raw, 2,134B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Finished task 144.0 in stage 10.0 (TID 1193). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 154.0 in stage 10.0 (TID 1203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,592B for [value] DOUBLE: 595 values, 4,767B raw, 2,548B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Running task 154.0 in stage 10.0 (TID 1203)
15/08/21 23:14:33 INFO Executor: Finished task 142.0 in stage 10.0 (TID 1191). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 145.0 in stage 10.0 (TID 1194) in 209 ms on localhost (138/200)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 140.0 in stage 10.0 (TID 1189) in 234 ms on localhost (139/200)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 155.0 in stage 10.0 (TID 1204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 144.0 in stage 10.0 (TID 1193) in 216 ms on localhost (140/200)
15/08/21 23:14:33 INFO Executor: Running task 155.0 in stage 10.0 (TID 1204)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 156.0 in stage 10.0 (TID 1205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 156.0 in stage 10.0 (TID 1205)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 142.0 in stage 10.0 (TID 1191) in 237 ms on localhost (141/200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000143
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000143_0: Committed
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,000
15/08/21 23:14:33 INFO Executor: Finished task 143.0 in stage 10.0 (TID 1192). 843 bytes result sent to driver
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000146
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000146_0: Committed
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,628B for [ps_partkey] INT32: 437 values, 1,755B raw, 1,592B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 157.0 in stage 10.0 (TID 1206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,925B for [value] DOUBLE: 437 values, 3,503B raw, 1,881B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Running task 157.0 in stage 10.0 (TID 1206)
15/08/21 23:14:33 INFO Executor: Finished task 146.0 in stage 10.0 (TID 1195). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 143.0 in stage 10.0 (TID 1192) in 248 ms on localhost (142/200)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,548
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,556B for [ps_partkey] INT32: 414 values, 1,663B raw, 1,520B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,874B for [value] DOUBLE: 414 values, 3,319B raw, 1,830B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 158.0 in stage 10.0 (TID 1207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 158.0 in stage 10.0 (TID 1207)
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,156
15/08/21 23:14:33 INFO TaskSetManager: Finished task 146.0 in stage 10.0 (TID 1195) in 223 ms on localhost (143/200)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,655B for [ps_partkey] INT32: 444 values, 1,783B raw, 1,619B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,972B for [value] DOUBLE: 444 values, 3,559B raw, 1,928B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000149
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000149_0: Committed
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO Executor: Finished task 149.0 in stage 10.0 (TID 1198). 843 bytes result sent to driver
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 159.0 in stage 10.0 (TID 1208, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 159.0 in stage 10.0 (TID 1208)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 149.0 in stage 10.0 (TID 1198) in 204 ms on localhost (144/200)
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000125
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000125_0: Committed
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO Executor: Finished task 125.0 in stage 10.0 (TID 1174). 843 bytes result sent to driver
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,120
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,156
15/08/21 23:14:33 INFO TaskSetManager: Starting task 160.0 in stage 10.0 (TID 1209, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,788
15/08/21 23:14:33 INFO Executor: Running task 160.0 in stage 10.0 (TID 1209)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,827B for [ps_partkey] INT32: 494 values, 1,983B raw, 1,791B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,649B for [ps_partkey] INT32: 443 values, 1,779B raw, 1,613B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,195B for [value] DOUBLE: 494 values, 3,959B raw, 2,151B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,998B for [value] DOUBLE: 443 values, 3,551B raw, 1,954B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO TaskSetManager: Finished task 125.0 in stage 10.0 (TID 1174) in 635 ms on localhost (145/200)
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,942B for [ps_partkey] INT32: 526 values, 2,111B raw, 1,906B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,324B for [value] DOUBLE: 526 values, 4,215B raw, 2,280B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,868
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,790B for [ps_partkey] INT32: 480 values, 1,927B raw, 1,754B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,132B for [value] DOUBLE: 480 values, 3,847B raw, 2,088B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000129
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000129_0: Committed
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,928
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000151
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000151_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 129.0 in stage 10.0 (TID 1178). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Finished task 151.0 in stage 10.0 (TID 1200). 843 bytes result sent to driver
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000150
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000150_0: Committed
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,449B for [ps_partkey] INT32: 383 values, 1,539B raw, 1,413B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,744B for [value] DOUBLE: 383 values, 3,071B raw, 1,700B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 161.0 in stage 10.0 (TID 1210, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 161.0 in stage 10.0 (TID 1210)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 162.0 in stage 10.0 (TID 1211, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 162.0 in stage 10.0 (TID 1211)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000154
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000154_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 150.0 in stage 10.0 (TID 1199). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Finished task 129.0 in stage 10.0 (TID 1178) in 627 ms on localhost (146/200)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 151.0 in stage 10.0 (TID 1200) in 225 ms on localhost (147/200)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 163.0 in stage 10.0 (TID 1212, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 163.0 in stage 10.0 (TID 1212)
15/08/21 23:14:33 INFO Executor: Finished task 154.0 in stage 10.0 (TID 1203). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 164.0 in stage 10.0 (TID 1213, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 164.0 in stage 10.0 (TID 1213)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 150.0 in stage 10.0 (TID 1199) in 242 ms on localhost (148/200)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 154.0 in stage 10.0 (TID 1203) in 200 ms on localhost (149/200)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000130
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000130_0: Committed
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,469,236
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,528
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000131
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000131_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 130.0 in stage 10.0 (TID 1179). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 165.0 in stage 10.0 (TID 1214, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,828
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,551B for [ps_partkey] INT32: 413 values, 1,659B raw, 1,515B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Running task 165.0 in stage 10.0 (TID 1214)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,852B for [value] DOUBLE: 413 values, 3,311B raw, 1,808B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Finished task 131.0 in stage 10.0 (TID 1180). 843 bytes result sent to driver
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,193B for [ps_partkey] INT32: 600 values, 2,407B raw, 2,157B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Starting task 166.0 in stage 10.0 (TID 1215, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,601B for [ps_partkey] INT32: 428 values, 1,719B raw, 1,565B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO TaskSetManager: Finished task 130.0 in stage 10.0 (TID 1179) in 648 ms on localhost (150/200)
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,616B for [value] DOUBLE: 600 values, 4,807B raw, 2,572B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,934B for [value] DOUBLE: 428 values, 3,431B raw, 1,890B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO Executor: Running task 166.0 in stage 10.0 (TID 1215)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 131.0 in stage 10.0 (TID 1180) in 648 ms on localhost (151/200)
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,096
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,640B for [ps_partkey] INT32: 441 values, 1,771B raw, 1,604B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,947B for [value] DOUBLE: 441 values, 3,535B raw, 1,903B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000153
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000153_0: Committed
15/08/21 23:14:33 INFO Executor: Finished task 153.0 in stage 10.0 (TID 1202). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 167.0 in stage 10.0 (TID 1216, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000155
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000155_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000156
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000156_0: Committed
15/08/21 23:14:33 INFO TaskSetManager: Finished task 153.0 in stage 10.0 (TID 1202) in 240 ms on localhost (152/200)
15/08/21 23:14:33 INFO Executor: Finished task 155.0 in stage 10.0 (TID 1204). 843 bytes result sent to driver
15/08/21 23:14:33 INFO Executor: Finished task 156.0 in stage 10.0 (TID 1205). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 168.0 in stage 10.0 (TID 1217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 168.0 in stage 10.0 (TID 1217)
15/08/21 23:14:33 INFO Executor: Running task 167.0 in stage 10.0 (TID 1216)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 169.0 in stage 10.0 (TID 1218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 169.0 in stage 10.0 (TID 1218)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000157
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000157_0: Committed
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000158
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000158_0: Committed
15/08/21 23:14:33 INFO TaskSetManager: Finished task 156.0 in stage 10.0 (TID 1205) in 219 ms on localhost (153/200)
15/08/21 23:14:33 INFO Executor: Finished task 157.0 in stage 10.0 (TID 1206). 843 bytes result sent to driver
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO TaskSetManager: Finished task 155.0 in stage 10.0 (TID 1204) in 237 ms on localhost (154/200)
15/08/21 23:14:33 INFO TaskSetManager: Starting task 170.0 in stage 10.0 (TID 1219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO Executor: Running task 170.0 in stage 10.0 (TID 1219)
15/08/21 23:14:33 INFO Executor: Finished task 158.0 in stage 10.0 (TID 1207). 843 bytes result sent to driver
15/08/21 23:14:33 INFO TaskSetManager: Starting task 171.0 in stage 10.0 (TID 1220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:33 INFO TaskSetManager: Finished task 157.0 in stage 10.0 (TID 1206) in 209 ms on localhost (155/200)
15/08/21 23:14:33 INFO Executor: Running task 171.0 in stage 10.0 (TID 1220)
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO TaskSetManager: Finished task 158.0 in stage 10.0 (TID 1207) in 199 ms on localhost (156/200)
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,776
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,765B for [ps_partkey] INT32: 475 values, 1,907B raw, 1,729B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 2,115B for [value] DOUBLE: 475 values, 3,807B raw, 2,071B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,208
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,663B for [ps_partkey] INT32: 447 values, 1,795B raw, 1,627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ColumnChunkPageWriteStore: written 1,970B for [value] DOUBLE: 447 values, 3,583B raw, 1,926B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000159
15/08/21 23:14:33 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000159_0: Committed
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,628
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,336
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO Executor: Finished task 159.0 in stage 10.0 (TID 1208). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Starting task 172.0 in stage 10.0 (TID 1221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 172.0 in stage 10.0 (TID 1221)
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,688B for [ps_partkey] INT32: 453 values, 1,819B raw, 1,652B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,910B for [ps_partkey] INT32: 518 values, 2,079B raw, 1,874B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,031B for [value] DOUBLE: 453 values, 3,631B raw, 1,987B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,273B for [value] DOUBLE: 518 values, 4,151B raw, 2,229B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,868
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO TaskSetManager: Finished task 159.0 in stage 10.0 (TID 1208) in 262 ms on localhost (157/200)
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000160
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000160_0: Committed
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,774B for [ps_partkey] INT32: 480 values, 1,927B raw, 1,738B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO Executor: Finished task 160.0 in stage 10.0 (TID 1209). 843 bytes result sent to driver
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,132B for [value] DOUBLE: 480 values, 3,847B raw, 2,088B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO TaskSetManager: Starting task 173.0 in stage 10.0 (TID 1222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO Executor: Running task 173.0 in stage 10.0 (TID 1222)
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO TaskSetManager: Finished task 160.0 in stage 10.0 (TID 1209) in 228 ms on localhost (158/200)
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,048
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,464,928
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,476
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,635B for [ps_partkey] INT32: 439 values, 1,763B raw, 1,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,932B for [value] DOUBLE: 439 values, 3,519B raw, 1,888B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,548B for [ps_partkey] INT32: 410 values, 1,647B raw, 1,512B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,832B for [value] DOUBLE: 410 values, 3,287B raw, 1,788B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,446B for [ps_partkey] INT32: 383 values, 1,539B raw, 1,410B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,734B for [value] DOUBLE: 383 values, 3,071B raw, 1,690B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000164
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000164_0: Committed
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000161
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000161_0: Committed
15/08/21 23:14:34 INFO Executor: Finished task 161.0 in stage 10.0 (TID 1210). 843 bytes result sent to driver
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000162
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000162_0: Committed
15/08/21 23:14:34 INFO Executor: Finished task 164.0 in stage 10.0 (TID 1213). 843 bytes result sent to driver
15/08/21 23:14:34 INFO Executor: Finished task 162.0 in stage 10.0 (TID 1211). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Starting task 174.0 in stage 10.0 (TID 1223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 174.0 in stage 10.0 (TID 1223)
15/08/21 23:14:34 INFO TaskSetManager: Starting task 175.0 in stage 10.0 (TID 1224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 175.0 in stage 10.0 (TID 1224)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 161.0 in stage 10.0 (TID 1210) in 201 ms on localhost (159/200)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 164.0 in stage 10.0 (TID 1213) in 195 ms on localhost (160/200)
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,463,896
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,076
15/08/21 23:14:34 INFO TaskSetManager: Finished task 162.0 in stage 10.0 (TID 1211) in 201 ms on localhost (161/200)
15/08/21 23:14:34 INFO TaskSetManager: Starting task 176.0 in stage 10.0 (TID 1225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 176.0 in stage 10.0 (TID 1225)
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,257B for [ps_partkey] INT32: 331 values, 1,331B raw, 1,221B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,756
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,488
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,541B for [value] DOUBLE: 331 values, 2,655B raw, 1,497B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,641B for [ps_partkey] INT32: 440 values, 1,767B raw, 1,605B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,954B for [value] DOUBLE: 440 values, 3,527B raw, 1,910B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,712B for [ps_partkey] INT32: 461 values, 1,851B raw, 1,676B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,053B for [value] DOUBLE: 461 values, 3,695B raw, 2,009B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,587B for [ps_partkey] INT32: 424 values, 1,703B raw, 1,551B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000163
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000163_0: Committed
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,872B for [value] DOUBLE: 424 values, 3,399B raw, 1,828B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000166
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000166_0: Committed
15/08/21 23:14:34 INFO Executor: Finished task 163.0 in stage 10.0 (TID 1212). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Starting task 177.0 in stage 10.0 (TID 1226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,268
15/08/21 23:14:34 INFO Executor: Finished task 166.0 in stage 10.0 (TID 1215). 843 bytes result sent to driver
15/08/21 23:14:34 INFO Executor: Running task 177.0 in stage 10.0 (TID 1226)
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000165
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000165_0: Committed
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO TaskSetManager: Starting task 178.0 in stage 10.0 (TID 1227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 163.0 in stage 10.0 (TID 1212) in 221 ms on localhost (162/200)
15/08/21 23:14:34 INFO Executor: Finished task 165.0 in stage 10.0 (TID 1214). 843 bytes result sent to driver
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,027B for [ps_partkey] INT32: 550 values, 2,207B raw, 1,991B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,399B for [value] DOUBLE: 550 values, 4,407B raw, 2,355B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO Executor: Running task 178.0 in stage 10.0 (TID 1227)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 166.0 in stage 10.0 (TID 1215) in 203 ms on localhost (163/200)
15/08/21 23:14:34 INFO TaskSetManager: Starting task 179.0 in stage 10.0 (TID 1228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 179.0 in stage 10.0 (TID 1228)
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO TaskSetManager: Finished task 165.0 in stage 10.0 (TID 1214) in 210 ms on localhost (164/200)
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000170
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000170_0: Committed
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000167
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000167_0: Committed
15/08/21 23:14:34 INFO Executor: Finished task 170.0 in stage 10.0 (TID 1219). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Starting task 180.0 in stage 10.0 (TID 1229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 180.0 in stage 10.0 (TID 1229)
15/08/21 23:14:34 INFO Executor: Finished task 167.0 in stage 10.0 (TID 1216). 843 bytes result sent to driver
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO TaskSetManager: Starting task 181.0 in stage 10.0 (TID 1230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000171
15/08/21 23:14:34 INFO TaskSetManager: Finished task 170.0 in stage 10.0 (TID 1219) in 206 ms on localhost (165/200)
15/08/21 23:14:34 INFO Executor: Running task 181.0 in stage 10.0 (TID 1230)
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000171_0: Committed
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 23:14:34 INFO TaskSetManager: Finished task 167.0 in stage 10.0 (TID 1216) in 222 ms on localhost (166/200)
15/08/21 23:14:34 INFO Executor: Finished task 171.0 in stage 10.0 (TID 1220). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Starting task 182.0 in stage 10.0 (TID 1231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 182.0 in stage 10.0 (TID 1231)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 171.0 in stage 10.0 (TID 1220) in 210 ms on localhost (167/200)
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,908
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,141B for [ps_partkey] INT32: 582 values, 2,335B raw, 2,105B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,519B for [value] DOUBLE: 582 values, 4,663B raw, 2,475B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000147
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000147_0: Committed
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO Executor: Finished task 147.0 in stage 10.0 (TID 1196). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Starting task 183.0 in stage 10.0 (TID 1232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO Executor: Running task 183.0 in stage 10.0 (TID 1232)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 147.0 in stage 10.0 (TID 1196) in 604 ms on localhost (168/200)
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000148
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000148_0: Committed
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO Executor: Finished task 148.0 in stage 10.0 (TID 1197). 843 bytes result sent to driver
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO TaskSetManager: Starting task 184.0 in stage 10.0 (TID 1233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 184.0 in stage 10.0 (TID 1233)
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO TaskSetManager: Finished task 148.0 in stage 10.0 (TID 1197) in 612 ms on localhost (169/200)
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,032
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,980B for [ps_partkey] INT32: 539 values, 2,163B raw, 1,944B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,347B for [value] DOUBLE: 539 values, 4,319B raw, 2,303B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000172
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000172_0: Committed
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO Executor: Finished task 172.0 in stage 10.0 (TID 1221). 843 bytes result sent to driver
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO TaskSetManager: Starting task 185.0 in stage 10.0 (TID 1234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 185.0 in stage 10.0 (TID 1234)
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO TaskSetManager: Finished task 172.0 in stage 10.0 (TID 1221) in 191 ms on localhost (170/200)
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,356
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,948
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,687B for [ps_partkey] INT32: 454 values, 1,823B raw, 1,651B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,999B for [value] DOUBLE: 454 values, 3,639B raw, 1,955B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,368
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,144B for [ps_partkey] INT32: 584 values, 2,343B raw, 2,108B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,545B for [value] DOUBLE: 584 values, 4,679B raw, 2,501B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,866B for [ps_partkey] INT32: 505 values, 2,027B raw, 1,830B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,209B for [value] DOUBLE: 505 values, 4,047B raw, 2,165B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000173
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000173_0: Committed
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,720
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000168
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000168_0: Committed
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000169
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,112
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000169_0: Committed
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000152
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000152_0: Committed
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,680
15/08/21 23:14:34 INFO Executor: Finished task 173.0 in stage 10.0 (TID 1222). 843 bytes result sent to driver
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO TaskSetManager: Starting task 186.0 in stage 10.0 (TID 1235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Finished task 169.0 in stage 10.0 (TID 1218). 843 bytes result sent to driver
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,920B for [ps_partkey] INT32: 523 values, 2,099B raw, 1,884B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO Executor: Running task 186.0 in stage 10.0 (TID 1235)
15/08/21 23:14:34 INFO Executor: Finished task 152.0 in stage 10.0 (TID 1201). 843 bytes result sent to driver
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,298B for [value] DOUBLE: 523 values, 4,191B raw, 2,254B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO TaskSetManager: Starting task 187.0 in stage 10.0 (TID 1236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 187.0 in stage 10.0 (TID 1236)
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,752B for [ps_partkey] INT32: 471 values, 1,891B raw, 1,716B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,111B for [value] DOUBLE: 471 values, 3,775B raw, 2,067B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO TaskSetManager: Starting task 188.0 in stage 10.0 (TID 1237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 188.0 in stage 10.0 (TID 1237)
15/08/21 23:14:34 INFO Executor: Finished task 168.0 in stage 10.0 (TID 1217). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Finished task 169.0 in stage 10.0 (TID 1218) in 1003 ms on localhost (171/200)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 152.0 in stage 10.0 (TID 1201) in 1267 ms on localhost (172/200)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 173.0 in stage 10.0 (TID 1222) in 897 ms on localhost (173/200)
15/08/21 23:14:34 INFO TaskSetManager: Starting task 189.0 in stage 10.0 (TID 1238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO TaskSetManager: Finished task 168.0 in stage 10.0 (TID 1217) in 1006 ms on localhost (174/200)
15/08/21 23:14:34 INFO Executor: Running task 189.0 in stage 10.0 (TID 1238)
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,650B for [ps_partkey] INT32: 443 values, 1,779B raw, 1,614B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,952B for [value] DOUBLE: 443 values, 3,551B raw, 1,908B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:34 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000176
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000176_0: Committed
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:34 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:34 INFO Executor: Finished task 176.0 in stage 10.0 (TID 1225). 843 bytes result sent to driver
15/08/21 23:14:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000179
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000174
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000174_0: Committed
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000177
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000177_0: Committed
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000179_0: Committed
15/08/21 23:14:34 INFO TaskSetManager: Starting task 190.0 in stage 10.0 (TID 1239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Finished task 174.0 in stage 10.0 (TID 1223). 843 bytes result sent to driver
15/08/21 23:14:34 INFO TaskSetManager: Starting task 191.0 in stage 10.0 (TID 1240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 190.0 in stage 10.0 (TID 1239)
15/08/21 23:14:34 INFO Executor: Running task 191.0 in stage 10.0 (TID 1240)
15/08/21 23:14:34 INFO Executor: Finished task 177.0 in stage 10.0 (TID 1226). 843 bytes result sent to driver
15/08/21 23:14:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000175
15/08/21 23:14:34 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000175_0: Committed
15/08/21 23:14:34 INFO TaskSetManager: Starting task 192.0 in stage 10.0 (TID 1241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 174.0 in stage 10.0 (TID 1223) in 914 ms on localhost (175/200)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 176.0 in stage 10.0 (TID 1225) in 903 ms on localhost (176/200)
15/08/21 23:14:34 INFO Executor: Running task 192.0 in stage 10.0 (TID 1241)
15/08/21 23:14:34 INFO TaskSetManager: Finished task 177.0 in stage 10.0 (TID 1226) in 890 ms on localhost (177/200)
15/08/21 23:14:34 INFO Executor: Finished task 179.0 in stage 10.0 (TID 1228). 843 bytes result sent to driver
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO TaskSetManager: Starting task 193.0 in stage 10.0 (TID 1242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Finished task 175.0 in stage 10.0 (TID 1224). 843 bytes result sent to driver
15/08/21 23:14:34 INFO Executor: Running task 193.0 in stage 10.0 (TID 1242)
15/08/21 23:14:34 INFO TaskSetManager: Starting task 194.0 in stage 10.0 (TID 1243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:34 INFO Executor: Running task 194.0 in stage 10.0 (TID 1243)
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,184
15/08/21 23:14:34 INFO TaskSetManager: Finished task 175.0 in stage 10.0 (TID 1224) in 922 ms on localhost (178/200)
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 23:14:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,316
15/08/21 23:14:34 INFO TaskSetManager: Finished task 179.0 in stage 10.0 (TID 1228) in 910 ms on localhost (179/200)
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,680B for [ps_partkey] INT32: 452 values, 1,815B raw, 1,644B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,029B for [value] DOUBLE: 452 values, 3,623B raw, 1,985B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 1,841B for [ps_partkey] INT32: 497 values, 1,995B raw, 1,805B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:34 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:40586 in memory (size: 8.3 KB, free: 20.7 GB)
15/08/21 23:14:34 INFO ColumnChunkPageWriteStore: written 2,155B for [value] DOUBLE: 497 values, 3,983B raw, 2,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,608
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,740B for [ps_partkey] INT32: 467 values, 1,875B raw, 1,704B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,060B for [value] DOUBLE: 467 values, 3,743B raw, 2,016B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,396
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,525B for [ps_partkey] INT32: 406 values, 1,631B raw, 1,489B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,809B for [value] DOUBLE: 406 values, 3,255B raw, 1,765B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,248
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,845B for [ps_partkey] INT32: 499 values, 2,003B raw, 1,809B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,210B for [value] DOUBLE: 499 values, 3,999B raw, 2,166B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000183
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000183_0: Committed
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO Executor: Finished task 183.0 in stage 10.0 (TID 1232). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Starting task 195.0 in stage 10.0 (TID 1244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO Executor: Running task 195.0 in stage 10.0 (TID 1244)
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO TaskSetManager: Finished task 183.0 in stage 10.0 (TID 1232) in 892 ms on localhost (180/200)
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000184
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000184_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 184.0 in stage 10.0 (TID 1233). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Starting task 196.0 in stage 10.0 (TID 1245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:35 INFO Executor: Running task 196.0 in stage 10.0 (TID 1245)
15/08/21 23:14:35 INFO TaskSetManager: Finished task 184.0 in stage 10.0 (TID 1233) in 912 ms on localhost (181/200)
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000181
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000181_0: Committed
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,076
15/08/21 23:14:35 INFO Executor: Finished task 181.0 in stage 10.0 (TID 1230). 843 bytes result sent to driver
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000180
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000180_0: Committed
15/08/21 23:14:35 INFO TaskSetManager: Starting task 197.0 in stage 10.0 (TID 1246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:35 INFO Executor: Running task 197.0 in stage 10.0 (TID 1246)
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,348
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,408
15/08/21 23:14:35 INFO TaskSetManager: Finished task 181.0 in stage 10.0 (TID 1230) in 953 ms on localhost (182/200)
15/08/21 23:14:35 INFO Executor: Finished task 180.0 in stage 10.0 (TID 1229). 843 bytes result sent to driver
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,691B for [ps_partkey] INT32: 454 values, 1,823B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,691B for [ps_partkey] INT32: 457 values, 1,835B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,003B for [value] DOUBLE: 457 values, 3,663B raw, 1,959B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,994B for [ps_partkey] INT32: 540 values, 2,167B raw, 1,958B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO TaskSetManager: Starting task 198.0 in stage 10.0 (TID 1247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,768
15/08/21 23:14:35 INFO TaskSetManager: Finished task 180.0 in stage 10.0 (TID 1229) in 958 ms on localhost (183/200)
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,991B for [value] DOUBLE: 454 values, 3,639B raw, 1,947B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO Executor: Running task 198.0 in stage 10.0 (TID 1247)
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,361B for [value] DOUBLE: 540 values, 4,327B raw, 2,317B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,591B for [ps_partkey] INT32: 425 values, 1,707B raw, 1,555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,896B for [value] DOUBLE: 425 values, 3,407B raw, 1,852B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000187
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000187_0: Committed
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,896
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000185
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000185_0: Committed
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000186
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000189
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000189_0: Committed
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000186_0: Committed
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO Executor: Finished task 187.0 in stage 10.0 (TID 1236). 843 bytes result sent to driver
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,953B for [ps_partkey] INT32: 531 values, 2,131B raw, 1,917B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO Executor: Finished task 185.0 in stage 10.0 (TID 1234). 843 bytes result sent to driver
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,298B for [value] DOUBLE: 531 values, 4,255B raw, 2,254B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO Executor: Finished task 189.0 in stage 10.0 (TID 1238). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Starting task 199.0 in stage 10.0 (TID 1248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 23:14:35 INFO Executor: Finished task 186.0 in stage 10.0 (TID 1235). 843 bytes result sent to driver
15/08/21 23:14:35 INFO Executor: Running task 199.0 in stage 10.0 (TID 1248)
15/08/21 23:14:35 INFO TaskSetManager: Finished task 187.0 in stage 10.0 (TID 1236) in 221 ms on localhost (184/200)
15/08/21 23:14:35 INFO TaskSetManager: Finished task 185.0 in stage 10.0 (TID 1234) in 937 ms on localhost (185/200)
15/08/21 23:14:35 INFO TaskSetManager: Finished task 189.0 in stage 10.0 (TID 1238) in 223 ms on localhost (186/200)
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,768
15/08/21 23:14:35 INFO TaskSetManager: Finished task 186.0 in stage 10.0 (TID 1235) in 228 ms on localhost (187/200)
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,076
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,474B for [ps_partkey] INT32: 390 values, 1,567B raw, 1,438B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,590B for [ps_partkey] INT32: 425 values, 1,707B raw, 1,554B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,731B for [value] DOUBLE: 390 values, 3,127B raw, 1,687B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,892B for [value] DOUBLE: 425 values, 3,407B raw, 1,848B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,440
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,720
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,750B for [ps_partkey] INT32: 473 values, 1,899B raw, 1,714B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,049B for [value] DOUBLE: 473 values, 3,791B raw, 2,005B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,700B for [ps_partkey] INT32: 459 values, 1,843B raw, 1,664B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,029B for [value] DOUBLE: 459 values, 3,679B raw, 1,985B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000193
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000193_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 193.0 in stage 10.0 (TID 1242). 843 bytes result sent to driver
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000194
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000194_0: Committed
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO TaskSetManager: Finished task 193.0 in stage 10.0 (TID 1242) in 216 ms on localhost (188/200)
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO Executor: Finished task 194.0 in stage 10.0 (TID 1243). 843 bytes result sent to driver
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,016
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000190
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000190_0: Committed
15/08/21 23:14:35 INFO TaskSetManager: Finished task 194.0 in stage 10.0 (TID 1243) in 217 ms on localhost (189/200)
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,797B for [ps_partkey] INT32: 487 values, 1,955B raw, 1,761B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,148B for [value] DOUBLE: 487 values, 3,903B raw, 2,104B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,466,248
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO Executor: Finished task 190.0 in stage 10.0 (TID 1239). 843 bytes result sent to driver
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000188
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000188_0: Committed
15/08/21 23:14:35 INFO TaskSetManager: Finished task 190.0 in stage 10.0 (TID 1239) in 238 ms on localhost (190/200)
15/08/21 23:14:35 INFO Executor: Finished task 188.0 in stage 10.0 (TID 1237). 843 bytes result sent to driver
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,673B for [ps_partkey] INT32: 449 values, 1,803B raw, 1,637B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,960B for [value] DOUBLE: 449 values, 3,599B raw, 1,916B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO TaskSetManager: Finished task 188.0 in stage 10.0 (TID 1237) in 288 ms on localhost (191/200)
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 23:14:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000195
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000195_0: Committed
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000191
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000191_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 195.0 in stage 10.0 (TID 1244). 843 bytes result sent to driver
15/08/21 23:14:35 INFO Executor: Finished task 191.0 in stage 10.0 (TID 1240). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Finished task 191.0 in stage 10.0 (TID 1240) in 273 ms on localhost (192/200)
15/08/21 23:14:35 INFO TaskSetManager: Finished task 195.0 in stage 10.0 (TID 1244) in 196 ms on localhost (193/200)
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,376
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,523B for [ps_partkey] INT32: 405 values, 1,627B raw, 1,487B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,793B for [value] DOUBLE: 405 values, 3,247B raw, 1,749B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 23:14:35 INFO CodecConfig: Compression: GZIP
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 23:14:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 23:14:35 INFO ParquetOutputFormat: Validation is off
15/08/21 23:14:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,465,368
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,468,896
15/08/21 23:14:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,527B for [ps_partkey] INT32: 405 values, 1,627B raw, 1,491B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,781B for [value] DOUBLE: 405 values, 3,247B raw, 1,737B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,133B for [ps_partkey] INT32: 581 values, 2,331B raw, 2,097B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,491B for [value] DOUBLE: 581 values, 4,655B raw, 2,447B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000196
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000196_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 196.0 in stage 10.0 (TID 1245). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Finished task 196.0 in stage 10.0 (TID 1245) in 208 ms on localhost (194/200)
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000198
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000198_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 198.0 in stage 10.0 (TID 1247). 843 bytes result sent to driver
15/08/21 23:14:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,467,168
15/08/21 23:14:35 INFO TaskSetManager: Finished task 198.0 in stage 10.0 (TID 1247) in 219 ms on localhost (195/200)
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 1,827B for [ps_partkey] INT32: 495 values, 1,987B raw, 1,791B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO ColumnChunkPageWriteStore: written 2,198B for [value] DOUBLE: 495 values, 3,967B raw, 2,154B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000199
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000199_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 199.0 in stage 10.0 (TID 1248). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Finished task 199.0 in stage 10.0 (TID 1248) in 203 ms on localhost (196/200)
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000178
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000178_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 178.0 in stage 10.0 (TID 1227). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Finished task 178.0 in stage 10.0 (TID 1227) in 1264 ms on localhost (197/200)
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000182
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000182_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 182.0 in stage 10.0 (TID 1231). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Finished task 182.0 in stage 10.0 (TID 1231) in 1325 ms on localhost (198/200)
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000192
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000192_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 192.0 in stage 10.0 (TID 1241). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Finished task 192.0 in stage 10.0 (TID 1241) in 617 ms on localhost (199/200)
15/08/21 23:14:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508212314_0010_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_temporary/0/task_201508212314_0010_m_000197
15/08/21 23:14:35 INFO SparkHadoopMapRedUtil: attempt_201508212314_0010_m_000197_0: Committed
15/08/21 23:14:35 INFO Executor: Finished task 197.0 in stage 10.0 (TID 1246). 843 bytes result sent to driver
15/08/21 23:14:35 INFO TaskSetManager: Finished task 197.0 in stage 10.0 (TID 1246) in 640 ms on localhost (200/200)
15/08/21 23:14:35 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/21 23:14:35 INFO DAGScheduler: ResultStage 10 (processCmd at CliDriver.java:423) finished in 5.426 s
15/08/21 23:14:35 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@45274dc
15/08/21 23:14:35 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 13.988534 s
15/08/21 23:14:35 INFO StatsReportListener: task runtime:(count: 200, mean: 403.625000, stdev: 249.523234, max: 1325.000000, min: 162.000000)
15/08/21 23:14:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:35 INFO StatsReportListener: 	162.0 ms	200.0 ms	209.0 ms	227.0 ms	289.0 ms	494.0 ms	897.0 ms	920.0 ms	1.3 s
15/08/21 23:14:35 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.250000, stdev: 0.870345, max: 11.000000, min: 0.000000)
15/08/21 23:14:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:35 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	11.0 ms
15/08/21 23:14:35 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 23:14:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:35 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 23:14:35 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 23:14:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:35 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 23:14:35 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 71.669904, stdev: 13.096855, max: 95.569620, min: 14.834578)
15/08/21 23:14:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:35 INFO StatsReportListener: 	15 %	55 %	57 %	64 %	71 %	81 %	89 %	92 %	96 %
15/08/21 23:14:35 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.085035, stdev: 0.366226, max: 4.824561, min: 0.000000)
15/08/21 23:14:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:35 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 5 %
15/08/21 23:14:35 INFO StatsReportListener: other time pct: (count: 200, mean: 28.245061, stdev: 13.074047, max: 85.165422, min: 4.351266)
15/08/21 23:14:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:35 INFO StatsReportListener: 	 4 %	 9 %	11 %	19 %	29 %	36 %	43 %	46 %	85 %
15/08/21 23:14:36 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 23:14:37 INFO DefaultWriterContainer: Job job_201508212314_0000 committed.
15/08/21 23:14:37 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 23:14:37 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par/_common_metadata
15/08/21 23:14:37 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 23:14:37 INFO DAGScheduler: Got job 5 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 23:14:37 INFO DAGScheduler: Final stage: ResultStage 11(processCmd at CliDriver.java:423)
15/08/21 23:14:37 INFO DAGScheduler: Parents of final stage: List()
15/08/21 23:14:37 INFO DAGScheduler: Missing parents: List()
15/08/21 23:14:37 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 23:14:37 INFO MemoryStore: ensureFreeSpace(2968) called with curMem=1865066, maxMem=22226833244
15/08/21 23:14:37 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 23:14:37 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=1868034, maxMem=22226833244
15/08/21 23:14:37 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 1776.0 B, free 20.7 GB)
15/08/21 23:14:37 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:40586 (size: 1776.0 B, free: 20.7 GB)
15/08/21 23:14:37 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:874
15/08/21 23:14:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at processCmd at CliDriver.java:423)
15/08/21 23:14:37 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
15/08/21 23:14:37 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 1249, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 23:14:37 INFO Executor: Running task 0.0 in stage 11.0 (TID 1249)
15/08/21 23:14:37 INFO Executor: Finished task 0.0 in stage 11.0 (TID 1249). 606 bytes result sent to driver
15/08/21 23:14:37 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 1249) in 13 ms on localhost (1/1)
15/08/21 23:14:37 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/21 23:14:37 INFO DAGScheduler: ResultStage 11 (processCmd at CliDriver.java:423) finished in 0.013 s
15/08/21 23:14:37 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2d1b95b5
15/08/21 23:14:37 INFO DAGScheduler: Job 5 finished: processCmd at CliDriver.java:423, took 0.027964 s
15/08/21 23:14:37 INFO StatsReportListener: task runtime:(count: 1, mean: 13.000000, stdev: 0.000000, max: 13.000000, min: 13.000000)
15/08/21 23:14:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:37 INFO StatsReportListener: 	13.0 ms	13.0 ms	13.0 ms	13.0 ms	13.0 ms	13.0 ms	13.0 ms	13.0 ms	13.0 ms
Time taken: 20.541 seconds
15/08/21 23:14:37 INFO CliDriver: Time taken: 20.541 seconds
15/08/21 23:14:37 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 23:14:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:37 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 23:14:37 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 23:14:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:37 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 23:14:37 INFO StatsReportListener: other time pct: (count: 1, mean: 100.000000, stdev: 0.000000, max: 100.000000, min: 100.000000)
15/08/21 23:14:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 23:14:37 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/21 23:14:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/21 23:14:37 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/21 23:14:37 INFO DAGScheduler: Stopping DAGScheduler
15/08/21 23:14:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/21 23:14:37 INFO Utils: path = /tmp/spark-69894f8f-1d49-46d4-a6ab-62fe441221ad/blockmgr-db311b79-dee5-46cd-aff8-06992e9dd85d, already present as root for deletion.
15/08/21 23:14:37 INFO MemoryStore: MemoryStore cleared
15/08/21 23:14:37 INFO BlockManager: BlockManager stopped
15/08/21 23:14:37 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/21 23:14:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/21 23:14:37 INFO SparkContext: Successfully stopped SparkContext
15/08/21 23:14:37 INFO Utils: Shutdown hook called
15/08/21 23:14:37 INFO Utils: Deleting directory /tmp/spark-69894f8f-1d49-46d4-a6ab-62fe441221ad
15/08/21 23:14:37 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/21 23:14:37 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/21 23:14:37 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/08/21 23:14:38 INFO Utils: Deleting directory /tmp/spark-85f57068-96fd-469c-97fe-4bf32c175514
15/08/21 23:14:38 INFO Utils: Deleting directory /tmp/spark-fdb962d5-1ddd-4670-b1b4-1859b7c66e0a
