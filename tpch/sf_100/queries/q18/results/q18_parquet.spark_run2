 -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey;

insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100;
15/08/21 11:04:29 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 11:04:29 INFO metastore: Connected to metastore.
15/08/21 11:04:30 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/21 11:04:30 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:30 INFO SparkContext: Running Spark version 1.4.1
15/08/21 11:04:31 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:31 INFO SecurityManager: Changing view acls to: hive
15/08/21 11:04:31 INFO SecurityManager: Changing modify acls to: hive
15/08/21 11:04:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/21 11:04:32 INFO Slf4jLogger: Slf4jLogger started
15/08/21 11:04:32 INFO Remoting: Starting remoting
15/08/21 11:04:32 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:41262]
15/08/21 11:04:32 INFO Utils: Successfully started service 'sparkDriver' on port 41262.
15/08/21 11:04:32 INFO SparkEnv: Registering MapOutputTracker
15/08/21 11:04:32 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:32 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:32 INFO SparkEnv: Registering BlockManagerMaster
15/08/21 11:04:32 INFO DiskBlockManager: Created local directory at /tmp/spark-8c304fc0-a8bf-4fd0-afbc-c3818c7aa7bf/blockmgr-344df924-f7f3-4e14-b551-a5542d88a00d
15/08/21 11:04:32 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/21 11:04:32 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:32 INFO HttpFileServer: HTTP File server directory is /tmp/spark-8c304fc0-a8bf-4fd0-afbc-c3818c7aa7bf/httpd-a7906236-a1cb-4d5a-9f50-4a884e3cf37b
15/08/21 11:04:32 INFO HttpServer: Starting HTTP Server
15/08/21 11:04:32 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 11:04:32 INFO AbstractConnector: Started SocketConnector@0.0.0.0:56065
15/08/21 11:04:32 INFO Utils: Successfully started service 'HTTP file server' on port 56065.
15/08/21 11:04:32 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/21 11:04:33 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 11:04:33 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/21 11:04:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/21 11:04:33 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/21 11:04:33 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:33 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:33 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:33 INFO Executor: Starting executor ID driver on host localhost
15/08/21 11:04:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53620.
15/08/21 11:04:33 INFO NettyBlockTransferService: Server created on 53620
15/08/21 11:04:33 INFO BlockManagerMaster: Trying to register BlockManager
15/08/21 11:04:33 INFO BlockManagerMasterEndpoint: Registering block manager localhost:53620 with 20.7 GB RAM, BlockManagerId(driver, localhost, 53620)
15/08/21 11:04:33 INFO BlockManagerMaster: Registered BlockManager
15/08/21 11:04:34 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 11:04:34 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/21 11:04:34 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/21 11:04:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/21 11:04:35 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 11:04:35 INFO metastore: Connected to metastore.
15/08/21 11:04:36 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/21 11:04:36 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/21 11:04:36 INFO ParseDriver: Parsing command: -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey
15/08/21 11:04:37 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/21 11:04:40 INFO MemoryStore: ensureFreeSpace(326528) called with curMem=0, maxMem=22226833244
15/08/21 11:04:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 11:04:41 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=326528, maxMem=22226833244
15/08/21 11:04:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 11:04:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:53620 (size: 22.3 KB, free: 20.7 GB)
15/08/21 11:04:41 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/21 11:04:41 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 11:04:41 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/21 11:04:41 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/21 11:04:41 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/21 11:04:41 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/21 11:04:41 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/21 11:04:41 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 11:04:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:04:41 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 11:04:41 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/21 11:04:41 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/21 11:04:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 11:04:42 INFO DAGScheduler: Registering RDD 3 (processCmd at CliDriver.java:423)
15/08/21 11:04:42 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 11:04:42 INFO DAGScheduler: Final stage: ResultStage 1(processCmd at CliDriver.java:423)
15/08/21 11:04:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/08/21 11:04:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/08/21 11:04:42 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:04:42 INFO MemoryStore: ensureFreeSpace(9208) called with curMem=349321, maxMem=22226833244
15/08/21 11:04:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 20.7 GB)
15/08/21 11:04:42 INFO MemoryStore: ensureFreeSpace(4552) called with curMem=358529, maxMem=22226833244
15/08/21 11:04:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 20.7 GB)
15/08/21 11:04:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:53620 (size: 4.4 KB, free: 20.7 GB)
15/08/21 11:04:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/08/21 11:04:42 INFO DAGScheduler: Submitting 170 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423)
15/08/21 11:04:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 170 tasks
15/08/21 11:04:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1758 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1770 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, ANY, 1757 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, ANY, 1769 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, ANY, 1757 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, ANY, 1771 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, ANY, 1758 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, ANY, 1773 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, ANY, 1758 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, ANY, 1770 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, localhost, ANY, 1757 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, localhost, ANY, 1767 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, localhost, ANY, 1757 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, localhost, ANY, 1772 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, localhost, ANY, 1758 bytes)
15/08/21 11:04:42 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, localhost, ANY, 1770 bytes)
15/08/21 11:04:43 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/08/21 11:04:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/21 11:04:43 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/08/21 11:04:43 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/08/21 11:04:43 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/08/21 11:04:43 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/08/21 11:04:43 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/08/21 11:04:43 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/21 11:04:43 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
15/08/21 11:04:43 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
15/08/21 11:04:43 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
15/08/21 11:04:43 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
15/08/21 11:04:43 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
15/08/21 11:04:43 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
15/08/21 11:04:43 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
15/08/21 11:04:43 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 134217728 end: 257463677 length: 123245949 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 134217728 end: 257568535 length: 123350807 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 134217728 end: 257329816 length: 123112088 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 134217728 end: 257425918 length: 123208190 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 134217728 end: 257477064 length: 123259336 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 134217728 end: 257420451 length: 123202723 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 259746640 length: 125528912 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 257171772 length: 122954044 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 11:04:43 INFO CodecPool: Got brand-new decompressor [.snappy]
21-Aug-2015 11:04:38 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 11:04:38 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 11:04:38 INFO: parquet.hadoop.ParquetFileReader: reading another 85 footers
21-Aug-2015 11:04:38 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573892 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572933 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501583 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572784 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501187 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572843 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573983 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574066 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573045 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501150 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627997 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 243 ms. row count = 3501150
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 247 ms. row count = 3500100
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 249 ms. row count = 3500100
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 249 ms. row count = 3500780
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 257 ms. row count = 15/08/21 11:05:15 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2125 bytes result sent to driver
15/08/21 11:05:15 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, localhost, ANY, 1755 bytes)
15/08/21 11:05:15 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
15/08/21 11:05:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:15 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 32878 ms on localhost (1/170)
15/08/21 11:05:15 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2125 bytes result sent to driver
15/08/21 11:05:15 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, localhost, ANY, 1770 bytes)
15/08/21 11:05:15 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
15/08/21 11:05:15 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 33182 ms on localhost (2/170)
15/08/21 11:05:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 261799262 length: 127581534 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:16 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2125 bytes result sent to driver
15/08/21 11:05:16 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, localhost, ANY, 1758 bytes)
15/08/21 11:05:16 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
15/08/21 11:05:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:16 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 33980 ms on localhost (3/170)
15/08/21 11:05:17 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2125 bytes result sent to driver
15/08/21 11:05:17 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, localhost, ANY, 1769 bytes)
15/08/21 11:05:17 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
15/08/21 11:05:17 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 34463 ms on localhost (4/170)
15/08/21 11:05:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 134217728 end: 257435404 length: 123217676 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2125 bytes result sent to driver
15/08/21 11:05:18 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, localhost, ANY, 1758 bytes)
15/08/21 11:05:18 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
15/08/21 11:05:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 35787 ms on localhost (5/170)
15/08/21 11:05:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:19 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 2125 bytes result sent to driver
15/08/21 11:05:19 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, localhost, ANY, 1772 bytes)
15/08/21 11:05:19 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
15/08/21 11:05:19 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 36401 ms on localhost (6/170)
15/08/21 11:05:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 134217728 end: 257347114 length: 123129386 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
3501191
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 257 ms. row count = 3501187
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 258 ms. row count = 3500100
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 258 ms. row count = 3500939
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 258 ms. row count = 3500100
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 264 ms. row count = 3501583
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 264 ms. row count = 3500100
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 264 ms. row count = 3500100
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 265 ms. row count = 3502678
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 265 ms. row count = 3503008
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 266 ms. row count = 3500100
21-Aug-2015 11:04:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 265 ms. row count = 3500100
21-Aug-2015 11:05:06 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502678 records from 2 columns in 22268 ms: 157.29648 rec/ms, 314.59296 cell/ms
21-Aug-2015 11:05:06 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 1% reading (265 ms) and 98% processing (22268 ms)
21-Aug-2015 11:05:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502678. reading next block
21-Aug-2015 11:05:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 70255
21-Aug-2015 11:05:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501183 records.
21-Aug-2015 11:05:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 44 ms. row count = 3501183
21-Aug-2015 11:05:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3689969 records.
21-Aug-2015 11:05:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 3501351
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32260 ms: 108.49659 rec/ms, 216.99318 cell/ms
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (266 ms) and 99% processing (32260 ms)
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 72684
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32356 ms: 108.17468 rec/ms, 216.34937 cell/ms
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (258 ms) and 99% processing (32356 ms)
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 73883
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32512 ms: 107.65563 rec/ms, 215.31126 cell/ms
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (265 ms) and 99% processing (32512 ms)
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 127897
21-Aug-2015 11:05:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 34 ms. row count = 3500100
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500939 records from 2 columns in 32978 ms: 106.159836 rec/ms, 212.31967 cell/ms
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (258 ms) and 99% processing (32978 ms)
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500939. reading next block
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 71904
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 33044 ms: 106.010414 rec/ms, 212.02083 cell/ms
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (265 ms) and 99% processing (33044 ms)
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503008. reading next block
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 70884
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500780 records from 2 columns in 33104 ms: 105.75097 rec/ms, 211.50194 cell/ms
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (249 ms) and 99% processing (33104 ms)
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500780. reading next block
21-Aug-2015 11:05:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73286
21-Aug-2015 11:05:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573923 records.
21-Aug-2015 11:05:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 40 ms. row count = 3501239
21-Aug-2015 11:05:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:05:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
21-Aug-2015 11:05:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.T15/08/21 11:05:19 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2125 bytes result sent to driver
15/08/21 11:05:19 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, localhost, ANY, 1758 bytes)
15/08/21 11:05:19 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
15/08/21 11:05:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:19 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 36648 ms on localhost (7/170)
15/08/21 11:05:19 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 2125 bytes result sent to driver
15/08/21 11:05:19 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, localhost, ANY, 1769 bytes)
15/08/21 11:05:19 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
15/08/21 11:05:19 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 36795 ms on localhost (8/170)
15/08/21 11:05:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 134217728 end: 257844810 length: 123627082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:19 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2125 bytes result sent to driver
15/08/21 11:05:19 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24, localhost, ANY, 1758 bytes)
15/08/21 11:05:19 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
15/08/21 11:05:19 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 37042 ms on localhost (9/170)
15/08/21 11:05:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:26 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 2125 bytes result sent to driver
15/08/21 11:05:26 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25, localhost, ANY, 1768 bytes)
15/08/21 11:05:26 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
15/08/21 11:05:26 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 43722 ms on localhost (10/170)
15/08/21 11:05:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 257827380 length: 123609652 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:26 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2125 bytes result sent to driver
15/08/21 11:05:26 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26, localhost, ANY, 1758 bytes)
15/08/21 11:05:26 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
15/08/21 11:05:26 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 43774 ms on localhost (11/170)
15/08/21 11:05:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:26 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2125 bytes result sent to driver
15/08/21 11:05:26 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27, localhost, ANY, 1770 bytes)
15/08/21 11:05:26 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
15/08/21 11:05:26 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 43857 ms on localhost (12/170)
15/08/21 11:05:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 134217728 end: 257458294 length: 123240566 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:26 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 2125 bytes result sent to driver
15/08/21 11:05:26 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28, localhost, ANY, 1758 bytes)
15/08/21 11:05:26 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
15/08/21 11:05:26 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 43936 ms on localhost (13/170)
15/08/21 11:05:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:26 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 2125 bytes result sent to driver
15/08/21 11:05:26 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29, localhost, ANY, 1770 bytes)
15/08/21 11:05:26 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
15/08/21 11:05:26 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 44048 ms on localhost (14/170)
15/08/21 11:05:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 134217728 end: 257369456 length: 123151728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:27 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 2125 bytes result sent to driver
15/08/21 11:05:27 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30, localhost, ANY, 1758 bytes)
15/08/21 11:05:27 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
15/08/21 11:05:27 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 45104 ms on localhost (15/170)
15/08/21 11:05:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:43 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 2125 bytes result sent to driver
15/08/21 11:05:43 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31, localhost, ANY, 1773 bytes)
15/08/21 11:05:43 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
15/08/21 11:05:43 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 27657 ms on localhost (16/170)
15/08/21 11:05:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 257340601 length: 123122873 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:43 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2125 bytes result sent to driver
15/08/21 11:05:43 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32, localhost, ANY, 1758 bytes)
15/08/21 11:05:43 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
15/08/21 11:05:43 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 61057 ms on localhost (17/170)
15/08/21 11:05:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
askAttemptContextImpl
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
21-Aug-2015 11:05:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501423 records.
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3501423
21-Aug-2015 11:05:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573035 records.
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 52 ms. row count = 3500100
21-Aug-2015 11:05:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:05:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 42181 ms: 82.97812 rec/ms, 165.95624 cell/ms
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (264 ms) and 99% processing (42181 ms)
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 72945
21-Aug-2015 11:05:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573921 records.
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501667 records.
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3501305
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3501667
21-Aug-2015 11:05:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572747 records.
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 74 ms. row count = 3500100
21-Aug-2015 11:05:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 48 ms. row count = 3500100
21-Aug-2015 11:05:27 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501351 records from 2 columns in 11655 ms: 300.41623 rec/ms, 600.83246 cell/ms
21-Aug-2015 11:05:27 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (35 ms) and 99% processing (11655 ms)
21-Aug-2015 11:05:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501351. reading next block
21-Aug-2015 11:05:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 188618
21-Aug-2015 11:05:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:05:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 84 ms. row count = 3500100
21-Aug-2015 11:05:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574247 records.
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 127 ms. row count = 3500100
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501239 records from 2 columns in 26257 ms: 133.34497 rec/ms, 266.68994 cell/ms
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (40 ms) and 99% processing (26257 ms)
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501239. reading next block
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 72684
21-Aug-2015 11:05:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:05:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:43 INFO: 15/08/21 11:05:48 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 2125 bytes result sent to driver
15/08/21 11:05:48 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33, localhost, ANY, 1768 bytes)
15/08/21 11:05:48 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
15/08/21 11:05:48 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 32256 ms on localhost (18/170)
15/08/21 11:05:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 259459156 length: 125241428 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:53 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 2125 bytes result sent to driver
15/08/21 11:05:53 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34, localhost, ANY, 1758 bytes)
15/08/21 11:05:53 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
15/08/21 11:05:53 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 36760 ms on localhost (19/170)
15/08/21 11:05:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:54 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 2125 bytes result sent to driver
15/08/21 11:05:54 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35, localhost, ANY, 1769 bytes)
15/08/21 11:05:54 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
15/08/21 11:05:54 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 37347 ms on localhost (20/170)
15/08/21 11:05:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 134217728 end: 257580347 length: 123362619 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:05:59 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 2125 bytes result sent to driver
15/08/21 11:05:59 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36, localhost, ANY, 1757 bytes)
15/08/21 11:05:59 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
15/08/21 11:05:59 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 41455 ms on localhost (21/170)
15/08/21 11:05:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:00 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 2125 bytes result sent to driver
15/08/21 11:06:00 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37, localhost, ANY, 1770 bytes)
15/08/21 11:06:00 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
15/08/21 11:06:00 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 41101 ms on localhost (22/170)
15/08/21 11:06:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 257857124 length: 123639396 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:21 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 2125 bytes result sent to driver
15/08/21 11:06:21 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38, localhost, ANY, 1757 bytes)
15/08/21 11:06:21 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
15/08/21 11:06:21 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 62033 ms on localhost (23/170)
15/08/21 11:06:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:21 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 2125 bytes result sent to driver
15/08/21 11:06:21 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, localhost, ANY, 1770 bytes)
15/08/21 11:06:21 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
15/08/21 11:06:21 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 62525 ms on localhost (24/170)
15/08/21 11:06:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 259199417 length: 124981689 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:22 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 2125 bytes result sent to driver
15/08/21 11:06:22 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, ANY, 1758 bytes)
15/08/21 11:06:22 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
15/08/21 11:06:22 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 55675 ms on localhost (25/170)
15/08/21 11:06:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:22 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 2125 bytes result sent to driver
15/08/21 11:06:22 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41, localhost, ANY, 1769 bytes)
15/08/21 11:06:22 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
15/08/21 11:06:22 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 62782 ms on localhost (26/170)
15/08/21 11:06:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 257564901 length: 123347173 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:22 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 2125 bytes result sent to driver
15/08/21 11:06:22 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42, localhost, ANY, 1757 bytes)
15/08/21 11:06:22 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
15/08/21 11:06:22 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 55886 ms on localhost (27/170)
15/08/21 11:06:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
parquet.hadoop.InternalParquetRecordReader: block read in memory in 172 ms. row count = 3500100
21-Aug-2015 11:05:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627517 records.
21-Aug-2015 11:05:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500968
21-Aug-2015 11:05:48 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29357 ms: 119.225395 rec/ms, 238.45079 cell/ms
21-Aug-2015 11:05:48 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (63 ms) and 99% processing (29357 ms)
21-Aug-2015 11:05:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 74238
21-Aug-2015 11:05:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501180 records.
21-Aug-2015 11:05:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 107 ms. row count = 3501180
21-Aug-2015 11:05:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501305 records from 2 columns in 27417 ms: 127.70562 rec/ms, 255.41124 cell/ms
21-Aug-2015 11:05:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (60 ms) and 99% processing (27417 ms)
21-Aug-2015 11:05:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501305. reading next block
21-Aug-2015 11:05:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 72616
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 34473 ms: 101.53163 rec/ms, 203.06326 cell/ms
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (52 ms) and 99% processing (34473 ms)
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 72935
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 27432 ms: 127.591866 rec/ms, 255.18373 cell/ms
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (63 ms) and 99% processing (27432 ms)
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 72647
21-Aug-2015 11:05:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573027 records.
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500932
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 27738 ms: 126.184296 rec/ms, 252.36859 cell/ms
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (48 ms) and 99% processing (27738 ms)
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:05:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 74054
21-Aug-2015 11:05:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:05:59 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:05:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:05:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 47 ms. row count = 3500100
21-Aug-2015 11:06:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574216 records.
21-Aug-2015 11:06:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 40 ms. row count = 3501165
21-Aug-2015 11:06:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
21-Aug-2015 11:06:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 102 ms. row count = 3501121
21-Aug-2015 11:06:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626725 records.
21-Aug-2015 11:06:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 66 ms. row count = 3502670
21-Aug-2015 11:06:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501407 records.
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 57 ms. row count = 3501407
21-Aug-2015 11:06:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572791 records.
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 65 ms. row count = 3500841
21-Aug-2015 11:06:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:22 INFO: parquet.15/08/21 11:06:22 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 2125 bytes result sent to driver
15/08/21 11:06:22 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43, localhost, ANY, 1769 bytes)
15/08/21 11:06:22 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
15/08/21 11:06:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 257460000 length: 123242272 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:22 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 55778 ms on localhost (28/170)
15/08/21 11:06:22 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 2125 bytes result sent to driver
15/08/21 11:06:22 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44, localhost, ANY, 1758 bytes)
15/08/21 11:06:22 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
15/08/21 11:06:22 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 56411 ms on localhost (29/170)
15/08/21 11:06:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:27 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 2125 bytes result sent to driver
15/08/21 11:06:27 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 2125 bytes result sent to driver
15/08/21 11:06:27 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45, localhost, ANY, 1768 bytes)
15/08/21 11:06:27 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
15/08/21 11:06:27 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46, localhost, ANY, 1758 bytes)
15/08/21 11:06:27 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
15/08/21 11:06:27 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 61230 ms on localhost (30/170)
15/08/21 11:06:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:27 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 60070 ms on localhost (31/170)
15/08/21 11:06:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 134217728 end: 257842743 length: 123625015 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:32 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 2125 bytes result sent to driver
15/08/21 11:06:32 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47, localhost, ANY, 1770 bytes)
15/08/21 11:06:32 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
15/08/21 11:06:32 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 48940 ms on localhost (32/170)
15/08/21 11:06:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 134217728 end: 257576472 length: 123358744 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:32 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 2125 bytes result sent to driver
15/08/21 11:06:35 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48, localhost, ANY, 1758 bytes)
15/08/21 11:06:35 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
15/08/21 11:06:35 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 51867 ms on localhost (33/170)
15/08/21 11:06:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:36 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 2125 bytes result sent to driver
15/08/21 11:06:36 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49, localhost, ANY, 1770 bytes)
15/08/21 11:06:36 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
15/08/21 11:06:36 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 48156 ms on localhost (34/170)
15/08/21 11:06:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 134217728 end: 256733155 length: 122515427 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:36 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 2125 bytes result sent to driver
15/08/21 11:06:36 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50, localhost, ANY, 1757 bytes)
15/08/21 11:06:36 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
15/08/21 11:06:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:36 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 43029 ms on localhost (35/170)
hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501143 records.
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573047 records.
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 136 ms. row count = 3501143
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 81 ms. row count = 3500100
21-Aug-2015 11:06:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501260 records.
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 103 ms. row count = 3501260
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 39611 ms: 88.36182 rec/ms, 176.72363 cell/ms
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (127 ms) and 99% processing (39611 ms)
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:06:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 74147
21-Aug-2015 11:06:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500968 records from 2 columns in 37171 ms: 94.18547 rec/ms, 188.37094 cell/ms
21-Aug-2015 11:06:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (39 ms) and 99% processing (37171 ms)
21-Aug-2015 11:06:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500968. reading next block
21-Aug-2015 11:06:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2209 ms. row count = 126549
21-Aug-2015 11:06:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501217 records.
21-Aug-2015 11:06:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573207 records.
21-Aug-2015 11:06:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 3500100
21-Aug-2015 11:06:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 56 ms. row count = 3501217
21-Aug-2015 11:06:32 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:32 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573258 records.
21-Aug-2015 11:06:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 165 ms. row count = 3501508
21-Aug-2015 11:06:32 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500932 records from 2 columns in 37835 ms: 92.53157 rec/ms, 185.06314 cell/ms
21-Aug-2015 11:06:32 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (69 ms) and 99% processing (37835 ms)
21-Aug-2015 11:06:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500932. reading next block
21-Aug-2015 11:06:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 72095
21-Aug-2015 11:06:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503050 records.
21-Aug-2015 11:06:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 52 ms. row count = 3503050
21-Aug-2015 11:06:36 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3570986 records.
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 75 ms. row count = 3503008
21-Aug-2015 11:06:36 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501200 records.
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 73 ms. row count = 3501200
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501165 records from 2 columns in 36234 ms: 96.62651 rec/ms, 193.25302 cell/ms
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (40 ms) and 99% processing (36234 ms)
21-Aug-2015 11:06:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501165. reading next block
21-Aug-2015 11:06:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2776 ms. row count = 73051
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502670 records from 2 columns in 18155 ms: 192.93143 rec/ms, 385.86285 cell/ms
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (66 ms) and 99% processing (18155 ms)
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502670. reading next block
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 124055
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500841 records from 2 columns in 18083 ms: 193.59846 rec/ms, 387.19693 cell/ms
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (65 ms) and 99% processing (18083 ms)
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500841. reading next block
21-Aug-2015 11:06:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 71950
21-Aug-2015 11:06:43 INFO: parquet.hadoop.InternalParquetRe15/08/21 11:06:43 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 2125 bytes result sent to driver
15/08/21 11:06:43 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51, localhost, ANY, 1768 bytes)
15/08/21 11:06:43 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
15/08/21 11:06:43 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 49419 ms on localhost (36/170)
15/08/21 11:06:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 134217728 end: 257568635 length: 123350907 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:44 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 2125 bytes result sent to driver
15/08/21 11:06:44 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52, localhost, ANY, 1757 bytes)
15/08/21 11:06:44 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 44498 ms on localhost (37/170)
15/08/21 11:06:44 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
15/08/21 11:06:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:44 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 2125 bytes result sent to driver
15/08/21 11:06:44 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53, localhost, ANY, 1768 bytes)
15/08/21 11:06:44 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
15/08/21 11:06:44 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 44505 ms on localhost (38/170)
15/08/21 11:06:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 257584480 length: 123366752 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:47 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 2125 bytes result sent to driver
15/08/21 11:06:47 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54, localhost, ANY, 1757 bytes)
15/08/21 11:06:47 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
15/08/21 11:06:47 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 26087 ms on localhost (39/170)
15/08/21 11:06:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:47 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 2125 bytes result sent to driver
15/08/21 11:06:47 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55, localhost, ANY, 1769 bytes)
15/08/21 11:06:47 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
15/08/21 11:06:48 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 25981 ms on localhost (40/170)
15/08/21 11:06:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 259050527 length: 124832799 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:48 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 2125 bytes result sent to driver
15/08/21 11:06:48 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56, localhost, ANY, 1758 bytes)
15/08/21 11:06:48 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
15/08/21 11:06:48 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 25831 ms on localhost (41/170)
15/08/21 11:06:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:48 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 2125 bytes result sent to driver
15/08/21 11:06:48 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57, localhost, ANY, 1772 bytes)
15/08/21 11:06:48 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
15/08/21 11:06:48 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 26535 ms on localhost (42/170)
15/08/21 11:06:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 257461890 length: 123244162 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:48 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 2125 bytes result sent to driver
15/08/21 11:06:48 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58, localhost, ANY, 1758 bytes)
15/08/21 11:06:48 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
15/08/21 11:06:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:48 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 25930 ms on localhost (43/170)
15/08/21 11:06:48 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 2125 bytes result sent to driver
15/08/21 11:06:48 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59, localhost, ANY, 1773 bytes)
15/08/21 11:06:48 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
15/08/21 11:06:48 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 26631 ms on localhost (44/170)
15/08/21 11:06:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 134217728 end: 257416343 length: 123198615 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:51 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 2125 bytes result sent to driver
15/08/21 11:06:51 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60, localhost, ANY, 1758 bytes)
15/08/21 11:06:51 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
15/08/21 11:06:51 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 29165 ms on localhost (45/170)
15/08/21 11:06:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:06:52 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 2125 bytes result sent to driver
15/08/21 11:06:52 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61, localhost, ANY, 1770 bytes)
15/08/21 11:06:52 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
15/08/21 11:06:52 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 24219 ms on localhost (46/170)
15/08/21 11:06:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 257883468 length: 123665740 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
cordReader: Assembled and processed 3500100 records from 2 columns in 21169 ms: 165.34084 rec/ms, 330.68167 cell/ms
21-Aug-2015 11:06:43 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (81 ms) and 99% processing (21169 ms)
21-Aug-2015 11:06:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:06:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 72947
21-Aug-2015 11:06:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573168 records.
21-Aug-2015 11:06:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 66 ms. row count = 3501341
21-Aug-2015 11:06:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500823 records.
21-Aug-2015 11:06:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 48 ms. row count = 3500823
21-Aug-2015 11:06:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573143 records.
21-Aug-2015 11:06:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 90 ms. row count = 3501035
21-Aug-2015 11:06:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503144 records.
21-Aug-2015 11:06:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 95 ms. row count = 3503144
21-Aug-2015 11:06:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3624571 records.
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 20137 ms: 173.81438 rec/ms, 347.62875 cell/ms
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (54 ms) and 99% processing (20137 ms)
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 73107
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500100
21-Aug-2015 11:06:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 72 ms. row count = 3500100
21-Aug-2015 11:06:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573988 records.
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 73 ms. row count = 3500824
21-Aug-2015 11:06:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 3500100
21-Aug-2015 11:06:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574223 records.
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 82 ms. row count = 3501614
21-Aug-2015 11:06:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
21-Aug-2015 11:06:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501191
21-Aug-2015 11:06:51 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501508 records from 2 columns in 19477 ms: 179.77655 rec/ms, 359.5531 cell/ms
21-Aug-2015 11:06:51 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (165 ms) and 99% processing (19477 ms)
21-Aug-2015 11:06:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501508. reading next block
21-Aug-2015 11:06:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 71750
21-Aug-2015 11:06:52 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:52 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573362 records.
21-Aug-2015 11:06:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 30 ms. row count = 3500100
21-Aug-2015 11:06:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 18606 ms: 188.27303 rec/ms, 376.54605 cell/ms
21-Aug-2015 11:06:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (75 ms) and 99% processing (18606 ms)
21-Aug-2015 11:06:54 INFO: parqu15/08/21 11:06:55 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 2125 bytes result sent to driver
15/08/21 11:06:55 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62, localhost, ANY, 1757 bytes)
15/08/21 11:06:55 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
15/08/21 11:06:55 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 27566 ms on localhost (47/170)
15/08/21 11:06:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:11 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 2125 bytes result sent to driver
15/08/21 11:07:11 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63, localhost, ANY, 1772 bytes)
15/08/21 11:07:11 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
15/08/21 11:07:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 134217728 end: 257592803 length: 123375075 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:11 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 38650 ms on localhost (48/170)
15/08/21 11:07:14 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 2125 bytes result sent to driver
15/08/21 11:07:14 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64, localhost, ANY, 1758 bytes)
15/08/21 11:07:14 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
15/08/21 11:07:14 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 42108 ms on localhost (49/170)
15/08/21 11:07:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:14 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 2125 bytes result sent to driver
15/08/21 11:07:14 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65, localhost, ANY, 1770 bytes)
15/08/21 11:07:14 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
15/08/21 11:07:14 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 38302 ms on localhost (50/170)
15/08/21 11:07:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258145407 length: 123927679 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:18 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 2125 bytes result sent to driver
15/08/21 11:07:18 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66, localhost, ANY, 1757 bytes)
15/08/21 11:07:18 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
15/08/21 11:07:18 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 42329 ms on localhost (51/170)
15/08/21 11:07:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:22 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 2125 bytes result sent to driver
15/08/21 11:07:22 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67, localhost, ANY, 1769 bytes)
15/08/21 11:07:22 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
15/08/21 11:07:22 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 38235 ms on localhost (52/170)
15/08/21 11:07:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259344407 length: 125126679 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:22 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 2125 bytes result sent to driver
15/08/21 11:07:22 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68, localhost, ANY, 1758 bytes)
15/08/21 11:07:22 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
15/08/21 11:07:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:22 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 38003 ms on localhost (53/170)
15/08/21 11:07:25 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 2125 bytes result sent to driver
15/08/21 11:07:25 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69, localhost, ANY, 1770 bytes)
15/08/21 11:07:25 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
15/08/21 11:07:25 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 37708 ms on localhost (54/170)
15/08/21 11:07:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 134217728 end: 257768338 length: 123550610 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:25 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 2125 bytes result sent to driver
15/08/21 11:07:25 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70, localhost, ANY, 1758 bytes)
15/08/21 11:07:25 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
15/08/21 11:07:25 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 41312 ms on localhost (55/170)
15/08/21 11:07:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:25 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 2125 bytes result sent to driver
15/08/21 11:07:25 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71, localhost, ANY, 1771 bytes)
15/08/21 11:07:25 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
et.hadoop.InternalParquetRecordReader: at row 3503008. reading next block
21-Aug-2015 11:06:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 67978
21-Aug-2015 11:06:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:06:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501059 records.
21-Aug-2015 11:06:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:06:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 79 ms. row count = 3501059
21-Aug-2015 11:07:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573291 records.
21-Aug-2015 11:07:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 88 ms. row count = 3501221
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501341 records from 2 columns in 30047 ms: 116.52881 rec/ms, 233.05762 cell/ms
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (66 ms) and 99% processing (30047 ms)
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501341. reading next block
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 71827
21-Aug-2015 11:07:14 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 97 ms. row count = 3500100
21-Aug-2015 11:07:14 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574053 records.
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501035 records from 2 columns in 33428 ms: 104.733604 rec/ms, 209.46721 cell/ms
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (90 ms) and 99% processing (33428 ms)
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501035. reading next block
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 72108
21-Aug-2015 11:07:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 72 ms. row count = 3500100
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500824 records from 2 columns in 30388 ms: 115.20416 rec/ms, 230.40833 cell/ms
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (73 ms) and 99% processing (30388 ms)
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500824. reading next block
21-Aug-2015 11:07:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 73164
21-Aug-2015 11:07:21 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501614 records from 2 columns in 30202 ms: 115.939804 rec/ms, 231.87961 cell/ms
21-Aug-2015 11:07:21 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (82 ms) and 99% processing (30202 ms)
21-Aug-2015 11:07:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501614. reading next block
21-Aug-2015 11:07:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 72609
21-Aug-2015 11:07:21 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 33829 ms: 103.464485 rec/ms, 206.92897 cell/ms
21-Aug-2015 11:07:21 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (91 ms) and 99% processing (33829 ms)
21-Aug-2015 11:07:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:07:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 124471
21-Aug-2015 11:07:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627630 records.
21-Aug-2015 11:07:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 40 ms. row count = 3500100
21-Aug-2015 11:07:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 3500100
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 33531 ms: 104.384 rec/ms, 208.768 cell/ms
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (30 ms) and 99% processing (33531 ms)
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 73262
21-Aug-2015 11:07:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574075 records.
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will 15/08/21 11:07:25 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 38243 ms on localhost (56/170)
15/08/21 11:07:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 134217728 end: 257102581 length: 122884853 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:26 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 2125 bytes result sent to driver
15/08/21 11:07:26 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72, localhost, ANY, 1758 bytes)
15/08/21 11:07:26 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
15/08/21 11:07:26 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 37784 ms on localhost (57/170)
15/08/21 11:07:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:26 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 2125 bytes result sent to driver
15/08/21 11:07:26 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73, localhost, ANY, 1772 bytes)
15/08/21 11:07:26 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
15/08/21 11:07:26 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 37647 ms on localhost (58/170)
15/08/21 11:07:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 134217728 end: 257873629 length: 123655901 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:29 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 2125 bytes result sent to driver
15/08/21 11:07:29 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74, localhost, ANY, 1757 bytes)
15/08/21 11:07:29 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
15/08/21 11:07:29 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 37732 ms on localhost (59/170)
15/08/21 11:07:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:29 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 2125 bytes result sent to driver
15/08/21 11:07:29 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75, localhost, ANY, 1767 bytes)
15/08/21 11:07:29 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
15/08/21 11:07:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 259470450 length: 125252722 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:29 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 40549 ms on localhost (60/170)
15/08/21 11:07:29 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 2125 bytes result sent to driver
15/08/21 11:07:29 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76, localhost, ANY, 1757 bytes)
15/08/21 11:07:29 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 41688 ms on localhost (61/170)
15/08/21 11:07:29 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
15/08/21 11:07:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:33 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 2125 bytes result sent to driver
15/08/21 11:07:33 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77, localhost, ANY, 1771 bytes)
15/08/21 11:07:33 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
15/08/21 11:07:33 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 41409 ms on localhost (62/170)
15/08/21 11:07:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 259842205 length: 125624477 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:33 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 2125 bytes result sent to driver
15/08/21 11:07:33 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78, localhost, ANY, 1758 bytes)
15/08/21 11:07:33 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
15/08/21 11:07:33 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 38206 ms on localhost (63/170)
15/08/21 11:07:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:40 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 2125 bytes result sent to driver
15/08/21 11:07:40 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79, localhost, ANY, 1773 bytes)
15/08/21 11:07:40 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
15/08/21 11:07:40 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 28790 ms on localhost (64/170)
15/08/21 11:07:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 257071082 length: 122853354 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:40 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 2125 bytes result sent to driver
15/08/21 11:07:40 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80, localhost, ANY, 1758 bytes)
15/08/21 11:07:40 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
15/08/21 11:07:40 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 26239 ms on localhost (65/170)
15/08/21 11:07:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:40 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 2125 bytes result sent to driver
15/08/21 11:07:40 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81, localhost, ANY, 1770 bytes)
15/08/21 11:07:40 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
15/08/21 11:07:40 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 25988 ms on localhost (66/170)
15/08/21 11:07:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 134217728 end: 257075001 length: 122857273 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
read a total of 3502727 records.
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 64 ms. row count = 3500100
21-Aug-2015 11:07:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571405 records.
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3502727
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3500100
21-Aug-2015 11:07:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501229 records.
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 43 ms. row count = 3501229
21-Aug-2015 11:07:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572739 records.
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 3500100
21-Aug-2015 11:07:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627768 records.
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3500100
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501221 records from 2 columns in 17862 ms: 196.01506 rec/ms, 392.03012 cell/ms
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (88 ms) and 99% processing (17862 ms)
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501221. reading next block
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 72070
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 115 ms. row count = 3501462
21-Aug-2015 11:07:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 80 ms. row count = 3500100
21-Aug-2015 11:07:32 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18189 ms: 192.42949 rec/ms, 384.85898 cell/ms
21-Aug-2015 11:07:32 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (50 ms) and 99% processing (18189 ms)
21-Aug-2015 11:07:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:07:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 73953
21-Aug-2015 11:07:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627712 records.
21-Aug-2015 11:07:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 3501302
21-Aug-2015 11:07:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503221 records.
21-Aug-2015 11:07:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 59 ms. row count = 3503221
21-Aug-2015 11:07:37 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 14953 ms: 234.07343 rec/ms, 468.14685 cell/ms
21-Aug-2015 11:07:37 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (40 ms) and 99% processing (14953 ms)
21-Aug-2015 11:07:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:07:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 127530
21-Aug-2015 11:07:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572089 records.
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 67 ms. row count = 3500100
21-Aug-2015 11:07:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574581 records.
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 90 ms. row count = 3500100
21-Aug-2015 11:07:40 INFO: p15/08/21 11:07:44 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 2125 bytes result sent to driver
15/08/21 11:07:44 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82, localhost, ANY, 1758 bytes)
15/08/21 11:07:44 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
15/08/21 11:07:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:44 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 25720 ms on localhost (67/170)
15/08/21 11:07:48 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 2125 bytes result sent to driver
15/08/21 11:07:48 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83, localhost, ANY, 1770 bytes)
15/08/21 11:07:48 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 26264 ms on localhost (68/170)
15/08/21 11:07:48 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
15/08/21 11:07:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 134217728 end: 257427527 length: 123209799 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:07:48 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 2125 bytes result sent to driver
15/08/21 11:07:48 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84, localhost, ANY, 1757 bytes)
15/08/21 11:07:48 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
15/08/21 11:07:48 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 26440 ms on localhost (69/170)
15/08/21 11:07:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:07 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 2125 bytes result sent to driver
15/08/21 11:08:07 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85, localhost, ANY, 1771 bytes)
15/08/21 11:08:07 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
15/08/21 11:08:07 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 41878 ms on localhost (70/170)
15/08/21 11:08:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 257349334 length: 123131606 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:08 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 2125 bytes result sent to driver
15/08/21 11:08:08 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86, localhost, ANY, 1757 bytes)
15/08/21 11:08:08 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
15/08/21 11:08:08 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 42197 ms on localhost (71/170)
15/08/21 11:08:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:08 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 2125 bytes result sent to driver
15/08/21 11:08:08 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87, localhost, ANY, 1770 bytes)
15/08/21 11:08:08 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
15/08/21 11:08:08 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 42444 ms on localhost (72/170)
15/08/21 11:08:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 257709471 length: 123491743 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:09 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 2125 bytes result sent to driver
15/08/21 11:08:09 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88, localhost, ANY, 1757 bytes)
15/08/21 11:08:09 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
15/08/21 11:08:09 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 43425 ms on localhost (73/170)
15/08/21 11:08:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:09 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 2125 bytes result sent to driver
15/08/21 11:08:09 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89, localhost, ANY, 1769 bytes)
15/08/21 11:08:09 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
15/08/21 11:08:09 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 43035 ms on localhost (74/170)
15/08/21 11:08:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 259741355 length: 125523627 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:09 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 2125 bytes result sent to driver
15/08/21 11:08:09 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90, localhost, ANY, 1758 bytes)
15/08/21 11:08:09 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
15/08/21 11:08:09 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 40211 ms on localhost (75/170)
15/08/21 11:08:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:09 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 2125 bytes result sent to driver
15/08/21 11:08:09 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91, localhost, ANY, 1773 bytes)
15/08/21 11:08:09 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
15/08/21 11:08:09 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 40156 ms on localhost (76/170)
15/08/21 11:08:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 134217728 end: 257333395 length: 123115667 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
arquet.hadoop.InternalParquetRecordReader: block read in memory in 104 ms. row count = 3503231
21-Aug-2015 11:07:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:44 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18375 ms: 190.48163 rec/ms, 380.96326 cell/ms
21-Aug-2015 11:07:44 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (46 ms) and 99% processing (18375 ms)
21-Aug-2015 11:07:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:07:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 71305
21-Aug-2015 11:07:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500100
21-Aug-2015 11:07:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574316 records.
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22372 ms: 156.45003 rec/ms, 312.90005 cell/ms
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (64 ms) and 99% processing (22372 ms)
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73975
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 3503274
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 19552 ms: 179.01494 rec/ms, 358.02988 cell/ms
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (58 ms) and 99% processing (19552 ms)
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 72639
21-Aug-2015 11:07:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:07:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500100
21-Aug-2015 11:07:49 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501462 records from 2 columns in 19516 ms: 179.41495 rec/ms, 358.8299 cell/ms
21-Aug-2015 11:07:49 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (115 ms) and 99% processing (19516 ms)
21-Aug-2015 11:07:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501462. reading next block
21-Aug-2015 11:07:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 126306
21-Aug-2015 11:08:07 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574256 records.
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501302 records from 2 columns in 34520 ms: 101.428215 rec/ms, 202.85643 cell/ms
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (89 ms) and 99% processing (34520 ms)
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501302. reading next block
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 126410
21-Aug-2015 11:08:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 117 ms. row count = 3500100
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 179 ms. row count = 3500100
21-Aug-2015 11:08:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
21-Aug-2015 11:08:09 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:09 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3500100
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627697 records.
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
21-Aug-2015 11:08:09 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500100
21-Aug-2015 11:08:09 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:09 INFO: parquet.h15/08/21 11:08:09 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 2125 bytes result sent to driver
15/08/21 11:08:09 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92, localhost, ANY, 1758 bytes)
15/08/21 11:08:09 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
15/08/21 11:08:10 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 40641 ms on localhost (77/170)
15/08/21 11:08:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:16 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 2125 bytes result sent to driver
15/08/21 11:08:16 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93, localhost, ANY, 1773 bytes)
15/08/21 11:08:16 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
15/08/21 11:08:16 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 42637 ms on localhost (78/170)
15/08/21 11:08:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 257330329 length: 123112601 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:16 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 2125 bytes result sent to driver
15/08/21 11:08:16 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94, localhost, ANY, 1758 bytes)
15/08/21 11:08:16 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
15/08/21 11:08:16 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 42881 ms on localhost (79/170)
15/08/21 11:08:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:21 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 2125 bytes result sent to driver
15/08/21 11:08:21 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95, localhost, ANY, 1768 bytes)
15/08/21 11:08:21 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 41248 ms on localhost (80/170)
15/08/21 11:08:21 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
15/08/21 11:08:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 134217728 end: 257756022 length: 123538294 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:21 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 2125 bytes result sent to driver
15/08/21 11:08:21 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96, localhost, ANY, 1757 bytes)
15/08/21 11:08:21 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
15/08/21 11:08:21 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 41419 ms on localhost (81/170)
15/08/21 11:08:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:21 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 2125 bytes result sent to driver
15/08/21 11:08:21 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97, localhost, ANY, 1771 bytes)
15/08/21 11:08:21 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
15/08/21 11:08:21 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 41451 ms on localhost (82/170)
15/08/21 11:08:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 257832393 length: 123614665 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:26 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 2125 bytes result sent to driver
15/08/21 11:08:26 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98, localhost, ANY, 1758 bytes)
15/08/21 11:08:26 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
15/08/21 11:08:26 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 41733 ms on localhost (83/170)
15/08/21 11:08:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:26 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 2125 bytes result sent to driver
15/08/21 11:08:26 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99, localhost, ANY, 1771 bytes)
15/08/21 11:08:26 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
15/08/21 11:08:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 134217728 end: 257486081 length: 123268353 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:26 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 38052 ms on localhost (84/170)
adoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573967 records.
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 62 ms. row count = 3500100
21-Aug-2015 11:08:10 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:10 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 1854 ms. row count = 3500100
21-Aug-2015 11:08:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31957 ms: 109.5253 rec/ms, 219.0506 cell/ms
21-Aug-2015 11:08:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (67 ms) and 99% processing (31957 ms)
21-Aug-2015 11:08:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:08:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 71989
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503231 records from 2 columns in 35432 ms: 98.87195 rec/ms, 197.7439 cell/ms
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (104 ms) and 99% processing (35432 ms)
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503231. reading next block
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 71350
21-Aug-2015 11:08:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574008 records.
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 3500100
21-Aug-2015 11:08:17 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503274 records from 2 columns in 28837 ms: 121.48538 rec/ms, 242.97076 cell/ms
21-Aug-2015 11:08:17 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (49 ms) and 99% processing (28837 ms)
21-Aug-2015 11:08:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503274. reading next block
21-Aug-2015 11:08:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3410 ms. row count = 71042
21-Aug-2015 11:08:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574308 records.
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 3500100
21-Aug-2015 11:08:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 87 ms. row count = 3500100
21-Aug-2015 11:08:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573690 records.
21-Aug-2015 11:08:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 82 ms. row count = 3500612
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 17445 ms: 200.63629 rec/ms, 401.27258 cell/ms
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (117 ms) and 99% processing (17445 ms)
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 74156
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 17342 ms: 201.82793 rec/ms, 403.65585 cell/ms
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (68 ms) and 99% processing (17342 ms)
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:08:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 74054
21-Aug-2015 11:08:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501124 records.
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 81 ms. row count = 3501124
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 16796 ms: 208.3889 rec/ms, 416.7778 cell/ms
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (63 ms) and 99% processing (16796 ms)
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 127597
21-Aug-2015 11:08:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573157 records.
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500100
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordRea15/08/21 11:08:26 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 2125 bytes result sent to driver
15/08/21 11:08:26 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100, localhost, ANY, 1757 bytes)
15/08/21 11:08:26 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 37955 ms on localhost (85/170)
15/08/21 11:08:26 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
15/08/21 11:08:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:31 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 2125 bytes result sent to driver
15/08/21 11:08:31 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101, localhost, ANY, 1769 bytes)
15/08/21 11:08:31 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
15/08/21 11:08:31 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 23365 ms on localhost (86/170)
15/08/21 11:08:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 134217728 end: 257368738 length: 123151010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:31 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 2125 bytes result sent to driver
15/08/21 11:08:31 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102, localhost, ANY, 1758 bytes)
15/08/21 11:08:31 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
15/08/21 11:08:31 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 23165 ms on localhost (87/170)
15/08/21 11:08:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:31 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 2125 bytes result sent to driver
15/08/21 11:08:31 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103, localhost, ANY, 1770 bytes)
15/08/21 11:08:31 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
15/08/21 11:08:31 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 23764 ms on localhost (88/170)
15/08/21 11:08:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 134217728 end: 257317174 length: 123099446 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:31 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 2125 bytes result sent to driver
15/08/21 11:08:31 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104, localhost, ANY, 1758 bytes)
15/08/21 11:08:31 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
15/08/21 11:08:31 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 22405 ms on localhost (89/170)
15/08/21 11:08:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:32 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 2125 bytes result sent to driver
15/08/21 11:08:32 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105, localhost, ANY, 1769 bytes)
15/08/21 11:08:32 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
15/08/21 11:08:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 257566042 length: 123348314 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:32 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 23306 ms on localhost (90/170)
15/08/21 11:08:32 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 2125 bytes result sent to driver
15/08/21 11:08:32 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106, localhost, ANY, 1758 bytes)
15/08/21 11:08:32 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
15/08/21 11:08:32 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 22810 ms on localhost (91/170)
15/08/21 11:08:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:36 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 2125 bytes result sent to driver
15/08/21 11:08:36 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107, localhost, ANY, 1771 bytes)
15/08/21 11:08:36 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
15/08/21 11:08:36 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 27202 ms on localhost (92/170)
15/08/21 11:08:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 134217728 end: 257458240 length: 123240512 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:37 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 2125 bytes result sent to driver
15/08/21 11:08:37 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108, localhost, ANY, 1758 bytes)
15/08/21 11:08:37 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
15/08/21 11:08:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:37 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 27701 ms on localhost (93/170)
15/08/21 11:08:37 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 2125 bytes result sent to driver
15/08/21 11:08:37 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109, localhost, ANY, 1770 bytes)
15/08/21 11:08:37 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
15/08/21 11:08:37 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 21622 ms on localhost (94/170)
15/08/21 11:08:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 134217728 end: 257458739 length: 123241011 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:41 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 2125 bytes result sent to driver
15/08/21 11:08:41 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110, localhost, ANY, 1757 bytes)
15/08/21 11:08:41 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
15/08/21 11:08:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:08:41 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 25535 ms on localhost (95/170)
der: Assembled and processed 3500100 records from 2 columns in 16605 ms: 210.7859 rec/ms, 421.5718 cell/ms
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (62 ms) and 99% processing (16605 ms)
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 73867
21-Aug-2015 11:08:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500100
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 15203 ms: 230.2243 rec/ms, 460.4486 cell/ms
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (63 ms) and 99% processing (15203 ms)
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:08:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 73908
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 88 ms. row count = 3500100
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500100
21-Aug-2015 11:08:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573826 records.
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 3500100
21-Aug-2015 11:08:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500786 records.
21-Aug-2015 11:08:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500786
21-Aug-2015 11:08:32 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:32 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:32 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573221 records.
21-Aug-2015 11:08:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:32 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502806 records.
21-Aug-2015 11:08:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3749 ms. row count = 3502806
21-Aug-2015 11:08:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3824 ms. row count = 3501171
21-Aug-2015 11:08:36 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:36 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571385 records.
21-Aug-2015 11:08:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 15983 ms: 218.98892 rec/ms, 437.97784 cell/ms
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (36 ms) and 99% processing (15983 ms)
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 74208
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500612 records from 2 columns in 15600 ms: 224.39821 rec/ms, 448.79642 cell/ms
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (82 ms) and 99% processing (15600 ms)
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500612. reading next block
21-Aug-2015 11:08:37 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 73078
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:37 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 76 ms. row count = 3500100
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574219 records.
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 3501364
21-Aug-2015 11:08:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:08:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501448 records.
21-Aug-2015 11:08:41 INFO: parque15/08/21 11:09:00 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 2125 bytes result sent to driver
15/08/21 11:09:00 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111, localhost, ANY, 1771 bytes)
15/08/21 11:09:00 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
15/08/21 11:09:00 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 39492 ms on localhost (96/170)
15/08/21 11:09:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 134217728 end: 257439181 length: 123221453 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:01 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 2125 bytes result sent to driver
15/08/21 11:09:01 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112, localhost, ANY, 1757 bytes)
15/08/21 11:09:01 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
15/08/21 11:09:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:01 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 39171 ms on localhost (97/170)
15/08/21 11:09:01 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 2125 bytes result sent to driver
15/08/21 11:09:01 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113, localhost, ANY, 1770 bytes)
15/08/21 11:09:01 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
15/08/21 11:09:01 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 40011 ms on localhost (98/170)
15/08/21 11:09:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 134217728 end: 181459518 length: 47241790 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:04 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 2125 bytes result sent to driver
15/08/21 11:09:04 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114, localhost, ANY, 1757 bytes)
15/08/21 11:09:04 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
15/08/21 11:09:04 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 38629 ms on localhost (99/170)
15/08/21 11:09:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:06 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 2125 bytes result sent to driver
15/08/21 11:09:06 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115, localhost, ANY, 1768 bytes)
15/08/21 11:09:06 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
15/08/21 11:09:06 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 39436 ms on localhost (100/170)
15/08/21 11:09:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 259458210 length: 125240482 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:09 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 2125 bytes result sent to driver
15/08/21 11:09:09 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116, localhost, ANY, 1758 bytes)
15/08/21 11:09:09 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
15/08/21 11:09:09 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 43544 ms on localhost (101/170)
15/08/21 11:09:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:10 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 2125 bytes result sent to driver
15/08/21 11:09:10 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117, localhost, ANY, 1769 bytes)
15/08/21 11:09:10 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
15/08/21 11:09:10 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 39468 ms on localhost (102/170)
15/08/21 11:09:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 257467186 length: 123249458 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:11 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 2125 bytes result sent to driver
15/08/21 11:09:11 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118, localhost, ANY, 1756 bytes)
15/08/21 11:09:11 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
15/08/21 11:09:11 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 9222 ms on localhost (103/170)
15/08/21 11:09:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:11 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 2125 bytes result sent to driver
15/08/21 11:09:11 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119, localhost, ANY, 1769 bytes)
15/08/21 11:09:11 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
15/08/21 11:09:11 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 39523 ms on localhost (104/170)
15/08/21 11:09:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 260141700 length: 125923972 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:11 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 2125 bytes result sent to driver
15/08/21 11:09:11 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120, localhost, ANY, 1756 bytes)
15/08/21 11:09:11 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
15/08/21 11:09:11 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 39188 ms on localhost (105/170)
15/08/21 11:09:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
t.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:08:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 84 ms. row count = 3501448
21-Aug-2015 11:09:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572926 records.
21-Aug-2015 11:09:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 109 ms. row count = 3500100
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 34598 ms: 101.16481 rec/ms, 202.32962 cell/ms
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (51 ms) and 99% processing (34598 ms)
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73057
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 164 ms. row count = 3500100
21-Aug-2015 11:09:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1466882 records.
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 82 ms. row count = 1466882
21-Aug-2015 11:09:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:09:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 139 ms. row count = 3500100
21-Aug-2015 11:09:04 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 33309 ms: 105.07971 rec/ms, 210.15942 cell/ms
21-Aug-2015 11:09:04 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (88 ms) and 99% processing (33309 ms)
21-Aug-2015 11:09:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 73871
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 33438 ms: 104.674324 rec/ms, 209.34865 cell/ms
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (118 ms) and 99% processing (33438 ms)
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73726
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29037 ms: 120.539314 rec/ms, 241.07863 cell/ms
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (70 ms) and 99% processing (29037 ms)
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 71285
21-Aug-2015 11:09:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:06 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501171 records from 2 columns in 29609 ms: 118.24685 rec/ms, 236.4937 cell/ms
21-Aug-2015 11:09:06 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 11% reading (3824 ms) and 88% processing (29609 ms)
21-Aug-2015 11:09:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501171. reading next block
21-Aug-2015 11:09:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 72050
21-Aug-2015 11:09:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627682 records.
21-Aug-2015 11:09:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3503132
21-Aug-2015 11:09:09 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:09 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:09:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500100
21-Aug-2015 11:09:10 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501364 records from 2 columns in 32698 ms: 107.0819 rec/ms, 214.1638 cell/ms
21-Aug-2015 11:09:10 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (71 ms) and 99% processing (32698 ms)
21-Aug-2015 11:09:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501364. reading next block
21-Aug-2015 11:09:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 72855
21-Aug-2015 11:09:10 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:10 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574192 records.
21-Aug-2015 11:09:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 31 ms. row count = 3501130
21-Aug-2015 11:09:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500631 records.
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized wil15/08/21 11:09:11 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 2125 bytes result sent to driver
15/08/21 11:09:11 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121, localhost, ANY, 1769 bytes)
15/08/21 11:09:11 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
15/08/21 11:09:11 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 38658 ms on localhost (106/170)
15/08/21 11:09:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 259582440 length: 125364712 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:11 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 2125 bytes result sent to driver
15/08/21 11:09:11 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122, localhost, ANY, 1758 bytes)
15/08/21 11:09:11 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
15/08/21 11:09:11 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 39635 ms on localhost (107/170)
15/08/21 11:09:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:11 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 2125 bytes result sent to driver
15/08/21 11:09:11 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123, localhost, ANY, 1770 bytes)
15/08/21 11:09:11 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
15/08/21 11:09:11 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 34957 ms on localhost (108/170)
15/08/21 11:09:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 257838232 length: 123620504 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:15 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 2125 bytes result sent to driver
15/08/21 11:09:15 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124, localhost, ANY, 1757 bytes)
15/08/21 11:09:15 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
15/08/21 11:09:15 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 42707 ms on localhost (109/170)
15/08/21 11:09:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:16 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 2125 bytes result sent to driver
15/08/21 11:09:16 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125, localhost, ANY, 1770 bytes)
15/08/21 11:09:16 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
15/08/21 11:09:16 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 34473 ms on localhost (110/170)
15/08/21 11:09:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 257849235 length: 123631507 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:16 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 2125 bytes result sent to driver
15/08/21 11:09:16 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126, localhost, ANY, 1758 bytes)
15/08/21 11:09:16 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 38544 ms on localhost (111/170)
15/08/21 11:09:16 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
15/08/21 11:09:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:20 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 2125 bytes result sent to driver
15/08/21 11:09:20 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127, localhost, ANY, 1770 bytes)
15/08/21 11:09:20 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
15/08/21 11:09:20 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 43211 ms on localhost (112/170)
15/08/21 11:09:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 134217728 end: 257181348 length: 122963620 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:25 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 2125 bytes result sent to driver
15/08/21 11:09:25 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128, localhost, ANY, 1758 bytes)
15/08/21 11:09:25 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
15/08/21 11:09:25 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 24525 ms on localhost (113/170)
15/08/21 11:09:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:26 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 2125 bytes result sent to driver
15/08/21 11:09:26 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129, localhost, ANY, 1769 bytes)
15/08/21 11:09:26 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
15/08/21 11:09:26 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 25038 ms on localhost (114/170)
15/08/21 11:09:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 257473792 length: 123256064 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:26 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 2125 bytes result sent to driver
15/08/21 11:09:26 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130, localhost, ANY, 1757 bytes)
15/08/21 11:09:26 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
15/08/21 11:09:26 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 21581 ms on localhost (115/170)
15/08/21 11:09:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
l read a total of 3648307 records.
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 62 ms. row count = 3500631
21-Aug-2015 11:09:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501115 records.
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501115
21-Aug-2015 11:09:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626900 records.
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3501235
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 34 ms. row count = 3500100
21-Aug-2015 11:09:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574036 records.
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 38 ms. row count = 3501487
21-Aug-2015 11:09:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501235 records.
21-Aug-2015 11:09:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 3501235
21-Aug-2015 11:09:15 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 14556 ms: 240.45755 rec/ms, 480.9151 cell/ms
21-Aug-2015 11:09:15 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (109 ms) and 99% processing (14556 ms)
21-Aug-2015 11:09:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 72826
21-Aug-2015 11:09:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573259 records.
21-Aug-2015 11:09:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502933 records.
21-Aug-2015 11:09:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
21-Aug-2015 11:09:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 87 ms. row count = 3502933
21-Aug-2015 11:09:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571471 records.
21-Aug-2015 11:09:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 3501332
21-Aug-2015 11:09:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500833 records.
21-Aug-2015 11:09:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500833
21-Aug-2015 11:09:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503132 records from 2 columns in 15747 ms: 222.46346 rec/ms, 444.9269 cell/ms
21-Aug-2015 11:09:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (63 ms) and 99% processing (15747 ms)
21-Aug-2015 11:09:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503132. reading next block
21-Aug-2015 11:09:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 124550
21-Aug-2015 11:09:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573329 records.
21-Aug-2015 11:09:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 3500100
21-Aug-2015 11:09:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502559 records.
21-Aug-2015 11:09:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3502559
21-Aug-2015 11:09:30 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501130 records from 2 columns in 19874 ms: 176.16635 rec/ms, 352.3327 cell/ms
21-Aug-2015 11:09:30 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (31 ms) and 99% processing (19874 ms)
21-Aug-2015 11:09:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501130. reading next block
21-Aug-2015 11:09:30 INFO:15/08/21 11:09:31 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 2125 bytes result sent to driver
15/08/21 11:09:31 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131, localhost, ANY, 1770 bytes)
15/08/21 11:09:31 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
15/08/21 11:09:31 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 25869 ms on localhost (116/170)
15/08/21 11:09:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 257455806 length: 123238078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:53 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 2125 bytes result sent to driver
15/08/21 11:09:53 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132, localhost, ANY, 1758 bytes)
15/08/21 11:09:53 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
15/08/21 11:09:53 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 43686 ms on localhost (117/170)
15/08/21 11:09:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:53 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 2125 bytes result sent to driver
15/08/21 11:09:53 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133, localhost, ANY, 1773 bytes)
15/08/21 11:09:53 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
15/08/21 11:09:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 134217728 end: 257547934 length: 123330206 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:53 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 42381 ms on localhost (118/170)
15/08/21 11:09:53 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 2125 bytes result sent to driver
15/08/21 11:09:53 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134, localhost, ANY, 1758 bytes)
15/08/21 11:09:53 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
15/08/21 11:09:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:53 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 42773 ms on localhost (119/170)
15/08/21 11:09:54 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 2125 bytes result sent to driver
15/08/21 11:09:54 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135, localhost, ANY, 1769 bytes)
15/08/21 11:09:54 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
15/08/21 11:09:54 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 43106 ms on localhost (120/170)
15/08/21 11:09:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 257790576 length: 123572848 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:54 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 2125 bytes result sent to driver
15/08/21 11:09:54 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136, localhost, ANY, 1757 bytes)
15/08/21 11:09:54 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
15/08/21 11:09:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:54 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 43319 ms on localhost (121/170)
15/08/21 11:09:54 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 2125 bytes result sent to driver
15/08/21 11:09:54 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137, localhost, ANY, 1770 bytes)
15/08/21 11:09:54 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
15/08/21 11:09:54 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 42796 ms on localhost (122/170)
15/08/21 11:09:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 134217728 end: 257571649 length: 123353921 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:54 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 2125 bytes result sent to driver
15/08/21 11:09:54 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138, localhost, ANY, 1758 bytes)
15/08/21 11:09:54 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
15/08/21 11:09:54 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 43306 ms on localhost (123/170)
15/08/21 11:09:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:54 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 2125 bytes result sent to driver
15/08/21 11:09:54 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139, localhost, ANY, 1770 bytes)
15/08/21 11:09:54 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
15/08/21 11:09:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 257573201 length: 123355473 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:09:54 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 43264 ms on localhost (124/170)
15/08/21 11:09:55 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 2125 bytes result sent to driver
15/08/21 11:09:55 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140, localhost, ANY, 1757 bytes)
15/08/21 11:09:55 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 40015 ms on localhost (125/170)
15/08/21 11:09:55 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
15/08/21 11:09:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
 parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 73062
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 19959 ms: 175.3645 rec/ms, 350.729 cell/ms
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (63 ms) and 99% processing (19959 ms)
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 148207
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501235 records from 2 columns in 19892 ms: 176.01222 rec/ms, 352.02444 cell/ms
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (60 ms) and 99% processing (19892 ms)
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501235. reading next block
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 125665
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501487 records from 2 columns in 19828 ms: 176.59305 rec/ms, 353.1861 cell/ms
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (38 ms) and 99% processing (19828 ms)
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501487. reading next block
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 72549
21-Aug-2015 11:09:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571559 records.
21-Aug-2015 11:09:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 130 ms. row count = 3500100
21-Aug-2015 11:09:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501043 records.
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 3501043
21-Aug-2015 11:09:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573366 records.
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 3501786
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3500100
21-Aug-2015 11:09:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 37802 ms: 92.59034 rec/ms, 185.18068 cell/ms
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (53 ms) and 99% processing (37802 ms)
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 73159
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574145 records.
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 62 ms. row count = 3500100
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501169 records.
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573313 records.
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501332 records from 2 columns in 33365 ms: 104.94027 rec/ms, 209.88054 cell/ms
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (55 ms) and 99% processing (33365 ms)
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501332. reading next block
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 67 ms. row count = 3501169
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 70139
21-Aug-2015 11:09:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501195 records.
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 150 ms. row count = 3501584
21-Aug-2015 11:09:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 99 ms. row count = 3501195
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572913 records.
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 84 ms. row count = 3500728
21-Aug-2015 11:09:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:55 INFO: parquet.hadoop.I15/08/21 11:09:55 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 2125 bytes result sent to driver
15/08/21 11:09:55 INFO TaskSetManager: Starting task 141.0 in stage 0.0 (TID 141, localhost, ANY, 1770 bytes)
15/08/21 11:09:55 INFO Executor: Running task 141.0 in stage 0.0 (TID 141)
15/08/21 11:09:55 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 39156 ms on localhost (126/170)
15/08/21 11:09:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 257466118 length: 123248390 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:02 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 2125 bytes result sent to driver
15/08/21 11:10:02 INFO TaskSetManager: Starting task 142.0 in stage 0.0 (TID 142, localhost, ANY, 1758 bytes)
15/08/21 11:10:02 INFO Executor: Running task 142.0 in stage 0.0 (TID 142)
15/08/21 11:10:02 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 41952 ms on localhost (127/170)
15/08/21 11:10:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:02 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 2125 bytes result sent to driver
15/08/21 11:10:02 INFO TaskSetManager: Starting task 143.0 in stage 0.0 (TID 143, localhost, ANY, 1770 bytes)
15/08/21 11:10:02 INFO Executor: Running task 143.0 in stage 0.0 (TID 143)
15/08/21 11:10:02 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 46619 ms on localhost (128/170)
15/08/21 11:10:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 257888240 length: 123670512 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:08 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 2125 bytes result sent to driver
15/08/21 11:10:08 INFO TaskSetManager: Starting task 144.0 in stage 0.0 (TID 144, localhost, ANY, 1757 bytes)
15/08/21 11:10:08 INFO Executor: Running task 144.0 in stage 0.0 (TID 144)
15/08/21 11:10:08 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 42990 ms on localhost (129/170)
15/08/21 11:10:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:08 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 2125 bytes result sent to driver
15/08/21 11:10:08 INFO TaskSetManager: Starting task 145.0 in stage 0.0 (TID 145, localhost, ANY, 1768 bytes)
15/08/21 11:10:08 INFO Executor: Running task 145.0 in stage 0.0 (TID 145)
15/08/21 11:10:08 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 42456 ms on localhost (130/170)
15/08/21 11:10:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 134217728 end: 258178393 length: 123960665 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:08 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 2125 bytes result sent to driver
15/08/21 11:10:08 INFO TaskSetManager: Starting task 146.0 in stage 0.0 (TID 146, localhost, ANY, 1758 bytes)
15/08/21 11:10:08 INFO Executor: Running task 146.0 in stage 0.0 (TID 146)
15/08/21 11:10:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:08 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 42382 ms on localhost (131/170)
15/08/21 11:10:14 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 2125 bytes result sent to driver
15/08/21 11:10:14 INFO TaskSetManager: Starting task 147.0 in stage 0.0 (TID 147, localhost, ANY, 1771 bytes)
15/08/21 11:10:14 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 20756 ms on localhost (132/170)
15/08/21 11:10:14 INFO Executor: Running task 147.0 in stage 0.0 (TID 147)
15/08/21 11:10:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 134217728 end: 257798680 length: 123580952 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:14 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 2125 bytes result sent to driver
15/08/21 11:10:14 INFO TaskSetManager: Starting task 148.0 in stage 0.0 (TID 148, localhost, ANY, 1757 bytes)
15/08/21 11:10:14 INFO Executor: Running task 148.0 in stage 0.0 (TID 148)
15/08/21 11:10:14 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 42732 ms on localhost (133/170)
15/08/21 11:10:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
nternalParquetRecordReader: RecordReader initialized will read a total of 3501339 records.
21-Aug-2015 11:09:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:09:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 123 ms. row count = 3501339
21-Aug-2015 11:09:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572985 records.
21-Aug-2015 11:09:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:09:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 171 ms. row count = 3500100
21-Aug-2015 11:09:58 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32538 ms: 107.56961 rec/ms, 215.13922 cell/ms
21-Aug-2015 11:09:58 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (42 ms) and 99% processing (32538 ms)
21-Aug-2015 11:09:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:09:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73229
21-Aug-2015 11:10:02 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:02 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:02 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:10:02 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:02 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
21-Aug-2015 11:10:02 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3500100
21-Aug-2015 11:10:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 120 ms. row count = 3501076
21-Aug-2015 11:10:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 36336 ms: 96.32596 rec/ms, 192.65192 cell/ms
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (130 ms) and 99% processing (36336 ms)
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 71459
21-Aug-2015 11:10:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573846 records.
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 59 ms. row count = 3500100
21-Aug-2015 11:10:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501395 records.
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 44 ms. row count = 3501395
21-Aug-2015 11:10:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501786 records from 2 columns in 19131 ms: 183.0425 rec/ms, 366.085 cell/ms
21-Aug-2015 11:10:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (68 ms) and 99% processing (19131 ms)
21-Aug-2015 11:10:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501786. reading next block
21-Aug-2015 11:10:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 71580
21-Aug-2015 11:10:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501584 records from 2 columns in 18468 ms: 189.60277 rec/ms, 379.20554 cell/ms
21-Aug-2015 11:10:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (150 ms) and 99% processing (18468 ms)
21-Aug-2015 11:10:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501584. reading next block
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 18 ms. row count = 71729
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500728 records from 2 columns in 18854 ms: 185.67561 rec/ms, 371.35123 cell/ms
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (84 ms) and 99% processing (18854 ms)
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500728. reading next block
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 72185
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 19371 ms: 180.68762 rec/ms, 361.37524 cell/ms
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (62 ms) and 99% processing (19371 ms)
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 74045
21-Aug-2015 11:10:14 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572381 records.
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 48 ms. row count = 3500100
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 19003 ms: 184.1867 rec/ms, 368.3734 cell/ms
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (171 ms) and 99% processing (19003 ms)
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:14 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-201515/08/21 11:10:19 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 2125 bytes result sent to driver
15/08/21 11:10:19 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 2125 bytes result sent to driver
15/08/21 11:10:19 INFO TaskSetManager: Starting task 149.0 in stage 0.0 (TID 149, localhost, ANY, 1771 bytes)
15/08/21 11:10:19 INFO Executor: Running task 149.0 in stage 0.0 (TID 149)
15/08/21 11:10:19 INFO TaskSetManager: Starting task 150.0 in stage 0.0 (TID 150, localhost, ANY, 1757 bytes)
15/08/21 11:10:19 INFO Executor: Running task 150.0 in stage 0.0 (TID 150)
15/08/21 11:10:19 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 25181 ms on localhost (134/170)
15/08/21 11:10:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 134217728 end: 257837778 length: 123620050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:19 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 25245 ms on localhost (135/170)
15/08/21 11:10:19 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 2125 bytes result sent to driver
15/08/21 11:10:19 INFO TaskSetManager: Starting task 151.0 in stage 0.0 (TID 151, localhost, ANY, 1771 bytes)
15/08/21 11:10:19 INFO Executor: Running task 151.0 in stage 0.0 (TID 151)
15/08/21 11:10:19 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 25536 ms on localhost (136/170)
15/08/21 11:10:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 134217728 end: 257748250 length: 123530522 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:19 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 2125 bytes result sent to driver
15/08/21 11:10:19 INFO TaskSetManager: Starting task 152.0 in stage 0.0 (TID 152, localhost, ANY, 1757 bytes)
15/08/21 11:10:19 INFO Executor: Running task 152.0 in stage 0.0 (TID 152)
15/08/21 11:10:19 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 24465 ms on localhost (137/170)
15/08/21 11:10:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:19 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 2125 bytes result sent to driver
15/08/21 11:10:19 INFO TaskSetManager: Starting task 153.0 in stage 0.0 (TID 153, localhost, ANY, 1772 bytes)
15/08/21 11:10:19 INFO Executor: Running task 153.0 in stage 0.0 (TID 153)
15/08/21 11:10:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 257331238 length: 123113510 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:19 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 25996 ms on localhost (138/170)
15/08/21 11:10:19 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 2125 bytes result sent to driver
15/08/21 11:10:19 INFO TaskSetManager: Starting task 154.0 in stage 0.0 (TID 154, localhost, ANY, 1758 bytes)
15/08/21 11:10:19 INFO Executor: Running task 154.0 in stage 0.0 (TID 154)
15/08/21 11:10:19 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 26279 ms on localhost (139/170)
15/08/21 11:10:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:19 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 2125 bytes result sent to driver
15/08/21 11:10:19 INFO TaskSetManager: Starting task 155.0 in stage 0.0 (TID 155, localhost, ANY, 1771 bytes)
15/08/21 11:10:19 INFO Executor: Running task 155.0 in stage 0.0 (TID 155)
15/08/21 11:10:19 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 25513 ms on localhost (140/170)
15/08/21 11:10:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 134217728 end: 257176539 length: 122958811 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:20 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 2125 bytes result sent to driver
15/08/21 11:10:20 INFO TaskSetManager: Starting task 156.0 in stage 0.0 (TID 156, localhost, ANY, 1758 bytes)
15/08/21 11:10:20 INFO Executor: Running task 156.0 in stage 0.0 (TID 156)
15/08/21 11:10:20 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 25727 ms on localhost (141/170)
15/08/21 11:10:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:33 INFO Executor: Finished task 141.0 in stage 0.0 (TID 141). 2125 bytes result sent to driver
15/08/21 11:10:33 INFO TaskSetManager: Starting task 157.0 in stage 0.0 (TID 157, localhost, ANY, 1771 bytes)
15/08/21 11:10:33 INFO Executor: Running task 157.0 in stage 0.0 (TID 157)
15/08/21 11:10:33 INFO TaskSetManager: Finished task 141.0 in stage 0.0 (TID 141) in 38212 ms on localhost (142/170)
15/08/21 11:10:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 134217728 end: 257446174 length: 123228446 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:34 INFO Executor: Finished task 142.0 in stage 0.0 (TID 142). 2125 bytes result sent to driver
15/08/21 11:10:34 INFO TaskSetManager: Starting task 158.0 in stage 0.0 (TID 158, localhost, ANY, 1758 bytes)
15/08/21 11:10:34 INFO Executor: Running task 158.0 in stage 0.0 (TID 158)
15/08/21 11:10:34 INFO TaskSetManager: Finished task 142.0 in stage 0.0 (TID 142) in 31876 ms on localhost (143/170)
15/08/21 11:10:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:34 INFO Executor: Finished task 143.0 in stage 0.0 (TID 143). 2125 bytes result sent to driver
15/08/21 11:10:34 INFO TaskSetManager: Starting task 159.0 in stage 0.0 (TID 159, localhost, ANY, 1769 bytes)
15/08/21 11:10:34 INFO Executor: Running task 159.0 in stage 0.0 (TID 159)
15/08/21 11:10:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 259884384 length: 125666656 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:34 INFO TaskSetManager: Finished task 143.0 in stage 0.0 (TID 143) in 31871 ms on localhost (144/170)
15/08/21 11:10:39 INFO Executor: Finished task 144.0 in stage 0.0 (TID 144). 2125 bytes result sent to driver
15/08/21 11:10:39 INFO TaskSetManager: Starting task 160.0 in stage 0.0 (TID 160, localhost, ANY, 1758 bytes)
15/08/21 11:10:39 INFO Executor: Running task 160.0 in stage 0.0 (TID 160)
15/08/21 11:10:39 INFO TaskSetManager: Finished task 144.0 in stage 0.0 (TID 144) in 30937 ms on localhost (145/170)
15/08/21 11:10:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 72885
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 103 ms. row count = 3501121
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501076 records from 2 columns in 16278 ms: 215.08023 rec/ms, 430.16046 cell/ms
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (120 ms) and 99% processing (16278 ms)
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501076. reading next block
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 16 ms. row count = 73262
21-Aug-2015 11:10:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573120 records.
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500100
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 3500100
21-Aug-2015 11:10:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573993 records.
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500100
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 43 ms. row count = 3500100
21-Aug-2015 11:10:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
21-Aug-2015 11:10:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503161 records.
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571357 records.
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3503161
21-Aug-2015 11:10:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 81 ms. row count = 3501317
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501689 records.
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 93 ms. row count = 3501689
21-Aug-2015 11:10:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572541 records.
21-Aug-2015 11:10:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 116 ms. row count = 3500100
21-Aug-2015 11:10:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501046 records.
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626733 records.
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 96 ms. row count = 3501046
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 3500100
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 26280 ms: 133.18494 rec/ms, 266.36987 cell/ms
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (59 ms) and 99% processing (26280 ms)
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 73746
21-Aug-2015 11:10:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501221 records.
21-Aug15/08/21 11:10:39 INFO Executor: Finished task 145.0 in stage 0.0 (TID 145). 2125 bytes result sent to driver
15/08/21 11:10:39 INFO TaskSetManager: Starting task 161.0 in stage 0.0 (TID 161, localhost, ANY, 1771 bytes)
15/08/21 11:10:39 INFO Executor: Running task 161.0 in stage 0.0 (TID 161)
15/08/21 11:10:39 INFO TaskSetManager: Finished task 145.0 in stage 0.0 (TID 145) in 30974 ms on localhost (146/170)
15/08/21 11:10:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259183553 length: 124965825 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:43 INFO Executor: Finished task 146.0 in stage 0.0 (TID 146). 2125 bytes result sent to driver
15/08/21 11:10:43 INFO TaskSetManager: Starting task 162.0 in stage 0.0 (TID 162, localhost, ANY, 1757 bytes)
15/08/21 11:10:43 INFO Executor: Running task 162.0 in stage 0.0 (TID 162)
15/08/21 11:10:43 INFO TaskSetManager: Finished task 146.0 in stage 0.0 (TID 146) in 35334 ms on localhost (147/170)
15/08/21 11:10:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:50 INFO Executor: Finished task 149.0 in stage 0.0 (TID 149). 2125 bytes result sent to driver
15/08/21 11:10:50 INFO Executor: Finished task 150.0 in stage 0.0 (TID 150). 2125 bytes result sent to driver
15/08/21 11:10:50 INFO Executor: Finished task 147.0 in stage 0.0 (TID 147). 2125 bytes result sent to driver
15/08/21 11:10:50 INFO TaskSetManager: Starting task 163.0 in stage 0.0 (TID 163, localhost, ANY, 1771 bytes)
15/08/21 11:10:50 INFO TaskSetManager: Starting task 164.0 in stage 0.0 (TID 164, localhost, ANY, 1758 bytes)
15/08/21 11:10:50 INFO Executor: Running task 164.0 in stage 0.0 (TID 164)
15/08/21 11:10:50 INFO TaskSetManager: Finished task 149.0 in stage 0.0 (TID 149) in 31205 ms on localhost (148/170)
15/08/21 11:10:50 INFO Executor: Running task 163.0 in stage 0.0 (TID 163)
15/08/21 11:10:50 INFO TaskSetManager: Finished task 147.0 in stage 0.0 (TID 147) in 36362 ms on localhost (149/170)
15/08/21 11:10:50 INFO TaskSetManager: Starting task 165.0 in stage 0.0 (TID 165, localhost, ANY, 1769 bytes)
15/08/21 11:10:50 INFO Executor: Running task 165.0 in stage 0.0 (TID 165)
15/08/21 11:10:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 257504450 length: 123286722 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:50 INFO TaskSetManager: Finished task 150.0 in stage 0.0 (TID 150) in 31217 ms on localhost (150/170)
15/08/21 11:10:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 134217728 end: 257173847 length: 122956119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:50 INFO Executor: Finished task 152.0 in stage 0.0 (TID 152). 2125 bytes result sent to driver
15/08/21 11:10:50 INFO TaskSetManager: Starting task 166.0 in stage 0.0 (TID 166, localhost, ANY, 1758 bytes)
15/08/21 11:10:50 INFO Executor: Running task 166.0 in stage 0.0 (TID 166)
15/08/21 11:10:50 INFO TaskSetManager: Finished task 152.0 in stage 0.0 (TID 152) in 31108 ms on localhost (151/170)
15/08/21 11:10:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:51 INFO Executor: Finished task 148.0 in stage 0.0 (TID 148). 2125 bytes result sent to driver
15/08/21 11:10:51 INFO TaskSetManager: Starting task 167.0 in stage 0.0 (TID 167, localhost, ANY, 1770 bytes)
15/08/21 11:10:51 INFO Executor: Running task 167.0 in stage 0.0 (TID 167)
15/08/21 11:10:51 INFO TaskSetManager: Finished task 148.0 in stage 0.0 (TID 148) in 36504 ms on localhost (152/170)
15/08/21 11:10:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 257494956 length: 123277228 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:57 INFO Executor: Finished task 151.0 in stage 0.0 (TID 151). 2125 bytes result sent to driver
15/08/21 11:10:57 INFO TaskSetManager: Starting task 168.0 in stage 0.0 (TID 168, localhost, ANY, 1757 bytes)
15/08/21 11:10:57 INFO Executor: Running task 168.0 in stage 0.0 (TID 168)
15/08/21 11:10:57 INFO TaskSetManager: Finished task 151.0 in stage 0.0 (TID 151) in 38156 ms on localhost (153/170)
15/08/21 11:10:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
-2015 11:10:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3501221
21-Aug-2015 11:10:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627071 records.
21-Aug-2015 11:10:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 85 ms. row count = 3503161
21-Aug-2015 11:10:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 87 ms. row count = 3500100
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29757 ms: 117.62274 rec/ms, 235.24548 cell/ms
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (48 ms) and 99% processing (29757 ms)
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 72281
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 25130 ms: 139.27974 rec/ms, 278.55948 cell/ms
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (51 ms) and 99% processing (25130 ms)
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 73020
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29246 ms: 119.6779 rec/ms, 239.3558 cell/ms
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (61 ms) and 99% processing (29246 ms)
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 73893
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29583 ms: 118.314575 rec/ms, 236.62915 cell/ms
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (43 ms) and 99% processing (29583 ms)
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 18 ms. row count = 73871
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501317 records from 2 columns in 29671 ms: 118.004684 rec/ms, 236.00937 cell/ms
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (81 ms) and 99% processing (29671 ms)
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501317. reading next block
21-Aug-2015 11:10:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 70040
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 16212 ms: 215.89563 rec/ms, 431.79126 cell/ms
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (116 ms) and 99% processing (16212 ms)
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 72441
21-Aug-2015 11:10:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573920 records.
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573142 records.
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501110 records.
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500779
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 3501110
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 128 ms. row count = 3502917
21-Aug-2015 11:10:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500519 records.
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 67 ms. row count = 3500519
21-Aug-2015 11:10:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 11:10:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573577 records.
21-Aug-2015 11:10:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 11:10:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 57 ms. row count = 3500100
21-Aug-2015 11:10:51 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 16491 ms: 212.24304 rec/ms, 424.48608 cell/ms
21-Aug-2015 11:10:51 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (71 ms) and 99% processing (16491 ms)
21-Aug-2015 11:10:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 11:10:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 126633
21-Aug-2015 11:10:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAtte15/08/21 11:10:57 INFO Executor: Finished task 156.0 in stage 0.0 (TID 156). 2125 bytes result sent to driver
15/08/21 11:10:57 INFO TaskSetManager: Starting task 169.0 in stage 0.0 (TID 169, localhost, ANY, 1771 bytes)
15/08/21 11:10:57 INFO Executor: Running task 169.0 in stage 0.0 (TID 169)
15/08/21 11:10:57 INFO TaskSetManager: Finished task 156.0 in stage 0.0 (TID 156) in 37778 ms on localhost (154/170)
15/08/21 11:10:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 134217728 end: 257035718 length: 122817990 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:10:58 INFO Executor: Finished task 154.0 in stage 0.0 (TID 154). 2125 bytes result sent to driver
15/08/21 11:10:58 INFO TaskSetManager: Finished task 154.0 in stage 0.0 (TID 154) in 38203 ms on localhost (155/170)
15/08/21 11:10:58 INFO Executor: Finished task 153.0 in stage 0.0 (TID 153). 2125 bytes result sent to driver
15/08/21 11:10:58 INFO TaskSetManager: Finished task 153.0 in stage 0.0 (TID 153) in 38635 ms on localhost (156/170)
15/08/21 11:10:58 INFO Executor: Finished task 157.0 in stage 0.0 (TID 157). 2125 bytes result sent to driver
15/08/21 11:10:58 INFO TaskSetManager: Finished task 157.0 in stage 0.0 (TID 157) in 25077 ms on localhost (157/170)
15/08/21 11:10:58 INFO Executor: Finished task 155.0 in stage 0.0 (TID 155). 2125 bytes result sent to driver
15/08/21 11:10:58 INFO TaskSetManager: Finished task 155.0 in stage 0.0 (TID 155) in 38743 ms on localhost (158/170)
15/08/21 11:10:59 INFO Executor: Finished task 158.0 in stage 0.0 (TID 158). 2125 bytes result sent to driver
15/08/21 11:10:59 INFO TaskSetManager: Finished task 158.0 in stage 0.0 (TID 158) in 24436 ms on localhost (159/170)
15/08/21 11:11:01 INFO Executor: Finished task 159.0 in stage 0.0 (TID 159). 2125 bytes result sent to driver
15/08/21 11:11:01 INFO TaskSetManager: Finished task 159.0 in stage 0.0 (TID 159) in 26624 ms on localhost (160/170)
15/08/21 11:11:02 INFO Executor: Finished task 160.0 in stage 0.0 (TID 160). 2125 bytes result sent to driver
15/08/21 11:11:02 INFO TaskSetManager: Finished task 160.0 in stage 0.0 (TID 160) in 23038 ms on localhost (161/170)
15/08/21 11:11:02 INFO Executor: Finished task 161.0 in stage 0.0 (TID 161). 2125 bytes result sent to driver
15/08/21 11:11:02 INFO TaskSetManager: Finished task 161.0 in stage 0.0 (TID 161) in 23109 ms on localhost (162/170)
15/08/21 11:11:02 INFO Executor: Finished task 162.0 in stage 0.0 (TID 162). 2125 bytes result sent to driver
15/08/21 11:11:02 INFO TaskSetManager: Finished task 162.0 in stage 0.0 (TID 162) in 18951 ms on localhost (163/170)
15/08/21 11:11:08 INFO Executor: Finished task 163.0 in stage 0.0 (TID 163). 2125 bytes result sent to driver
15/08/21 11:11:08 INFO TaskSetManager: Finished task 163.0 in stage 0.0 (TID 163) in 17753 ms on localhost (164/170)
15/08/21 11:11:08 INFO Executor: Finished task 166.0 in stage 0.0 (TID 166). 2125 bytes result sent to driver
15/08/21 11:11:08 INFO TaskSetManager: Finished task 166.0 in stage 0.0 (TID 166) in 17579 ms on localhost (165/170)
15/08/21 11:11:08 INFO Executor: Finished task 164.0 in stage 0.0 (TID 164). 2125 bytes result sent to driver
15/08/21 11:11:08 INFO TaskSetManager: Finished task 164.0 in stage 0.0 (TID 164) in 17825 ms on localhost (166/170)
15/08/21 11:11:08 INFO Executor: Finished task 165.0 in stage 0.0 (TID 165). 2125 bytes result sent to driver
15/08/21 11:11:08 INFO TaskSetManager: Finished task 165.0 in stage 0.0 (TID 165) in 18125 ms on localhost (167/170)
15/08/21 11:11:08 INFO Executor: Finished task 167.0 in stage 0.0 (TID 167). 2125 bytes result sent to driver
15/08/21 11:11:08 INFO TaskSetManager: Finished task 167.0 in stage 0.0 (TID 167) in 17663 ms on localhost (168/170)
15/08/21 11:11:08 INFO Executor: Finished task 168.0 in stage 0.0 (TID 168). 2125 bytes result sent to driver
15/08/21 11:11:08 INFO TaskSetManager: Finished task 168.0 in stage 0.0 (TID 168) in 11125 ms on localhost (169/170)
15/08/21 11:11:09 INFO Executor: Finished task 169.0 in stage 0.0 (TID 169). 2125 bytes result sent to driver
15/08/21 11:11:09 INFO TaskSetManager: Finished task 169.0 in stage 0.0 (TID 169) in 11427 ms on localhost (170/170)
15/08/21 11:11:09 INFO DAGScheduler: ShuffleMapStage 0 (processCmd at CliDriver.java:423) finished in 386.690 s
15/08/21 11:11:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/21 11:11:09 INFO DAGScheduler: looking for newly runnable stages
15/08/21 11:11:09 INFO DAGScheduler: running: Set()
15/08/21 11:11:09 INFO DAGScheduler: waiting: Set(ResultStage 1)
15/08/21 11:11:09 INFO DAGScheduler: failed: Set()
15/08/21 11:11:09 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@21e91a20
15/08/21 11:11:09 INFO DAGScheduler: Missing parents for ResultStage 1: List()
15/08/21 11:11:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 11:11:09 INFO StatsReportListener: task runtime:(count: 170, mean: 35917.311765, stdev: 10304.853575, max: 62782.000000, min: 9222.000000)
15/08/21 11:11:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:11:09 INFO StatsReportListener: 	9.2 s	19.0 s	23.3 s	26.3 s	38.2 s	42.4 s	44.5 s	55.8 s	1.0 min
15/08/21 11:11:09 INFO StatsReportListener: shuffle bytes written:(count: 170, mean: 9629242.888235, stdev: 468850.402633, max: 10377140.000000, min: 4098365.000000)
15/08/21 11:11:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:11:09 INFO StatsReportListener: 	3.9 MB	9.0 MB	9.0 MB	9.1 MB	9.2 MB	9.3 MB	9.5 MB	9.6 MB	9.9 MB
15/08/21 11:11:09 INFO StatsReportListener: task result size:(count: 170, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 11:11:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:11:09 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 11:11:09 INFO MemoryStore: ensureFreeSpace(78888) called with curMem=363081, maxMem=22226833244
15/08/21 11:11:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 77.0 KB, free 20.7 GB)
15/08/21 11:11:09 INFO StatsReportListener: executor (non-fetch) time pct: (count: 170, mean: 99.628631, stdev: 1.059095, max: 99.944494, min: 89.960182)
15/08/21 11:11:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:11:09 INFO StatsReportListener: 	90 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 11:11:09 INFO MemoryStore: ensureFreeSpace(30046) called with curMem=441969, maxMem=22226833244
15/08/21 11:11:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.3 KB, free 20.7 GB)
15/08/21 11:11:09 INFO StatsReportListener: other time pct: (count: 170, mean: 0.371369, stdev: 1.059095, max: 10.039818, min: 0.055506)
15/08/21 11:11:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:11:09 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	10 %
15/08/21 11:11:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:53620 (size: 29.3 KB, free: 20.7 GB)
15/08/21 11:11:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/08/21 11:11:09 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423)
15/08/21 11:11:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
15/08/21 11:11:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:09 INFO Executor: Running task 4.0 in stage 1.0 (TID 174)
15/08/21 11:11:09 INFO Executor: Running task 6.0 in stage 1.0 (TID 176)
15/08/21 11:11:09 INFO Executor: Running task 12.0 in stage 1.0 (TID 182)
15/08/21 11:11:09 INFO Executor: Running task 3.0 in stage 1.0 (TID 173)
15/08/21 11:11:09 INFO Executor: Running task 5.0 in stage 1.0 (TID 175)
15/08/21 11:11:09 INFO Executor: Running task 15.0 in stage 1.0 (TID 185)
15/08/21 11:11:09 INFO Executor: Running task 1.0 in stage 1.0 (TID 171)
15/08/21 11:11:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 170)
15/08/21 11:11:09 INFO Executor: Running task 2.0 in stage 1.0 (TID 172)
15/08/21 11:11:09 INFO Executor: Running task 13.0 in stage 1.0 (TID 183)
15/08/21 11:11:09 INFO Executor: Running task 11.0 in stage 1.0 (TID 181)
15/08/21 11:11:09 INFO Executor: Running task 14.0 in stage 1.0 (TID 184)
15/08/21 11:11:09 INFO Executor: Running task 9.0 in stage 1.0 (TID 179)
15/08/21 11:11:09 INFO Executor: Running task 10.0 in stage 1.0 (TID 180)
15/08/21 11:11:09 INFO Executor: Running task 7.0 in stage 1.0 (TID 177)
15/08/21 11:11:09 INFO Executor: Running task 8.0 in stage 1.0 (TID 178)
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:53620 in memory (size: 4.4 KB, free: 20.7 GB)
15/08/21 11:11:36 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:36 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/21 11:11:36 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:37 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:37 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:38 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:38 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:38 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:38 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:38 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:38 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:38 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:38 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:38 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:38 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:38 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:38 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:11:38 INFO CodecConfig: Compression: GZIP
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:11:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:11:38 INFO ParquetOutputFormat: Validation is off
15/08/21 11:11:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:11:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 2,670,872B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,753B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 834,621B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,339B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 2,664,829B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,710B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 833,548B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,266B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 2,670,799B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,680B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 832,054B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,772B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:11:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000009
15/08/21 11:11:44 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000009_0: Committed
15/08/21 11:11:44 INFO Executor: Finished task 9.0 in stage 1.0 (TID 179). 843 bytes result sent to driver
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 2,670,778B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,659B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:11:44 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000008
15/08/21 11:11:44 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000008_0: Committed
15/08/21 11:11:44 INFO Executor: Finished task 8.0 in stage 1.0 (TID 178). 843 bytes result sent to driver
15/08/21 11:11:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000006
15/08/21 11:11:44 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000006_0: Committed
15/08/21 11:11:44 INFO Executor: Running task 16.0 in stage 1.0 (TID 186)
15/08/21 11:11:44 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:44 INFO Executor: Running task 17.0 in stage 1.0 (TID 187)
15/08/21 11:11:44 INFO Executor: Finished task 6.0 in stage 1.0 (TID 176). 843 bytes result sent to driver
15/08/21 11:11:44 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 179) in 35391 ms on localhost (1/200)
15/08/21 11:11:44 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:44 INFO Executor: Running task 18.0 in stage 1.0 (TID 188)
15/08/21 11:11:44 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 178) in 35405 ms on localhost (2/200)
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 834,080B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,798B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:11:44 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 176) in 35411 ms on localhost (3/200)
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 2,671,049B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,930B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 834,267B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,985B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:11:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000007
15/08/21 11:11:44 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000007_0: Committed
15/08/21 11:11:44 INFO Executor: Finished task 7.0 in stage 1.0 (TID 177). 843 bytes result sent to driver
15/08/21 11:11:44 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:44 INFO Executor: Running task 19.0 in stage 1.0 (TID 189)
15/08/21 11:11:44 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 177) in 35469 ms on localhost (4/200)
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000005
15/08/21 11:11:44 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000005_0: Committed
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 2,664,882B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,763B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:44 INFO ColumnChunkPageWriteStore: written 833,967B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,685B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:11:44 INFO Executor: Finished task 5.0 in stage 1.0 (TID 175). 843 bytes result sent to driver
15/08/21 11:11:44 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:44 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 175) in 35541 ms on localhost (5/200)
15/08/21 11:11:44 INFO Executor: Running task 20.0 in stage 1.0 (TID 190)
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000001
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000001_0: Committed
15/08/21 11:11:45 INFO Executor: Finished task 1.0 in stage 1.0 (TID 171). 843 bytes result sent to driver
15/08/21 11:11:45 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 21.0 in stage 1.0 (TID 191)
15/08/21 11:11:45 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 171) in 35637 ms on localhost (6/200)
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,665,058B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,939B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 832,232B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,950B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,671,003B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,884B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 833,807B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,525B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000013
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000013_0: Committed
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:11:45 INFO Executor: Finished task 13.0 in stage 1.0 (TID 183). 843 bytes result sent to driver
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000011
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000011_0: Committed
15/08/21 11:11:45 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:11:45 INFO Executor: Running task 22.0 in stage 1.0 (TID 192)
15/08/21 11:11:45 INFO Executor: Finished task 11.0 in stage 1.0 (TID 181). 843 bytes result sent to driver
15/08/21 11:11:45 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 23.0 in stage 1.0 (TID 193)
15/08/21 11:11:45 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 183) in 35744 ms on localhost (7/200)
15/08/21 11:11:45 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 181) in 35750 ms on localhost (8/200)
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,671,085B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,966B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 833,328B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,046B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,671,226B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,107B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 833,273B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,991B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,665,833B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,714B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 832,984B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,702B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,664,920B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,801B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 833,650B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,368B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,671,125B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,006B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 836,802B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 836,520B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000015
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000015_0: Committed
15/08/21 11:11:45 INFO Executor: Finished task 15.0 in stage 1.0 (TID 185). 843 bytes result sent to driver
15/08/21 11:11:45 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 24.0 in stage 1.0 (TID 194)
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000002
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000002_0: Committed
15/08/21 11:11:45 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 185) in 35972 ms on localhost (9/200)
15/08/21 11:11:45 INFO Executor: Finished task 2.0 in stage 1.0 (TID 172). 843 bytes result sent to driver
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,665,860B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,741B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 833,117B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,835B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,664,834B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,715B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 833,045B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,763B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:11:45 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 172) in 36036 ms on localhost (10/200)
15/08/21 11:11:45 INFO Executor: Running task 25.0 in stage 1.0 (TID 195)
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000000
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000000_0: Committed
15/08/21 11:11:45 INFO Executor: Finished task 0.0 in stage 1.0 (TID 170). 843 bytes result sent to driver
15/08/21 11:11:45 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 26.0 in stage 1.0 (TID 196)
15/08/21 11:11:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 170) in 36057 ms on localhost (11/200)
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000014
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000014_0: Committed
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:45 INFO Executor: Finished task 14.0 in stage 1.0 (TID 184). 843 bytes result sent to driver
15/08/21 11:11:45 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 27.0 in stage 1.0 (TID 197)
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000012
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000012_0: Committed
15/08/21 11:11:45 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 184) in 36101 ms on localhost (12/200)
15/08/21 11:11:45 INFO Executor: Finished task 12.0 in stage 1.0 (TID 182). 843 bytes result sent to driver
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000004
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000004_0: Committed
15/08/21 11:11:45 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 28.0 in stage 1.0 (TID 198)
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000003
15/08/21 11:11:45 INFO Executor: Finished task 4.0 in stage 1.0 (TID 174). 843 bytes result sent to driver
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000003_0: Committed
15/08/21 11:11:45 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 29.0 in stage 1.0 (TID 199)
15/08/21 11:11:45 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 182) in 36110 ms on localhost (13/200)
15/08/21 11:11:45 INFO Executor: Finished task 3.0 in stage 1.0 (TID 173). 843 bytes result sent to driver
15/08/21 11:11:45 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 174) in 36121 ms on localhost (14/200)
15/08/21 11:11:45 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 30.0 in stage 1.0 (TID 200)
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 173) in 36127 ms on localhost (15/200)
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,681
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 2,665,013B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,894B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:11:45 INFO ColumnChunkPageWriteStore: written 833,233B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,951B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 320 entries, 2,560B raw, 320B comp}
15/08/21 11:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211111_0001_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211111_0001_m_000010
15/08/21 11:11:45 INFO SparkHadoopMapRedUtil: attempt_201508211111_0001_m_000010_0: Committed
15/08/21 11:11:45 INFO Executor: Finished task 10.0 in stage 1.0 (TID 180). 843 bytes result sent to driver
15/08/21 11:11:45 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:11:45 INFO Executor: Running task 31.0 in stage 1.0 (TID 201)
15/08/21 11:11:45 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 180) in 36413 ms on localhost (16/200)
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:11:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:06 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:06 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 2,670,946B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,827B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 833,075B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,793B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:12:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:12:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:12:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000029
15/08/21 11:12:08 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000029_0: Committed
15/08/21 11:12:08 INFO Executor: Finished task 29.0 in stage 1.0 (TID 199). 843 bytes result sent to driver
15/08/21 11:12:08 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:08 INFO Executor: Running task 32.0 in stage 1.0 (TID 202)
15/08/21 11:12:08 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 199) in 23017 ms on localhost (17/200)
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 2,670,944B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,825B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 834,137B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,855B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:12:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 2,671,237B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,118B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 833,850B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,568B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:12:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000030
15/08/21 11:12:08 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000030_0: Committed
15/08/21 11:12:08 INFO Executor: Finished task 30.0 in stage 1.0 (TID 200). 843 bytes result sent to driver
15/08/21 11:12:08 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:08 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 200) in 23125 ms on localhost (18/200)
15/08/21 11:12:08 INFO Executor: Running task 33.0 in stage 1.0 (TID 203)
15/08/21 11:12:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000021
15/08/21 11:12:08 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000021_0: Committed
15/08/21 11:12:08 INFO Executor: Finished task 21.0 in stage 1.0 (TID 191). 843 bytes result sent to driver
15/08/21 11:12:08 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:08 INFO Executor: Running task 34.0 in stage 1.0 (TID 204)
15/08/21 11:12:08 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 191) in 23676 ms on localhost (19/200)
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 2,671,512B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,393B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 834,404B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,122B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000023
15/08/21 11:12:08 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000023_0: Committed
15/08/21 11:12:08 INFO Executor: Finished task 23.0 in stage 1.0 (TID 193). 843 bytes result sent to driver
15/08/21 11:12:08 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:08 INFO Executor: Running task 35.0 in stage 1.0 (TID 205)
15/08/21 11:12:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:08 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 193) in 23696 ms on localhost (20/200)
15/08/21 11:12:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 2,665,097B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,978B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 833,850B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,568B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 2,671,314B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,195B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 2,664,826B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,707B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 830,496B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,214B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:08 INFO ColumnChunkPageWriteStore: written 832,355B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,073B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:12:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000018
15/08/21 11:12:09 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000018_0: Committed
15/08/21 11:12:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000024
15/08/21 11:12:09 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000024_0: Committed
15/08/21 11:12:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000026
15/08/21 11:12:09 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000026_0: Committed
15/08/21 11:12:09 INFO Executor: Finished task 24.0 in stage 1.0 (TID 194). 843 bytes result sent to driver
15/08/21 11:12:09 INFO Executor: Finished task 18.0 in stage 1.0 (TID 188). 843 bytes result sent to driver
15/08/21 11:12:09 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:09 INFO Executor: Running task 36.0 in stage 1.0 (TID 206)
15/08/21 11:12:09 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:09 INFO Executor: Running task 37.0 in stage 1.0 (TID 207)
15/08/21 11:12:09 INFO Executor: Finished task 26.0 in stage 1.0 (TID 196). 843 bytes result sent to driver
15/08/21 11:12:09 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 188) in 24343 ms on localhost (21/200)
15/08/21 11:12:09 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 208, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:09 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 194) in 23767 ms on localhost (22/200)
15/08/21 11:12:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:12:09 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 196) in 23702 ms on localhost (23/200)
15/08/21 11:12:09 INFO Executor: Running task 38.0 in stage 1.0 (TID 208)
15/08/21 11:12:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:09 INFO ColumnChunkPageWriteStore: written 2,666,255B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,136B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:09 INFO ColumnChunkPageWriteStore: written 834,498B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,216B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:12:13 INFO ColumnChunkPageWriteStore: written 2,671,426B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,307B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:13 INFO ColumnChunkPageWriteStore: written 834,577B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,295B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:12:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:12:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
15/08/21 11:12:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:12:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000020
15/08/21 11:12:13 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000020_0: Committed
15/08/21 11:12:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000016
15/08/21 11:12:13 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000016_0: Committed
15/08/21 11:12:13 INFO Executor: Finished task 16.0 in stage 1.0 (TID 186). 843 bytes result sent to driver
15/08/21 11:12:13 INFO Executor: Finished task 20.0 in stage 1.0 (TID 190). 843 bytes result sent to driver
15/08/21 11:12:13 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 209, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:13 INFO Executor: Running task 39.0 in stage 1.0 (TID 209)
15/08/21 11:12:13 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 210, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:13 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 186) in 29137 ms on localhost (24/200)
15/08/21 11:12:13 INFO Executor: Running task 40.0 in stage 1.0 (TID 210)
15/08/21 11:12:13 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 190) in 28976 ms on localhost (25/200)
15/08/21 11:12:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:12:13 INFO ColumnChunkPageWriteStore: written 2,671,180B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,061B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:13 INFO ColumnChunkPageWriteStore: written 834,246B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,964B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:12:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 2,664,899B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,780B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 834,265B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,983B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:12:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000022
15/08/21 11:12:14 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000022_0: Committed
15/08/21 11:12:14 INFO Executor: Finished task 22.0 in stage 1.0 (TID 192). 843 bytes result sent to driver
15/08/21 11:12:14 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 211, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000025
15/08/21 11:12:14 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000025_0: Committed
15/08/21 11:12:14 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 192) in 29002 ms on localhost (26/200)
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 2,665,197B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,078B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:14 INFO Executor: Finished task 25.0 in stage 1.0 (TID 195). 843 bytes result sent to driver
15/08/21 11:12:14 INFO Executor: Running task 41.0 in stage 1.0 (TID 211)
15/08/21 11:12:14 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 212, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:14 INFO Executor: Running task 42.0 in stage 1.0 (TID 212)
15/08/21 11:12:14 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 195) in 28736 ms on localhost (27/200)
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 835,387B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,105B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:12:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000019
15/08/21 11:12:14 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000019_0: Committed
15/08/21 11:12:14 INFO Executor: Finished task 19.0 in stage 1.0 (TID 189). 843 bytes result sent to driver
15/08/21 11:12:14 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 213, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:14 INFO Executor: Running task 43.0 in stage 1.0 (TID 213)
15/08/21 11:12:14 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 189) in 29565 ms on localhost (28/200)
15/08/21 11:12:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:12:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 2,664,955B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,836B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 834,916B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,634B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 2,671,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,926B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 835,320B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,038B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:12:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000027
15/08/21 11:12:14 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000027_0: Committed
15/08/21 11:12:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000031
15/08/21 11:12:14 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000031_0: Committed
15/08/21 11:12:14 INFO Executor: Finished task 27.0 in stage 1.0 (TID 197). 843 bytes result sent to driver
15/08/21 11:12:14 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 214, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:14 INFO Executor: Running task 44.0 in stage 1.0 (TID 214)
15/08/21 11:12:14 INFO Executor: Finished task 31.0 in stage 1.0 (TID 201). 843 bytes result sent to driver
15/08/21 11:12:14 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 215, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:14 INFO Executor: Running task 45.0 in stage 1.0 (TID 215)
15/08/21 11:12:14 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 197) in 29222 ms on localhost (29/200)
15/08/21 11:12:14 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 201) in 28916 ms on localhost (30/200)
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 2,665,155B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,036B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:14 INFO ColumnChunkPageWriteStore: written 833,825B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,543B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:12:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000017
15/08/21 11:12:15 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000017_0: Committed
15/08/21 11:12:15 INFO Executor: Finished task 17.0 in stage 1.0 (TID 187). 843 bytes result sent to driver
15/08/21 11:12:15 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 216, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:15 INFO Executor: Running task 46.0 in stage 1.0 (TID 216)
15/08/21 11:12:15 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 187) in 30348 ms on localhost (31/200)
15/08/21 11:12:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:12:15 INFO ColumnChunkPageWriteStore: written 2,666,072B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,953B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:15 INFO ColumnChunkPageWriteStore: written 833,917B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,635B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:12:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000028
15/08/21 11:12:15 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000028_0: Committed
15/08/21 11:12:15 INFO Executor: Finished task 28.0 in stage 1.0 (TID 198). 843 bytes result sent to driver
15/08/21 11:12:15 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:15 INFO Executor: Running task 47.0 in stage 1.0 (TID 217)
15/08/21 11:12:15 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 198) in 30163 ms on localhost (32/200)
15/08/21 11:12:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
15/08/21 11:12:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:42 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:42 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:42 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:42 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:42 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:42 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:45 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:45 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:45 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:45 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:45 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:45 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:45 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:45 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:46 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:46 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:46 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:46 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:46 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:46 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:46 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:46 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:46 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:46 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:47 INFO ColumnChunkPageWriteStore: written 2,664,725B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,606B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:47 INFO ColumnChunkPageWriteStore: written 833,737B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,455B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000035
15/08/21 11:12:48 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000035_0: Committed
15/08/21 11:12:48 INFO Executor: Finished task 35.0 in stage 1.0 (TID 205). 843 bytes result sent to driver
15/08/21 11:12:48 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:48 INFO Executor: Running task 48.0 in stage 1.0 (TID 218)
15/08/21 11:12:48 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 205) in 39212 ms on localhost (33/200)
15/08/21 11:12:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 2,665,950B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,831B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 833,179B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,897B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 2,671,004B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,885B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 834,687B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,405B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 2,664,808B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,689B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 832,830B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,548B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:12:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000032
15/08/21 11:12:48 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000032_0: Committed
15/08/21 11:12:48 INFO Executor: Finished task 32.0 in stage 1.0 (TID 202). 843 bytes result sent to driver
15/08/21 11:12:48 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000036
15/08/21 11:12:48 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000036_0: Committed
15/08/21 11:12:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000034
15/08/21 11:12:48 INFO Executor: Running task 49.0 in stage 1.0 (TID 219)
15/08/21 11:12:48 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000034_0: Committed
15/08/21 11:12:48 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 202) in 39886 ms on localhost (34/200)
15/08/21 11:12:48 INFO Executor: Finished task 36.0 in stage 1.0 (TID 206). 843 bytes result sent to driver
15/08/21 11:12:48 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:48 INFO Executor: Finished task 34.0 in stage 1.0 (TID 204). 843 bytes result sent to driver
15/08/21 11:12:48 INFO Executor: Running task 50.0 in stage 1.0 (TID 220)
15/08/21 11:12:48 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:48 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 206) in 39291 ms on localhost (35/200)
15/08/21 11:12:48 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 204) in 39729 ms on localhost (36/200)
15/08/21 11:12:48 INFO Executor: Running task 51.0 in stage 1.0 (TID 221)
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 2,664,648B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,529B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:48 INFO ColumnChunkPageWriteStore: written 832,636B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,354B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:12:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:12:48 INFO CodecConfig: Compression: GZIP
15/08/21 11:12:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:12:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:12:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:12:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:12:48 INFO ParquetOutputFormat: Validation is off
15/08/21 11:12:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:12:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:12:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000033
15/08/21 11:12:48 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000033_0: Committed
15/08/21 11:12:48 INFO Executor: Finished task 33.0 in stage 1.0 (TID 203). 843 bytes result sent to driver
15/08/21 11:12:48 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:48 INFO Executor: Running task 52.0 in stage 1.0 (TID 222)
15/08/21 11:12:48 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 203) in 40239 ms on localhost (37/200)
15/08/21 11:12:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 2,670,816B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,697B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 834,052B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,770B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000039
15/08/21 11:12:49 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000039_0: Committed
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 2,670,777B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,658B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 833,951B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,669B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:12:49 INFO Executor: Finished task 39.0 in stage 1.0 (TID 209). 843 bytes result sent to driver
15/08/21 11:12:49 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:49 INFO Executor: Running task 53.0 in stage 1.0 (TID 223)
15/08/21 11:12:49 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 209) in 35268 ms on localhost (38/200)
15/08/21 11:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000037
15/08/21 11:12:49 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000037_0: Committed
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:49 INFO Executor: Finished task 37.0 in stage 1.0 (TID 207). 843 bytes result sent to driver
15/08/21 11:12:49 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:49 INFO Executor: Running task 54.0 in stage 1.0 (TID 224)
15/08/21 11:12:49 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 207) in 40146 ms on localhost (39/200)
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 2,665,178B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,059B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 834,058B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,776B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:12:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000041
15/08/21 11:12:49 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000041_0: Committed
15/08/21 11:12:49 INFO Executor: Finished task 41.0 in stage 1.0 (TID 211). 843 bytes result sent to driver
15/08/21 11:12:49 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:49 INFO Executor: Running task 55.0 in stage 1.0 (TID 225)
15/08/21 11:12:49 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 211) in 35521 ms on localhost (40/200)
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 2,665,054B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,935B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 836,409B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 836,127B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:12:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 2,670,885B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,766B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 834,145B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,863B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000043
15/08/21 11:12:49 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000043_0: Committed
15/08/21 11:12:49 INFO Executor: Finished task 43.0 in stage 1.0 (TID 213). 843 bytes result sent to driver
15/08/21 11:12:49 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:49 INFO Executor: Running task 56.0 in stage 1.0 (TID 226)
15/08/21 11:12:49 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 213) in 35492 ms on localhost (41/200)
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 2,665,042B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,923B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:49 INFO ColumnChunkPageWriteStore: written 832,856B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,574B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000038
15/08/21 11:12:54 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000038_0: Committed
15/08/21 11:12:54 INFO Executor: Finished task 38.0 in stage 1.0 (TID 208). 843 bytes result sent to driver
15/08/21 11:12:54 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:54 INFO Executor: Running task 57.0 in stage 1.0 (TID 227)
15/08/21 11:12:54 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 208) in 45679 ms on localhost (42/200)
15/08/21 11:12:54 INFO ColumnChunkPageWriteStore: written 2,671,186B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,067B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:54 INFO ColumnChunkPageWriteStore: written 833,435B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,153B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000042
15/08/21 11:12:54 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000042_0: Committed
15/08/21 11:12:54 INFO Executor: Finished task 42.0 in stage 1.0 (TID 212). 843 bytes result sent to driver
15/08/21 11:12:54 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:54 INFO Executor: Running task 58.0 in stage 1.0 (TID 228)
15/08/21 11:12:54 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 212) in 40749 ms on localhost (43/200)
15/08/21 11:12:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:12:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000040
15/08/21 11:12:55 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000040_0: Committed
15/08/21 11:12:55 INFO Executor: Finished task 40.0 in stage 1.0 (TID 210). 843 bytes result sent to driver
15/08/21 11:12:55 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:55 INFO Executor: Running task 59.0 in stage 1.0 (TID 229)
15/08/21 11:12:55 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 210) in 41124 ms on localhost (44/200)
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:12:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:55 INFO ColumnChunkPageWriteStore: written 2,671,149B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,030B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:12:55 INFO ColumnChunkPageWriteStore: written 833,990B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,708B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:55 INFO ColumnChunkPageWriteStore: written 2,671,276B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,157B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:55 INFO ColumnChunkPageWriteStore: written 832,176B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,894B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:12:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000045
15/08/21 11:12:55 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000045_0: Committed
15/08/21 11:12:55 INFO Executor: Finished task 45.0 in stage 1.0 (TID 215). 843 bytes result sent to driver
15/08/21 11:12:55 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:55 INFO Executor: Running task 60.0 in stage 1.0 (TID 230)
15/08/21 11:12:55 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 215) in 40919 ms on localhost (45/200)
15/08/21 11:12:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000047
15/08/21 11:12:55 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000047_0: Committed
15/08/21 11:12:55 INFO Executor: Finished task 47.0 in stage 1.0 (TID 217). 843 bytes result sent to driver
15/08/21 11:12:55 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:55 INFO Executor: Running task 61.0 in stage 1.0 (TID 231)
15/08/21 11:12:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:12:55 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 217) in 40010 ms on localhost (46/200)
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:55 INFO ColumnChunkPageWriteStore: written 2,671,206B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,087B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:55 INFO ColumnChunkPageWriteStore: written 835,007B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,725B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:12:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000046
15/08/21 11:12:55 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000046_0: Committed
15/08/21 11:12:55 INFO Executor: Finished task 46.0 in stage 1.0 (TID 216). 843 bytes result sent to driver
15/08/21 11:12:55 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:55 INFO Executor: Running task 62.0 in stage 1.0 (TID 232)
15/08/21 11:12:55 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 216) in 40814 ms on localhost (47/200)
15/08/21 11:12:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:12:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:12:56 INFO ColumnChunkPageWriteStore: written 2,665,941B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,822B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:12:56 INFO ColumnChunkPageWriteStore: written 833,292B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,010B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:12:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211112_0001_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211112_0001_m_000044
15/08/21 11:12:56 INFO SparkHadoopMapRedUtil: attempt_201508211112_0001_m_000044_0: Committed
15/08/21 11:12:56 INFO Executor: Finished task 44.0 in stage 1.0 (TID 214). 843 bytes result sent to driver
15/08/21 11:12:56 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:12:56 INFO Executor: Running task 63.0 in stage 1.0 (TID 233)
15/08/21 11:12:56 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 214) in 41860 ms on localhost (48/200)
15/08/21 11:12:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:12:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:09 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:09 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:10 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:10 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:10 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:10 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:10 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:10 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:13:11 INFO ColumnChunkPageWriteStore: written 2,671,229B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,110B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:11 INFO ColumnChunkPageWriteStore: written 834,384B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,102B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:13:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:11 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:11 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:11 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:11 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000048
15/08/21 11:13:11 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000048_0: Committed
15/08/21 11:13:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:11 INFO Executor: Finished task 48.0 in stage 1.0 (TID 218). 843 bytes result sent to driver
15/08/21 11:13:11 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:11 INFO Executor: Running task 64.0 in stage 1.0 (TID 234)
15/08/21 11:13:11 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 218) in 23147 ms on localhost (49/200)
15/08/21 11:13:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:11 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:11 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:16 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:16 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:13:17 INFO ColumnChunkPageWriteStore: written 2,665,307B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,188B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:17 INFO ColumnChunkPageWriteStore: written 834,754B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,472B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:13:17 INFO ColumnChunkPageWriteStore: written 2,664,912B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,793B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO ColumnChunkPageWriteStore: written 835,880B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,598B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:13:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000050
15/08/21 11:13:17 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000050_0: Committed
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO Executor: Finished task 50.0 in stage 1.0 (TID 220). 843 bytes result sent to driver
15/08/21 11:13:17 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:17 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 220) in 29338 ms on localhost (50/200)
15/08/21 11:13:17 INFO Executor: Running task 65.0 in stage 1.0 (TID 235)
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ColumnChunkPageWriteStore: written 2,666,261B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,142B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:17 INFO ColumnChunkPageWriteStore: written 834,132B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,850B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000052
15/08/21 11:13:17 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000052_0: Committed
15/08/21 11:13:17 INFO Executor: Finished task 52.0 in stage 1.0 (TID 222). 843 bytes result sent to driver
15/08/21 11:13:17 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:17 INFO Executor: Running task 66.0 in stage 1.0 (TID 236)
15/08/21 11:13:17 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 222) in 29029 ms on localhost (51/200)
15/08/21 11:13:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000049
15/08/21 11:13:18 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000049_0: Committed
15/08/21 11:13:18 INFO Executor: Finished task 49.0 in stage 1.0 (TID 219). 843 bytes result sent to driver
15/08/21 11:13:18 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:18 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 219) in 29802 ms on localhost (52/200)
15/08/21 11:13:18 INFO Executor: Running task 67.0 in stage 1.0 (TID 237)
15/08/21 11:13:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:18 INFO ColumnChunkPageWriteStore: written 2,671,319B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,200B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:18 INFO ColumnChunkPageWriteStore: written 831,020B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,738B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000056
15/08/21 11:13:18 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000056_0: Committed
15/08/21 11:13:18 INFO Executor: Finished task 56.0 in stage 1.0 (TID 226). 843 bytes result sent to driver
15/08/21 11:13:18 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:18 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 226) in 28647 ms on localhost (53/200)
15/08/21 11:13:18 INFO Executor: Running task 68.0 in stage 1.0 (TID 238)
15/08/21 11:13:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:13:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:18 INFO ColumnChunkPageWriteStore: written 2,671,541B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,422B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:18 INFO ColumnChunkPageWriteStore: written 833,666B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,384B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:13:18 INFO ColumnChunkPageWriteStore: written 2,665,201B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,082B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:18 INFO ColumnChunkPageWriteStore: written 833,868B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,586B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:13:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000053
15/08/21 11:13:18 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000053_0: Committed
15/08/21 11:13:18 INFO Executor: Finished task 53.0 in stage 1.0 (TID 223). 843 bytes result sent to driver
15/08/21 11:13:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000051
15/08/21 11:13:18 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000051_0: Committed
15/08/21 11:13:18 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:18 INFO Executor: Running task 69.0 in stage 1.0 (TID 239)
15/08/21 11:13:18 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 223) in 29720 ms on localhost (54/200)
15/08/21 11:13:18 INFO Executor: Finished task 51.0 in stage 1.0 (TID 221). 843 bytes result sent to driver
15/08/21 11:13:18 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:18 INFO Executor: Running task 70.0 in stage 1.0 (TID 240)
15/08/21 11:13:18 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 221) in 30471 ms on localhost (55/200)
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:13:19 INFO ColumnChunkPageWriteStore: written 2,666,043B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,924B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:19 INFO ColumnChunkPageWriteStore: written 836,248B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,966B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:19 INFO ColumnChunkPageWriteStore: written 2,671,231B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,112B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:19 INFO ColumnChunkPageWriteStore: written 836,159B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,877B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:13:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000054
15/08/21 11:13:20 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000054_0: Committed
15/08/21 11:13:20 INFO Executor: Finished task 54.0 in stage 1.0 (TID 224). 843 bytes result sent to driver
15/08/21 11:13:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:13:20 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:20 INFO Executor: Running task 71.0 in stage 1.0 (TID 241)
15/08/21 11:13:20 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 224) in 30774 ms on localhost (56/200)
15/08/21 11:13:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000060
15/08/21 11:13:20 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000060_0: Committed
15/08/21 11:13:20 INFO Executor: Finished task 60.0 in stage 1.0 (TID 230). 843 bytes result sent to driver
15/08/21 11:13:20 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:20 INFO Executor: Running task 72.0 in stage 1.0 (TID 242)
15/08/21 11:13:20 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 230) in 24424 ms on localhost (57/200)
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 2,664,718B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,599B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 831,971B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,689B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000058
15/08/21 11:13:20 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000058_0: Committed
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 2,671,403B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,284B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 833,349B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,067B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:20 INFO Executor: Finished task 58.0 in stage 1.0 (TID 228). 843 bytes result sent to driver
15/08/21 11:13:20 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:20 INFO Executor: Running task 73.0 in stage 1.0 (TID 243)
15/08/21 11:13:20 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 228) in 25310 ms on localhost (58/200)
15/08/21 11:13:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:13:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:13:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000055
15/08/21 11:13:20 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000055_0: Committed
15/08/21 11:13:20 INFO Executor: Finished task 55.0 in stage 1.0 (TID 225). 843 bytes result sent to driver
15/08/21 11:13:20 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:20 INFO Executor: Running task 74.0 in stage 1.0 (TID 244)
15/08/21 11:13:20 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 225) in 30666 ms on localhost (59/200)
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 2,664,734B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,615B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 835,655B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,373B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 2,670,898B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,779B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:20 INFO ColumnChunkPageWriteStore: written 834,402B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,120B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:13:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000057
15/08/21 11:13:25 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000057_0: Committed
15/08/21 11:13:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:13:25 INFO Executor: Finished task 57.0 in stage 1.0 (TID 227). 843 bytes result sent to driver
15/08/21 11:13:25 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:25 INFO Executor: Running task 75.0 in stage 1.0 (TID 245)
15/08/21 11:13:25 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 227) in 30888 ms on localhost (60/200)
15/08/21 11:13:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000061
15/08/21 11:13:25 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000061_0: Committed
15/08/21 11:13:25 INFO Executor: Finished task 61.0 in stage 1.0 (TID 231). 843 bytes result sent to driver
15/08/21 11:13:25 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:25 INFO Executor: Running task 76.0 in stage 1.0 (TID 246)
15/08/21 11:13:25 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 231) in 30043 ms on localhost (61/200)
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:13:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:13:25 INFO ColumnChunkPageWriteStore: written 2,670,957B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,838B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:25 INFO ColumnChunkPageWriteStore: written 831,272B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,990B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:13:25 INFO ColumnChunkPageWriteStore: written 2,664,687B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,568B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:25 INFO ColumnChunkPageWriteStore: written 833,617B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,335B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:13:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000063
15/08/21 11:13:25 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000063_0: Committed
15/08/21 11:13:25 INFO Executor: Finished task 63.0 in stage 1.0 (TID 233). 843 bytes result sent to driver
15/08/21 11:13:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:25 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:25 INFO Executor: Running task 77.0 in stage 1.0 (TID 247)
15/08/21 11:13:25 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 233) in 29340 ms on localhost (62/200)
15/08/21 11:13:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000059
15/08/21 11:13:25 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000059_0: Committed
15/08/21 11:13:25 INFO Executor: Finished task 59.0 in stage 1.0 (TID 229). 843 bytes result sent to driver
15/08/21 11:13:25 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:25 INFO Executor: Running task 78.0 in stage 1.0 (TID 248)
15/08/21 11:13:25 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 229) in 30900 ms on localhost (63/200)
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:25 INFO ColumnChunkPageWriteStore: written 2,670,932B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,813B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:25 INFO ColumnChunkPageWriteStore: written 830,638B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,356B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000062
15/08/21 11:13:26 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000062_0: Committed
15/08/21 11:13:26 INFO Executor: Finished task 62.0 in stage 1.0 (TID 232). 843 bytes result sent to driver
15/08/21 11:13:26 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:26 INFO Executor: Running task 79.0 in stage 1.0 (TID 249)
15/08/21 11:13:26 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 232) in 30145 ms on localhost (64/200)
15/08/21 11:13:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:42 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:42 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:42 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:13:46 INFO ColumnChunkPageWriteStore: written 2,671,438B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,319B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:46 INFO ColumnChunkPageWriteStore: written 831,981B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,699B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:13:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000064
15/08/21 11:13:47 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000064_0: Committed
15/08/21 11:13:47 INFO Executor: Finished task 64.0 in stage 1.0 (TID 234). 843 bytes result sent to driver
15/08/21 11:13:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:47 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:47 INFO Executor: Running task 80.0 in stage 1.0 (TID 250)
15/08/21 11:13:47 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 234) in 35917 ms on localhost (65/200)
15/08/21 11:13:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:48 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:48 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:48 INFO ColumnChunkPageWriteStore: written 2,664,716B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,597B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:48 INFO ColumnChunkPageWriteStore: written 833,304B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,022B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:48 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:48 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000066
15/08/21 11:13:48 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000066_0: Committed
15/08/21 11:13:48 INFO ColumnChunkPageWriteStore: written 2,664,507B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,388B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:48 INFO ColumnChunkPageWriteStore: written 834,830B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,548B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:48 INFO Executor: Finished task 66.0 in stage 1.0 (TID 236). 843 bytes result sent to driver
15/08/21 11:13:48 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:48 INFO Executor: Running task 81.0 in stage 1.0 (TID 251)
15/08/21 11:13:48 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 236) in 31002 ms on localhost (66/200)
15/08/21 11:13:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:48 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:48 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000065
15/08/21 11:13:48 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000065_0: Committed
15/08/21 11:13:48 INFO Executor: Finished task 65.0 in stage 1.0 (TID 235). 843 bytes result sent to driver
15/08/21 11:13:48 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:48 INFO Executor: Running task 82.0 in stage 1.0 (TID 252)
15/08/21 11:13:48 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 235) in 31256 ms on localhost (67/200)
15/08/21 11:13:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:13:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:13:53 INFO ColumnChunkPageWriteStore: written 2,664,870B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,751B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:53 INFO ColumnChunkPageWriteStore: written 834,960B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,678B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:13:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000067
15/08/21 11:13:53 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000067_0: Committed
15/08/21 11:13:53 INFO Executor: Finished task 67.0 in stage 1.0 (TID 237). 843 bytes result sent to driver
15/08/21 11:13:53 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:53 INFO Executor: Running task 83.0 in stage 1.0 (TID 253)
15/08/21 11:13:53 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 237) in 35177 ms on localhost (68/200)
15/08/21 11:13:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:13:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:53 INFO ColumnChunkPageWriteStore: written 2,665,867B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,748B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:53 INFO ColumnChunkPageWriteStore: written 833,718B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,436B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:13:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000068
15/08/21 11:13:53 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000068_0: Committed
15/08/21 11:13:53 INFO Executor: Finished task 68.0 in stage 1.0 (TID 238). 843 bytes result sent to driver
15/08/21 11:13:53 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:53 INFO Executor: Running task 84.0 in stage 1.0 (TID 254)
15/08/21 11:13:53 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 238) in 35256 ms on localhost (69/200)
15/08/21 11:13:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:13:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:13:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:13:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:13:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:13:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:13:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:13:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:13:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:13:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:13:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:13:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:13:54 INFO ColumnChunkPageWriteStore: written 2,671,149B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,030B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:54 INFO ColumnChunkPageWriteStore: written 833,093B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,811B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:13:54 INFO ColumnChunkPageWriteStore: written 2,670,847B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,728B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:54 INFO ColumnChunkPageWriteStore: written 831,987B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,705B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:13:54 INFO ColumnChunkPageWriteStore: written 2,670,691B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,572B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 835,083B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,801B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000078
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000078_0: Committed
15/08/21 11:13:55 INFO Executor: Finished task 78.0 in stage 1.0 (TID 248). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 85.0 in stage 1.0 (TID 255)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 248) in 29124 ms on localhost (70/200)
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000069
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000069_0: Committed
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000070
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000070_0: Committed
15/08/21 11:13:55 INFO Executor: Finished task 69.0 in stage 1.0 (TID 239). 843 bytes result sent to driver
15/08/21 11:13:55 INFO Executor: Finished task 70.0 in stage 1.0 (TID 240). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 86.0 in stage 1.0 (TID 256)
15/08/21 11:13:55 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 87.0 in stage 1.0 (TID 257)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 240) in 36174 ms on localhost (71/200)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 239) in 36181 ms on localhost (72/200)
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 2,671,325B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,206B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 833,913B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,631B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:13:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:13:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000072
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000072_0: Committed
15/08/21 11:13:55 INFO Executor: Finished task 72.0 in stage 1.0 (TID 242). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 88.0 in stage 1.0 (TID 258)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 242) in 35387 ms on localhost (73/200)
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 2,664,859B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,740B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 2,664,740B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,621B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 831,489B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,207B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 835,568B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,286B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000075
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000075_0: Committed
15/08/21 11:13:55 INFO Executor: Finished task 75.0 in stage 1.0 (TID 245). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 89.0 in stage 1.0 (TID 259)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 245) in 29897 ms on localhost (74/200)
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000073
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000073_0: Committed
15/08/21 11:13:55 INFO Executor: Finished task 73.0 in stage 1.0 (TID 243). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 90.0 in stage 1.0 (TID 260)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 243) in 35419 ms on localhost (75/200)
15/08/21 11:13:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:13:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 2,671,148B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,029B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 833,406B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,124B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:13:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 2,671,144B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,025B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 832,813B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,531B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000077
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000077_0: Committed
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 2,665,533B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,414B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO Executor: Finished task 77.0 in stage 1.0 (TID 247). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 91.0 in stage 1.0 (TID 261)
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 833,356B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,074B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:13:55 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 247) in 29928 ms on localhost (76/200)
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 2,671,025B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,906B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:55 INFO ColumnChunkPageWriteStore: written 832,782B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,500B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000079
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000079_0: Committed
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000076
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000076_0: Committed
15/08/21 11:13:55 INFO Executor: Finished task 79.0 in stage 1.0 (TID 249). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 92.0 in stage 1.0 (TID 262)
15/08/21 11:13:55 INFO Executor: Finished task 76.0 in stage 1.0 (TID 246). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 93.0 in stage 1.0 (TID 263)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 249) in 29877 ms on localhost (77/200)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 246) in 30246 ms on localhost (78/200)
15/08/21 11:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000071
15/08/21 11:13:55 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000071_0: Committed
15/08/21 11:13:55 INFO Executor: Finished task 71.0 in stage 1.0 (TID 241). 843 bytes result sent to driver
15/08/21 11:13:55 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:55 INFO Executor: Running task 94.0 in stage 1.0 (TID 264)
15/08/21 11:13:55 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 241) in 35933 ms on localhost (79/200)
15/08/21 11:13:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:13:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:13:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:13:56 INFO ColumnChunkPageWriteStore: written 2,664,772B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,653B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:13:56 INFO ColumnChunkPageWriteStore: written 834,014B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,732B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:13:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508211113_0001_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211113_0001_m_000074
15/08/21 11:13:56 INFO SparkHadoopMapRedUtil: attempt_201508211113_0001_m_000074_0: Committed
15/08/21 11:13:56 INFO Executor: Finished task 74.0 in stage 1.0 (TID 244). 843 bytes result sent to driver
15/08/21 11:13:56 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:13:56 INFO Executor: Running task 95.0 in stage 1.0 (TID 265)
15/08/21 11:14:01 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 244) in 40929 ms on localhost (80/200)
15/08/21 11:14:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:08 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:08 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:09 INFO ColumnChunkPageWriteStore: written 2,671,198B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,079B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:09 INFO ColumnChunkPageWriteStore: written 836,069B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,787B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000080
15/08/21 11:14:09 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000080_0: Committed
15/08/21 11:14:09 INFO Executor: Finished task 80.0 in stage 1.0 (TID 250). 843 bytes result sent to driver
15/08/21 11:14:09 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:09 INFO Executor: Running task 96.0 in stage 1.0 (TID 266)
15/08/21 11:14:09 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 250) in 22883 ms on localhost (81/200)
15/08/21 11:14:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:09 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:09 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:10 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:10 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:14:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 2,665,242B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,123B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 834,874B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,592B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 2,666,194B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,075B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 831,501B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,219B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:14:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:16 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:16 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000082
15/08/21 11:14:16 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000082_0: Committed
15/08/21 11:14:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000084
15/08/21 11:14:16 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000084_0: Committed
15/08/21 11:14:16 INFO Executor: Finished task 84.0 in stage 1.0 (TID 254). 843 bytes result sent to driver
15/08/21 11:14:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:16 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:16 INFO Executor: Running task 97.0 in stage 1.0 (TID 267)
15/08/21 11:14:16 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 254) in 22420 ms on localhost (82/200)
15/08/21 11:14:16 INFO Executor: Finished task 82.0 in stage 1.0 (TID 252). 843 bytes result sent to driver
15/08/21 11:14:16 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:16 INFO Executor: Running task 98.0 in stage 1.0 (TID 268)
15/08/21 11:14:16 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 252) in 27236 ms on localhost (83/200)
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 2,665,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,926B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 834,589B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,307B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000081
15/08/21 11:14:16 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000081_0: Committed
15/08/21 11:14:16 INFO Executor: Finished task 81.0 in stage 1.0 (TID 251). 843 bytes result sent to driver
15/08/21 11:14:16 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:16 INFO Executor: Running task 99.0 in stage 1.0 (TID 269)
15/08/21 11:14:16 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 251) in 27539 ms on localhost (84/200)
15/08/21 11:14:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:16 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:16 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:16 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:16 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 2,665,382B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,263B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:16 INFO ColumnChunkPageWriteStore: written 832,836B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,554B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:14:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:16 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:16 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000083
15/08/21 11:14:16 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000083_0: Committed
15/08/21 11:14:16 INFO Executor: Finished task 83.0 in stage 1.0 (TID 253). 843 bytes result sent to driver
15/08/21 11:14:16 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:16 INFO Executor: Running task 100.0 in stage 1.0 (TID 270)
15/08/21 11:14:16 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 253) in 23400 ms on localhost (85/200)
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:17 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:17 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:21 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:21 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:22 INFO ColumnChunkPageWriteStore: written 2,671,098B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,979B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:22 INFO ColumnChunkPageWriteStore: written 832,460B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,178B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000085
15/08/21 11:14:22 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000085_0: Committed
15/08/21 11:14:22 INFO Executor: Finished task 85.0 in stage 1.0 (TID 255). 843 bytes result sent to driver
15/08/21 11:14:22 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:22 INFO Executor: Running task 101.0 in stage 1.0 (TID 271)
15/08/21 11:14:22 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 255) in 27516 ms on localhost (86/200)
15/08/21 11:14:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:14:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:22 INFO ColumnChunkPageWriteStore: written 2,671,429B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,310B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:22 INFO ColumnChunkPageWriteStore: written 833,569B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,287B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000087
15/08/21 11:14:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:14:22 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000087_0: Committed
15/08/21 11:14:22 INFO Executor: Finished task 87.0 in stage 1.0 (TID 257). 843 bytes result sent to driver
15/08/21 11:14:22 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:22 INFO Executor: Running task 102.0 in stage 1.0 (TID 272)
15/08/21 11:14:22 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 257) in 27799 ms on localhost (87/200)
15/08/21 11:14:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:22 INFO ColumnChunkPageWriteStore: written 2,671,438B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,319B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:22 INFO ColumnChunkPageWriteStore: written 834,780B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,498B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:14:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000086
15/08/21 11:14:22 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000086_0: Committed
15/08/21 11:14:22 INFO Executor: Finished task 86.0 in stage 1.0 (TID 256). 843 bytes result sent to driver
15/08/21 11:14:22 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:22 INFO Executor: Running task 103.0 in stage 1.0 (TID 273)
15/08/21 11:14:22 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 256) in 27944 ms on localhost (88/200)
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:14:23 INFO ColumnChunkPageWriteStore: written 2,664,909B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,790B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:23 INFO ColumnChunkPageWriteStore: written 830,715B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,433B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:14:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000090
15/08/21 11:14:23 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000090_0: Committed
15/08/21 11:14:23 INFO Executor: Finished task 90.0 in stage 1.0 (TID 260). 843 bytes result sent to driver
15/08/21 11:14:23 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:23 INFO Executor: Running task 104.0 in stage 1.0 (TID 274)
15/08/21 11:14:23 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 260) in 27791 ms on localhost (89/200)
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 11:14:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,673
15/08/21 11:14:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,585
15/08/21 11:14:23 INFO ColumnChunkPageWriteStore: written 2,671,325B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,206B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:23 INFO ColumnChunkPageWriteStore: written 835,794B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,512B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 319 entries, 2,552B raw, 319B comp}
15/08/21 11:14:23 INFO ColumnChunkPageWriteStore: written 2,671,043B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,924B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:23 INFO ColumnChunkPageWriteStore: written 834,855B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,573B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 308 entries, 2,464B raw, 308B comp}
15/08/21 11:14:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000088
15/08/21 11:14:23 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000088_0: Committed
15/08/21 11:14:23 INFO Executor: Finished task 88.0 in stage 1.0 (TID 258). 843 bytes result sent to driver
15/08/21 11:14:23 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:23 INFO Executor: Running task 105.0 in stage 1.0 (TID 275)
15/08/21 11:14:23 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 258) in 28371 ms on localhost (90/200)
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000093
15/08/21 11:14:23 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000093_0: Committed
15/08/21 11:14:23 INFO Executor: Finished task 93.0 in stage 1.0 (TID 263). 843 bytes result sent to driver
15/08/21 11:14:23 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:23 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 263) in 27976 ms on localhost (91/200)
15/08/21 11:14:23 INFO Executor: Running task 106.0 in stage 1.0 (TID 276)
15/08/21 11:14:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:14:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 2,665,894B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,775B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 833,016B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,734B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 2,665,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,926B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 832,530B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,248B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:14:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:14:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:14:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000092
15/08/21 11:14:24 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000092_0: Committed
15/08/21 11:14:24 INFO Executor: Finished task 92.0 in stage 1.0 (TID 262). 843 bytes result sent to driver
15/08/21 11:14:24 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:24 INFO Executor: Running task 107.0 in stage 1.0 (TID 277)
15/08/21 11:14:24 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 262) in 28225 ms on localhost (92/200)
15/08/21 11:14:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000091
15/08/21 11:14:24 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000091_0: Committed
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 2,670,977B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,858B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 832,600B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,318B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:14:24 INFO Executor: Finished task 91.0 in stage 1.0 (TID 261). 843 bytes result sent to driver
15/08/21 11:14:24 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:24 INFO Executor: Running task 108.0 in stage 1.0 (TID 278)
15/08/21 11:14:24 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 261) in 28383 ms on localhost (93/200)
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 2,670,798B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,679B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 833,138B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,856B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000094
15/08/21 11:14:24 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000094_0: Committed
15/08/21 11:14:24 INFO Executor: Finished task 94.0 in stage 1.0 (TID 264). 843 bytes result sent to driver
15/08/21 11:14:24 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:24 INFO Executor: Running task 109.0 in stage 1.0 (TID 279)
15/08/21 11:14:24 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 264) in 28357 ms on localhost (94/200)
15/08/21 11:14:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 11:14:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000095
15/08/21 11:14:24 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000095_0: Committed
15/08/21 11:14:24 INFO Executor: Finished task 95.0 in stage 1.0 (TID 265). 843 bytes result sent to driver
15/08/21 11:14:24 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:24 INFO Executor: Running task 110.0 in stage 1.0 (TID 280)
15/08/21 11:14:24 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 265) in 27903 ms on localhost (95/200)
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 2,665,088B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,969B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:24 INFO ColumnChunkPageWriteStore: written 834,046B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,764B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 11:14:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000089
15/08/21 11:14:24 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000089_0: Committed
15/08/21 11:14:24 INFO Executor: Finished task 89.0 in stage 1.0 (TID 259). 843 bytes result sent to driver
15/08/21 11:14:24 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:24 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 259) in 29052 ms on localhost (96/200)
15/08/21 11:14:24 INFO Executor: Running task 111.0 in stage 1.0 (TID 281)
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:14:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:31 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:31 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:33 INFO ColumnChunkPageWriteStore: written 2,671,367B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,248B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:33 INFO ColumnChunkPageWriteStore: written 835,262B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,980B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000096
15/08/21 11:14:33 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000096_0: Committed
15/08/21 11:14:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:33 INFO Executor: Finished task 96.0 in stage 1.0 (TID 266). 843 bytes result sent to driver
15/08/21 11:14:33 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:33 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:33 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:33 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 266) in 23403 ms on localhost (97/200)
15/08/21 11:14:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:33 INFO Executor: Running task 112.0 in stage 1.0 (TID 282)
15/08/21 11:14:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:48 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:48 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:48 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:48 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:49 INFO ColumnChunkPageWriteStore: written 2,664,905B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,786B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:49 INFO ColumnChunkPageWriteStore: written 834,064B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,782B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,673
15/08/21 11:14:49 INFO ColumnChunkPageWriteStore: written 2,665,205B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,086B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:49 INFO ColumnChunkPageWriteStore: written 833,315B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,033B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 319 entries, 2,552B raw, 319B comp}
15/08/21 11:14:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000099
15/08/21 11:14:49 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000099_0: Committed
15/08/21 11:14:49 INFO Executor: Finished task 99.0 in stage 1.0 (TID 269). 843 bytes result sent to driver
15/08/21 11:14:49 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:49 INFO Executor: Running task 113.0 in stage 1.0 (TID 283)
15/08/21 11:14:49 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 269) in 33137 ms on localhost (98/200)
15/08/21 11:14:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000097
15/08/21 11:14:49 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000097_0: Committed
15/08/21 11:14:49 INFO Executor: Finished task 97.0 in stage 1.0 (TID 267). 843 bytes result sent to driver
15/08/21 11:14:49 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:49 INFO Executor: Running task 114.0 in stage 1.0 (TID 284)
15/08/21 11:14:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:49 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 267) in 33394 ms on localhost (99/200)
15/08/21 11:14:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:14:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:14:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:14:50 INFO ColumnChunkPageWriteStore: written 2,664,831B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,712B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:50 INFO ColumnChunkPageWriteStore: written 834,250B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,968B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:14:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000098
15/08/21 11:14:50 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000098_0: Committed
15/08/21 11:14:50 INFO Executor: Finished task 98.0 in stage 1.0 (TID 268). 843 bytes result sent to driver
15/08/21 11:14:50 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 285, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:50 INFO Executor: Running task 115.0 in stage 1.0 (TID 285)
15/08/21 11:14:50 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 268) in 34000 ms on localhost (100/200)
15/08/21 11:14:50 INFO ColumnChunkPageWriteStore: written 2,665,707B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,588B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:50 INFO ColumnChunkPageWriteStore: written 833,973B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,691B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:14:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000100
15/08/21 11:14:52 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000100_0: Committed
15/08/21 11:14:52 INFO Executor: Finished task 100.0 in stage 1.0 (TID 270). 843 bytes result sent to driver
15/08/21 11:14:52 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 286, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:52 INFO Executor: Running task 116.0 in stage 1.0 (TID 286)
15/08/21 11:14:52 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 270) in 35451 ms on localhost (101/200)
15/08/21 11:14:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:52 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:52 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:52 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:52 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:52 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:52 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:53 INFO ColumnChunkPageWriteStore: written 2,671,228B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,109B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:53 INFO ColumnChunkPageWriteStore: written 837,388B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 837,106B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000101
15/08/21 11:14:53 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000101_0: Committed
15/08/21 11:14:53 INFO Executor: Finished task 101.0 in stage 1.0 (TID 271). 843 bytes result sent to driver
15/08/21 11:14:53 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 287, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:53 INFO Executor: Running task 117.0 in stage 1.0 (TID 287)
15/08/21 11:14:53 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 271) in 31452 ms on localhost (102/200)
15/08/21 11:14:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:14:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:14:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:14:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:14:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:14:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:14:54 INFO ColumnChunkPageWriteStore: written 2,670,961B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,842B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:54 INFO ColumnChunkPageWriteStore: written 834,751B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,469B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:14:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:14:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:14:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000102
15/08/21 11:14:54 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000102_0: Committed
15/08/21 11:14:54 INFO Executor: Finished task 102.0 in stage 1.0 (TID 272). 843 bytes result sent to driver
15/08/21 11:14:54 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 288, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:54 INFO Executor: Running task 118.0 in stage 1.0 (TID 288)
15/08/21 11:14:54 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 272) in 31621 ms on localhost (103/200)
15/08/21 11:14:54 INFO ColumnChunkPageWriteStore: written 2,671,439B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,320B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:54 INFO ColumnChunkPageWriteStore: written 834,740B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,458B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:14:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000104
15/08/21 11:14:54 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000104_0: Committed
15/08/21 11:14:54 INFO Executor: Finished task 104.0 in stage 1.0 (TID 274). 843 bytes result sent to driver
15/08/21 11:14:54 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 289, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:54 INFO Executor: Running task 119.0 in stage 1.0 (TID 289)
15/08/21 11:14:54 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 274) in 31242 ms on localhost (104/200)
15/08/21 11:14:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:14:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:14:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:14:59 INFO ColumnChunkPageWriteStore: written 2,671,218B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,099B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:59 INFO ColumnChunkPageWriteStore: written 831,764B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,482B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:14:59 INFO ColumnChunkPageWriteStore: written 2,665,021B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,902B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:59 INFO ColumnChunkPageWriteStore: written 832,356B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,074B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:14:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000103
15/08/21 11:14:59 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000103_0: Committed
15/08/21 11:14:59 INFO Executor: Finished task 103.0 in stage 1.0 (TID 273). 843 bytes result sent to driver
15/08/21 11:14:59 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 290, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:59 INFO Executor: Running task 120.0 in stage 1.0 (TID 290)
15/08/21 11:14:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:14:59 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 273) in 36925 ms on localhost (105/200)
15/08/21 11:14:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000106
15/08/21 11:14:59 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000106_0: Committed
15/08/21 11:14:59 INFO Executor: Finished task 106.0 in stage 1.0 (TID 276). 843 bytes result sent to driver
15/08/21 11:14:59 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 291, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:14:59 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 276) in 36016 ms on localhost (106/200)
15/08/21 11:14:59 INFO Executor: Running task 121.0 in stage 1.0 (TID 291)
15/08/21 11:14:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:14:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:14:59 INFO ColumnChunkPageWriteStore: written 2,664,933B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,814B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:14:59 INFO ColumnChunkPageWriteStore: written 834,256B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,974B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000105
15/08/21 11:15:00 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000105_0: Committed
15/08/21 11:15:00 INFO Executor: Finished task 105.0 in stage 1.0 (TID 275). 843 bytes result sent to driver
15/08/21 11:15:00 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 292, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:00 INFO Executor: Running task 122.0 in stage 1.0 (TID 292)
15/08/21 11:15:00 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 275) in 36347 ms on localhost (107/200)
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 11:15:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:15:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:15:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 2,666,321B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,202B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 833,519B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,237B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 2,671,095B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,976B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 830,535B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,253B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 2,665,132B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,013B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 833,912B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,630B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:15:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000108
15/08/21 11:15:00 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000108_0: Committed
15/08/21 11:15:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000109
15/08/21 11:15:00 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000109_0: Committed
15/08/21 11:15:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000107
15/08/21 11:15:00 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000107_0: Committed
15/08/21 11:15:00 INFO Executor: Finished task 108.0 in stage 1.0 (TID 278). 843 bytes result sent to driver
15/08/21 11:15:00 INFO Executor: Finished task 109.0 in stage 1.0 (TID 279). 843 bytes result sent to driver
15/08/21 11:15:00 INFO Executor: Finished task 107.0 in stage 1.0 (TID 277). 843 bytes result sent to driver
15/08/21 11:15:00 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 293, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:00 INFO Executor: Running task 123.0 in stage 1.0 (TID 293)
15/08/21 11:15:00 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 294, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:00 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 295, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:00 INFO Executor: Running task 125.0 in stage 1.0 (TID 295)
15/08/21 11:15:00 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 278) in 36595 ms on localhost (108/200)
15/08/21 11:15:00 INFO Executor: Running task 124.0 in stage 1.0 (TID 294)
15/08/21 11:15:00 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 279) in 36481 ms on localhost (109/200)
15/08/21 11:15:00 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 277) in 36639 ms on localhost (110/200)
15/08/21 11:15:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:15:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 2,671,480B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,361B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:00 INFO ColumnChunkPageWriteStore: written 834,612B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,330B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:15:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000111
15/08/21 11:15:00 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000111_0: Committed
15/08/21 11:15:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:15:00 INFO Executor: Finished task 111.0 in stage 1.0 (TID 281). 843 bytes result sent to driver
15/08/21 11:15:00 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 296, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:00 INFO Executor: Running task 126.0 in stage 1.0 (TID 296)
15/08/21 11:15:00 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 281) in 36371 ms on localhost (111/200)
15/08/21 11:15:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:01 INFO ColumnChunkPageWriteStore: written 2,671,231B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,112B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:01 INFO ColumnChunkPageWriteStore: written 834,861B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,579B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:15:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211114_0001_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211114_0001_m_000110
15/08/21 11:15:01 INFO SparkHadoopMapRedUtil: attempt_201508211114_0001_m_000110_0: Committed
15/08/21 11:15:01 INFO Executor: Finished task 110.0 in stage 1.0 (TID 280). 843 bytes result sent to driver
15/08/21 11:15:01 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 297, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:01 INFO Executor: Running task 127.0 in stage 1.0 (TID 297)
15/08/21 11:15:01 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 280) in 36834 ms on localhost (112/200)
15/08/21 11:15:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:15:07 INFO ColumnChunkPageWriteStore: written 2,671,213B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,094B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:07 INFO ColumnChunkPageWriteStore: written 833,615B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,333B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:15:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000112
15/08/21 11:15:07 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000112_0: Committed
15/08/21 11:15:07 INFO Executor: Finished task 112.0 in stage 1.0 (TID 282). 843 bytes result sent to driver
15/08/21 11:15:07 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 298, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:07 INFO Executor: Running task 128.0 in stage 1.0 (TID 298)
15/08/21 11:15:07 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 282) in 33848 ms on localhost (113/200)
15/08/21 11:15:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:08 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:08 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:08 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:08 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:15:09 INFO ColumnChunkPageWriteStore: written 2,665,084B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,965B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:09 INFO ColumnChunkPageWriteStore: written 834,882B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,600B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:15:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:15:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000114
15/08/21 11:15:09 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000114_0: Committed
15/08/21 11:15:09 INFO Executor: Finished task 114.0 in stage 1.0 (TID 284). 843 bytes result sent to driver
15/08/21 11:15:09 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 299, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:09 INFO Executor: Running task 129.0 in stage 1.0 (TID 299)
15/08/21 11:15:09 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 284) in 19716 ms on localhost (114/200)
15/08/21 11:15:09 INFO ColumnChunkPageWriteStore: written 2,665,196B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,077B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:09 INFO ColumnChunkPageWriteStore: written 835,647B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,365B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:15:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000113
15/08/21 11:15:09 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000113_0: Committed
15/08/21 11:15:09 INFO Executor: Finished task 113.0 in stage 1.0 (TID 283). 843 bytes result sent to driver
15/08/21 11:15:09 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 300, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:09 INFO Executor: Running task 130.0 in stage 1.0 (TID 300)
15/08/21 11:15:09 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 283) in 19907 ms on localhost (115/200)
15/08/21 11:15:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:15:14 INFO ColumnChunkPageWriteStore: written 2,665,047B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,928B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:14 INFO ColumnChunkPageWriteStore: written 834,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,713B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:15:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:15:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000115
15/08/21 11:15:14 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000115_0: Committed
15/08/21 11:15:14 INFO Executor: Finished task 115.0 in stage 1.0 (TID 285). 843 bytes result sent to driver
15/08/21 11:15:14 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 301, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:14 INFO Executor: Running task 131.0 in stage 1.0 (TID 301)
15/08/21 11:15:14 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 285) in 24467 ms on localhost (116/200)
15/08/21 11:15:14 INFO ColumnChunkPageWriteStore: written 2,666,031B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,912B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:14 INFO ColumnChunkPageWriteStore: written 837,303B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 837,021B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:15:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000116
15/08/21 11:15:14 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000116_0: Committed
15/08/21 11:15:14 INFO Executor: Finished task 116.0 in stage 1.0 (TID 286). 843 bytes result sent to driver
15/08/21 11:15:14 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 302, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:14 INFO Executor: Running task 132.0 in stage 1.0 (TID 302)
15/08/21 11:15:14 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 286) in 22635 ms on localhost (117/200)
15/08/21 11:15:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:15 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:15 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:15 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:15 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:15:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:20 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:20 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:20 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:20 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:20 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:20 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:20 INFO ColumnChunkPageWriteStore: written 2,671,112B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,993B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:20 INFO ColumnChunkPageWriteStore: written 832,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,713B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:15:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000117
15/08/21 11:15:20 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000117_0: Committed
15/08/21 11:15:20 INFO Executor: Finished task 117.0 in stage 1.0 (TID 287). 843 bytes result sent to driver
15/08/21 11:15:20 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 303, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:20 INFO Executor: Running task 133.0 in stage 1.0 (TID 303)
15/08/21 11:15:20 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 287) in 26946 ms on localhost (118/200)
15/08/21 11:15:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:21 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:21 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:15:21 INFO ColumnChunkPageWriteStore: written 2,671,269B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,150B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:21 INFO ColumnChunkPageWriteStore: written 834,770B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,488B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:15:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000118
15/08/21 11:15:21 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000118_0: Committed
15/08/21 11:15:21 INFO Executor: Finished task 118.0 in stage 1.0 (TID 288). 843 bytes result sent to driver
15/08/21 11:15:21 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 304, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:21 INFO Executor: Running task 134.0 in stage 1.0 (TID 304)
15/08/21 11:15:21 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 288) in 27266 ms on localhost (119/200)
15/08/21 11:15:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:21 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:21 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:21 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:21 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:15:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:22 INFO ColumnChunkPageWriteStore: written 2,671,208B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,089B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:22 INFO ColumnChunkPageWriteStore: written 832,665B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,383B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:15:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000120
15/08/21 11:15:22 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000120_0: Committed
15/08/21 11:15:22 INFO Executor: Finished task 120.0 in stage 1.0 (TID 290). 843 bytes result sent to driver
15/08/21 11:15:22 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 305, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:22 INFO Executor: Running task 135.0 in stage 1.0 (TID 305)
15/08/21 11:15:22 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 290) in 22659 ms on localhost (120/200)
15/08/21 11:15:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:15:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:15:22 INFO ColumnChunkPageWriteStore: written 2,671,005B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,886B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:22 INFO ColumnChunkPageWriteStore: written 834,168B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,886B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:15:22 INFO ColumnChunkPageWriteStore: written 2,665,084B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,965B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:22 INFO ColumnChunkPageWriteStore: written 836,016B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,734B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:15:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000119
15/08/21 11:15:22 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000119_0: Committed
15/08/21 11:15:22 INFO Executor: Finished task 119.0 in stage 1.0 (TID 289). 843 bytes result sent to driver
15/08/21 11:15:22 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 306, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:22 INFO Executor: Running task 136.0 in stage 1.0 (TID 306)
15/08/21 11:15:22 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 289) in 28357 ms on localhost (121/200)
15/08/21 11:15:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000121
15/08/21 11:15:23 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000121_0: Committed
15/08/21 11:15:23 INFO Executor: Finished task 121.0 in stage 1.0 (TID 291). 843 bytes result sent to driver
15/08/21 11:15:23 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 307, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:23 INFO Executor: Running task 137.0 in stage 1.0 (TID 307)
15/08/21 11:15:23 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 291) in 23483 ms on localhost (122/200)
15/08/21 11:15:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:15:23 INFO ColumnChunkPageWriteStore: written 2,664,913B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,794B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:23 INFO ColumnChunkPageWriteStore: written 833,998B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,716B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:15:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:15:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000122
15/08/21 11:15:23 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000122_0: Committed
15/08/21 11:15:23 INFO Executor: Finished task 122.0 in stage 1.0 (TID 292). 843 bytes result sent to driver
15/08/21 11:15:23 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 308, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:23 INFO Executor: Running task 138.0 in stage 1.0 (TID 308)
15/08/21 11:15:23 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 292) in 23653 ms on localhost (123/200)
15/08/21 11:15:23 INFO ColumnChunkPageWriteStore: written 2,671,132B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,013B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:23 INFO ColumnChunkPageWriteStore: written 833,511B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,229B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:15:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:15:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000125
15/08/21 11:15:23 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000125_0: Committed
15/08/21 11:15:23 INFO Executor: Finished task 125.0 in stage 1.0 (TID 295). 843 bytes result sent to driver
15/08/21 11:15:23 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 309, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:23 INFO Executor: Running task 139.0 in stage 1.0 (TID 309)
15/08/21 11:15:23 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 295) in 23143 ms on localhost (124/200)
15/08/21 11:15:28 INFO ColumnChunkPageWriteStore: written 2,671,421B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,302B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:28 INFO ColumnChunkPageWriteStore: written 832,191B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,909B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:15:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000127
15/08/21 11:15:29 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000127_0: Committed
15/08/21 11:15:29 INFO Executor: Finished task 127.0 in stage 1.0 (TID 297). 843 bytes result sent to driver
15/08/21 11:15:29 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 310, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:29 INFO Executor: Running task 140.0 in stage 1.0 (TID 310)
15/08/21 11:15:29 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 297) in 27884 ms on localhost (125/200)
15/08/21 11:15:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:15:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:15:29 INFO ColumnChunkPageWriteStore: written 2,665,196B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,077B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:29 INFO ColumnChunkPageWriteStore: written 834,711B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,429B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:15:29 INFO ColumnChunkPageWriteStore: written 2,671,062B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,943B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:29 INFO ColumnChunkPageWriteStore: written 834,555B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,273B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:15:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000123
15/08/21 11:15:29 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000123_0: Committed
15/08/21 11:15:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000126
15/08/21 11:15:29 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000126_0: Committed
15/08/21 11:15:29 INFO Executor: Finished task 123.0 in stage 1.0 (TID 293). 843 bytes result sent to driver
15/08/21 11:15:29 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 311, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:29 INFO Executor: Running task 141.0 in stage 1.0 (TID 311)
15/08/21 11:15:29 INFO Executor: Finished task 126.0 in stage 1.0 (TID 296). 843 bytes result sent to driver
15/08/21 11:15:29 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 312, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:29 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 293) in 29074 ms on localhost (126/200)
15/08/21 11:15:29 INFO Executor: Running task 142.0 in stage 1.0 (TID 312)
15/08/21 11:15:29 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 296) in 28879 ms on localhost (127/200)
15/08/21 11:15:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:15:30 INFO ColumnChunkPageWriteStore: written 2,665,944B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,825B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:30 INFO ColumnChunkPageWriteStore: written 835,183B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,901B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000124
15/08/21 11:15:30 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000124_0: Committed
15/08/21 11:15:30 INFO Executor: Finished task 124.0 in stage 1.0 (TID 294). 843 bytes result sent to driver
15/08/21 11:15:30 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 313, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:30 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 294) in 29797 ms on localhost (128/200)
15/08/21 11:15:30 INFO Executor: Running task 143.0 in stage 1.0 (TID 313)
15/08/21 11:15:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:31 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:31 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:31 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:31 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:15:31 INFO ColumnChunkPageWriteStore: written 2,671,683B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,564B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:31 INFO ColumnChunkPageWriteStore: written 834,278B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,996B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:15:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000128
15/08/21 11:15:31 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000128_0: Committed
15/08/21 11:15:31 INFO Executor: Finished task 128.0 in stage 1.0 (TID 298). 843 bytes result sent to driver
15/08/21 11:15:31 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 314, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:31 INFO Executor: Running task 144.0 in stage 1.0 (TID 314)
15/08/21 11:15:31 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 298) in 24382 ms on localhost (129/200)
15/08/21 11:15:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:15:48 INFO ColumnChunkPageWriteStore: written 2,664,800B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,681B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:48 INFO ColumnChunkPageWriteStore: written 834,840B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,558B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:15:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000130
15/08/21 11:15:48 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000130_0: Committed
15/08/21 11:15:48 INFO Executor: Finished task 130.0 in stage 1.0 (TID 300). 843 bytes result sent to driver
15/08/21 11:15:48 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 315, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:48 INFO Executor: Running task 145.0 in stage 1.0 (TID 315)
15/08/21 11:15:48 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 300) in 39046 ms on localhost (130/200)
15/08/21 11:15:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:15:48 INFO ColumnChunkPageWriteStore: written 2,665,074B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,955B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:48 INFO ColumnChunkPageWriteStore: written 831,680B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,398B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:15:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000129
15/08/21 11:15:48 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000129_0: Committed
15/08/21 11:15:48 INFO Executor: Finished task 129.0 in stage 1.0 (TID 299). 843 bytes result sent to driver
15/08/21 11:15:48 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 316, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:48 INFO Executor: Running task 146.0 in stage 1.0 (TID 316)
15/08/21 11:15:48 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 299) in 39414 ms on localhost (131/200)
15/08/21 11:15:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:48 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:48 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:49 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:49 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:15:50 INFO ColumnChunkPageWriteStore: written 2,665,010B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,891B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:50 INFO ColumnChunkPageWriteStore: written 835,357B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,075B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:15:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000131
15/08/21 11:15:50 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000131_0: Committed
15/08/21 11:15:50 INFO Executor: Finished task 131.0 in stage 1.0 (TID 301). 843 bytes result sent to driver
15/08/21 11:15:50 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 317, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:50 INFO Executor: Running task 147.0 in stage 1.0 (TID 317)
15/08/21 11:15:50 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 301) in 35443 ms on localhost (132/200)
15/08/21 11:15:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:52 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:52 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 11:15:53 INFO ColumnChunkPageWriteStore: written 2,665,953B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,834B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:53 INFO ColumnChunkPageWriteStore: written 835,522B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,240B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 11:15:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000132
15/08/21 11:15:53 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000132_0: Committed
15/08/21 11:15:53 INFO Executor: Finished task 132.0 in stage 1.0 (TID 302). 843 bytes result sent to driver
15/08/21 11:15:53 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 318, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:53 INFO Executor: Running task 148.0 in stage 1.0 (TID 318)
15/08/21 11:15:53 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 302) in 38715 ms on localhost (133/200)
15/08/21 11:15:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:15:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:15:54 INFO ColumnChunkPageWriteStore: written 2,671,115B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,996B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:54 INFO ColumnChunkPageWriteStore: written 833,766B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,484B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:15:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000133
15/08/21 11:15:54 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000133_0: Committed
15/08/21 11:15:54 INFO Executor: Finished task 133.0 in stage 1.0 (TID 303). 843 bytes result sent to driver
15/08/21 11:15:54 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 319, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:54 INFO Executor: Running task 149.0 in stage 1.0 (TID 319)
15/08/21 11:15:54 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 303) in 33454 ms on localhost (134/200)
15/08/21 11:15:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:15:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:15:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:54 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:54 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:55 INFO ColumnChunkPageWriteStore: written 2,670,779B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,660B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:15:55 INFO ColumnChunkPageWriteStore: written 834,758B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,476B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:15:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000134
15/08/21 11:15:55 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000134_0: Committed
15/08/21 11:15:55 INFO Executor: Finished task 134.0 in stage 1.0 (TID 304). 843 bytes result sent to driver
15/08/21 11:15:55 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 320, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:15:55 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 304) in 33421 ms on localhost (135/200)
15/08/21 11:15:55 INFO Executor: Running task 150.0 in stage 1.0 (TID 320)
15/08/21 11:15:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:15:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
15/08/21 11:15:55 INFO CodecConfig: Compression: GZIP
15/08/21 11:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:15:55 INFO ParquetOutputFormat: Validation is off
15/08/21 11:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:16:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 2,671,262B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,143B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 833,200B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,918B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 2,670,949B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,830B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 834,040B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,758B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:16:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000135
15/08/21 11:16:01 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000135_0: Committed
15/08/21 11:16:01 INFO Executor: Finished task 135.0 in stage 1.0 (TID 305). 843 bytes result sent to driver
15/08/21 11:16:01 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 321, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:01 INFO Executor: Running task 151.0 in stage 1.0 (TID 321)
15/08/21 11:16:01 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 305) in 38561 ms on localhost (136/200)
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:01 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:01 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000136
15/08/21 11:16:01 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000136_0: Committed
15/08/21 11:16:01 INFO Executor: Finished task 136.0 in stage 1.0 (TID 306). 843 bytes result sent to driver
15/08/21 11:16:01 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 322, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:01 INFO Executor: Running task 152.0 in stage 1.0 (TID 322)
15/08/21 11:16:01 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 306) in 38572 ms on localhost (137/200)
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 2,665,032B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,913B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 832,389B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,107B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000137
15/08/21 11:16:01 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000137_0: Committed
15/08/21 11:16:01 INFO Executor: Finished task 137.0 in stage 1.0 (TID 307). 843 bytes result sent to driver
15/08/21 11:16:01 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 323, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:01 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 307) in 38236 ms on localhost (138/200)
15/08/21 11:16:01 INFO Executor: Running task 153.0 in stage 1.0 (TID 323)
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 2,665,071B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,952B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 835,821B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,539B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000139
15/08/21 11:16:01 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000139_0: Committed
15/08/21 11:16:01 INFO Executor: Finished task 139.0 in stage 1.0 (TID 309). 843 bytes result sent to driver
15/08/21 11:16:01 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 324, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:01 INFO Executor: Running task 154.0 in stage 1.0 (TID 324)
15/08/21 11:16:01 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 309) in 37782 ms on localhost (139/200)
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 2,665,250B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,131B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:01 INFO ColumnChunkPageWriteStore: written 831,478B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,196B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:16:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211115_0001_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211115_0001_m_000138
15/08/21 11:16:01 INFO SparkHadoopMapRedUtil: attempt_201508211115_0001_m_000138_0: Committed
15/08/21 11:16:01 INFO Executor: Finished task 138.0 in stage 1.0 (TID 308). 843 bytes result sent to driver
15/08/21 11:16:01 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 325, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:01 INFO Executor: Running task 155.0 in stage 1.0 (TID 325)
15/08/21 11:16:01 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 308) in 38037 ms on localhost (140/200)
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:16:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 2,665,955B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,836B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 832,522B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,240B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 2,671,252B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,133B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 833,254B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,972B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000140
15/08/21 11:16:02 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000140_0: Committed
15/08/21 11:16:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000142
15/08/21 11:16:02 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000142_0: Committed
15/08/21 11:16:02 INFO Executor: Finished task 140.0 in stage 1.0 (TID 310). 843 bytes result sent to driver
15/08/21 11:16:02 INFO Executor: Finished task 142.0 in stage 1.0 (TID 312). 843 bytes result sent to driver
15/08/21 11:16:02 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 326, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:02 INFO Executor: Running task 156.0 in stage 1.0 (TID 326)
15/08/21 11:16:02 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 327, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:02 INFO Executor: Running task 157.0 in stage 1.0 (TID 327)
15/08/21 11:16:02 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 310) in 33422 ms on localhost (141/200)
15/08/21 11:16:02 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 312) in 32630 ms on localhost (142/200)
15/08/21 11:16:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 2,670,980B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,861B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 834,170B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,888B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 11:16:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000141
15/08/21 11:16:02 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000141_0: Committed
15/08/21 11:16:02 INFO Executor: Finished task 141.0 in stage 1.0 (TID 311). 843 bytes result sent to driver
15/08/21 11:16:02 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 328, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:02 INFO Executor: Running task 158.0 in stage 1.0 (TID 328)
15/08/21 11:16:02 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 311) in 32796 ms on localhost (143/200)
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 2,671,240B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,121B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:02 INFO ColumnChunkPageWriteStore: written 832,390B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,108B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:02 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:02 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:02 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:02 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000143
15/08/21 11:16:02 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000143_0: Committed
15/08/21 11:16:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:02 INFO Executor: Finished task 143.0 in stage 1.0 (TID 313). 843 bytes result sent to driver
15/08/21 11:16:02 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 329, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:02 INFO Executor: Running task 159.0 in stage 1.0 (TID 329)
15/08/21 11:16:02 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 313) in 32176 ms on localhost (144/200)
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:07 INFO ColumnChunkPageWriteStore: written 2,671,505B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,386B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:07 INFO ColumnChunkPageWriteStore: written 834,710B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,428B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000144
15/08/21 11:16:07 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000144_0: Committed
15/08/21 11:16:07 INFO Executor: Finished task 144.0 in stage 1.0 (TID 314). 843 bytes result sent to driver
15/08/21 11:16:07 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 330, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:07 INFO Executor: Running task 160.0 in stage 1.0 (TID 330)
15/08/21 11:16:07 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 314) in 35834 ms on localhost (145/200)
15/08/21 11:16:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:16:08 INFO ColumnChunkPageWriteStore: written 2,665,008B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,889B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:08 INFO ColumnChunkPageWriteStore: written 835,046B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,764B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:16:08 INFO ColumnChunkPageWriteStore: written 2,664,977B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,858B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:08 INFO ColumnChunkPageWriteStore: written 832,215B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,933B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000145
15/08/21 11:16:08 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000145_0: Committed
15/08/21 11:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000146
15/08/21 11:16:08 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000146_0: Committed
15/08/21 11:16:08 INFO Executor: Finished task 145.0 in stage 1.0 (TID 315). 843 bytes result sent to driver
15/08/21 11:16:08 INFO Executor: Finished task 146.0 in stage 1.0 (TID 316). 843 bytes result sent to driver
15/08/21 11:16:08 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 331, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:08 INFO Executor: Running task 161.0 in stage 1.0 (TID 331)
15/08/21 11:16:08 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 332, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:08 INFO Executor: Running task 162.0 in stage 1.0 (TID 332)
15/08/21 11:16:08 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 315) in 20225 ms on localhost (146/200)
15/08/21 11:16:08 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 316) in 19998 ms on localhost (147/200)
15/08/21 11:16:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:16:09 INFO ColumnChunkPageWriteStore: written 2,664,987B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,868B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:09 INFO ColumnChunkPageWriteStore: written 833,748B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,466B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:16:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000147
15/08/21 11:16:09 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000147_0: Committed
15/08/21 11:16:09 INFO Executor: Finished task 147.0 in stage 1.0 (TID 317). 843 bytes result sent to driver
15/08/21 11:16:09 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 333, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:09 INFO Executor: Running task 163.0 in stage 1.0 (TID 333)
15/08/21 11:16:09 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 317) in 19638 ms on localhost (148/200)
15/08/21 11:16:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:15 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:15 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:15 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:15 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:15 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:15 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:16:17 INFO ColumnChunkPageWriteStore: written 2,671,374B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,255B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:17 INFO ColumnChunkPageWriteStore: written 833,179B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,897B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:16:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000149
15/08/21 11:16:17 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000149_0: Committed
15/08/21 11:16:17 INFO Executor: Finished task 149.0 in stage 1.0 (TID 319). 843 bytes result sent to driver
15/08/21 11:16:17 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 334, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:17 INFO Executor: Running task 164.0 in stage 1.0 (TID 334)
15/08/21 11:16:17 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 319) in 22719 ms on localhost (149/200)
15/08/21 11:16:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:16:21 INFO ColumnChunkPageWriteStore: written 2,666,190B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,071B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:21 INFO ColumnChunkPageWriteStore: written 831,826B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,544B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:16:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000148
15/08/21 11:16:21 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000148_0: Committed
15/08/21 11:16:21 INFO Executor: Finished task 148.0 in stage 1.0 (TID 318). 843 bytes result sent to driver
15/08/21 11:16:21 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 335, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:21 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 318) in 28295 ms on localhost (150/200)
15/08/21 11:16:21 INFO Executor: Running task 165.0 in stage 1.0 (TID 335)
15/08/21 11:16:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:16:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:16:22 INFO ColumnChunkPageWriteStore: written 2,671,251B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,132B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:22 INFO ColumnChunkPageWriteStore: written 833,479B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,197B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:16:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000150
15/08/21 11:16:22 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000150_0: Committed
15/08/21 11:16:22 INFO Executor: Finished task 150.0 in stage 1.0 (TID 320). 843 bytes result sent to driver
15/08/21 11:16:22 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 336, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:22 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 320) in 27247 ms on localhost (151/200)
15/08/21 11:16:22 INFO Executor: Running task 166.0 in stage 1.0 (TID 336)
15/08/21 11:16:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:16:24 INFO ColumnChunkPageWriteStore: written 2,665,172B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,053B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:24 INFO ColumnChunkPageWriteStore: written 833,923B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,641B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:16:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000153
15/08/21 11:16:24 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000153_0: Committed
15/08/21 11:16:24 INFO Executor: Finished task 153.0 in stage 1.0 (TID 323). 843 bytes result sent to driver
15/08/21 11:16:24 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 337, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:24 INFO Executor: Running task 167.0 in stage 1.0 (TID 337)
15/08/21 11:16:24 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 323) in 22761 ms on localhost (152/200)
15/08/21 11:16:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:16:24 INFO ColumnChunkPageWriteStore: written 2,671,152B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,033B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:24 INFO ColumnChunkPageWriteStore: written 831,567B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,285B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:16:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000151
15/08/21 11:16:24 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000151_0: Committed
15/08/21 11:16:24 INFO Executor: Finished task 151.0 in stage 1.0 (TID 321). 843 bytes result sent to driver
15/08/21 11:16:24 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 338, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:24 INFO Executor: Running task 168.0 in stage 1.0 (TID 338)
15/08/21 11:16:24 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 321) in 23469 ms on localhost (153/200)
15/08/21 11:16:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:29 INFO ColumnChunkPageWriteStore: written 2,665,303B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,184B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 834,375B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,093B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000154
15/08/21 11:16:30 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000154_0: Committed
15/08/21 11:16:30 INFO Executor: Finished task 154.0 in stage 1.0 (TID 324). 843 bytes result sent to driver
15/08/21 11:16:30 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 339, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:30 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 324) in 28400 ms on localhost (154/200)
15/08/21 11:16:30 INFO Executor: Running task 169.0 in stage 1.0 (TID 339)
15/08/21 11:16:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:16:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:30 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:30 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 2,665,144B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,025B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 832,740B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,458B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000155
15/08/21 11:16:30 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000155_0: Committed
15/08/21 11:16:30 INFO Executor: Finished task 155.0 in stage 1.0 (TID 325). 843 bytes result sent to driver
15/08/21 11:16:30 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 340, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:30 INFO Executor: Running task 170.0 in stage 1.0 (TID 340)
15/08/21 11:16:30 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 325) in 28682 ms on localhost (155/200)
15/08/21 11:16:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:16:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 2,670,940B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,821B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 833,949B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,667B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 2,671,444B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,325B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 832,700B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,418B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000157
15/08/21 11:16:30 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000157_0: Committed
15/08/21 11:16:30 INFO Executor: Finished task 157.0 in stage 1.0 (TID 327). 843 bytes result sent to driver
15/08/21 11:16:30 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 341, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:30 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 327) in 28299 ms on localhost (156/200)
15/08/21 11:16:30 INFO Executor: Running task 171.0 in stage 1.0 (TID 341)
15/08/21 11:16:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000152
15/08/21 11:16:30 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000152_0: Committed
15/08/21 11:16:30 INFO Executor: Finished task 152.0 in stage 1.0 (TID 322). 843 bytes result sent to driver
15/08/21 11:16:30 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 342, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:30 INFO Executor: Running task 172.0 in stage 1.0 (TID 342)
15/08/21 11:16:30 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 322) in 29240 ms on localhost (157/200)
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:16:30 INFO ColumnChunkPageWriteStore: written 2,671,373B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,254B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:31 INFO ColumnChunkPageWriteStore: written 830,066B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 829,784B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:16:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:31 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:31 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000158
15/08/21 11:16:31 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000158_0: Committed
15/08/21 11:16:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:31 INFO Executor: Finished task 158.0 in stage 1.0 (TID 328). 843 bytes result sent to driver
15/08/21 11:16:31 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 343, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:31 INFO Executor: Running task 173.0 in stage 1.0 (TID 343)
15/08/21 11:16:31 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 328) in 28442 ms on localhost (158/200)
15/08/21 11:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:16:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:31 INFO ColumnChunkPageWriteStore: written 2,666,080B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,961B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:31 INFO ColumnChunkPageWriteStore: written 834,306B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,024B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000156
15/08/21 11:16:31 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000156_0: Committed
15/08/21 11:16:31 INFO Executor: Finished task 156.0 in stage 1.0 (TID 326). 843 bytes result sent to driver
15/08/21 11:16:31 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 344, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:31 INFO Executor: Running task 174.0 in stage 1.0 (TID 344)
15/08/21 11:16:31 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 326) in 28821 ms on localhost (159/200)
15/08/21 11:16:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:16:31 INFO ColumnChunkPageWriteStore: written 2,671,157B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,038B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:31 INFO ColumnChunkPageWriteStore: written 833,432B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,150B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000159
15/08/21 11:16:31 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000159_0: Committed
15/08/21 11:16:31 INFO Executor: Finished task 159.0 in stage 1.0 (TID 329). 843 bytes result sent to driver
15/08/21 11:16:31 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 345, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:31 INFO Executor: Running task 175.0 in stage 1.0 (TID 345)
15/08/21 11:16:31 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 329) in 28898 ms on localhost (160/200)
15/08/21 11:16:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:16:31 INFO ColumnChunkPageWriteStore: written 2,671,419B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,300B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:31 INFO ColumnChunkPageWriteStore: written 831,056B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,774B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000160
15/08/21 11:16:31 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000160_0: Committed
15/08/21 11:16:31 INFO Executor: Finished task 160.0 in stage 1.0 (TID 330). 843 bytes result sent to driver
15/08/21 11:16:31 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 346, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:31 INFO Executor: Running task 176.0 in stage 1.0 (TID 346)
15/08/21 11:16:31 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 330) in 24518 ms on localhost (161/200)
15/08/21 11:16:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 11:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:16:32 INFO ColumnChunkPageWriteStore: written 2,664,814B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,695B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:32 INFO ColumnChunkPageWriteStore: written 833,829B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,547B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000161
15/08/21 11:16:32 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000161_0: Committed
15/08/21 11:16:32 INFO Executor: Finished task 161.0 in stage 1.0 (TID 331). 843 bytes result sent to driver
15/08/21 11:16:32 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 347, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:32 INFO Executor: Running task 177.0 in stage 1.0 (TID 347)
15/08/21 11:16:32 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 331) in 23565 ms on localhost (162/200)
15/08/21 11:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:32 INFO ColumnChunkPageWriteStore: written 2,665,004B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,885B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:32 INFO ColumnChunkPageWriteStore: written 832,594B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,312B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000162
15/08/21 11:16:32 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000162_0: Committed
15/08/21 11:16:32 INFO Executor: Finished task 162.0 in stage 1.0 (TID 332). 843 bytes result sent to driver
15/08/21 11:16:32 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 348, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:32 INFO Executor: Running task 178.0 in stage 1.0 (TID 348)
15/08/21 11:16:32 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 332) in 23755 ms on localhost (163/200)
15/08/21 11:16:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 11:16:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:16:46 INFO ColumnChunkPageWriteStore: written 2,665,184B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,065B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:46 INFO ColumnChunkPageWriteStore: written 833,514B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,232B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:16:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000163
15/08/21 11:16:46 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000163_0: Committed
15/08/21 11:16:46 INFO Executor: Finished task 163.0 in stage 1.0 (TID 333). 843 bytes result sent to driver
15/08/21 11:16:46 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 349, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:46 INFO Executor: Running task 179.0 in stage 1.0 (TID 349)
15/08/21 11:16:46 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 333) in 36718 ms on localhost (164/200)
15/08/21 11:16:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:16:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:47 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:47 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:51 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:51 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:51 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:51 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:16:52 INFO ColumnChunkPageWriteStore: written 2,665,751B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,632B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:52 INFO ColumnChunkPageWriteStore: written 833,909B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,627B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:16:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000164
15/08/21 11:16:52 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000164_0: Committed
15/08/21 11:16:52 INFO Executor: Finished task 164.0 in stage 1.0 (TID 334). 843 bytes result sent to driver
15/08/21 11:16:52 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 350, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:52 INFO Executor: Running task 180.0 in stage 1.0 (TID 350)
15/08/21 11:16:52 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 334) in 35175 ms on localhost (165/200)
15/08/21 11:16:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 11:16:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:53 INFO ColumnChunkPageWriteStore: written 2,671,029B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,910B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:53 INFO ColumnChunkPageWriteStore: written 836,258B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,976B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000165
15/08/21 11:16:53 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000165_0: Committed
15/08/21 11:16:53 INFO Executor: Finished task 165.0 in stage 1.0 (TID 335). 843 bytes result sent to driver
15/08/21 11:16:53 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 351, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:53 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 335) in 31819 ms on localhost (166/200)
15/08/21 11:16:53 INFO Executor: Running task 181.0 in stage 1.0 (TID 351)
15/08/21 11:16:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:16:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:53 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:53 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:53 INFO ColumnChunkPageWriteStore: written 2,670,794B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,675B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:53 INFO ColumnChunkPageWriteStore: written 832,013B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,731B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000166
15/08/21 11:16:54 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000166_0: Committed
15/08/21 11:16:54 INFO Executor: Finished task 166.0 in stage 1.0 (TID 336). 843 bytes result sent to driver
15/08/21 11:16:54 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 352, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:54 INFO Executor: Running task 182.0 in stage 1.0 (TID 352)
15/08/21 11:16:54 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 336) in 31701 ms on localhost (167/200)
15/08/21 11:16:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:58 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:58 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:59 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:59 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:59 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:59 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:59 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:59 INFO ColumnChunkPageWriteStore: written 2,671,233B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,114B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:59 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:59 INFO ColumnChunkPageWriteStore: written 834,702B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,420B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000167
15/08/21 11:16:59 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000167_0: Committed
15/08/21 11:16:59 INFO Executor: Finished task 167.0 in stage 1.0 (TID 337). 843 bytes result sent to driver
15/08/21 11:16:59 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 353, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:59 INFO Executor: Running task 183.0 in stage 1.0 (TID 353)
15/08/21 11:16:59 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 337) in 35161 ms on localhost (168/200)
15/08/21 11:16:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:16:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:59 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:59 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:59 INFO ColumnChunkPageWriteStore: written 2,671,218B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,099B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:16:59 INFO ColumnChunkPageWriteStore: written 832,689B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,407B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:16:59 INFO CodecConfig: Compression: GZIP
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:16:59 INFO ParquetOutputFormat: Validation is off
15/08/21 11:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:16:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000168
15/08/21 11:16:59 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000168_0: Committed
15/08/21 11:16:59 INFO Executor: Finished task 168.0 in stage 1.0 (TID 338). 843 bytes result sent to driver
15/08/21 11:16:59 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 354, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:16:59 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 338) in 35334 ms on localhost (169/200)
15/08/21 11:16:59 INFO Executor: Running task 184.0 in stage 1.0 (TID 354)
15/08/21 11:16:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:16:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:17:00 INFO ColumnChunkPageWriteStore: written 2,665,102B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,983B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:00 INFO ColumnChunkPageWriteStore: written 830,633B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,351B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000169
15/08/21 11:17:00 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000169_0: Committed
15/08/21 11:17:00 INFO Executor: Finished task 169.0 in stage 1.0 (TID 339). 843 bytes result sent to driver
15/08/21 11:17:00 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 355, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:00 INFO Executor: Running task 185.0 in stage 1.0 (TID 355)
15/08/21 11:17:00 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 339) in 30302 ms on localhost (170/200)
15/08/21 11:17:00 INFO ColumnChunkPageWriteStore: written 2,665,025B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,906B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:00 INFO ColumnChunkPageWriteStore: written 831,175B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,893B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:17:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000171
15/08/21 11:17:00 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000171_0: Committed
15/08/21 11:17:00 INFO Executor: Finished task 171.0 in stage 1.0 (TID 341). 843 bytes result sent to driver
15/08/21 11:17:00 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 356, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:00 INFO Executor: Running task 186.0 in stage 1.0 (TID 356)
15/08/21 11:17:00 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 341) in 29716 ms on localhost (171/200)
15/08/21 11:17:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:17:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:00 INFO ColumnChunkPageWriteStore: written 2,671,354B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,235B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:00 INFO ColumnChunkPageWriteStore: written 833,246B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,964B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000174
15/08/21 11:17:00 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000174_0: Committed
15/08/21 11:17:00 INFO Executor: Finished task 174.0 in stage 1.0 (TID 344). 843 bytes result sent to driver
15/08/21 11:17:00 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 357, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:00 INFO Executor: Running task 187.0 in stage 1.0 (TID 357)
15/08/21 11:17:00 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 344) in 29625 ms on localhost (172/200)
15/08/21 11:17:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 11:17:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:00 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:00 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:01 INFO ColumnChunkPageWriteStore: written 2,671,396B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,277B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:01 INFO ColumnChunkPageWriteStore: written 836,094B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,812B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 11:17:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000173
15/08/21 11:17:01 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000173_0: Committed
15/08/21 11:17:01 INFO Executor: Finished task 173.0 in stage 1.0 (TID 343). 843 bytes result sent to driver
15/08/21 11:17:01 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 358, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:01 INFO Executor: Running task 188.0 in stage 1.0 (TID 358)
15/08/21 11:17:01 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 343) in 30070 ms on localhost (173/200)
15/08/21 11:17:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:17:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:01 INFO ColumnChunkPageWriteStore: written 2,666,198B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,079B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:01 INFO ColumnChunkPageWriteStore: written 834,962B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,680B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:17:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000172
15/08/21 11:17:01 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000172_0: Committed
15/08/21 11:17:01 INFO Executor: Finished task 172.0 in stage 1.0 (TID 342). 843 bytes result sent to driver
15/08/21 11:17:01 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 359, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:01 INFO Executor: Running task 189.0 in stage 1.0 (TID 359)
15/08/21 11:17:01 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 342) in 30524 ms on localhost (174/200)
15/08/21 11:17:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:17:01 INFO ColumnChunkPageWriteStore: written 2,671,213B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,094B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:01 INFO ColumnChunkPageWriteStore: written 832,436B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,154B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:17:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000175
15/08/21 11:17:06 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000175_0: Committed
15/08/21 11:17:06 INFO Executor: Finished task 175.0 in stage 1.0 (TID 345). 843 bytes result sent to driver
15/08/21 11:17:06 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 360, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:06 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 345) in 35023 ms on localhost (175/200)
15/08/21 11:17:06 INFO Executor: Running task 190.0 in stage 1.0 (TID 360)
15/08/21 11:17:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 11:17:06 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:17:06 INFO ColumnChunkPageWriteStore: written 2,665,329B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,210B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:06 INFO ColumnChunkPageWriteStore: written 832,235B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,953B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 11:17:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508211116_0001_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211116_0001_m_000170
15/08/21 11:17:06 INFO SparkHadoopMapRedUtil: attempt_201508211116_0001_m_000170_0: Committed
15/08/21 11:17:06 INFO Executor: Finished task 170.0 in stage 1.0 (TID 340). 843 bytes result sent to driver
15/08/21 11:17:06 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 361, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:06 INFO Executor: Running task 191.0 in stage 1.0 (TID 361)
15/08/21 11:17:06 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 340) in 36400 ms on localhost (176/200)
15/08/21 11:17:06 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:07 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:07 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:17:07 INFO ColumnChunkPageWriteStore: written 2,671,155B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,036B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:07 INFO ColumnChunkPageWriteStore: written 831,754B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,472B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:17:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000176
15/08/21 11:17:07 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000176_0: Committed
15/08/21 11:17:07 INFO Executor: Finished task 176.0 in stage 1.0 (TID 346). 843 bytes result sent to driver
15/08/21 11:17:07 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 362, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:07 INFO Executor: Running task 192.0 in stage 1.0 (TID 362)
15/08/21 11:17:07 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 346) in 35474 ms on localhost (177/200)
15/08/21 11:17:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:17:07 INFO ColumnChunkPageWriteStore: written 2,665,184B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,065B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:07 INFO ColumnChunkPageWriteStore: written 835,844B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,562B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:17:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000177
15/08/21 11:17:07 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000177_0: Committed
15/08/21 11:17:07 INFO Executor: Finished task 177.0 in stage 1.0 (TID 347). 843 bytes result sent to driver
15/08/21 11:17:07 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 363, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:07 INFO Executor: Running task 193.0 in stage 1.0 (TID 363)
15/08/21 11:17:07 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 347) in 35597 ms on localhost (178/200)
15/08/21 11:17:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:17:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:17:07 INFO ColumnChunkPageWriteStore: written 2,665,190B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,071B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:07 INFO ColumnChunkPageWriteStore: written 833,011B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,729B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:17:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000178
15/08/21 11:17:08 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000178_0: Committed
15/08/21 11:17:08 INFO Executor: Finished task 178.0 in stage 1.0 (TID 348). 843 bytes result sent to driver
15/08/21 11:17:08 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 364, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:08 INFO Executor: Running task 194.0 in stage 1.0 (TID 364)
15/08/21 11:17:08 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 348) in 35592 ms on localhost (179/200)
15/08/21 11:17:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:17:09 INFO ColumnChunkPageWriteStore: written 2,665,124B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,005B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:09 INFO ColumnChunkPageWriteStore: written 834,872B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,590B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:17:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000179
15/08/21 11:17:09 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000179_0: Committed
15/08/21 11:17:09 INFO Executor: Finished task 179.0 in stage 1.0 (TID 349). 843 bytes result sent to driver
15/08/21 11:17:09 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 365, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:09 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 349) in 22853 ms on localhost (180/200)
15/08/21 11:17:09 INFO Executor: Running task 195.0 in stage 1.0 (TID 365)
15/08/21 11:17:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:17:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:14 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:14 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:15 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:15 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:15 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:15 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 11:17:16 INFO ColumnChunkPageWriteStore: written 2,666,139B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,020B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:16 INFO ColumnChunkPageWriteStore: written 833,416B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,134B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 11:17:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000180
15/08/21 11:17:16 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000180_0: Committed
15/08/21 11:17:16 INFO Executor: Finished task 180.0 in stage 1.0 (TID 350). 843 bytes result sent to driver
15/08/21 11:17:16 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 366, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:16 INFO Executor: Running task 196.0 in stage 1.0 (TID 366)
15/08/21 11:17:16 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 350) in 24102 ms on localhost (181/200)
15/08/21 11:17:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:17:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:17:22 INFO ColumnChunkPageWriteStore: written 2,671,432B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,313B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:22 INFO ColumnChunkPageWriteStore: written 830,734B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,452B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:17:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000181
15/08/21 11:17:22 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000181_0: Committed
15/08/21 11:17:22 INFO Executor: Finished task 181.0 in stage 1.0 (TID 351). 843 bytes result sent to driver
15/08/21 11:17:22 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 367, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:22 INFO Executor: Running task 197.0 in stage 1.0 (TID 367)
15/08/21 11:17:22 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 351) in 28536 ms on localhost (182/200)
15/08/21 11:17:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:17:22 INFO ColumnChunkPageWriteStore: written 2,671,236B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,117B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:22 INFO ColumnChunkPageWriteStore: written 833,872B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,590B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:17:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000182
15/08/21 11:17:22 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000182_0: Committed
15/08/21 11:17:22 INFO Executor: Finished task 182.0 in stage 1.0 (TID 352). 843 bytes result sent to driver
15/08/21 11:17:22 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 368, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:22 INFO Executor: Running task 198.0 in stage 1.0 (TID 368)
15/08/21 11:17:22 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 352) in 28549 ms on localhost (183/200)
15/08/21 11:17:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:17:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:22 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:22 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:23 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:23 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:23 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:17:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:24 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:24 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:24 INFO ColumnChunkPageWriteStore: written 2,671,254B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,135B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:24 INFO ColumnChunkPageWriteStore: written 835,132B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,850B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:17:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000184
15/08/21 11:17:24 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000184_0: Committed
15/08/21 11:17:24 INFO Executor: Finished task 184.0 in stage 1.0 (TID 354). 843 bytes result sent to driver
15/08/21 11:17:24 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 369, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 11:17:24 INFO Executor: Running task 199.0 in stage 1.0 (TID 369)
15/08/21 11:17:24 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 354) in 24484 ms on localhost (184/200)
15/08/21 11:17:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:17:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:17:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:17:24 INFO ColumnChunkPageWriteStore: written 2,671,394B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,275B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:24 INFO ColumnChunkPageWriteStore: written 833,536B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,254B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:17:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000183
15/08/21 11:17:24 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000183_0: Committed
15/08/21 11:17:24 INFO Executor: Finished task 183.0 in stage 1.0 (TID 353). 843 bytes result sent to driver
15/08/21 11:17:24 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 353) in 25045 ms on localhost (185/200)
15/08/21 11:17:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:17:24 INFO ColumnChunkPageWriteStore: written 2,666,015B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,896B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:24 INFO ColumnChunkPageWriteStore: written 834,611B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,329B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:17:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000188
15/08/21 11:17:24 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000188_0: Committed
15/08/21 11:17:24 INFO Executor: Finished task 188.0 in stage 1.0 (TID 358). 843 bytes result sent to driver
15/08/21 11:17:29 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 358) in 27944 ms on localhost (186/200)
15/08/21 11:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:17:29 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:29 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 2,664,952B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,833B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 835,727B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,445B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 2,665,090B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,971B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 832,815B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,533B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:17:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:30 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:30 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000186
15/08/21 11:17:30 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000186_0: Committed
15/08/21 11:17:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:17:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000185
15/08/21 11:17:30 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000185_0: Committed
15/08/21 11:17:30 INFO Executor: Finished task 185.0 in stage 1.0 (TID 355). 843 bytes result sent to driver
15/08/21 11:17:30 INFO Executor: Finished task 186.0 in stage 1.0 (TID 356). 843 bytes result sent to driver
15/08/21 11:17:30 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 355) in 29734 ms on localhost (187/200)
15/08/21 11:17:30 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 356) in 29645 ms on localhost (188/200)
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 2,671,442B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,323B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 833,079B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,797B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:17:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:30 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:30 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000190
15/08/21 11:17:30 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000190_0: Committed
15/08/21 11:17:30 INFO Executor: Finished task 190.0 in stage 1.0 (TID 360). 843 bytes result sent to driver
15/08/21 11:17:30 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 360) in 23613 ms on localhost (189/200)
15/08/21 11:17:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 2,665,171B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,052B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:30 INFO ColumnChunkPageWriteStore: written 834,877B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,595B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:17:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000187
15/08/21 11:17:30 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000187_0: Committed
15/08/21 11:17:30 INFO Executor: Finished task 187.0 in stage 1.0 (TID 357). 843 bytes result sent to driver
15/08/21 11:17:30 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 357) in 29886 ms on localhost (190/200)
15/08/21 11:17:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 2,671,450B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,331B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 831,022B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,740B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:17:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000189
15/08/21 11:17:31 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000189_0: Committed
15/08/21 11:17:31 INFO Executor: Finished task 189.0 in stage 1.0 (TID 359). 843 bytes result sent to driver
15/08/21 11:17:31 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 359) in 29768 ms on localhost (191/200)
15/08/21 11:17:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 2,671,177B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,058B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 835,289B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,007B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:17:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000192
15/08/21 11:17:31 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000192_0: Committed
15/08/21 11:17:31 INFO Executor: Finished task 192.0 in stage 1.0 (TID 362). 843 bytes result sent to driver
15/08/21 11:17:31 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 362) in 23894 ms on localhost (192/200)
15/08/21 11:17:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 2,671,529B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,410B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 833,348B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,066B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 11:17:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000191
15/08/21 11:17:31 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000191_0: Committed
15/08/21 11:17:31 INFO Executor: Finished task 191.0 in stage 1.0 (TID 361). 843 bytes result sent to driver
15/08/21 11:17:31 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 361) in 24760 ms on localhost (193/200)
15/08/21 11:17:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 2,665,052B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,933B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 834,559B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,277B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:17:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000195
15/08/21 11:17:31 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000195_0: Committed
15/08/21 11:17:31 INFO Executor: Finished task 195.0 in stage 1.0 (TID 365). 843 bytes result sent to driver
15/08/21 11:17:31 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 365) in 22487 ms on localhost (194/200)
15/08/21 11:17:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 2,664,917B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,798B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:31 INFO ColumnChunkPageWriteStore: written 833,921B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,639B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:17:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000193
15/08/21 11:17:31 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000193_0: Committed
15/08/21 11:17:31 INFO Executor: Finished task 193.0 in stage 1.0 (TID 363). 843 bytes result sent to driver
15/08/21 11:17:31 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 363) in 24080 ms on localhost (195/200)
15/08/21 11:17:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 11:17:32 INFO ColumnChunkPageWriteStore: written 2,665,340B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,221B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:32 INFO ColumnChunkPageWriteStore: written 831,556B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,274B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 11:17:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000194
15/08/21 11:17:32 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000194_0: Committed
15/08/21 11:17:32 INFO Executor: Finished task 194.0 in stage 1.0 (TID 364). 843 bytes result sent to driver
15/08/21 11:17:32 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 364) in 24101 ms on localhost (196/200)
15/08/21 11:17:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:32 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:32 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:32 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:32 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:33 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:33 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:17:33 INFO CodecConfig: Compression: GZIP
15/08/21 11:17:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:17:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:17:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:17:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:17:33 INFO ParquetOutputFormat: Validation is off
15/08/21 11:17:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:17:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:17:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 11:17:34 INFO ColumnChunkPageWriteStore: written 2,666,316B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,197B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:34 INFO ColumnChunkPageWriteStore: written 832,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,713B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 11:17:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000196
15/08/21 11:17:34 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000196_0: Committed
15/08/21 11:17:34 INFO Executor: Finished task 196.0 in stage 1.0 (TID 366). 843 bytes result sent to driver
15/08/21 11:17:34 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 366) in 18190 ms on localhost (197/200)
15/08/21 11:17:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 11:17:34 INFO ColumnChunkPageWriteStore: written 2,671,199B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,080B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:34 INFO ColumnChunkPageWriteStore: written 832,452B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,170B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 11:17:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 11:17:35 INFO ColumnChunkPageWriteStore: written 2,671,458B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,339B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:35 INFO ColumnChunkPageWriteStore: written 833,958B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,676B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 11:17:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000198
15/08/21 11:17:35 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000198_0: Committed
15/08/21 11:17:35 INFO Executor: Finished task 198.0 in stage 1.0 (TID 368). 843 bytes result sent to driver
15/08/21 11:17:35 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 368) in 12602 ms on localhost (198/200)
15/08/21 11:17:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000197
15/08/21 11:17:35 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000197_0: Committed
15/08/21 11:17:35 INFO Executor: Finished task 197.0 in stage 1.0 (TID 367). 843 bytes result sent to driver
15/08/21 11:17:35 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 367) in 13058 ms on localhost (199/200)
15/08/21 11:17:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 11:17:36 INFO ColumnChunkPageWriteStore: written 2,671,110B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,991B comp, 3 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:17:36 INFO ColumnChunkPageWriteStore: written 832,892B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,610B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 11:17:36 INFO FileOutputCommitter: Saved output of task 'attempt_201508211117_0001_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211117_0001_m_000199
15/08/21 11:17:36 INFO SparkHadoopMapRedUtil: attempt_201508211117_0001_m_000199_0: Committed
15/08/21 11:17:36 INFO Executor: Finished task 199.0 in stage 1.0 (TID 369). 843 bytes result sent to driver
15/08/21 11:17:36 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 369) in 11699 ms on localhost (200/200)
15/08/21 11:17:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/21 11:17:36 INFO DAGScheduler: ResultStage 1 (processCmd at CliDriver.java:423) finished in 386.695 s
15/08/21 11:17:36 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@13320e9b
15/08/21 11:17:36 INFO StatsReportListener: task runtime:(count: 200, mean: 30607.665000, stdev: 5964.703094, max: 45679.000000, min: 11699.000000)
15/08/21 11:17:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:36 INFO StatsReportListener: 	11.7 s	22.5 s	23.1 s	27.5 s	29.9 s	35.6 s	38.2 s	40.0 s	45.7 s
15/08/21 11:17:36 INFO StatsReportListener: fetch wait time:(count: 200, mean: 1.100000, stdev: 1.679286, max: 15.000000, min: 0.000000)
15/08/21 11:17:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:36 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	4.0 ms	15.0 ms
15/08/21 11:17:36 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 11:17:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:36 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 11:17:36 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 11:17:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:36 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 11:17:36 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:423, took 774.211369 s
15/08/21 11:17:36 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 99.122263, stdev: 2.706112, max: 99.872965, min: 82.722288)
15/08/21 11:17:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:36 INFO StatsReportListener: 	83 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 11:17:36 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.003461, stdev: 0.005034, max: 0.041550, min: 0.000000)
15/08/21 11:17:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:36 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 11:17:36 INFO StatsReportListener: other time pct: (count: 200, mean: 0.874276, stdev: 2.706379, max: 17.277712, min: 0.121850)
15/08/21 11:17:36 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:36 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	17 %
15/08/21 11:17:37 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:17:37 INFO DefaultWriterContainer: Job job_201508211104_0000 committed.
15/08/21 11:17:37 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:17:37 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_common_metadata
15/08/21 11:17:38 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 11:17:38 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 11:17:38 INFO DAGScheduler: Final stage: ResultStage 2(processCmd at CliDriver.java:423)
15/08/21 11:17:38 INFO DAGScheduler: Parents of final stage: List()
15/08/21 11:17:38 INFO DAGScheduler: Missing parents: List()
15/08/21 11:17:38 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:17:38 INFO MemoryStore: ensureFreeSpace(2976) called with curMem=458255, maxMem=22226833244
15/08/21 11:17:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 11:17:38 INFO MemoryStore: ensureFreeSpace(1784) called with curMem=461231, maxMem=22226833244
15/08/21 11:17:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1784.0 B, free 20.7 GB)
15/08/21 11:17:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:53620 (size: 1784.0 B, free: 20.7 GB)
15/08/21 11:17:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/21 11:17:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423)
15/08/21 11:17:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/21 11:17:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 370, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 11:17:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 370)
15/08/21 11:17:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 370). 606 bytes result sent to driver
15/08/21 11:17:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 370) in 61 ms on localhost (1/1)
15/08/21 11:17:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/21 11:17:38 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:423) finished in 0.063 s
15/08/21 11:17:38 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@6aa0807
15/08/21 11:17:38 INFO StatsReportListener: task runtime:(count: 1, mean: 61.000000, stdev: 0.000000, max: 61.000000, min: 61.000000)
15/08/21 11:17:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:38 INFO StatsReportListener: 	61.0 ms	61.0 ms	61.0 ms	61.0 ms	61.0 ms	61.0 ms	61.0 ms	61.0 ms	61.0 ms
15/08/21 11:17:38 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 0.083696 s
15/08/21 11:17:38 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 11:17:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:38 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 11:17:38 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 22.950820, stdev: 0.000000, max: 22.950820, min: 22.950820)
15/08/21 11:17:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:38 INFO StatsReportListener: 	23 %	23 %	23 %	23 %	23 %	23 %	23 %	23 %	23 %
15/08/21 11:17:38 INFO StatsReportListener: other time pct: (count: 1, mean: 77.049180, stdev: 0.000000, max: 77.049180, min: 77.049180)
15/08/21 11:17:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:38 INFO StatsReportListener: 	77 %	77 %	77 %	77 %	77 %	77 %	77 %	77 %	77 %
Time taken: 781.788 seconds
15/08/21 11:17:38 INFO CliDriver: Time taken: 781.788 seconds
15/08/21 11:17:38 INFO ParseDriver: Parsing command: insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100
15/08/21 11:17:38 INFO ParseDriver: Parse Completed
15/08/21 11:17:38 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:17:38 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:17:38 INFO ParquetFileReader: reading another 10 footers
15/08/21 11:17:38 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:17:38 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:17:38 INFO ParquetFileReader: reading another 19 footers
15/08/21 11:17:38 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:17:38 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=463015, maxMem=22226833244
15/08/21 11:17:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 11:17:39 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=789623, maxMem=22226833244
15/08/21 11:17:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 11:17:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:53620 (size: 22.3 KB, free: 20.7 GB)
15/08/21 11:17:39 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:423
15/08/21 11:17:39 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=812416, maxMem=22226833244
15/08/21 11:17:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 11:17:39 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1139024, maxMem=22226833244
15/08/21 11:17:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 11:17:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:53620 (size: 22.3 KB, free: 20.7 GB)
15/08/21 11:17:39 INFO SparkContext: Created broadcast 5 from processCmd at CliDriver.java:423
15/08/21 11:17:39 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1161817, maxMem=22226833244
15/08/21 11:17:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 11:17:39 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1488425, maxMem=22226833244
15/08/21 11:17:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 11:17:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:53620 (size: 22.3 KB, free: 20.7 GB)
15/08/21 11:17:39 INFO SparkContext: Created broadcast 6 from processCmd at CliDriver.java:423
15/08/21 11:17:39 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1511218, maxMem=22226833244
15/08/21 11:17:39 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 11:17:39 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1837826, maxMem=22226833244
15/08/21 11:17:39 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 11:17:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:53620 (size: 22.3 KB, free: 20.7 GB)
15/08/21 11:17:39 INFO SparkContext: Created broadcast 7 from processCmd at CliDriver.java:423
15/08/21 11:17:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 11:17:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 11:17:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 11:17:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 11:17:39 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 11:17:39 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 11:17:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 11:17:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 11:17:39 INFO DAGScheduler: Registering RDD 25 (processCmd at CliDriver.java:423)
15/08/21 11:17:39 INFO DAGScheduler: Registering RDD 28 (processCmd at CliDriver.java:423)
15/08/21 11:17:39 INFO DAGScheduler: Registering RDD 33 (processCmd at CliDriver.java:423)
15/08/21 11:17:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 11:17:40 INFO DAGScheduler: Registering RDD 22 (processCmd at CliDriver.java:423)
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 11:17:40 INFO DAGScheduler: Registering RDD 17 (processCmd at CliDriver.java:423)
15/08/21 11:17:40 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 11:17:40 INFO DAGScheduler: Final stage: ResultStage 8(processCmd at CliDriver.java:423)
15/08/21 11:17:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 11:17:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 11:17:40 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(6632) called with curMem=1860619, maxMem=22226833244
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.5 KB, free 20.7 GB)
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(3624) called with curMem=1867251, maxMem=22226833244
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 20.7 GB)
15/08/21 11:17:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:53620 (size: 3.5 KB, free: 20.7 GB)
15/08/21 11:17:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/21 11:17:40 INFO DAGScheduler: Submitting 19 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/21 11:17:40 INFO TaskSchedulerImpl: Adding task set 3.0 with 19 tasks
15/08/21 11:17:40 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 371, localhost, ANY, 1706 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 372, localhost, ANY, 1715 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 373, localhost, ANY, 1709 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 374, localhost, ANY, 1706 bytes)
15/08/21 11:17:40 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:17:40 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 375, localhost, ANY, 1714 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 376, localhost, ANY, 1706 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 377, localhost, ANY, 1714 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 378, localhost, ANY, 1706 bytes)
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(6728) called with curMem=1870875, maxMem=22226833244
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 6.6 KB, free 20.7 GB)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 379, localhost, ANY, 1713 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 380, localhost, ANY, 1705 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 381, localhost, ANY, 1715 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 382, localhost, ANY, 1706 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 383, localhost, ANY, 1716 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 384, localhost, ANY, 1706 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 385, localhost, ANY, 1713 bytes)
15/08/21 11:17:40 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 386, localhost, ANY, 1706 bytes)
15/08/21 11:17:40 INFO Executor: Running task 0.0 in stage 3.0 (TID 371)
15/08/21 11:17:40 INFO Executor: Running task 1.0 in stage 3.0 (TID 372)
15/08/21 11:17:40 INFO Executor: Running task 7.0 in stage 3.0 (TID 378)
15/08/21 11:17:40 INFO Executor: Running task 2.0 in stage 3.0 (TID 373)
15/08/21 11:17:40 INFO Executor: Running task 4.0 in stage 3.0 (TID 375)
15/08/21 11:17:40 INFO Executor: Running task 3.0 in stage 3.0 (TID 374)
15/08/21 11:17:40 INFO Executor: Running task 5.0 in stage 3.0 (TID 376)
15/08/21 11:17:40 INFO Executor: Running task 6.0 in stage 3.0 (TID 377)
15/08/21 11:17:40 INFO Executor: Running task 8.0 in stage 3.0 (TID 379)
15/08/21 11:17:40 INFO Executor: Running task 9.0 in stage 3.0 (TID 380)
15/08/21 11:17:40 INFO Executor: Running task 10.0 in stage 3.0 (TID 381)
15/08/21 11:17:40 INFO Executor: Running task 12.0 in stage 3.0 (TID 383)
15/08/21 11:17:40 INFO Executor: Running task 14.0 in stage 3.0 (TID 385)
15/08/21 11:17:40 INFO Executor: Running task 11.0 in stage 3.0 (TID 382)
15/08/21 11:17:40 INFO Executor: Running task 13.0 in stage 3.0 (TID 384)
15/08/21 11:17:40 INFO Executor: Running task 15.0 in stage 3.0 (TID 386)
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(3650) called with curMem=1877603, maxMem=22226833244
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.6 KB, free 20.7 GB)
15/08/21 11:17:40 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:53620 (size: 3.6 KB, free: 20.7 GB)
15/08/21 11:17:40 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000000_0 start: 134217728 end: 138596223 length: 4378495 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000005_0 start: 134217728 end: 138026641 length: 3808913 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000001_0 start: 134217728 end: 138025614 length: 3807886 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000006_0 start: 134217728 end: 137279350 length: 3061622 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 INFO DAGScheduler: Submitting 57 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423)
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 57 tasks
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000007_0 start: 134217728 end: 137178560 length: 2960832 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000002_0 start: 134217728 end: 138011074 length: 3793346 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000009_0 start: 0 end: 24310349 length: 24310349 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000004_0 start: 134217728 end: 138024796 length: 3807068 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76861 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560031 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 83694 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560462 records.
15/08/21 11:17:40 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560128 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76962 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1559896 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 68086 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560100 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 288335 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 67074 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560100 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560144 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560003 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 77014 records.
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76866 records.
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(7320) called with curMem=1881253, maxMem=22226833244
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.1 KB, free 20.7 GB)
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 83694
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 76962
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 68086
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 77014
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 67074
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 76861
15/08/21 11:17:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 288335
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 76866
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(3874) called with curMem=1888573, maxMem=22226833244
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.8 KB, free 20.7 GB)
15/08/21 11:17:40 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:53620 (size: 3.8 KB, free: 20.7 GB)
15/08/21 11:17:40 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:874
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 143 ms. row count = 1560031
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 152 ms. row count = 1560462
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 141 ms. row count = 1560128
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 139 ms. row count = 1560100
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 134 ms. row count = 1560100
15/08/21 11:17:40 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423)
15/08/21 11:17:40 INFO TaskSchedulerImpl: Adding task set 6.0 with 200 tasks
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 152 ms. row count = 1560003
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 158 ms. row count = 1560144
15/08/21 11:17:40 INFO InternalParquetRecordReader: block read in memory in 170 ms. row count = 1559896
15/08/21 11:17:40 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(6952) called with curMem=1892447, maxMem=22226833244
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 6.8 KB, free 20.7 GB)
15/08/21 11:17:40 INFO MemoryStore: ensureFreeSpace(3749) called with curMem=1899399, maxMem=22226833244
15/08/21 11:17:40 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 20.7 GB)
15/08/21 11:17:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:53620 (size: 3.7 KB, free: 20.7 GB)
15/08/21 11:17:40 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:874
15/08/21 11:17:40 INFO DAGScheduler: Submitting 170 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423)
15/08/21 11:17:40 INFO TaskSchedulerImpl: Adding task set 7.0 with 170 tasks
15/08/21 11:17:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:53620 in memory (size: 1784.0 B, free: 20.7 GB)
15/08/21 11:17:45 INFO Executor: Finished task 14.0 in stage 3.0 (TID 385). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 387, localhost, ANY, 1714 bytes)
15/08/21 11:17:45 INFO Executor: Running task 16.0 in stage 3.0 (TID 387)
15/08/21 11:17:45 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 385) in 4925 ms on localhost (1/19)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000008_0 start: 134217728 end: 137173148 length: 2955420 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 66882 records.
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 66882
15/08/21 11:17:45 INFO Executor: Finished task 6.0 in stage 3.0 (TID 377). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 388, localhost, ANY, 1706 bytes)
15/08/21 11:17:45 INFO Executor: Running task 17.0 in stage 3.0 (TID 388)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 377) in 5043 ms on localhost (2/19)
15/08/21 11:17:45 INFO Executor: Finished task 1.0 in stage 3.0 (TID 372). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560365 records.
15/08/21 11:17:45 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 389, localhost, ANY, 1712 bytes)
15/08/21 11:17:45 INFO Executor: Running task 18.0 in stage 3.0 (TID 389)
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 372) in 5069 ms on localhost (3/19)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000003_0 start: 134217728 end: 138036519 length: 3818791 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76997 records.
15/08/21 11:17:45 INFO Executor: Finished task 10.0 in stage 3.0 (TID 381). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 390, localhost, ANY, 1740 bytes)
15/08/21 11:17:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 390)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 381) in 5100 ms on localhost (4/19)
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 76997
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 1560365
15/08/21 11:17:45 INFO Executor: Finished task 4.0 in stage 3.0 (TID 375). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061701 records.
15/08/21 11:17:45 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 391, localhost, ANY, 1746 bytes)
15/08/21 11:17:45 INFO Executor: Finished task 12.0 in stage 3.0 (TID 383). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO Executor: Running task 1.0 in stage 4.0 (TID 391)
15/08/21 11:17:45 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 392, localhost, ANY, 1752 bytes)
15/08/21 11:17:45 INFO Executor: Running task 2.0 in stage 4.0 (TID 392)
15/08/21 11:17:45 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 375) in 5151 ms on localhost (5/19)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 268435456 end: 340165230 length: 71729774 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 383) in 5147 ms on localhost (6/19)
15/08/21 11:17:45 INFO Executor: Finished task 8.0 in stage 3.0 (TID 379). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 393, localhost, ANY, 1739 bytes)
15/08/21 11:17:45 INFO Executor: Running task 3.0 in stage 4.0 (TID 393)
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 379) in 5162 ms on localhost (7/19)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1785023 records.
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061593 records.
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061449 records.
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 151 ms. row count = 1785023
15/08/21 11:17:45 INFO Executor: Finished task 2.0 in stage 3.0 (TID 373). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 394, localhost, ANY, 1745 bytes)
15/08/21 11:17:45 INFO Executor: Running task 4.0 in stage 4.0 (TID 394)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 373) in 5374 ms on localhost (8/19)
15/08/21 11:17:45 INFO Executor: Finished task 16.0 in stage 3.0 (TID 387). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 395, localhost, ANY, 1756 bytes)
15/08/21 11:17:45 INFO Executor: Running task 5.0 in stage 4.0 (TID 395)
15/08/21 11:17:45 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 387) in 485 ms on localhost (9/19)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 268435456 end: 344047583 length: 75612127 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061600 records.
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 220 ms. row count = 3061701
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1875761 records.
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 256 ms. row count = 3061593
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 289 ms. row count = 3061449
15/08/21 11:17:45 INFO Executor: Finished task 18.0 in stage 3.0 (TID 389). 2125 bytes result sent to driver
15/08/21 11:17:45 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 396, localhost, ANY, 1741 bytes)
15/08/21 11:17:45 INFO Executor: Running task 6.0 in stage 4.0 (TID 396)
15/08/21 11:17:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:45 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 389) in 524 ms on localhost (10/19)
15/08/21 11:17:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 132 ms. row count = 1875761
15/08/21 11:17:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061554 records.
15/08/21 11:17:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:45 INFO InternalParquetRecordReader: block read in memory in 254 ms. row count = 3061600
15/08/21 11:17:46 INFO InternalParquetRecordReader: block read in memory in 422 ms. row count = 3061554
15/08/21 11:17:47 INFO Executor: Finished task 0.0 in stage 3.0 (TID 371). 2125 bytes result sent to driver
15/08/21 11:17:47 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 397, localhost, ANY, 1746 bytes)
15/08/21 11:17:47 INFO Executor: Running task 7.0 in stage 4.0 (TID 397)
15/08/21 11:17:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 371) in 7385 ms on localhost (11/19)
15/08/21 11:17:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061231 records.
15/08/21 11:17:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:47 INFO InternalParquetRecordReader: block read in memory in 181 ms. row count = 3061231
15/08/21 11:17:48 INFO Executor: Finished task 5.0 in stage 3.0 (TID 376). 2125 bytes result sent to driver
15/08/21 11:17:48 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 398, localhost, ANY, 1752 bytes)
15/08/21 11:17:48 INFO Executor: Running task 8.0 in stage 4.0 (TID 398)
15/08/21 11:17:48 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 376) in 7749 ms on localhost (12/19)
15/08/21 11:17:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 268435456 end: 340526195 length: 72090739 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1793520 records.
15/08/21 11:17:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:48 INFO Executor: Finished task 11.0 in stage 3.0 (TID 382). 2125 bytes result sent to driver
15/08/21 11:17:48 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 399, localhost, ANY, 1742 bytes)
15/08/21 11:17:48 INFO Executor: Running task 9.0 in stage 4.0 (TID 399)
15/08/21 11:17:48 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 382) in 7851 ms on localhost (13/19)
15/08/21 11:17:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061301 records.
15/08/21 11:17:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:48 INFO Executor: Finished task 15.0 in stage 3.0 (TID 386). 2125 bytes result sent to driver
15/08/21 11:17:48 INFO InternalParquetRecordReader: block read in memory in 127 ms. row count = 1793520
15/08/21 11:17:48 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 400, localhost, ANY, 1747 bytes)
15/08/21 11:17:48 INFO Executor: Running task 10.0 in stage 4.0 (TID 400)
15/08/21 11:17:48 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 386) in 7922 ms on localhost (14/19)
15/08/21 11:17:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061650 records.
15/08/21 11:17:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:48 INFO Executor: Finished task 9.0 in stage 3.0 (TID 380). 2125 bytes result sent to driver
15/08/21 11:17:48 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 401, localhost, ANY, 1753 bytes)
15/08/21 11:17:48 INFO Executor: Running task 11.0 in stage 4.0 (TID 401)
15/08/21 11:17:48 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 380) in 7998 ms on localhost (15/19)
15/08/21 11:17:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 268435456 end: 340154417 length: 71718961 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:48 INFO Executor: Finished task 7.0 in stage 3.0 (TID 378). 2125 bytes result sent to driver
15/08/21 11:17:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784493 records.
15/08/21 11:17:48 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 402, localhost, ANY, 1742 bytes)
15/08/21 11:17:48 INFO Executor: Running task 12.0 in stage 4.0 (TID 402)
15/08/21 11:17:48 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 378) in 8038 ms on localhost (16/19)
15/08/21 11:17:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:48 INFO Executor: Finished task 3.0 in stage 3.0 (TID 374). 2125 bytes result sent to driver
15/08/21 11:17:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:48 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 403, localhost, ANY, 1747 bytes)
15/08/21 11:17:48 INFO Executor: Running task 13.0 in stage 4.0 (TID 403)
15/08/21 11:17:48 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 374) in 8060 ms on localhost (17/19)
15/08/21 11:17:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061813 records.
15/08/21 11:17:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061669 records.
15/08/21 11:17:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:48 INFO InternalParquetRecordReader: block read in memory in 234 ms. row count = 3061301
15/08/21 11:17:48 INFO InternalParquetRecordReader: block read in memory in 152 ms. row count = 1784493
15/08/21 11:17:48 INFO InternalParquetRecordReader: block read in memory in 252 ms. row count = 3061650
15/08/21 11:17:48 INFO InternalParquetRecordReader: block read in memory in 232 ms. row count = 3061669
15/08/21 11:17:48 INFO Executor: Finished task 13.0 in stage 3.0 (TID 384). 2125 bytes result sent to driver
15/08/21 11:17:48 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 404, localhost, ANY, 1754 bytes)
15/08/21 11:17:48 INFO Executor: Running task 14.0 in stage 4.0 (TID 404)
15/08/21 11:17:48 INFO InternalParquetRecordReader: block read in memory in 259 ms. row count = 3061813
15/08/21 11:17:48 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 384) in 8365 ms on localhost (18/19)
15/08/21 11:17:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 268435456 end: 340147334 length: 71711878 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784858 records.
15/08/21 11:17:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:48 INFO InternalParquetRecordReader: block read in memory in 116 ms. row count = 1784858
15/08/21 11:17:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:53620 in memory (size: 29.3 KB, free: 20.7 GB)
15/08/21 11:17:52 INFO ContextCleaner: Cleaned shuffle 0
15/08/21 11:17:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:53620 in memory (size: 22.3 KB, free: 20.7 GB)
15/08/21 11:17:52 INFO Executor: Finished task 17.0 in stage 3.0 (TID 388). 2125 bytes result sent to driver
15/08/21 11:17:52 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 405, localhost, ANY, 1740 bytes)
15/08/21 11:17:52 INFO Executor: Running task 15.0 in stage 4.0 (TID 405)
15/08/21 11:17:52 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 388) in 6973 ms on localhost (19/19)
15/08/21 11:17:52 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 12.007 s
15/08/21 11:17:52 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/21 11:17:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:52 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5260ebe1
15/08/21 11:17:52 INFO StatsReportListener: task runtime:(count: 19, mean: 5911.631579, stdev: 2266.120130, max: 8365.000000, min: 485.000000)
15/08/21 11:17:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:52 INFO StatsReportListener: 	485.0 ms	485.0 ms	524.0 ms	5.1 s	5.4 s	7.9 s	8.1 s	8.4 s	8.4 s
15/08/21 11:17:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:52 INFO StatsReportListener: shuffle bytes written:(count: 19, mean: 8918860.526316, stdev: 8262561.827176, max: 17626413.000000, min: 769785.000000)
15/08/21 11:17:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:52 INFO StatsReportListener: 	751.7 KB	751.7 KB	753.5 KB	860.8 KB	3.1 MB	16.8 MB	16.8 MB	16.8 MB	16.8 MB
15/08/21 11:17:52 INFO DAGScheduler: looking for newly runnable stages
15/08/21 11:17:52 INFO DAGScheduler: running: Set(ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 4)
15/08/21 11:17:52 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/21 11:17:52 INFO StatsReportListener: task result size:(count: 19, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 11:17:52 INFO DAGScheduler: failed: Set()
15/08/21 11:17:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:52 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 11:17:52 INFO StatsReportListener: executor (non-fetch) time pct: (count: 19, mean: 98.197209, stdev: 2.680443, max: 99.670156, min: 87.835052)
15/08/21 11:17:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:52 INFO StatsReportListener: 	88 %	88 %	94 %	99 %	99 %	99 %	99 %	100 %	100 %
15/08/21 11:17:52 INFO StatsReportListener: other time pct: (count: 19, mean: 1.802791, stdev: 2.680443, max: 12.164948, min: 0.329844)
15/08/21 11:17:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:17:52 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 1 %	 6 %	12 %	12 %
15/08/21 11:17:52 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List(ShuffleMapStage 4)
15/08/21 11:17:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061498 records.
15/08/21 11:17:52 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 11:17:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:52 INFO InternalParquetRecordReader: block read in memory in 309 ms. row count = 3061498
15/08/21 11:17:53 INFO Executor: Finished task 2.0 in stage 4.0 (TID 392). 2125 bytes result sent to driver
15/08/21 11:17:53 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 406, localhost, ANY, 1746 bytes)
15/08/21 11:17:53 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 392) in 8472 ms on localhost (1/57)
15/08/21 11:17:53 INFO Executor: Running task 16.0 in stage 4.0 (TID 406)
15/08/21 11:17:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061799 records.
15/08/21 11:17:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:54 INFO Executor: Finished task 5.0 in stage 4.0 (TID 395). 2125 bytes result sent to driver
15/08/21 11:17:54 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 407, localhost, ANY, 1752 bytes)
15/08/21 11:17:54 INFO Executor: Running task 17.0 in stage 4.0 (TID 407)
15/08/21 11:17:54 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 395) in 8346 ms on localhost (2/57)
15/08/21 11:17:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 268435456 end: 343033905 length: 74598449 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1851904 records.
15/08/21 11:17:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:54 INFO InternalParquetRecordReader: block read in memory in 185 ms. row count = 3061799
15/08/21 11:17:54 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 1851904
15/08/21 11:17:55 INFO Executor: Finished task 8.0 in stage 4.0 (TID 398). 2125 bytes result sent to driver
15/08/21 11:17:55 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 408, localhost, ANY, 1741 bytes)
15/08/21 11:17:55 INFO Executor: Running task 18.0 in stage 4.0 (TID 408)
15/08/21 11:17:55 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 398) in 7620 ms on localhost (3/57)
15/08/21 11:17:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061581 records.
15/08/21 11:17:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:56 INFO InternalParquetRecordReader: block read in memory in 282 ms. row count = 3061581
15/08/21 11:17:56 INFO Executor: Finished task 14.0 in stage 4.0 (TID 404). 2125 bytes result sent to driver
15/08/21 11:17:56 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 409, localhost, ANY, 1747 bytes)
15/08/21 11:17:56 INFO Executor: Running task 19.0 in stage 4.0 (TID 409)
15/08/21 11:17:56 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 404) in 7434 ms on localhost (4/57)
15/08/21 11:17:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061649 records.
15/08/21 11:17:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:56 INFO InternalParquetRecordReader: block read in memory in 174 ms. row count = 3061649
15/08/21 11:17:56 INFO Executor: Finished task 3.0 in stage 4.0 (TID 393). 2125 bytes result sent to driver
15/08/21 11:17:56 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 410, localhost, ANY, 1753 bytes)
15/08/21 11:17:56 INFO Executor: Running task 20.0 in stage 4.0 (TID 410)
15/08/21 11:17:56 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 393) in 11314 ms on localhost (5/57)
15/08/21 11:17:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 268435456 end: 340167695 length: 71732239 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784882 records.
15/08/21 11:17:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:56 INFO Executor: Finished task 11.0 in stage 4.0 (TID 401). 2125 bytes result sent to driver
15/08/21 11:17:56 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 411, localhost, ANY, 1741 bytes)
15/08/21 11:17:56 INFO Executor: Running task 21.0 in stage 4.0 (TID 411)
15/08/21 11:17:56 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 401) in 8523 ms on localhost (6/57)
15/08/21 11:17:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061580 records.
15/08/21 11:17:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:56 INFO InternalParquetRecordReader: block read in memory in 108 ms. row count = 1784882
15/08/21 11:17:57 INFO InternalParquetRecordReader: block read in memory in 167 ms. row count = 3061580
15/08/21 11:17:57 INFO Executor: Finished task 1.0 in stage 4.0 (TID 391). 2125 bytes result sent to driver
15/08/21 11:17:57 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 412, localhost, ANY, 1747 bytes)
15/08/21 11:17:57 INFO Executor: Running task 22.0 in stage 4.0 (TID 412)
15/08/21 11:17:57 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 391) in 12407 ms on localhost (7/57)
15/08/21 11:17:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 390). 2125 bytes result sent to driver
15/08/21 11:17:57 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 413, localhost, ANY, 1754 bytes)
15/08/21 11:17:57 INFO Executor: Running task 23.0 in stage 4.0 (TID 413)
15/08/21 11:17:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 390) in 12471 ms on localhost (8/57)
15/08/21 11:17:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 268435456 end: 340141911 length: 71706455 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061754 records.
15/08/21 11:17:57 INFO Executor: Finished task 4.0 in stage 4.0 (TID 394). 2125 bytes result sent to driver
15/08/21 11:17:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784589 records.
15/08/21 11:17:57 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 414, localhost, ANY, 1742 bytes)
15/08/21 11:17:57 INFO Executor: Running task 24.0 in stage 4.0 (TID 414)
15/08/21 11:17:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:57 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 394) in 12223 ms on localhost (9/57)
15/08/21 11:17:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061826 records.
15/08/21 11:17:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:58 INFO InternalParquetRecordReader: block read in memory in 115 ms. row count = 1784589
15/08/21 11:17:58 INFO InternalParquetRecordReader: block read in memory in 181 ms. row count = 3061754
15/08/21 11:17:58 INFO InternalParquetRecordReader: block read in memory in 193 ms. row count = 3061826
15/08/21 11:17:58 INFO Executor: Finished task 6.0 in stage 4.0 (TID 396). 2125 bytes result sent to driver
15/08/21 11:17:58 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 415, localhost, ANY, 1747 bytes)
15/08/21 11:17:58 INFO Executor: Running task 25.0 in stage 4.0 (TID 415)
15/08/21 11:17:58 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 396) in 12465 ms on localhost (10/57)
15/08/21 11:17:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061906 records.
15/08/21 11:17:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:58 INFO InternalParquetRecordReader: block read in memory in 300 ms. row count = 3061906
15/08/21 11:17:58 INFO Executor: Finished task 9.0 in stage 4.0 (TID 399). 2125 bytes result sent to driver
15/08/21 11:17:58 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 416, localhost, ANY, 1755 bytes)
15/08/21 11:17:58 INFO Executor: Running task 26.0 in stage 4.0 (TID 416)
15/08/21 11:17:58 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 399) in 10710 ms on localhost (11/57)
15/08/21 11:17:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 268435456 end: 340152459 length: 71717003 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784988 records.
15/08/21 11:17:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:59 INFO InternalParquetRecordReader: block read in memory in 106 ms. row count = 1784988
15/08/21 11:17:59 INFO Executor: Finished task 17.0 in stage 4.0 (TID 407). 2125 bytes result sent to driver
15/08/21 11:17:59 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 417, localhost, ANY, 1741 bytes)
15/08/21 11:17:59 INFO Executor: Running task 27.0 in stage 4.0 (TID 417)
15/08/21 11:17:59 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 407) in 5469 ms on localhost (12/57)
15/08/21 11:17:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:59 INFO Executor: Finished task 7.0 in stage 4.0 (TID 397). 2125 bytes result sent to driver
15/08/21 11:17:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061659 records.
15/08/21 11:17:59 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 418, localhost, ANY, 1747 bytes)
15/08/21 11:17:59 INFO Executor: Running task 28.0 in stage 4.0 (TID 418)
15/08/21 11:17:59 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 397) in 11844 ms on localhost (13/57)
15/08/21 11:17:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061649 records.
15/08/21 11:17:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:59 INFO InternalParquetRecordReader: block read in memory in 170 ms. row count = 3061659
15/08/21 11:17:59 INFO Executor: Finished task 10.0 in stage 4.0 (TID 400). 2125 bytes result sent to driver
15/08/21 11:17:59 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 419, localhost, ANY, 1754 bytes)
15/08/21 11:17:59 INFO Executor: Running task 29.0 in stage 4.0 (TID 419)
15/08/21 11:17:59 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 400) in 11562 ms on localhost (14/57)
15/08/21 11:17:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 268435456 end: 340149052 length: 71713596 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:59 INFO InternalParquetRecordReader: block read in memory in 219 ms. row count = 3061649
15/08/21 11:17:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784790 records.
15/08/21 11:17:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:59 INFO Executor: Finished task 13.0 in stage 4.0 (TID 403). 2125 bytes result sent to driver
15/08/21 11:17:59 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 420, localhost, ANY, 1742 bytes)
15/08/21 11:17:59 INFO Executor: Running task 30.0 in stage 4.0 (TID 420)
15/08/21 11:17:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:59 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 403) in 11505 ms on localhost (15/57)
15/08/21 11:17:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061804 records.
15/08/21 11:17:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:17:59 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 1784790
15/08/21 11:17:59 INFO Executor: Finished task 12.0 in stage 4.0 (TID 402). 2125 bytes result sent to driver
15/08/21 11:17:59 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 421, localhost, ANY, 1747 bytes)
15/08/21 11:17:59 INFO Executor: Running task 31.0 in stage 4.0 (TID 421)
15/08/21 11:17:59 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 402) in 11613 ms on localhost (16/57)
15/08/21 11:17:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:17:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:17:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061148 records.
15/08/21 11:18:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:00 INFO InternalParquetRecordReader: block read in memory in 151 ms. row count = 3061804
15/08/21 11:18:00 INFO InternalParquetRecordReader: block read in memory in 314 ms. row count = 3061148
15/08/21 11:18:01 INFO Executor: Finished task 15.0 in stage 4.0 (TID 405). 2125 bytes result sent to driver
15/08/21 11:18:01 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 422, localhost, ANY, 1754 bytes)
15/08/21 11:18:01 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 405) in 8757 ms on localhost (17/57)
15/08/21 11:18:01 INFO Executor: Running task 32.0 in stage 4.0 (TID 422)
15/08/21 11:18:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 268435456 end: 340153987 length: 71718531 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784820 records.
15/08/21 11:18:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:01 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 1784820
15/08/21 11:18:01 INFO Executor: Finished task 20.0 in stage 4.0 (TID 410). 2125 bytes result sent to driver
15/08/21 11:18:01 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 423, localhost, ANY, 1741 bytes)
15/08/21 11:18:01 INFO Executor: Running task 33.0 in stage 4.0 (TID 423)
15/08/21 11:18:01 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 410) in 4604 ms on localhost (18/57)
15/08/21 11:18:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061090 records.
15/08/21 11:18:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:01 INFO InternalParquetRecordReader: block read in memory in 129 ms. row count = 3061090
15/08/21 11:18:02 INFO Executor: Finished task 23.0 in stage 4.0 (TID 413). 2125 bytes result sent to driver
15/08/21 11:18:02 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 424, localhost, ANY, 1747 bytes)
15/08/21 11:18:02 INFO Executor: Running task 34.0 in stage 4.0 (TID 424)
15/08/21 11:18:02 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 413) in 4834 ms on localhost (19/57)
15/08/21 11:18:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061657 records.
15/08/21 11:18:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:03 INFO InternalParquetRecordReader: block read in memory in 261 ms. row count = 3061657
15/08/21 11:18:03 INFO Executor: Finished task 16.0 in stage 4.0 (TID 406). 2125 bytes result sent to driver
15/08/21 11:18:03 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 425, localhost, ANY, 1755 bytes)
15/08/21 11:18:03 INFO Executor: Running task 35.0 in stage 4.0 (TID 425)
15/08/21 11:18:03 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 406) in 9256 ms on localhost (20/57)
15/08/21 11:18:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 268435456 end: 340154038 length: 71718582 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784628 records.
15/08/21 11:18:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:03 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 1784628
15/08/21 11:18:03 INFO Executor: Finished task 26.0 in stage 4.0 (TID 416). 2125 bytes result sent to driver
15/08/21 11:18:03 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 426, localhost, ANY, 1741 bytes)
15/08/21 11:18:03 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 416) in 4875 ms on localhost (21/57)
15/08/21 11:18:03 INFO Executor: Running task 36.0 in stage 4.0 (TID 426)
15/08/21 11:18:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061565 records.
15/08/21 11:18:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:03 INFO Executor: Finished task 18.0 in stage 4.0 (TID 408). 2125 bytes result sent to driver
15/08/21 11:18:03 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 427, localhost, ANY, 1747 bytes)
15/08/21 11:18:03 INFO Executor: Running task 37.0 in stage 4.0 (TID 427)
15/08/21 11:18:03 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 408) in 8139 ms on localhost (22/57)
15/08/21 11:18:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061553 records.
15/08/21 11:18:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:03 INFO Executor: Finished task 19.0 in stage 4.0 (TID 409). 2125 bytes result sent to driver
15/08/21 11:18:03 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 428, localhost, ANY, 1753 bytes)
15/08/21 11:18:03 INFO Executor: Running task 38.0 in stage 4.0 (TID 428)
15/08/21 11:18:03 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 409) in 7753 ms on localhost (23/57)
15/08/21 11:18:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 268435456 end: 340156978 length: 71721522 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1785031 records.
15/08/21 11:18:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:03 INFO InternalParquetRecordReader: block read in memory in 123 ms. row count = 3061565
15/08/21 11:18:03 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 1785031
15/08/21 11:18:03 INFO InternalParquetRecordReader: block read in memory in 147 ms. row count = 3061553
15/08/21 11:18:04 INFO Executor: Finished task 29.0 in stage 4.0 (TID 419). 2125 bytes result sent to driver
15/08/21 11:18:04 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 429, localhost, ANY, 1741 bytes)
15/08/21 11:18:04 INFO Executor: Running task 39.0 in stage 4.0 (TID 429)
15/08/21 11:18:04 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 419) in 4671 ms on localhost (24/57)
15/08/21 11:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061805 records.
15/08/21 11:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:04 INFO InternalParquetRecordReader: block read in memory in 119 ms. row count = 3061805
15/08/21 11:18:05 INFO Executor: Finished task 32.0 in stage 4.0 (TID 422). 2125 bytes result sent to driver
15/08/21 11:18:05 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 430, localhost, ANY, 1747 bytes)
15/08/21 11:18:05 INFO Executor: Running task 40.0 in stage 4.0 (TID 430)
15/08/21 11:18:05 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 422) in 4178 ms on localhost (25/57)
15/08/21 11:18:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061589 records.
15/08/21 11:18:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:05 INFO Executor: Finished task 21.0 in stage 4.0 (TID 411). 2125 bytes result sent to driver
15/08/21 11:18:05 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 431, localhost, ANY, 1754 bytes)
15/08/21 11:18:05 INFO Executor: Running task 41.0 in stage 4.0 (TID 431)
15/08/21 11:18:05 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 411) in 8648 ms on localhost (26/57)
15/08/21 11:18:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 268435456 end: 340173405 length: 71737949 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:05 INFO InternalParquetRecordReader: block read in memory in 214 ms. row count = 3061589
15/08/21 11:18:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784699 records.
15/08/21 11:18:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:05 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 1784699
15/08/21 11:18:05 INFO Executor: Finished task 24.0 in stage 4.0 (TID 414). 2125 bytes result sent to driver
15/08/21 11:18:05 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 432, localhost, ANY, 1741 bytes)
15/08/21 11:18:05 INFO Executor: Running task 42.0 in stage 4.0 (TID 432)
15/08/21 11:18:05 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 414) in 7746 ms on localhost (27/57)
15/08/21 11:18:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061319 records.
15/08/21 11:18:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:06 INFO InternalParquetRecordReader: block read in memory in 734 ms. row count = 3061319
15/08/21 11:18:06 INFO Executor: Finished task 25.0 in stage 4.0 (TID 415). 2125 bytes result sent to driver
15/08/21 11:18:06 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 433, localhost, ANY, 1747 bytes)
15/08/21 11:18:06 INFO Executor: Running task 43.0 in stage 4.0 (TID 433)
15/08/21 11:18:06 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 415) in 8162 ms on localhost (28/57)
15/08/21 11:18:06 INFO Executor: Finished task 22.0 in stage 4.0 (TID 412). 2125 bytes result sent to driver
15/08/21 11:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:06 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 434, localhost, ANY, 1752 bytes)
15/08/21 11:18:06 INFO Executor: Running task 44.0 in stage 4.0 (TID 434)
15/08/21 11:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:06 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 412) in 8659 ms on localhost (29/57)
15/08/21 11:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 268435456 end: 340161363 length: 71725907 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061685 records.
15/08/21 11:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784830 records.
15/08/21 11:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:06 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 1784830
15/08/21 11:18:06 INFO InternalParquetRecordReader: block read in memory in 153 ms. row count = 3061685
15/08/21 11:18:07 INFO Executor: Finished task 27.0 in stage 4.0 (TID 417). 2125 bytes result sent to driver
15/08/21 11:18:07 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 435, localhost, ANY, 1741 bytes)
15/08/21 11:18:07 INFO Executor: Running task 45.0 in stage 4.0 (TID 435)
15/08/21 11:18:07 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 417) in 7695 ms on localhost (30/57)
15/08/21 11:18:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061466 records.
15/08/21 11:18:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:07 INFO Executor: Finished task 28.0 in stage 4.0 (TID 418). 2125 bytes result sent to driver
15/08/21 11:18:07 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 436, localhost, ANY, 1747 bytes)
15/08/21 11:18:07 INFO Executor: Running task 46.0 in stage 4.0 (TID 436)
15/08/21 11:18:07 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 418) in 7753 ms on localhost (31/57)
15/08/21 11:18:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061853 records.
15/08/21 11:18:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:07 INFO InternalParquetRecordReader: block read in memory in 147 ms. row count = 3061466
15/08/21 11:18:07 INFO InternalParquetRecordReader: block read in memory in 180 ms. row count = 3061853
15/08/21 11:18:07 INFO Executor: Finished task 35.0 in stage 4.0 (TID 425). 2125 bytes result sent to driver
15/08/21 11:18:07 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 437, localhost, ANY, 1755 bytes)
15/08/21 11:18:07 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 425) in 4624 ms on localhost (32/57)
15/08/21 11:18:07 INFO Executor: Running task 47.0 in stage 4.0 (TID 437)
15/08/21 11:18:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 268435456 end: 319211896 length: 50776440 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1297836 records.
15/08/21 11:18:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:07 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 1297836
15/08/21 11:18:07 INFO Executor: Finished task 31.0 in stage 4.0 (TID 421). 2125 bytes result sent to driver
15/08/21 11:18:07 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 438, localhost, ANY, 1741 bytes)
15/08/21 11:18:07 INFO Executor: Running task 48.0 in stage 4.0 (TID 438)
15/08/21 11:18:07 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 421) in 7966 ms on localhost (33/57)
15/08/21 11:18:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062023 records.
15/08/21 11:18:07 INFO Executor: Finished task 30.0 in stage 4.0 (TID 420). 2125 bytes result sent to driver
15/08/21 11:18:07 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 439, localhost, ANY, 1747 bytes)
15/08/21 11:18:07 INFO Executor: Running task 49.0 in stage 4.0 (TID 439)
15/08/21 11:18:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:07 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 420) in 8079 ms on localhost (34/57)
15/08/21 11:18:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061647 records.
15/08/21 11:18:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:08 INFO InternalParquetRecordReader: block read in memory in 178 ms. row count = 3062023
15/08/21 11:18:08 INFO InternalParquetRecordReader: block read in memory in 166 ms. row count = 3061647
15/08/21 11:18:08 INFO Executor: Finished task 38.0 in stage 4.0 (TID 428). 2125 bytes result sent to driver
15/08/21 11:18:08 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 440, localhost, ANY, 1755 bytes)
15/08/21 11:18:08 INFO Executor: Running task 50.0 in stage 4.0 (TID 440)
15/08/21 11:18:08 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 428) in 4449 ms on localhost (35/57)
15/08/21 11:18:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 268435456 end: 340156068 length: 71720612 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784477 records.
15/08/21 11:18:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:08 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 1784477
15/08/21 11:18:08 INFO Executor: Finished task 33.0 in stage 4.0 (TID 423). 2125 bytes result sent to driver
15/08/21 11:18:08 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 441, localhost, ANY, 1741 bytes)
15/08/21 11:18:08 INFO Executor: Running task 51.0 in stage 4.0 (TID 441)
15/08/21 11:18:08 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 423) in 7375 ms on localhost (36/57)
15/08/21 11:18:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062000 records.
15/08/21 11:18:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:08 INFO InternalParquetRecordReader: block read in memory in 153 ms. row count = 3062000
15/08/21 11:18:11 INFO Executor: Finished task 34.0 in stage 4.0 (TID 424). 2125 bytes result sent to driver
15/08/21 11:18:11 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 442, localhost, ANY, 1746 bytes)
15/08/21 11:18:11 INFO Executor: Running task 52.0 in stage 4.0 (TID 442)
15/08/21 11:18:11 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 424) in 8363 ms on localhost (37/57)
15/08/21 11:18:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:11 INFO Executor: Finished task 41.0 in stage 4.0 (TID 431). 2125 bytes result sent to driver
15/08/21 11:18:11 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 443, localhost, ANY, 1754 bytes)
15/08/21 11:18:11 INFO Executor: Running task 53.0 in stage 4.0 (TID 443)
15/08/21 11:18:11 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 431) in 5621 ms on localhost (38/57)
15/08/21 11:18:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 268435456 end: 343036825 length: 74601369 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062082 records.
15/08/21 11:18:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1851764 records.
15/08/21 11:18:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:11 INFO InternalParquetRecordReader: block read in memory in 154 ms. row count = 1851764
15/08/21 11:18:11 INFO InternalParquetRecordReader: block read in memory in 192 ms. row count = 3062082
15/08/21 11:18:11 INFO Executor: Finished task 47.0 in stage 4.0 (TID 437). 2125 bytes result sent to driver
15/08/21 11:18:11 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 444, localhost, ANY, 1741 bytes)
15/08/21 11:18:11 INFO Executor: Running task 54.0 in stage 4.0 (TID 444)
15/08/21 11:18:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:11 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 437) in 4002 ms on localhost (39/57)
15/08/21 11:18:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:11 INFO Executor: Finished task 44.0 in stage 4.0 (TID 434). 2125 bytes result sent to driver
15/08/21 11:18:11 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 445, localhost, ANY, 1747 bytes)
15/08/21 11:18:11 INFO Executor: Running task 55.0 in stage 4.0 (TID 445)
15/08/21 11:18:11 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 434) in 5298 ms on localhost (40/57)
15/08/21 11:18:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061869 records.
15/08/21 11:18:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062071 records.
15/08/21 11:18:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:12 INFO InternalParquetRecordReader: block read in memory in 187 ms. row count = 3062071
15/08/21 11:18:12 INFO InternalParquetRecordReader: block read in memory in 200 ms. row count = 3061869
15/08/21 11:18:12 INFO Executor: Finished task 36.0 in stage 4.0 (TID 426). 2125 bytes result sent to driver
15/08/21 11:18:12 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 446, localhost, ANY, 1753 bytes)
15/08/21 11:18:12 INFO Executor: Running task 56.0 in stage 4.0 (TID 446)
15/08/21 11:18:12 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 426) in 8385 ms on localhost (41/57)
15/08/21 11:18:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 268435456 end: 340157695 length: 71722239 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784419 records.
15/08/21 11:18:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:12 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 1784419
15/08/21 11:18:12 INFO Executor: Finished task 37.0 in stage 4.0 (TID 427). 2125 bytes result sent to driver
15/08/21 11:18:12 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 447, localhost, ANY, 1693 bytes)
15/08/21 11:18:12 INFO Executor: Running task 0.0 in stage 6.0 (TID 447)
15/08/21 11:18:12 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 427) in 8569 ms on localhost (42/57)
15/08/21 11:18:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00140-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499669 length: 3499669 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:12 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:12 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 750000
15/08/21 11:18:12 INFO Executor: Finished task 39.0 in stage 4.0 (TID 429). 2125 bytes result sent to driver
15/08/21 11:18:12 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 448, localhost, ANY, 1692 bytes)
15/08/21 11:18:12 INFO Executor: Running task 1.0 in stage 6.0 (TID 448)
15/08/21 11:18:12 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 429) in 8263 ms on localhost (43/57)
15/08/21 11:18:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00191-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506064 length: 3506064 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:12 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:12 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 11:18:12 INFO Executor: Finished task 0.0 in stage 6.0 (TID 447). 2125 bytes result sent to driver
15/08/21 11:18:12 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 449, localhost, ANY, 1691 bytes)
15/08/21 11:18:12 INFO Executor: Running task 2.0 in stage 6.0 (TID 449)
15/08/21 11:18:12 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 447) in 629 ms on localhost (1/200)
15/08/21 11:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00004-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500159 length: 3500159 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:13 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 11:18:13 INFO Executor: Finished task 50.0 in stage 4.0 (TID 440). 2125 bytes result sent to driver
15/08/21 11:18:13 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 450, localhost, ANY, 1695 bytes)
15/08/21 11:18:13 INFO Executor: Running task 3.0 in stage 6.0 (TID 450)
15/08/21 11:18:13 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 440) in 4849 ms on localhost (44/57)
15/08/21 11:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00173-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3508672 length: 3508672 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:13 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 11:18:13 INFO Executor: Finished task 1.0 in stage 6.0 (TID 448). 2125 bytes result sent to driver
15/08/21 11:18:13 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 451, localhost, ANY, 1694 bytes)
15/08/21 11:18:13 INFO Executor: Running task 4.0 in stage 6.0 (TID 451)
15/08/21 11:18:13 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 448) in 507 ms on localhost (2/200)
15/08/21 11:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00022-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506607 length: 3506607 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:13 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:13 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:13 INFO Executor: Finished task 2.0 in stage 6.0 (TID 449). 2125 bytes result sent to driver
15/08/21 11:18:13 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 452, localhost, ANY, 1693 bytes)
15/08/21 11:18:13 INFO Executor: Running task 5.0 in stage 6.0 (TID 452)
15/08/21 11:18:13 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 449) in 504 ms on localhost (3/200)
15/08/21 11:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00188-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501816 length: 3501816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 11:18:13 INFO Executor: Finished task 3.0 in stage 6.0 (TID 450). 2125 bytes result sent to driver
15/08/21 11:18:13 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 453, localhost, ANY, 1693 bytes)
15/08/21 11:18:13 INFO Executor: Running task 6.0 in stage 6.0 (TID 453)
15/08/21 11:18:13 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 450) in 447 ms on localhost (4/200)
15/08/21 11:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00000-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3509124 length: 3509124 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:13 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 750000
15/08/21 11:18:13 INFO Executor: Finished task 4.0 in stage 6.0 (TID 451). 2125 bytes result sent to driver
15/08/21 11:18:13 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 454, localhost, ANY, 1694 bytes)
15/08/21 11:18:13 INFO Executor: Running task 7.0 in stage 6.0 (TID 454)
15/08/21 11:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00127-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504800 length: 3504800 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:13 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 451) in 485 ms on localhost (5/200)
15/08/21 11:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 11:18:13 INFO Executor: Finished task 5.0 in stage 6.0 (TID 452). 2125 bytes result sent to driver
15/08/21 11:18:13 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 455, localhost, ANY, 1692 bytes)
15/08/21 11:18:13 INFO Executor: Running task 8.0 in stage 6.0 (TID 455)
15/08/21 11:18:13 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 452) in 408 ms on localhost (6/200)
15/08/21 11:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00179-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501177 length: 3501177 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:14 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 11:18:14 INFO Executor: Finished task 6.0 in stage 6.0 (TID 453). 2125 bytes result sent to driver
15/08/21 11:18:14 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 456, localhost, ANY, 1693 bytes)
15/08/21 11:18:14 INFO Executor: Running task 9.0 in stage 6.0 (TID 456)
15/08/21 11:18:14 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 453) in 605 ms on localhost (7/200)
15/08/21 11:18:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00145-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498381 length: 3498381 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:14 INFO Executor: Finished task 7.0 in stage 6.0 (TID 454). 2125 bytes result sent to driver
15/08/21 11:18:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:14 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 457, localhost, ANY, 1693 bytes)
15/08/21 11:18:14 INFO Executor: Running task 10.0 in stage 6.0 (TID 457)
15/08/21 11:18:14 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 454) in 532 ms on localhost (8/200)
15/08/21 11:18:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00182-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506307 length: 3506307 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:14 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 11:18:14 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 750000
15/08/21 11:18:14 INFO Executor: Finished task 40.0 in stage 4.0 (TID 430). 2125 bytes result sent to driver
15/08/21 11:18:14 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 458, localhost, ANY, 1694 bytes)
15/08/21 11:18:14 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 430) in 9144 ms on localhost (45/57)
15/08/21 11:18:14 INFO Executor: Running task 11.0 in stage 6.0 (TID 458)
15/08/21 11:18:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00118-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507229 length: 3507229 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:14 INFO Executor: Finished task 8.0 in stage 6.0 (TID 455). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 459, localhost, ANY, 1694 bytes)
15/08/21 11:18:15 INFO Executor: Running task 12.0 in stage 6.0 (TID 459)
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 11:18:15 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 455) in 1220 ms on localhost (9/200)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00072-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506406 length: 3506406 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:15 INFO Executor: Finished task 10.0 in stage 6.0 (TID 457). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO Executor: Finished task 9.0 in stage 6.0 (TID 456). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 460, localhost, ANY, 1692 bytes)
15/08/21 11:18:15 INFO Executor: Running task 13.0 in stage 6.0 (TID 460)
15/08/21 11:18:15 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 461, localhost, ANY, 1693 bytes)
15/08/21 11:18:15 INFO Executor: Running task 14.0 in stage 6.0 (TID 461)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00062-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502754 length: 3502754 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 456) in 1129 ms on localhost (10/200)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00017-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500155 length: 3500155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 457) in 1108 ms on localhost (11/200)
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 750000
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 750000
15/08/21 11:18:15 INFO Executor: Finished task 12.0 in stage 6.0 (TID 459). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 462, localhost, ANY, 1692 bytes)
15/08/21 11:18:15 INFO Executor: Running task 15.0 in stage 6.0 (TID 462)
15/08/21 11:18:15 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 459) in 409 ms on localhost (12/200)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00036-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500319 length: 3500319 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 750000
15/08/21 11:18:15 INFO Executor: Finished task 11.0 in stage 6.0 (TID 458). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 463, localhost, ANY, 1693 bytes)
15/08/21 11:18:15 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 458) in 1259 ms on localhost (13/200)
15/08/21 11:18:15 INFO Executor: Running task 16.0 in stage 6.0 (TID 463)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00126-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506809 length: 3506809 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO Executor: Finished task 13.0 in stage 6.0 (TID 460). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 464, localhost, ANY, 1693 bytes)
15/08/21 11:18:15 INFO Executor: Running task 17.0 in stage 6.0 (TID 464)
15/08/21 11:18:15 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 460) in 379 ms on localhost (14/200)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00172-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502333 length: 3502333 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 11:18:15 INFO Executor: Finished task 14.0 in stage 6.0 (TID 461). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 465, localhost, ANY, 1692 bytes)
15/08/21 11:18:15 INFO Executor: Running task 18.0 in stage 6.0 (TID 465)
15/08/21 11:18:15 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 461) in 498 ms on localhost (15/200)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00106-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498575 length: 3498575 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 750000
15/08/21 11:18:15 INFO Executor: Finished task 42.0 in stage 4.0 (TID 432). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 466, localhost, ANY, 1692 bytes)
15/08/21 11:18:15 INFO Executor: Running task 19.0 in stage 6.0 (TID 466)
15/08/21 11:18:15 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 432) in 10254 ms on localhost (46/57)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00070-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504014 length: 3504014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:15 INFO Executor: Finished task 15.0 in stage 6.0 (TID 462). 2125 bytes result sent to driver
15/08/21 11:18:15 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 467, localhost, ANY, 1694 bytes)
15/08/21 11:18:15 INFO Executor: Running task 20.0 in stage 6.0 (TID 467)
15/08/21 11:18:15 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 462) in 514 ms on localhost (16/200)
15/08/21 11:18:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00012-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500010 length: 3500010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:15 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 11:18:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 17.0 in stage 6.0 (TID 464). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 468, localhost, ANY, 1691 bytes)
15/08/21 11:18:16 INFO Executor: Finished task 16.0 in stage 6.0 (TID 463). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 469, localhost, ANY, 1694 bytes)
15/08/21 11:18:16 INFO Executor: Running task 22.0 in stage 6.0 (TID 469)
15/08/21 11:18:16 INFO Executor: Running task 21.0 in stage 6.0 (TID 468)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 464) in 458 ms on localhost (17/200)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 463) in 512 ms on localhost (18/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00030-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506260 length: 3506260 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00178-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499383 length: 3499383 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 19.0 in stage 6.0 (TID 466). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 11:18:16 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 470, localhost, ANY, 1693 bytes)
15/08/21 11:18:16 INFO Executor: Running task 23.0 in stage 6.0 (TID 470)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 466) in 319 ms on localhost (19/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00125-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505822 length: 3505822 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 43.0 in stage 4.0 (TID 433). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 471, localhost, ANY, 1694 bytes)
15/08/21 11:18:16 INFO Executor: Finished task 18.0 in stage 6.0 (TID 465). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 433) in 9762 ms on localhost (47/57)
15/08/21 11:18:16 INFO Executor: Running task 24.0 in stage 6.0 (TID 471)
15/08/21 11:18:16 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 472, localhost, ANY, 1693 bytes)
15/08/21 11:18:16 INFO Executor: Running task 25.0 in stage 6.0 (TID 472)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 465) in 473 ms on localhost (20/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00120-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505053 length: 3505053 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00160-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503635 length: 3503635 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 11:18:16 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 20.0 in stage 6.0 (TID 467). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 473, localhost, ANY, 1694 bytes)
15/08/21 11:18:16 INFO Executor: Running task 26.0 in stage 6.0 (TID 473)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 467) in 434 ms on localhost (21/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00175-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504840 length: 3504840 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 21.0 in stage 6.0 (TID 468). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 474, localhost, ANY, 1693 bytes)
15/08/21 11:18:16 INFO Executor: Running task 27.0 in stage 6.0 (TID 474)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 468) in 480 ms on localhost (22/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00098-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500259 length: 3500259 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO Executor: Finished task 22.0 in stage 6.0 (TID 469). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 475, localhost, ANY, 1693 bytes)
15/08/21 11:18:16 INFO Executor: Running task 28.0 in stage 6.0 (TID 475)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 469) in 524 ms on localhost (23/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00148-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499209 length: 3499209 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 23.0 in stage 6.0 (TID 470). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 476, localhost, ANY, 1692 bytes)
15/08/21 11:18:16 INFO Executor: Running task 29.0 in stage 6.0 (TID 476)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 470) in 565 ms on localhost (24/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00137-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498602 length: 3498602 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 24.0 in stage 6.0 (TID 471). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO Executor: Finished task 25.0 in stage 6.0 (TID 472). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 477, localhost, ANY, 1694 bytes)
15/08/21 11:18:16 INFO Executor: Running task 30.0 in stage 6.0 (TID 477)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00124-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502312 length: 3502312 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 478, localhost, ANY, 1694 bytes)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 471) in 555 ms on localhost (25/200)
15/08/21 11:18:16 INFO Executor: Running task 31.0 in stage 6.0 (TID 478)
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00119-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506360 length: 3506360 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 472) in 558 ms on localhost (26/200)
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 11:18:16 INFO Executor: Finished task 26.0 in stage 6.0 (TID 473). 2125 bytes result sent to driver
15/08/21 11:18:16 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 479, localhost, ANY, 1693 bytes)
15/08/21 11:18:16 INFO Executor: Running task 32.0 in stage 6.0 (TID 479)
15/08/21 11:18:16 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 473) in 538 ms on localhost (27/200)
15/08/21 11:18:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00044-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500433 length: 3500433 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:16 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 11:18:17 INFO Executor: Finished task 27.0 in stage 6.0 (TID 474). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 480, localhost, ANY, 1693 bytes)
15/08/21 11:18:17 INFO Executor: Running task 33.0 in stage 6.0 (TID 480)
15/08/21 11:18:17 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 474) in 562 ms on localhost (28/200)
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00111-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507275 length: 3507275 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 750000
15/08/21 11:18:17 INFO Executor: Finished task 28.0 in stage 6.0 (TID 475). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 481, localhost, ANY, 1693 bytes)
15/08/21 11:18:17 INFO Executor: Running task 34.0 in stage 6.0 (TID 481)
15/08/21 11:18:17 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 475) in 715 ms on localhost (29/200)
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00024-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502988 length: 3502988 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO Executor: Finished task 29.0 in stage 6.0 (TID 476). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 482, localhost, ANY, 1692 bytes)
15/08/21 11:18:17 INFO Executor: Running task 35.0 in stage 6.0 (TID 482)
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00011-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498475 length: 3498475 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 476) in 638 ms on localhost (30/200)
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 750000
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 11:18:17 INFO Executor: Finished task 30.0 in stage 6.0 (TID 477). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 483, localhost, ANY, 1693 bytes)
15/08/21 11:18:17 INFO Executor: Running task 36.0 in stage 6.0 (TID 483)
15/08/21 11:18:17 INFO Executor: Finished task 31.0 in stage 6.0 (TID 478). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 484, localhost, ANY, 1692 bytes)
15/08/21 11:18:17 INFO Executor: Running task 37.0 in stage 6.0 (TID 484)
15/08/21 11:18:17 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 477) in 672 ms on localhost (31/200)
15/08/21 11:18:17 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 478) in 669 ms on localhost (32/200)
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00100-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500864 length: 3500864 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00039-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506046 length: 3506046 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO Executor: Finished task 32.0 in stage 6.0 (TID 479). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 485, localhost, ANY, 1691 bytes)
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO Executor: Running task 38.0 in stage 6.0 (TID 485)
15/08/21 11:18:17 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 479) in 639 ms on localhost (33/200)
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00008-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506690 length: 3506690 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 85 ms. row count = 750000
15/08/21 11:18:17 INFO Executor: Finished task 33.0 in stage 6.0 (TID 480). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 486, localhost, ANY, 1693 bytes)
15/08/21 11:18:17 INFO Executor: Running task 39.0 in stage 6.0 (TID 486)
15/08/21 11:18:17 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 480) in 638 ms on localhost (34/200)
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00021-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506265 length: 3506265 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO Executor: Finished task 45.0 in stage 4.0 (TID 435). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 487, localhost, ANY, 1693 bytes)
15/08/21 11:18:17 INFO Executor: Running task 40.0 in stage 6.0 (TID 487)
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00027-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501052 length: 3501052 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 435) in 10670 ms on localhost (48/57)
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:17 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:17 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 11:18:17 INFO Executor: Finished task 34.0 in stage 6.0 (TID 481). 2125 bytes result sent to driver
15/08/21 11:18:17 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 488, localhost, ANY, 1694 bytes)
15/08/21 11:18:17 INFO Executor: Running task 41.0 in stage 6.0 (TID 488)
15/08/21 11:18:17 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 481) in 605 ms on localhost (35/200)
15/08/21 11:18:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00086-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507402 length: 3507402 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO Executor: Finished task 35.0 in stage 6.0 (TID 482). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 11:18:18 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 489, localhost, ANY, 1692 bytes)
15/08/21 11:18:18 INFO Executor: Running task 42.0 in stage 6.0 (TID 489)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 482) in 642 ms on localhost (36/200)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00174-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505785 length: 3505785 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 750000
15/08/21 11:18:18 INFO Executor: Finished task 36.0 in stage 6.0 (TID 483). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 490, localhost, ANY, 1692 bytes)
15/08/21 11:18:18 INFO Executor: Running task 43.0 in stage 6.0 (TID 490)
15/08/21 11:18:18 INFO Executor: Finished task 39.0 in stage 6.0 (TID 486). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 491, localhost, ANY, 1694 bytes)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 483) in 615 ms on localhost (37/200)
15/08/21 11:18:18 INFO Executor: Running task 44.0 in stage 6.0 (TID 491)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 486) in 318 ms on localhost (38/200)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00029-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505200 length: 3505200 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00015-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505595 length: 3505595 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 INFO Executor: Finished task 37.0 in stage 6.0 (TID 484). 2125 bytes result sent to driver
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 492, localhost, ANY, 1694 bytes)
15/08/21 11:18:18 INFO Executor: Running task 45.0 in stage 6.0 (TID 492)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 484) in 618 ms on localhost (39/200)
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00135-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505648 length: 3505648 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO Executor: Finished task 53.0 in stage 4.0 (TID 443). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 493, localhost, ANY, 1692 bytes)
15/08/21 11:18:18 INFO Executor: Running task 46.0 in stage 6.0 (TID 493)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 443) in 7032 ms on localhost (49/57)
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00103-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504162 length: 3504162 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 INFO Executor: Finished task 38.0 in stage 6.0 (TID 485). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 494, localhost, ANY, 1691 bytes)
15/08/21 11:18:18 INFO Executor: Running task 47.0 in stage 6.0 (TID 494)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00018-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500127 length: 3500127 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 485) in 625 ms on localhost (40/200)
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 82 ms. row count = 750000
15/08/21 11:18:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 97 ms. row count = 750000
15/08/21 11:18:18 INFO Executor: Finished task 46.0 in stage 4.0 (TID 436). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 495, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO Executor: Running task 48.0 in stage 6.0 (TID 495)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 436) in 11017 ms on localhost (50/57)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00177-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502215 length: 3502215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO Executor: Finished task 40.0 in stage 6.0 (TID 487). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 496, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO Executor: Running task 49.0 in stage 6.0 (TID 496)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 487) in 466 ms on localhost (41/200)
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00094-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504770 length: 3504770 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 750000
15/08/21 11:18:18 INFO Executor: Finished task 41.0 in stage 6.0 (TID 488). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 50.0 in stage 6.0 (TID 497, localhost, ANY, 1692 bytes)
15/08/21 11:18:18 INFO Executor: Running task 50.0 in stage 6.0 (TID 497)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 488) in 556 ms on localhost (42/200)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00002-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499762 length: 3499762 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO Executor: Finished task 46.0 in stage 6.0 (TID 493). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 51.0 in stage 6.0 (TID 498, localhost, ANY, 1694 bytes)
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO Executor: Running task 51.0 in stage 6.0 (TID 498)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 493) in 412 ms on localhost (43/200)
15/08/21 11:18:18 INFO Executor: Finished task 42.0 in stage 6.0 (TID 489). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00143-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504817 length: 3504817 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 INFO TaskSetManager: Starting task 52.0 in stage 6.0 (TID 499, localhost, ANY, 1692 bytes)
15/08/21 11:18:18 INFO Executor: Running task 52.0 in stage 6.0 (TID 499)
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 489) in 526 ms on localhost (44/200)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00071-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505003 length: 3505003 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 750000
15/08/21 11:18:18 INFO Executor: Finished task 45.0 in stage 6.0 (TID 492). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 53.0 in stage 6.0 (TID 500, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO Executor: Running task 53.0 in stage 6.0 (TID 500)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 492) in 627 ms on localhost (45/200)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00133-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506051 length: 3506051 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO Executor: Finished task 44.0 in stage 6.0 (TID 491). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO TaskSetManager: Starting task 54.0 in stage 6.0 (TID 501, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO Executor: Running task 54.0 in stage 6.0 (TID 501)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00042-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499078 length: 3499078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 491) in 679 ms on localhost (46/200)
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO Executor: Finished task 47.0 in stage 6.0 (TID 494). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 55.0 in stage 6.0 (TID 502, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 494) in 683 ms on localhost (47/200)
15/08/21 11:18:18 INFO Executor: Running task 55.0 in stage 6.0 (TID 502)
15/08/21 11:18:18 INFO Executor: Finished task 43.0 in stage 6.0 (TID 490). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00108-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501015 length: 3501015 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO TaskSetManager: Starting task 56.0 in stage 6.0 (TID 503, localhost, ANY, 1692 bytes)
15/08/21 11:18:18 INFO Executor: Running task 56.0 in stage 6.0 (TID 503)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00085-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504735 length: 3504735 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 490) in 739 ms on localhost (48/200)
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 750000
15/08/21 11:18:18 INFO Executor: Finished task 48.0 in stage 6.0 (TID 495). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 57.0 in stage 6.0 (TID 504, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO Executor: Running task 57.0 in stage 6.0 (TID 504)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 495) in 605 ms on localhost (49/200)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00049-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501976 length: 3501976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO Executor: Finished task 56.0 in stage 4.0 (TID 446). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO TaskSetManager: Starting task 58.0 in stage 6.0 (TID 505, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO Executor: Running task 58.0 in stage 6.0 (TID 505)
15/08/21 11:18:18 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 446) in 6796 ms on localhost (51/57)
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00054-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3508574 length: 3508574 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO Executor: Finished task 49.0 in stage 6.0 (TID 496). 2125 bytes result sent to driver
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO TaskSetManager: Starting task 59.0 in stage 6.0 (TID 506, localhost, ANY, 1693 bytes)
15/08/21 11:18:18 INFO Executor: Running task 59.0 in stage 6.0 (TID 506)
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 11:18:18 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 496) in 629 ms on localhost (50/200)
15/08/21 11:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00142-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505692 length: 3505692 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:18 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 11:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 11:18:19 INFO Executor: Finished task 50.0 in stage 6.0 (TID 497). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO TaskSetManager: Starting task 60.0 in stage 6.0 (TID 507, localhost, ANY, 1694 bytes)
15/08/21 11:18:19 INFO Executor: Running task 60.0 in stage 6.0 (TID 507)
15/08/21 11:18:19 INFO Executor: Finished task 52.0 in stage 6.0 (TID 499). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO TaskSetManager: Starting task 61.0 in stage 6.0 (TID 508, localhost, ANY, 1694 bytes)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 50.0 in stage 6.0 (TID 497) in 932 ms on localhost (51/200)
15/08/21 11:18:19 INFO Executor: Running task 61.0 in stage 6.0 (TID 508)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 52.0 in stage 6.0 (TID 499) in 891 ms on localhost (52/200)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00183-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506119 length: 3506119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00069-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506964 length: 3506964 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO Executor: Finished task 51.0 in stage 6.0 (TID 498). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO TaskSetManager: Starting task 62.0 in stage 6.0 (TID 509, localhost, ANY, 1693 bytes)
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO Executor: Running task 62.0 in stage 6.0 (TID 509)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00194-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498074 length: 3498074 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO TaskSetManager: Finished task 51.0 in stage 6.0 (TID 498) in 922 ms on localhost (53/200)
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 750000
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 11:18:19 INFO Executor: Finished task 53.0 in stage 6.0 (TID 500). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO TaskSetManager: Starting task 63.0 in stage 6.0 (TID 510, localhost, ANY, 1692 bytes)
15/08/21 11:18:19 INFO Executor: Running task 63.0 in stage 6.0 (TID 510)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 53.0 in stage 6.0 (TID 500) in 961 ms on localhost (54/200)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00081-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500815 length: 3500815 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO Executor: Finished task 54.0 in stage 6.0 (TID 501). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO Executor: Finished task 55.0 in stage 6.0 (TID 502). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO TaskSetManager: Starting task 64.0 in stage 6.0 (TID 511, localhost, ANY, 1693 bytes)
15/08/21 11:18:19 INFO Executor: Running task 64.0 in stage 6.0 (TID 511)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00115-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501230 length: 3501230 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO TaskSetManager: Starting task 65.0 in stage 6.0 (TID 512, localhost, ANY, 1692 bytes)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 54.0 in stage 6.0 (TID 501) in 992 ms on localhost (55/200)
15/08/21 11:18:19 INFO Executor: Running task 65.0 in stage 6.0 (TID 512)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 55.0 in stage 6.0 (TID 502) in 955 ms on localhost (56/200)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00041-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500431 length: 3500431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO Executor: Finished task 62.0 in stage 6.0 (TID 509). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO Executor: Finished task 61.0 in stage 6.0 (TID 508). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO Executor: Finished task 56.0 in stage 6.0 (TID 503). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO TaskSetManager: Starting task 66.0 in stage 6.0 (TID 513, localhost, ANY, 1695 bytes)
15/08/21 11:18:19 INFO Executor: Running task 66.0 in stage 6.0 (TID 513)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00102-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506891 length: 3506891 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO TaskSetManager: Starting task 67.0 in stage 6.0 (TID 514, localhost, ANY, 1694 bytes)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 62.0 in stage 6.0 (TID 509) in 366 ms on localhost (57/200)
15/08/21 11:18:19 INFO Executor: Running task 67.0 in stage 6.0 (TID 514)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00129-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3497927 length: 3497927 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:19 INFO TaskSetManager: Starting task 68.0 in stage 6.0 (TID 515, localhost, ANY, 1693 bytes)
15/08/21 11:18:19 INFO Executor: Running task 68.0 in stage 6.0 (TID 515)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 61.0 in stage 6.0 (TID 508) in 391 ms on localhost (58/200)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 56.0 in stage 6.0 (TID 503) in 990 ms on localhost (59/200)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00056-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503531 length: 3503531 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO Executor: Finished task 57.0 in stage 6.0 (TID 504). 2125 bytes result sent to driver
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO TaskSetManager: Starting task 69.0 in stage 6.0 (TID 516, localhost, ANY, 1695 bytes)
15/08/21 11:18:19 INFO Executor: Running task 69.0 in stage 6.0 (TID 516)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 57.0 in stage 6.0 (TID 504) in 956 ms on localhost (60/200)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00032-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506885 length: 3506885 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 750000
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 750000
15/08/21 11:18:19 INFO Executor: Finished task 58.0 in stage 6.0 (TID 505). 2125 bytes result sent to driver
15/08/21 11:18:19 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 750000
15/08/21 11:18:19 INFO TaskSetManager: Starting task 70.0 in stage 6.0 (TID 517, localhost, ANY, 1693 bytes)
15/08/21 11:18:19 INFO Executor: Running task 70.0 in stage 6.0 (TID 517)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 58.0 in stage 6.0 (TID 505) in 1056 ms on localhost (61/200)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00040-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505811 length: 3505811 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO Executor: Finished task 59.0 in stage 6.0 (TID 506). 2125 bytes result sent to driver
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO TaskSetManager: Starting task 71.0 in stage 6.0 (TID 518, localhost, ANY, 1693 bytes)
15/08/21 11:18:19 INFO Executor: Running task 71.0 in stage 6.0 (TID 518)
15/08/21 11:18:19 INFO TaskSetManager: Finished task 59.0 in stage 6.0 (TID 506) in 1042 ms on localhost (62/200)
15/08/21 11:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00019-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501762 length: 3501762 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 11:18:20 INFO Executor: Finished task 60.0 in stage 6.0 (TID 507). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 72.0 in stage 6.0 (TID 519, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 72.0 in stage 6.0 (TID 519)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 60.0 in stage 6.0 (TID 507) in 616 ms on localhost (63/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00077-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505733 length: 3505733 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 47 ms. row count = 750000
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO Executor: Finished task 48.0 in stage 4.0 (TID 438). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 73.0 in stage 6.0 (TID 520, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 73.0 in stage 6.0 (TID 520)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 438) in 12169 ms on localhost (52/57)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00059-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499497 length: 3499497 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO Executor: Finished task 67.0 in stage 6.0 (TID 514). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 74.0 in stage 6.0 (TID 521, localhost, ANY, 1691 bytes)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 67.0 in stage 6.0 (TID 514) in 378 ms on localhost (64/200)
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 11:18:20 INFO Executor: Finished task 49.0 in stage 4.0 (TID 439). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO Executor: Running task 74.0 in stage 6.0 (TID 521)
15/08/21 11:18:20 INFO TaskSetManager: Starting task 75.0 in stage 6.0 (TID 522, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 75.0 in stage 6.0 (TID 522)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00078-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505418 length: 3505418 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 439) in 12223 ms on localhost (53/57)
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00117-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505296 length: 3505296 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 11:18:20 INFO Executor: Finished task 63.0 in stage 6.0 (TID 510). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 76.0 in stage 6.0 (TID 523, localhost, ANY, 1694 bytes)
15/08/21 11:18:20 INFO Executor: Running task 76.0 in stage 6.0 (TID 523)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 63.0 in stage 6.0 (TID 510) in 574 ms on localhost (65/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00180-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500745 length: 3500745 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 750000
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:20 INFO Executor: Finished task 66.0 in stage 6.0 (TID 513). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 77.0 in stage 6.0 (TID 524, localhost, ANY, 1691 bytes)
15/08/21 11:18:20 INFO Executor: Running task 77.0 in stage 6.0 (TID 524)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 66.0 in stage 6.0 (TID 513) in 618 ms on localhost (66/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00083-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499387 length: 3499387 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO Executor: Finished task 64.0 in stage 6.0 (TID 511). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:20 INFO TaskSetManager: Starting task 78.0 in stage 6.0 (TID 525, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 78.0 in stage 6.0 (TID 525)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 64.0 in stage 6.0 (TID 511) in 736 ms on localhost (67/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00130-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500816 length: 3500816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO Executor: Finished task 68.0 in stage 6.0 (TID 515). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 79.0 in stage 6.0 (TID 526, localhost, ANY, 1691 bytes)
15/08/21 11:18:20 INFO Executor: Running task 79.0 in stage 6.0 (TID 526)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 68.0 in stage 6.0 (TID 515) in 692 ms on localhost (68/200)
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00009-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499561 length: 3499561 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO Executor: Finished task 69.0 in stage 6.0 (TID 516). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 80.0 in stage 6.0 (TID 527, localhost, ANY, 1694 bytes)
15/08/21 11:18:20 INFO Executor: Running task 80.0 in stage 6.0 (TID 527)
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00074-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499991 length: 3499991 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 INFO TaskSetManager: Finished task 69.0 in stage 6.0 (TID 516) in 720 ms on localhost (69/200)
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO Executor: Finished task 65.0 in stage 6.0 (TID 512). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 81.0 in stage 6.0 (TID 528, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 81.0 in stage 6.0 (TID 528)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 65.0 in stage 6.0 (TID 512) in 850 ms on localhost (70/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00055-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505938 length: 3505938 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO Executor: Finished task 70.0 in stage 6.0 (TID 517). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 82.0 in stage 6.0 (TID 529, localhost, ANY, 1692 bytes)
15/08/21 11:18:20 INFO Executor: Running task 82.0 in stage 6.0 (TID 529)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 70.0 in stage 6.0 (TID 517) in 648 ms on localhost (71/200)
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00116-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504528 length: 3504528 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 INFO Executor: Finished task 71.0 in stage 6.0 (TID 518). 2125 bytes result sent to driver
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 750000
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO TaskSetManager: Starting task 83.0 in stage 6.0 (TID 530, localhost, ANY, 1692 bytes)
15/08/21 11:18:20 INFO Executor: Running task 83.0 in stage 6.0 (TID 530)
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO TaskSetManager: Finished task 71.0 in stage 6.0 (TID 518) in 679 ms on localhost (72/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00007-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506054 length: 3506054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 132 ms. row count = 750000
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 750000
15/08/21 11:18:20 INFO Executor: Finished task 73.0 in stage 6.0 (TID 520). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 84.0 in stage 6.0 (TID 531, localhost, ANY, 1692 bytes)
15/08/21 11:18:20 INFO Executor: Running task 84.0 in stage 6.0 (TID 531)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 73.0 in stage 6.0 (TID 520) in 622 ms on localhost (73/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00058-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3497871 length: 3497871 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 750000
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO Executor: Finished task 77.0 in stage 6.0 (TID 524). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO TaskSetManager: Starting task 85.0 in stage 6.0 (TID 532, localhost, ANY, 1692 bytes)
15/08/21 11:18:20 INFO Executor: Running task 85.0 in stage 6.0 (TID 532)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00090-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3496809 length: 3496809 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 INFO TaskSetManager: Finished task 77.0 in stage 6.0 (TID 524) in 367 ms on localhost (74/200)
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 750000
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO Executor: Finished task 74.0 in stage 6.0 (TID 521). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 86.0 in stage 6.0 (TID 533, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 86.0 in stage 6.0 (TID 533)
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 11:18:20 INFO TaskSetManager: Finished task 74.0 in stage 6.0 (TID 521) in 624 ms on localhost (75/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00149-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505750 length: 3505750 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 11:18:20 INFO Executor: Finished task 72.0 in stage 6.0 (TID 519). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 87.0 in stage 6.0 (TID 534, localhost, ANY, 1692 bytes)
15/08/21 11:18:20 INFO Executor: Running task 87.0 in stage 6.0 (TID 534)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 72.0 in stage 6.0 (TID 519) in 821 ms on localhost (76/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00159-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505777 length: 3505777 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO Executor: Finished task 76.0 in stage 6.0 (TID 523). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO TaskSetManager: Starting task 88.0 in stage 6.0 (TID 535, localhost, ANY, 1692 bytes)
15/08/21 11:18:20 INFO Executor: Running task 88.0 in stage 6.0 (TID 535)
15/08/21 11:18:20 INFO TaskSetManager: Finished task 76.0 in stage 6.0 (TID 523) in 644 ms on localhost (77/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00006-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504043 length: 3504043 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 11:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:20 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 750000
15/08/21 11:18:20 INFO Executor: Finished task 75.0 in stage 6.0 (TID 522). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Starting task 89.0 in stage 6.0 (TID 536, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 89.0 in stage 6.0 (TID 536)
15/08/21 11:18:20 INFO Executor: Finished task 78.0 in stage 6.0 (TID 525). 2125 bytes result sent to driver
15/08/21 11:18:20 INFO TaskSetManager: Finished task 75.0 in stage 6.0 (TID 522) in 817 ms on localhost (78/200)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00031-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507536 length: 3507536 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:20 INFO TaskSetManager: Starting task 90.0 in stage 6.0 (TID 537, localhost, ANY, 1693 bytes)
15/08/21 11:18:20 INFO Executor: Running task 90.0 in stage 6.0 (TID 537)
15/08/21 11:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00121-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502280 length: 3502280 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:20 INFO TaskSetManager: Finished task 78.0 in stage 6.0 (TID 525) in 562 ms on localhost (79/200)
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO Executor: Finished task 51.0 in stage 4.0 (TID 441). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO TaskSetManager: Starting task 91.0 in stage 6.0 (TID 538, localhost, ANY, 1695 bytes)
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 441) in 12276 ms on localhost (54/57)
15/08/21 11:18:21 INFO Executor: Running task 91.0 in stage 6.0 (TID 538)
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00046-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507400 length: 3507400 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 750000
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 85 ms. row count = 750000
15/08/21 11:18:21 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 750000
15/08/21 11:18:21 INFO Executor: Finished task 80.0 in stage 6.0 (TID 527). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Starting task 92.0 in stage 6.0 (TID 539, localhost, ANY, 1691 bytes)
15/08/21 11:18:21 INFO Executor: Running task 92.0 in stage 6.0 (TID 539)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 80.0 in stage 6.0 (TID 527) in 743 ms on localhost (80/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00161-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499824 length: 3499824 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO Executor: Finished task 79.0 in stage 6.0 (TID 526). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO TaskSetManager: Starting task 93.0 in stage 6.0 (TID 540, localhost, ANY, 1694 bytes)
15/08/21 11:18:21 INFO Executor: Running task 93.0 in stage 6.0 (TID 540)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 79.0 in stage 6.0 (TID 526) in 813 ms on localhost (81/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00157-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506076 length: 3506076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 INFO Executor: Finished task 82.0 in stage 6.0 (TID 529). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:21 INFO TaskSetManager: Starting task 94.0 in stage 6.0 (TID 541, localhost, ANY, 1694 bytes)
15/08/21 11:18:21 INFO Executor: Running task 94.0 in stage 6.0 (TID 541)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 82.0 in stage 6.0 (TID 529) in 715 ms on localhost (82/200)
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00067-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501014 length: 3501014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 750000
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO Executor: Finished task 81.0 in stage 6.0 (TID 528). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Starting task 95.0 in stage 6.0 (TID 542, localhost, ANY, 1694 bytes)
15/08/21 11:18:21 INFO Executor: Running task 95.0 in stage 6.0 (TID 542)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 81.0 in stage 6.0 (TID 528) in 826 ms on localhost (83/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00093-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507082 length: 3507082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 62 ms. row count = 750000
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO Executor: Finished task 83.0 in stage 6.0 (TID 530). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO Executor: Finished task 84.0 in stage 6.0 (TID 531). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Starting task 96.0 in stage 6.0 (TID 543, localhost, ANY, 1695 bytes)
15/08/21 11:18:21 INFO Executor: Running task 96.0 in stage 6.0 (TID 543)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00192-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507651 length: 3507651 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 INFO TaskSetManager: Starting task 97.0 in stage 6.0 (TID 544, localhost, ANY, 1694 bytes)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 83.0 in stage 6.0 (TID 530) in 841 ms on localhost (84/200)
15/08/21 11:18:21 INFO Executor: Running task 97.0 in stage 6.0 (TID 544)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 84.0 in stage 6.0 (TID 531) in 776 ms on localhost (85/200)
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00198-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506610 length: 3506610 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO Executor: Finished task 85.0 in stage 6.0 (TID 532). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 750000
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO TaskSetManager: Starting task 98.0 in stage 6.0 (TID 545, localhost, ANY, 1694 bytes)
15/08/21 11:18:21 INFO Executor: Running task 98.0 in stage 6.0 (TID 545)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 85.0 in stage 6.0 (TID 532) in 776 ms on localhost (86/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00193-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500019 length: 3500019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 INFO Executor: Finished task 86.0 in stage 6.0 (TID 533). 2125 bytes result sent to driver
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 11:18:21 INFO TaskSetManager: Starting task 99.0 in stage 6.0 (TID 546, localhost, ANY, 1692 bytes)
15/08/21 11:18:21 INFO Executor: Running task 99.0 in stage 6.0 (TID 546)
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO TaskSetManager: Finished task 86.0 in stage 6.0 (TID 533) in 784 ms on localhost (87/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00063-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503402 length: 3503402 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO Executor: Finished task 87.0 in stage 6.0 (TID 534). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Starting task 100.0 in stage 6.0 (TID 547, localhost, ANY, 1692 bytes)
15/08/21 11:18:21 INFO Executor: Running task 100.0 in stage 6.0 (TID 547)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 87.0 in stage 6.0 (TID 534) in 799 ms on localhost (88/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00025-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500335 length: 3500335 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO Executor: Finished task 88.0 in stage 6.0 (TID 535). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Starting task 101.0 in stage 6.0 (TID 548, localhost, ANY, 1692 bytes)
15/08/21 11:18:21 INFO Executor: Running task 101.0 in stage 6.0 (TID 548)
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 52 ms. row count = 750000
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00092-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500083 length: 3500083 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 INFO TaskSetManager: Finished task 88.0 in stage 6.0 (TID 535) in 786 ms on localhost (89/200)
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:21 INFO Executor: Finished task 90.0 in stage 6.0 (TID 537). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Starting task 102.0 in stage 6.0 (TID 549, localhost, ANY, 1692 bytes)
15/08/21 11:18:21 INFO Executor: Running task 102.0 in stage 6.0 (TID 549)
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:21 INFO TaskSetManager: Finished task 90.0 in stage 6.0 (TID 537) in 731 ms on localhost (90/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00047-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504640 length: 3504640 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO Executor: Finished task 91.0 in stage 6.0 (TID 538). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO TaskSetManager: Starting task 103.0 in stage 6.0 (TID 550, localhost, ANY, 1693 bytes)
15/08/21 11:18:21 INFO Executor: Running task 103.0 in stage 6.0 (TID 550)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 91.0 in stage 6.0 (TID 538) in 748 ms on localhost (91/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00050-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501241 length: 3501241 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 750000
15/08/21 11:18:21 INFO Executor: Finished task 89.0 in stage 6.0 (TID 536). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO TaskSetManager: Starting task 104.0 in stage 6.0 (TID 551, localhost, ANY, 1694 bytes)
15/08/21 11:18:21 INFO Executor: Running task 104.0 in stage 6.0 (TID 551)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 89.0 in stage 6.0 (TID 536) in 819 ms on localhost (92/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00196-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500486 length: 3500486 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:21 INFO Executor: Finished task 93.0 in stage 6.0 (TID 540). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Starting task 105.0 in stage 6.0 (TID 552, localhost, ANY, 1691 bytes)
15/08/21 11:18:21 INFO Executor: Running task 105.0 in stage 6.0 (TID 552)
15/08/21 11:18:21 INFO Executor: Finished task 92.0 in stage 6.0 (TID 539). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO TaskSetManager: Finished task 93.0 in stage 6.0 (TID 540) in 646 ms on localhost (93/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00028-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501169 length: 3501169 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO TaskSetManager: Starting task 106.0 in stage 6.0 (TID 553, localhost, ANY, 1692 bytes)
15/08/21 11:18:21 INFO Executor: Running task 106.0 in stage 6.0 (TID 553)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 92.0 in stage 6.0 (TID 539) in 692 ms on localhost (94/200)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00184-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507565 length: 3507565 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO Executor: Finished task 100.0 in stage 6.0 (TID 547). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO TaskSetManager: Starting task 107.0 in stage 6.0 (TID 554, localhost, ANY, 1692 bytes)
15/08/21 11:18:21 INFO Executor: Running task 107.0 in stage 6.0 (TID 554)
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00190-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505708 length: 3505708 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 INFO TaskSetManager: Finished task 100.0 in stage 6.0 (TID 547) in 356 ms on localhost (95/200)
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:21 INFO Executor: Finished task 94.0 in stage 6.0 (TID 541). 2125 bytes result sent to driver
15/08/21 11:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:21 INFO TaskSetManager: Starting task 108.0 in stage 6.0 (TID 555, localhost, ANY, 1693 bytes)
15/08/21 11:18:21 INFO Executor: Running task 108.0 in stage 6.0 (TID 555)
15/08/21 11:18:21 INFO TaskSetManager: Finished task 94.0 in stage 6.0 (TID 541) in 693 ms on localhost (96/200)
15/08/21 11:18:21 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 11:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00136-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506182 length: 3506182 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 750000
15/08/21 11:18:22 INFO Executor: Finished task 95.0 in stage 6.0 (TID 542). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO TaskSetManager: Starting task 109.0 in stage 6.0 (TID 556, localhost, ANY, 1692 bytes)
15/08/21 11:18:22 INFO Executor: Running task 109.0 in stage 6.0 (TID 556)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 95.0 in stage 6.0 (TID 542) in 701 ms on localhost (97/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00035-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499639 length: 3499639 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:22 INFO Executor: Finished task 96.0 in stage 6.0 (TID 543). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO Executor: Finished task 97.0 in stage 6.0 (TID 544). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO TaskSetManager: Starting task 110.0 in stage 6.0 (TID 557, localhost, ANY, 1694 bytes)
15/08/21 11:18:22 INFO Executor: Running task 110.0 in stage 6.0 (TID 557)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00176-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504086 length: 3504086 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO TaskSetManager: Starting task 111.0 in stage 6.0 (TID 558, localhost, ANY, 1692 bytes)
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO Executor: Running task 111.0 in stage 6.0 (TID 558)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 96.0 in stage 6.0 (TID 543) in 719 ms on localhost (98/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00163-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499870 length: 3499870 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO TaskSetManager: Finished task 97.0 in stage 6.0 (TID 544) in 719 ms on localhost (99/200)
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO Executor: Finished task 98.0 in stage 6.0 (TID 545). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO TaskSetManager: Starting task 112.0 in stage 6.0 (TID 559, localhost, ANY, 1694 bytes)
15/08/21 11:18:22 INFO Executor: Running task 112.0 in stage 6.0 (TID 559)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00171-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3497374 length: 3497374 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO TaskSetManager: Finished task 98.0 in stage 6.0 (TID 545) in 725 ms on localhost (100/200)
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO Executor: Finished task 99.0 in stage 6.0 (TID 546). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO TaskSetManager: Starting task 113.0 in stage 6.0 (TID 560, localhost, ANY, 1693 bytes)
15/08/21 11:18:22 INFO Executor: Running task 113.0 in stage 6.0 (TID 560)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 99.0 in stage 6.0 (TID 546) in 739 ms on localhost (101/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00187-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501231 length: 3501231 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO Executor: Finished task 101.0 in stage 6.0 (TID 548). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 750000
15/08/21 11:18:22 INFO TaskSetManager: Starting task 114.0 in stage 6.0 (TID 561, localhost, ANY, 1693 bytes)
15/08/21 11:18:22 INFO Executor: Running task 114.0 in stage 6.0 (TID 561)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 101.0 in stage 6.0 (TID 548) in 670 ms on localhost (102/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00152-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505322 length: 3505322 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO Executor: Finished task 102.0 in stage 6.0 (TID 549). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 11:18:22 INFO TaskSetManager: Starting task 115.0 in stage 6.0 (TID 562, localhost, ANY, 1692 bytes)
15/08/21 11:18:22 INFO Executor: Running task 115.0 in stage 6.0 (TID 562)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 102.0 in stage 6.0 (TID 549) in 644 ms on localhost (103/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00189-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503657 length: 3503657 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO Executor: Finished task 103.0 in stage 6.0 (TID 550). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO Executor: Finished task 104.0 in stage 6.0 (TID 551). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO TaskSetManager: Starting task 116.0 in stage 6.0 (TID 563, localhost, ANY, 1695 bytes)
15/08/21 11:18:22 INFO Executor: Running task 116.0 in stage 6.0 (TID 563)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00023-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507108 length: 3507108 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 750000
15/08/21 11:18:22 INFO TaskSetManager: Starting task 117.0 in stage 6.0 (TID 564, localhost, ANY, 1692 bytes)
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO TaskSetManager: Finished task 103.0 in stage 6.0 (TID 550) in 667 ms on localhost (104/200)
15/08/21 11:18:22 INFO Executor: Running task 117.0 in stage 6.0 (TID 564)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 104.0 in stage 6.0 (TID 551) in 625 ms on localhost (105/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00139-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502079 length: 3502079 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 11:18:22 INFO Executor: Finished task 105.0 in stage 6.0 (TID 552). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO TaskSetManager: Starting task 118.0 in stage 6.0 (TID 565, localhost, ANY, 1694 bytes)
15/08/21 11:18:22 INFO Executor: Running task 118.0 in stage 6.0 (TID 565)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 105.0 in stage 6.0 (TID 552) in 634 ms on localhost (106/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00197-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504816 length: 3504816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO Executor: Finished task 106.0 in stage 6.0 (TID 553). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO TaskSetManager: Starting task 119.0 in stage 6.0 (TID 566, localhost, ANY, 1694 bytes)
15/08/21 11:18:22 INFO Executor: Running task 119.0 in stage 6.0 (TID 566)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 106.0 in stage 6.0 (TID 553) in 645 ms on localhost (107/200)
15/08/21 11:18:22 INFO Executor: Finished task 107.0 in stage 6.0 (TID 554). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00134-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506718 length: 3506718 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO Executor: Finished task 108.0 in stage 6.0 (TID 555). 2125 bytes result sent to driver
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO TaskSetManager: Starting task 120.0 in stage 6.0 (TID 567, localhost, ANY, 1693 bytes)
15/08/21 11:18:22 INFO Executor: Running task 120.0 in stage 6.0 (TID 567)
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 750000
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00138-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3497916 length: 3497916 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO TaskSetManager: Starting task 121.0 in stage 6.0 (TID 568, localhost, ANY, 1694 bytes)
15/08/21 11:18:22 INFO Executor: Running task 121.0 in stage 6.0 (TID 568)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 107.0 in stage 6.0 (TID 554) in 652 ms on localhost (108/200)
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO TaskSetManager: Finished task 108.0 in stage 6.0 (TID 555) in 617 ms on localhost (109/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00037-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505909 length: 3505909 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 11:18:22 INFO Executor: Finished task 109.0 in stage 6.0 (TID 556). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO TaskSetManager: Starting task 122.0 in stage 6.0 (TID 569, localhost, ANY, 1693 bytes)
15/08/21 11:18:22 INFO Executor: Running task 122.0 in stage 6.0 (TID 569)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 109.0 in stage 6.0 (TID 556) in 630 ms on localhost (110/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00156-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501563 length: 3501563 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 11:18:22 INFO Executor: Finished task 110.0 in stage 6.0 (TID 557). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO Executor: Finished task 111.0 in stage 6.0 (TID 558). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO TaskSetManager: Starting task 123.0 in stage 6.0 (TID 570, localhost, ANY, 1692 bytes)
15/08/21 11:18:22 INFO Executor: Running task 123.0 in stage 6.0 (TID 570)
15/08/21 11:18:22 INFO Executor: Finished task 112.0 in stage 6.0 (TID 559). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00034-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498833 length: 3498833 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO TaskSetManager: Starting task 124.0 in stage 6.0 (TID 571, localhost, ANY, 1693 bytes)
15/08/21 11:18:22 INFO Executor: Running task 124.0 in stage 6.0 (TID 571)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 110.0 in stage 6.0 (TID 557) in 681 ms on localhost (111/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00010-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499442 length: 3499442 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO TaskSetManager: Starting task 125.0 in stage 6.0 (TID 572, localhost, ANY, 1692 bytes)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 111.0 in stage 6.0 (TID 558) in 666 ms on localhost (112/200)
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO Executor: Running task 125.0 in stage 6.0 (TID 572)
15/08/21 11:18:22 INFO TaskSetManager: Finished task 112.0 in stage 6.0 (TID 559) in 640 ms on localhost (113/200)
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00101-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3509794 length: 3509794 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO Executor: Finished task 113.0 in stage 6.0 (TID 560). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO TaskSetManager: Starting task 126.0 in stage 6.0 (TID 573, localhost, ANY, 1694 bytes)
15/08/21 11:18:22 INFO Executor: Running task 126.0 in stage 6.0 (TID 573)
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO TaskSetManager: Finished task 113.0 in stage 6.0 (TID 560) in 647 ms on localhost (114/200)
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 11:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00014-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505682 length: 3505682 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 750000
15/08/21 11:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:22 INFO Executor: Finished task 114.0 in stage 6.0 (TID 561). 2125 bytes result sent to driver
15/08/21 11:18:22 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 750000
15/08/21 11:18:23 INFO TaskSetManager: Starting task 127.0 in stage 6.0 (TID 574, localhost, ANY, 1695 bytes)
15/08/21 11:18:23 INFO Executor: Running task 127.0 in stage 6.0 (TID 574)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 114.0 in stage 6.0 (TID 561) in 845 ms on localhost (115/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00128-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507147 length: 3507147 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 220 ms. row count = 750000
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO Executor: Finished task 115.0 in stage 6.0 (TID 562). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 128.0 in stage 6.0 (TID 575, localhost, ANY, 1691 bytes)
15/08/21 11:18:23 INFO Executor: Running task 128.0 in stage 6.0 (TID 575)
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 11:18:23 INFO TaskSetManager: Finished task 115.0 in stage 6.0 (TID 562) in 850 ms on localhost (116/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00060-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503479 length: 3503479 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO Executor: Finished task 117.0 in stage 6.0 (TID 564). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO TaskSetManager: Starting task 129.0 in stage 6.0 (TID 576, localhost, ANY, 1691 bytes)
15/08/21 11:18:23 INFO Executor: Running task 129.0 in stage 6.0 (TID 576)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 117.0 in stage 6.0 (TID 564) in 814 ms on localhost (117/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00186-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501858 length: 3501858 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO Executor: Finished task 116.0 in stage 6.0 (TID 563). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 750000
15/08/21 11:18:23 INFO TaskSetManager: Starting task 130.0 in stage 6.0 (TID 577, localhost, ANY, 1693 bytes)
15/08/21 11:18:23 INFO Executor: Running task 130.0 in stage 6.0 (TID 577)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 116.0 in stage 6.0 (TID 563) in 853 ms on localhost (118/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00038-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506215 length: 3506215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 11:18:23 INFO Executor: Finished task 118.0 in stage 6.0 (TID 565). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO Executor: Finished task 119.0 in stage 6.0 (TID 566). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 131.0 in stage 6.0 (TID 578, localhost, ANY, 1692 bytes)
15/08/21 11:18:23 INFO Executor: Running task 131.0 in stage 6.0 (TID 578)
15/08/21 11:18:23 INFO Executor: Finished task 121.0 in stage 6.0 (TID 568). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO Executor: Finished task 120.0 in stage 6.0 (TID 567). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00166-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503996 length: 3503996 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 INFO TaskSetManager: Starting task 132.0 in stage 6.0 (TID 579, localhost, ANY, 1693 bytes)
15/08/21 11:18:23 INFO Executor: Running task 132.0 in stage 6.0 (TID 579)
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO TaskSetManager: Starting task 133.0 in stage 6.0 (TID 580, localhost, ANY, 1693 bytes)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00005-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506506 length: 3506506 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 INFO TaskSetManager: Finished task 118.0 in stage 6.0 (TID 565) in 864 ms on localhost (119/200)
15/08/21 11:18:23 INFO Executor: Running task 133.0 in stage 6.0 (TID 580)
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO TaskSetManager: Finished task 119.0 in stage 6.0 (TID 566) in 837 ms on localhost (120/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00181-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503343 length: 3503343 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO TaskSetManager: Starting task 134.0 in stage 6.0 (TID 581, localhost, ANY, 1692 bytes)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 121.0 in stage 6.0 (TID 568) in 824 ms on localhost (121/200)
15/08/21 11:18:23 INFO Executor: Running task 134.0 in stage 6.0 (TID 581)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 120.0 in stage 6.0 (TID 567) in 841 ms on localhost (122/200)
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00151-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3503903 length: 3503903 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO Executor: Finished task 122.0 in stage 6.0 (TID 569). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:23 INFO TaskSetManager: Starting task 135.0 in stage 6.0 (TID 582, localhost, ANY, 1693 bytes)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 122.0 in stage 6.0 (TID 569) in 750 ms on localhost (123/200)
15/08/21 11:18:23 INFO Executor: Running task 135.0 in stage 6.0 (TID 582)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00131-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501547 length: 3501547 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:23 INFO Executor: Finished task 123.0 in stage 6.0 (TID 570). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 136.0 in stage 6.0 (TID 583, localhost, ANY, 1694 bytes)
15/08/21 11:18:23 INFO Executor: Running task 136.0 in stage 6.0 (TID 583)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 123.0 in stage 6.0 (TID 570) in 766 ms on localhost (124/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00068-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500774 length: 3500774 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO Executor: Finished task 124.0 in stage 6.0 (TID 571). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 137.0 in stage 6.0 (TID 584, localhost, ANY, 1692 bytes)
15/08/21 11:18:23 INFO Executor: Running task 137.0 in stage 6.0 (TID 584)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 124.0 in stage 6.0 (TID 571) in 814 ms on localhost (125/200)
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00195-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500801 length: 3500801 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 11:18:23 INFO Executor: Finished task 126.0 in stage 6.0 (TID 573). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 138.0 in stage 6.0 (TID 585, localhost, ANY, 1694 bytes)
15/08/21 11:18:23 INFO Executor: Running task 138.0 in stage 6.0 (TID 585)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 126.0 in stage 6.0 (TID 573) in 810 ms on localhost (126/200)
15/08/21 11:18:23 INFO Executor: Finished task 125.0 in stage 6.0 (TID 572). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00168-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505102 length: 3505102 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 INFO TaskSetManager: Starting task 139.0 in stage 6.0 (TID 586, localhost, ANY, 1691 bytes)
15/08/21 11:18:23 INFO Executor: Running task 139.0 in stage 6.0 (TID 586)
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO TaskSetManager: Finished task 125.0 in stage 6.0 (TID 572) in 858 ms on localhost (127/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00114-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501155 length: 3501155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO Executor: Finished task 127.0 in stage 6.0 (TID 574). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO TaskSetManager: Starting task 140.0 in stage 6.0 (TID 587, localhost, ANY, 1692 bytes)
15/08/21 11:18:23 INFO Executor: Running task 140.0 in stage 6.0 (TID 587)
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO TaskSetManager: Finished task 127.0 in stage 6.0 (TID 574) in 765 ms on localhost (128/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00147-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499919 length: 3499919 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 11:18:23 INFO Executor: Finished task 129.0 in stage 6.0 (TID 576). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO Executor: Finished task 128.0 in stage 6.0 (TID 575). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 141.0 in stage 6.0 (TID 588, localhost, ANY, 1694 bytes)
15/08/21 11:18:23 INFO Executor: Running task 141.0 in stage 6.0 (TID 588)
15/08/21 11:18:23 INFO TaskSetManager: Starting task 142.0 in stage 6.0 (TID 589, localhost, ANY, 1693 bytes)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00097-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499721 length: 3499721 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 INFO Executor: Running task 142.0 in stage 6.0 (TID 589)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 129.0 in stage 6.0 (TID 576) in 572 ms on localhost (129/200)
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO TaskSetManager: Finished task 128.0 in stage 6.0 (TID 575) in 608 ms on localhost (130/200)
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00033-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498478 length: 3498478 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO Executor: Finished task 130.0 in stage 6.0 (TID 577). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 143.0 in stage 6.0 (TID 590, localhost, ANY, 1694 bytes)
15/08/21 11:18:23 INFO Executor: Running task 143.0 in stage 6.0 (TID 590)
15/08/21 11:18:23 INFO TaskSetManager: Finished task 130.0 in stage 6.0 (TID 577) in 567 ms on localhost (131/200)
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00066-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499206 length: 3499206 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 750000
15/08/21 11:18:23 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 750000
15/08/21 11:18:23 INFO Executor: Finished task 133.0 in stage 6.0 (TID 580). 2125 bytes result sent to driver
15/08/21 11:18:23 INFO TaskSetManager: Starting task 144.0 in stage 6.0 (TID 591, localhost, ANY, 1693 bytes)
15/08/21 11:18:23 INFO Executor: Running task 144.0 in stage 6.0 (TID 591)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 133.0 in stage 6.0 (TID 580) in 619 ms on localhost (132/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00079-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505149 length: 3505149 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO Executor: Finished task 132.0 in stage 6.0 (TID 579). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO Executor: Finished task 134.0 in stage 6.0 (TID 581). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO TaskSetManager: Starting task 145.0 in stage 6.0 (TID 592, localhost, ANY, 1695 bytes)
15/08/21 11:18:24 INFO Executor: Running task 145.0 in stage 6.0 (TID 592)
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00165-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3508457 length: 3508457 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO TaskSetManager: Starting task 146.0 in stage 6.0 (TID 593, localhost, ANY, 1691 bytes)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 132.0 in stage 6.0 (TID 579) in 684 ms on localhost (133/200)
15/08/21 11:18:24 INFO Executor: Running task 146.0 in stage 6.0 (TID 593)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 134.0 in stage 6.0 (TID 581) in 661 ms on localhost (134/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00164-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500845 length: 3500845 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO Executor: Finished task 131.0 in stage 6.0 (TID 578). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO Executor: Finished task 135.0 in stage 6.0 (TID 582). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:24 INFO TaskSetManager: Starting task 147.0 in stage 6.0 (TID 594, localhost, ANY, 1694 bytes)
15/08/21 11:18:24 INFO Executor: Running task 147.0 in stage 6.0 (TID 594)
15/08/21 11:18:24 INFO TaskSetManager: Starting task 148.0 in stage 6.0 (TID 595, localhost, ANY, 1694 bytes)
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO TaskSetManager: Finished task 131.0 in stage 6.0 (TID 578) in 724 ms on localhost (135/200)
15/08/21 11:18:24 INFO Executor: Running task 148.0 in stage 6.0 (TID 595)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00169-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3496922 length: 3496922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 INFO TaskSetManager: Finished task 135.0 in stage 6.0 (TID 582) in 656 ms on localhost (136/200)
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00110-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507283 length: 3507283 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 11:18:24 INFO Executor: Finished task 136.0 in stage 6.0 (TID 583). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 149.0 in stage 6.0 (TID 596, localhost, ANY, 1692 bytes)
15/08/21 11:18:24 INFO Executor: Running task 149.0 in stage 6.0 (TID 596)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 136.0 in stage 6.0 (TID 583) in 691 ms on localhost (137/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00123-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501098 length: 3501098 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO Executor: Finished task 137.0 in stage 6.0 (TID 584). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO TaskSetManager: Starting task 150.0 in stage 6.0 (TID 597, localhost, ANY, 1691 bytes)
15/08/21 11:18:24 INFO Executor: Running task 150.0 in stage 6.0 (TID 597)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 137.0 in stage 6.0 (TID 584) in 706 ms on localhost (138/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00089-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500320 length: 3500320 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO Executor: Finished task 138.0 in stage 6.0 (TID 585). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 151.0 in stage 6.0 (TID 598, localhost, ANY, 1691 bytes)
15/08/21 11:18:24 INFO Executor: Running task 151.0 in stage 6.0 (TID 598)
15/08/21 11:18:24 INFO Executor: Finished task 139.0 in stage 6.0 (TID 586). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Finished task 138.0 in stage 6.0 (TID 585) in 668 ms on localhost (139/200)
15/08/21 11:18:24 INFO Executor: Finished task 140.0 in stage 6.0 (TID 587). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO TaskSetManager: Starting task 152.0 in stage 6.0 (TID 599, localhost, ANY, 1693 bytes)
15/08/21 11:18:24 INFO Executor: Running task 152.0 in stage 6.0 (TID 599)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00043-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502639 length: 3502639 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 INFO TaskSetManager: Starting task 153.0 in stage 6.0 (TID 600, localhost, ANY, 1693 bytes)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00109-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502812 length: 3502812 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 INFO Executor: Running task 153.0 in stage 6.0 (TID 600)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 139.0 in stage 6.0 (TID 586) in 664 ms on localhost (140/200)
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO TaskSetManager: Finished task 140.0 in stage 6.0 (TID 587) in 652 ms on localhost (141/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00122-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500106 length: 3500106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO Executor: Finished task 148.0 in stage 6.0 (TID 595). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 154.0 in stage 6.0 (TID 601, localhost, ANY, 1694 bytes)
15/08/21 11:18:24 INFO Executor: Running task 154.0 in stage 6.0 (TID 601)
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00073-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501504 length: 3501504 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 INFO TaskSetManager: Finished task 148.0 in stage 6.0 (TID 595) in 318 ms on localhost (142/200)
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO Executor: Finished task 142.0 in stage 6.0 (TID 589). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 11:18:24 INFO TaskSetManager: Starting task 155.0 in stage 6.0 (TID 602, localhost, ANY, 1692 bytes)
15/08/21 11:18:24 INFO Executor: Running task 155.0 in stage 6.0 (TID 602)
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO TaskSetManager: Finished task 142.0 in stage 6.0 (TID 589) in 680 ms on localhost (143/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00075-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3497524 length: 3497524 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO Executor: Finished task 143.0 in stage 6.0 (TID 590). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 156.0 in stage 6.0 (TID 603, localhost, ANY, 1693 bytes)
15/08/21 11:18:24 INFO Executor: Running task 156.0 in stage 6.0 (TID 603)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 143.0 in stage 6.0 (TID 590) in 697 ms on localhost (144/200)
15/08/21 11:18:24 INFO Executor: Finished task 141.0 in stage 6.0 (TID 588). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00162-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498776 length: 3498776 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 INFO TaskSetManager: Starting task 157.0 in stage 6.0 (TID 604, localhost, ANY, 1695 bytes)
15/08/21 11:18:24 INFO Executor: Running task 157.0 in stage 6.0 (TID 604)
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00144-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507412 length: 3507412 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 INFO TaskSetManager: Finished task 141.0 in stage 6.0 (TID 588) in 739 ms on localhost (145/200)
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 750000
15/08/21 11:18:24 INFO Executor: Finished task 145.0 in stage 6.0 (TID 592). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 158.0 in stage 6.0 (TID 605, localhost, ANY, 1694 bytes)
15/08/21 11:18:24 INFO Executor: Running task 158.0 in stage 6.0 (TID 605)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 145.0 in stage 6.0 (TID 592) in 608 ms on localhost (146/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00061-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506496 length: 3506496 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO Executor: Finished task 144.0 in stage 6.0 (TID 591). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 159.0 in stage 6.0 (TID 606, localhost, ANY, 1693 bytes)
15/08/21 11:18:24 INFO Executor: Running task 159.0 in stage 6.0 (TID 606)
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO TaskSetManager: Finished task 144.0 in stage 6.0 (TID 591) in 648 ms on localhost (147/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00087-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506179 length: 3506179 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO Executor: Finished task 146.0 in stage 6.0 (TID 593). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 160.0 in stage 6.0 (TID 607, localhost, ANY, 1694 bytes)
15/08/21 11:18:24 INFO Executor: Running task 160.0 in stage 6.0 (TID 607)
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO TaskSetManager: Finished task 146.0 in stage 6.0 (TID 593) in 637 ms on localhost (148/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00107-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500234 length: 3500234 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 11:18:24 INFO Executor: Finished task 147.0 in stage 6.0 (TID 594). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 161.0 in stage 6.0 (TID 608, localhost, ANY, 1694 bytes)
15/08/21 11:18:24 INFO Executor: Running task 161.0 in stage 6.0 (TID 608)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 147.0 in stage 6.0 (TID 594) in 667 ms on localhost (149/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00065-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500529 length: 3500529 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO Executor: Finished task 52.0 in stage 4.0 (TID 442). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO TaskSetManager: Starting task 162.0 in stage 6.0 (TID 609, localhost, ANY, 1693 bytes)
15/08/21 11:18:24 INFO Executor: Running task 162.0 in stage 6.0 (TID 609)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00105-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500366 length: 3500366 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 442) in 13711 ms on localhost (55/57)
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:24 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 11:18:24 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:24 INFO Executor: Finished task 149.0 in stage 6.0 (TID 596). 2125 bytes result sent to driver
15/08/21 11:18:24 INFO TaskSetManager: Starting task 163.0 in stage 6.0 (TID 610, localhost, ANY, 1693 bytes)
15/08/21 11:18:24 INFO Executor: Running task 163.0 in stage 6.0 (TID 610)
15/08/21 11:18:24 INFO TaskSetManager: Finished task 149.0 in stage 6.0 (TID 596) in 705 ms on localhost (150/200)
15/08/21 11:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00088-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3508314 length: 3508314 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Finished task 150.0 in stage 6.0 (TID 597). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO TaskSetManager: Starting task 164.0 in stage 6.0 (TID 611, localhost, ANY, 1692 bytes)
15/08/21 11:18:25 INFO Executor: Running task 164.0 in stage 6.0 (TID 611)
15/08/21 11:18:25 INFO Executor: Finished task 152.0 in stage 6.0 (TID 599). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00084-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498871 length: 3498871 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO TaskSetManager: Starting task 165.0 in stage 6.0 (TID 612, localhost, ANY, 1693 bytes)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 150.0 in stage 6.0 (TID 597) in 748 ms on localhost (151/200)
15/08/21 11:18:25 INFO Executor: Running task 165.0 in stage 6.0 (TID 612)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 152.0 in stage 6.0 (TID 599) in 695 ms on localhost (152/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00048-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506806 length: 3506806 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 750000
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Finished task 153.0 in stage 6.0 (TID 600). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO Executor: Finished task 162.0 in stage 6.0 (TID 609). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO TaskSetManager: Starting task 166.0 in stage 6.0 (TID 613, localhost, ANY, 1693 bytes)
15/08/21 11:18:25 INFO Executor: Running task 166.0 in stage 6.0 (TID 613)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00146-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501235 length: 3501235 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO TaskSetManager: Starting task 167.0 in stage 6.0 (TID 614, localhost, ANY, 1694 bytes)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 153.0 in stage 6.0 (TID 600) in 822 ms on localhost (153/200)
15/08/21 11:18:25 INFO Executor: Running task 167.0 in stage 6.0 (TID 614)
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00095-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505119 length: 3505119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 INFO TaskSetManager: Finished task 162.0 in stage 6.0 (TID 609) in 410 ms on localhost (154/200)
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Finished task 151.0 in stage 6.0 (TID 598). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO Executor: Finished task 154.0 in stage 6.0 (TID 601). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 11:18:25 INFO TaskSetManager: Starting task 168.0 in stage 6.0 (TID 615, localhost, ANY, 1692 bytes)
15/08/21 11:18:25 INFO Executor: Running task 168.0 in stage 6.0 (TID 615)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00112-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506027 length: 3506027 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO TaskSetManager: Starting task 169.0 in stage 6.0 (TID 616, localhost, ANY, 1692 bytes)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 151.0 in stage 6.0 (TID 598) in 916 ms on localhost (155/200)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 154.0 in stage 6.0 (TID 601) in 869 ms on localhost (156/200)
15/08/21 11:18:25 INFO Executor: Running task 169.0 in stage 6.0 (TID 616)
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00158-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502636 length: 3502636 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Finished task 157.0 in stage 6.0 (TID 604). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO TaskSetManager: Starting task 170.0 in stage 6.0 (TID 617, localhost, ANY, 1693 bytes)
15/08/21 11:18:25 INFO Executor: Running task 170.0 in stage 6.0 (TID 617)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 157.0 in stage 6.0 (TID 604) in 831 ms on localhost (157/200)
15/08/21 11:18:25 INFO Executor: Finished task 155.0 in stage 6.0 (TID 602). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00082-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501309 length: 3501309 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 11:18:25 INFO TaskSetManager: Starting task 171.0 in stage 6.0 (TID 618, localhost, ANY, 1694 bytes)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 155.0 in stage 6.0 (TID 602) in 884 ms on localhost (158/200)
15/08/21 11:18:25 INFO Executor: Running task 171.0 in stage 6.0 (TID 618)
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO Executor: Finished task 156.0 in stage 6.0 (TID 603). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00141-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506341 length: 3506341 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 INFO TaskSetManager: Starting task 172.0 in stage 6.0 (TID 619, localhost, ANY, 1694 bytes)
15/08/21 11:18:25 INFO Executor: Running task 172.0 in stage 6.0 (TID 619)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00053-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506395 length: 3506395 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO TaskSetManager: Finished task 156.0 in stage 6.0 (TID 603) in 858 ms on localhost (159/200)
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO Executor: Finished task 159.0 in stage 6.0 (TID 606). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 11:18:25 INFO TaskSetManager: Starting task 173.0 in stage 6.0 (TID 620, localhost, ANY, 1694 bytes)
15/08/21 11:18:25 INFO Executor: Running task 173.0 in stage 6.0 (TID 620)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 159.0 in stage 6.0 (TID 606) in 752 ms on localhost (160/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00080-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3508460 length: 3508460 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO Executor: Finished task 158.0 in stage 6.0 (TID 605). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 750000
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO TaskSetManager: Starting task 174.0 in stage 6.0 (TID 621, localhost, ANY, 1695 bytes)
15/08/21 11:18:25 INFO Executor: Running task 174.0 in stage 6.0 (TID 621)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 158.0 in stage 6.0 (TID 605) in 800 ms on localhost (161/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00016-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507180 length: 3507180 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO Executor: Finished task 160.0 in stage 6.0 (TID 607). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 11:18:25 INFO TaskSetManager: Starting task 175.0 in stage 6.0 (TID 622, localhost, ANY, 1692 bytes)
15/08/21 11:18:25 INFO Executor: Running task 175.0 in stage 6.0 (TID 622)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 160.0 in stage 6.0 (TID 607) in 799 ms on localhost (162/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00026-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498365 length: 3498365 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO Executor: Finished task 161.0 in stage 6.0 (TID 608). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO TaskSetManager: Starting task 176.0 in stage 6.0 (TID 623, localhost, ANY, 1693 bytes)
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Running task 176.0 in stage 6.0 (TID 623)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 161.0 in stage 6.0 (TID 608) in 771 ms on localhost (163/200)
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00185-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499076 length: 3499076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Finished task 165.0 in stage 6.0 (TID 612). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO TaskSetManager: Starting task 177.0 in stage 6.0 (TID 624, localhost, ANY, 1691 bytes)
15/08/21 11:18:25 INFO Executor: Running task 177.0 in stage 6.0 (TID 624)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 165.0 in stage 6.0 (TID 612) in 580 ms on localhost (164/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00099-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500145 length: 3500145 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO Executor: Finished task 163.0 in stage 6.0 (TID 610). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 11:18:25 INFO TaskSetManager: Starting task 178.0 in stage 6.0 (TID 625, localhost, ANY, 1692 bytes)
15/08/21 11:18:25 INFO Executor: Running task 178.0 in stage 6.0 (TID 625)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 163.0 in stage 6.0 (TID 610) in 728 ms on localhost (165/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00170-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498753 length: 3498753 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO Executor: Finished task 166.0 in stage 6.0 (TID 613). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO Executor: Finished task 164.0 in stage 6.0 (TID 611). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 11:18:25 INFO TaskSetManager: Starting task 179.0 in stage 6.0 (TID 626, localhost, ANY, 1693 bytes)
15/08/21 11:18:25 INFO Executor: Running task 179.0 in stage 6.0 (TID 626)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00052-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501586 length: 3501586 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO TaskSetManager: Starting task 180.0 in stage 6.0 (TID 627, localhost, ANY, 1693 bytes)
15/08/21 11:18:25 INFO Executor: Running task 180.0 in stage 6.0 (TID 627)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 166.0 in stage 6.0 (TID 613) in 573 ms on localhost (166/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00045-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3506318 length: 3506318 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 INFO TaskSetManager: Finished task 164.0 in stage 6.0 (TID 611) in 699 ms on localhost (167/200)
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Finished task 167.0 in stage 6.0 (TID 614). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 11:18:25 INFO TaskSetManager: Starting task 181.0 in stage 6.0 (TID 628, localhost, ANY, 1690 bytes)
15/08/21 11:18:25 INFO Executor: Running task 181.0 in stage 6.0 (TID 628)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 167.0 in stage 6.0 (TID 614) in 652 ms on localhost (168/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00001-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500034 length: 3500034 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO Executor: Finished task 168.0 in stage 6.0 (TID 615). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO TaskSetManager: Starting task 182.0 in stage 6.0 (TID 629, localhost, ANY, 1692 bytes)
15/08/21 11:18:25 INFO Executor: Running task 182.0 in stage 6.0 (TID 629)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 168.0 in stage 6.0 (TID 615) in 621 ms on localhost (169/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00155-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499054 length: 3499054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO Executor: Finished task 169.0 in stage 6.0 (TID 616). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO TaskSetManager: Starting task 183.0 in stage 6.0 (TID 630, localhost, ANY, 1690 bytes)
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO Executor: Running task 183.0 in stage 6.0 (TID 630)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 169.0 in stage 6.0 (TID 616) in 636 ms on localhost (170/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00003-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3499057 length: 3499057 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 11:18:25 INFO Executor: Finished task 170.0 in stage 6.0 (TID 617). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO TaskSetManager: Starting task 184.0 in stage 6.0 (TID 631, localhost, ANY, 1695 bytes)
15/08/21 11:18:25 INFO Executor: Running task 184.0 in stage 6.0 (TID 631)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 170.0 in stage 6.0 (TID 617) in 647 ms on localhost (171/200)
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00167-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507097 length: 3507097 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:25 INFO Executor: Finished task 172.0 in stage 6.0 (TID 619). 2125 bytes result sent to driver
15/08/21 11:18:25 INFO TaskSetManager: Starting task 185.0 in stage 6.0 (TID 632, localhost, ANY, 1693 bytes)
15/08/21 11:18:25 INFO Executor: Running task 185.0 in stage 6.0 (TID 632)
15/08/21 11:18:25 INFO TaskSetManager: Finished task 172.0 in stage 6.0 (TID 619) in 651 ms on localhost (172/200)
15/08/21 11:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00057-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501590 length: 3501590 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:25 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO Executor: Finished task 171.0 in stage 6.0 (TID 618). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO TaskSetManager: Starting task 186.0 in stage 6.0 (TID 633, localhost, ANY, 1694 bytes)
15/08/21 11:18:26 INFO Executor: Running task 186.0 in stage 6.0 (TID 633)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 171.0 in stage 6.0 (TID 618) in 730 ms on localhost (173/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00051-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500251 length: 3500251 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 750000
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 11:18:26 INFO Executor: Finished task 173.0 in stage 6.0 (TID 620). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO Executor: Finished task 174.0 in stage 6.0 (TID 621). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO TaskSetManager: Starting task 187.0 in stage 6.0 (TID 634, localhost, ANY, 1693 bytes)
15/08/21 11:18:26 INFO Executor: Running task 187.0 in stage 6.0 (TID 634)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00013-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505997 length: 3505997 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 INFO TaskSetManager: Starting task 188.0 in stage 6.0 (TID 635, localhost, ANY, 1693 bytes)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 173.0 in stage 6.0 (TID 620) in 760 ms on localhost (174/200)
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO Executor: Running task 188.0 in stage 6.0 (TID 635)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 174.0 in stage 6.0 (TID 621) in 724 ms on localhost (175/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00091-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3498759 length: 3498759 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO Executor: Finished task 175.0 in stage 6.0 (TID 622). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 750000
15/08/21 11:18:26 INFO TaskSetManager: Starting task 189.0 in stage 6.0 (TID 636, localhost, ANY, 1692 bytes)
15/08/21 11:18:26 INFO Executor: Running task 189.0 in stage 6.0 (TID 636)
15/08/21 11:18:26 INFO Executor: Finished task 176.0 in stage 6.0 (TID 623). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00132-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502651 length: 3502651 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO TaskSetManager: Starting task 190.0 in stage 6.0 (TID 637, localhost, ANY, 1690 bytes)
15/08/21 11:18:26 INFO Executor: Running task 190.0 in stage 6.0 (TID 637)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 175.0 in stage 6.0 (TID 622) in 758 ms on localhost (176/200)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 176.0 in stage 6.0 (TID 623) in 743 ms on localhost (177/200)
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00020-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3501937 length: 3501937 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 750000
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 11:18:26 INFO Executor: Finished task 177.0 in stage 6.0 (TID 624). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO TaskSetManager: Starting task 191.0 in stage 6.0 (TID 638, localhost, ANY, 1692 bytes)
15/08/21 11:18:26 INFO Executor: Running task 191.0 in stage 6.0 (TID 638)
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 11:18:26 INFO TaskSetManager: Finished task 177.0 in stage 6.0 (TID 624) in 723 ms on localhost (178/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00199-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505199 length: 3505199 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 750000
15/08/21 11:18:26 INFO Executor: Finished task 178.0 in stage 6.0 (TID 625). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO TaskSetManager: Starting task 192.0 in stage 6.0 (TID 639, localhost, ANY, 1694 bytes)
15/08/21 11:18:26 INFO Executor: Running task 192.0 in stage 6.0 (TID 639)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 178.0 in stage 6.0 (TID 625) in 748 ms on localhost (179/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00096-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507797 length: 3507797 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO Executor: Finished task 179.0 in stage 6.0 (TID 626). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO TaskSetManager: Starting task 193.0 in stage 6.0 (TID 640, localhost, ANY, 1693 bytes)
15/08/21 11:18:26 INFO Executor: Running task 193.0 in stage 6.0 (TID 640)
15/08/21 11:18:26 INFO Executor: Finished task 180.0 in stage 6.0 (TID 627). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00113-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3502019 length: 3502019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 11:18:26 INFO TaskSetManager: Starting task 194.0 in stage 6.0 (TID 641, localhost, ANY, 1692 bytes)
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO TaskSetManager: Finished task 179.0 in stage 6.0 (TID 626) in 787 ms on localhost (180/200)
15/08/21 11:18:26 INFO Executor: Running task 194.0 in stage 6.0 (TID 641)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 180.0 in stage 6.0 (TID 627) in 791 ms on localhost (181/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00076-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500080 length: 3500080 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO Executor: Finished task 181.0 in stage 6.0 (TID 628). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 291 ms. row count = 750000
15/08/21 11:18:26 INFO TaskSetManager: Starting task 195.0 in stage 6.0 (TID 642, localhost, ANY, 1694 bytes)
15/08/21 11:18:26 INFO Executor: Running task 195.0 in stage 6.0 (TID 642)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 181.0 in stage 6.0 (TID 628) in 1056 ms on localhost (182/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00064-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3504621 length: 3504621 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: block read in memory in 316 ms. row count = 750000
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO Executor: Finished task 182.0 in stage 6.0 (TID 629). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO TaskSetManager: Starting task 196.0 in stage 6.0 (TID 643, localhost, ANY, 1692 bytes)
15/08/21 11:18:26 INFO Executor: Running task 196.0 in stage 6.0 (TID 643)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 182.0 in stage 6.0 (TID 629) in 1085 ms on localhost (183/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00150-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3505922 length: 3505922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:26 INFO Executor: Finished task 183.0 in stage 6.0 (TID 630). 2125 bytes result sent to driver
15/08/21 11:18:26 INFO TaskSetManager: Starting task 197.0 in stage 6.0 (TID 644, localhost, ANY, 1691 bytes)
15/08/21 11:18:26 INFO Executor: Running task 197.0 in stage 6.0 (TID 644)
15/08/21 11:18:26 INFO TaskSetManager: Finished task 183.0 in stage 6.0 (TID 630) in 1134 ms on localhost (184/200)
15/08/21 11:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00154-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500858 length: 3500858 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 750000
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 750000
15/08/21 11:18:27 INFO Executor: Finished task 184.0 in stage 6.0 (TID 631). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 198.0 in stage 6.0 (TID 645, localhost, ANY, 1693 bytes)
15/08/21 11:18:27 INFO Executor: Running task 198.0 in stage 6.0 (TID 645)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 184.0 in stage 6.0 (TID 631) in 1135 ms on localhost (185/200)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00153-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3500279 length: 3500279 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 750000
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO Executor: Finished task 185.0 in stage 6.0 (TID 632). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 199.0 in stage 6.0 (TID 646, localhost, ANY, 1694 bytes)
15/08/21 11:18:27 INFO Executor: Running task 199.0 in stage 6.0 (TID 646)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00104-8ce39c9b-28c8-4911-9dbd-154dc8e06081.gz.parquet start: 0 end: 3507353 length: 3507353 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 INFO TaskSetManager: Finished task 185.0 in stage 6.0 (TID 632) in 1189 ms on localhost (186/200)
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 750000
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO Executor: Finished task 186.0 in stage 6.0 (TID 633). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO Executor: Finished task 187.0 in stage 6.0 (TID 634). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 647, localhost, ANY, 1762 bytes)
15/08/21 11:18:27 INFO Executor: Running task 0.0 in stage 7.0 (TID 647)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 648, localhost, ANY, 1773 bytes)
15/08/21 11:18:27 INFO Executor: Running task 1.0 in stage 7.0 (TID 648)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 134217728 end: 257568535 length: 123350807 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 INFO TaskSetManager: Finished task 187.0 in stage 6.0 (TID 634) in 1112 ms on localhost (187/200)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 186.0 in stage 6.0 (TID 633) in 1209 ms on localhost (188/200)
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 750000
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572843 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO Executor: Finished task 188.0 in stage 6.0 (TID 635). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 649, localhost, ANY, 1761 bytes)
15/08/21 11:18:27 INFO Executor: Running task 2.0 in stage 7.0 (TID 649)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 188.0 in stage 6.0 (TID 635) in 1198 ms on localhost (189/200)
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3501191
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO Executor: Finished task 189.0 in stage 6.0 (TID 636). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501150 records.
15/08/21 11:18:27 INFO Executor: Finished task 190.0 in stage 6.0 (TID 637). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 650, localhost, ANY, 1772 bytes)
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO TaskSetManager: Finished task 189.0 in stage 6.0 (TID 636) in 1184 ms on localhost (190/200)
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 3500939
15/08/21 11:18:27 INFO Executor: Running task 3.0 in stage 7.0 (TID 650)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 257171772 length: 122954044 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 INFO Executor: Finished task 191.0 in stage 6.0 (TID 638). 2125 bytes result sent to driver
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 651, localhost, ANY, 1761 bytes)
15/08/21 11:18:27 INFO Executor: Running task 4.0 in stage 7.0 (TID 651)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572933 records.
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 652, localhost, ANY, 1775 bytes)
15/08/21 11:18:27 INFO Executor: Running task 5.0 in stage 7.0 (TID 652)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 190.0 in stage 6.0 (TID 637) in 1235 ms on localhost (191/200)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 134217728 end: 257477064 length: 123259336 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3501150
15/08/21 11:18:27 INFO TaskSetManager: Finished task 191.0 in stage 6.0 (TID 638) in 1151 ms on localhost (192/200)
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501187 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573045 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 3502678
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 3501187
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 3500100
15/08/21 11:18:27 INFO Executor: Finished task 192.0 in stage 6.0 (TID 639). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 653, localhost, ANY, 1762 bytes)
15/08/21 11:18:27 INFO Executor: Running task 6.0 in stage 7.0 (TID 653)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 192.0 in stage 6.0 (TID 639) in 1168 ms on localhost (193/200)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 3500100
15/08/21 11:18:27 INFO Executor: Finished task 193.0 in stage 6.0 (TID 640). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO Executor: Finished task 194.0 in stage 6.0 (TID 641). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 654, localhost, ANY, 1777 bytes)
15/08/21 11:18:27 INFO Executor: Running task 7.0 in stage 7.0 (TID 654)
15/08/21 11:18:27 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 655, localhost, ANY, 1762 bytes)
15/08/21 11:18:27 INFO Executor: Running task 8.0 in stage 7.0 (TID 655)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 193.0 in stage 6.0 (TID 640) in 1236 ms on localhost (194/200)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 194.0 in stage 6.0 (TID 641) in 1221 ms on localhost (195/200)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 134217728 end: 257329816 length: 123112088 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573983 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501583 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 3501583
15/08/21 11:18:27 INFO InternalParquetRecordReader: block read in memory in 83 ms. row count = 3500100
15/08/21 11:18:27 INFO Executor: Finished task 196.0 in stage 6.0 (TID 643). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO Executor: Finished task 197.0 in stage 6.0 (TID 644). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 656, localhost, ANY, 1774 bytes)
15/08/21 11:18:27 INFO Executor: Running task 9.0 in stage 7.0 (TID 656)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 134217728 end: 257425918 length: 123208190 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO Executor: Finished task 195.0 in stage 6.0 (TID 642). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 657, localhost, ANY, 1760 bytes)
15/08/21 11:18:27 INFO Executor: Running task 10.0 in stage 7.0 (TID 657)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 658, localhost, ANY, 1771 bytes)
15/08/21 11:18:27 INFO Executor: Running task 11.0 in stage 7.0 (TID 658)
15/08/21 11:18:27 INFO TaskSetManager: Finished task 197.0 in stage 6.0 (TID 644) in 948 ms on localhost (196/200)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 259746640 length: 125528912 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO TaskSetManager: Finished task 195.0 in stage 6.0 (TID 642) in 1098 ms on localhost (197/200)
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572784 records.
15/08/21 11:18:27 INFO TaskSetManager: Finished task 196.0 in stage 6.0 (TID 643) in 1062 ms on localhost (198/200)
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627997 records.
15/08/21 11:18:27 INFO Executor: Finished task 198.0 in stage 6.0 (TID 645). 2125 bytes result sent to driver
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 659, localhost, ANY, 1762 bytes)
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:27 INFO Executor: Running task 12.0 in stage 7.0 (TID 659)
15/08/21 11:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:27 INFO TaskSetManager: Finished task 198.0 in stage 6.0 (TID 645) in 907 ms on localhost (199/200)
15/08/21 11:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:18:28 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 3500100
15/08/21 11:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:28 INFO InternalParquetRecordReader: block read in memory in 99 ms. row count = 3500100
15/08/21 11:18:28 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500100
15/08/21 11:18:28 INFO Executor: Finished task 199.0 in stage 6.0 (TID 646). 2125 bytes result sent to driver
15/08/21 11:18:28 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 660, localhost, ANY, 1775 bytes)
15/08/21 11:18:28 INFO TaskSetManager: Finished task 199.0 in stage 6.0 (TID 646) in 943 ms on localhost (200/200)
15/08/21 11:18:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/21 11:18:28 INFO DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:423) finished in 47.457 s
15/08/21 11:18:28 INFO DAGScheduler: looking for newly runnable stages
15/08/21 11:18:28 INFO DAGScheduler: running: Set(ShuffleMapStage 7, ShuffleMapStage 4)
15/08/21 11:18:28 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/21 11:18:28 INFO DAGScheduler: failed: Set()
15/08/21 11:18:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@114fc89d
15/08/21 11:18:28 INFO Executor: Running task 13.0 in stage 7.0 (TID 660)
15/08/21 11:18:28 INFO StatsReportListener: task runtime:(count: 255, mean: 2416.305882, stdev: 3446.673630, max: 13711.000000, min: 318.000000)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	318.0 ms	412.0 ms	524.0 ms	638.0 ms	750.0 ms	1.2 s	8.5 s	11.3 s	13.7 s
15/08/21 11:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 134217728 end: 257420451 length: 123202723 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:28 INFO StatsReportListener: shuffle bytes written:(count: 255, mean: 14057791.258824, stdev: 27732446.546454, max: 76375224.000000, min: 0.000000)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	0.0 B	3.1 KB	3.1 KB	3.2 KB	3.2 KB	3.2 KB	72.8 MB	72.8 MB	72.8 MB
15/08/21 11:18:28 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List(ShuffleMapStage 4)
15/08/21 11:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:28 INFO StatsReportListener: task result size:(count: 255, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 11:18:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 255, mean: 95.264246, stdev: 4.893967, max: 99.869099, min: 41.885246)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	42 %	91 %	92 %	94 %	96 %	98 %	100 %	100 %	100 %
15/08/21 11:18:28 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 7)
15/08/21 11:18:28 INFO StatsReportListener: other time pct: (count: 255, mean: 4.735754, stdev: 4.893967, max: 58.114754, min: 0.130901)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 2 %	 4 %	 6 %	 8 %	 9 %	58 %
15/08/21 11:18:28 INFO InternalParquetRecordReader: block read in memory in 96 ms. row count = 3500100
15/08/21 11:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573892 records.
15/08/21 11:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:28 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 3503008
15/08/21 11:18:28 INFO Executor: Finished task 55.0 in stage 4.0 (TID 445). 2125 bytes result sent to driver
15/08/21 11:18:28 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 661, localhost, ANY, 1762 bytes)
15/08/21 11:18:28 INFO Executor: Running task 14.0 in stage 7.0 (TID 661)
15/08/21 11:18:28 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 445) in 16401 ms on localhost (56/57)
15/08/21 11:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:28 INFO Executor: Finished task 54.0 in stage 4.0 (TID 444). 2125 bytes result sent to driver
15/08/21 11:18:28 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 662, localhost, ANY, 1773 bytes)
15/08/21 11:18:28 INFO Executor: Running task 15.0 in stage 7.0 (TID 662)
15/08/21 11:18:28 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 444) in 16471 ms on localhost (57/57)
15/08/21 11:18:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/21 11:18:28 INFO DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:423) finished in 47.903 s
15/08/21 11:18:28 INFO DAGScheduler: looking for newly runnable stages
15/08/21 11:18:28 INFO DAGScheduler: running: Set(ShuffleMapStage 7)
15/08/21 11:18:28 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/21 11:18:28 INFO DAGScheduler: failed: Set()
15/08/21 11:18:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@f205cec
15/08/21 11:18:28 INFO StatsReportListener: task runtime:(count: 2, mean: 16436.000000, stdev: 35.000000, max: 16471.000000, min: 16401.000000)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	16.4 s	16.4 s	16.4 s	16.4 s	16.5 s	16.5 s	16.5 s	16.5 s	16.5 s
15/08/21 11:18:28 INFO StatsReportListener: shuffle bytes written:(count: 2, mean: 76367999.000000, stdev: 3587.000000, max: 76371586.000000, min: 76364412.000000)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	72.8 MB	72.8 MB	72.8 MB	72.8 MB	72.8 MB	72.8 MB	72.8 MB	72.8 MB	72.8 MB
15/08/21 11:18:28 INFO StatsReportListener: task result size:(count: 2, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 11:18:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 2, mean: 99.805097, stdev: 0.097762, max: 99.902860, min: 99.707335)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 11:18:28 INFO StatsReportListener: other time pct: (count: 2, mean: 0.194903, stdev: 0.097762, max: 0.292665, min: 0.097140)
15/08/21 11:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:18:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 11:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 134217728 end: 257463677 length: 123245949 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:18:28 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List()
15/08/21 11:18:28 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 7)
15/08/21 11:18:28 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 11:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574066 records.
15/08/21 11:18:28 INFO MemoryStore: ensureFreeSpace(9704) called with curMem=1440133, maxMem=22226833244
15/08/21 11:18:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.5 KB, free 20.7 GB)
15/08/21 11:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:18:28 INFO MemoryStore: ensureFreeSpace(5005) called with curMem=1449837, maxMem=22226833244
15/08/21 11:18:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.9 KB, free 20.7 GB)
15/08/21 11:18:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:53620 (size: 4.9 KB, free: 20.7 GB)
15/08/21 11:18:28 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/21 11:18:28 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 3500100
15/08/21 11:18:28 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500780
15/08/21 11:18:28 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423)
15/08/21 11:18:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 200 tasks
15/08/21 11:18:32 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5066 ms: 690.90015 rec/ms, 1381.8003 cell/ms
15/08/21 11:18:32 INFO InternalParquetRecordReader: time spent so far 1% reading (57 ms) and 98% processing (5066 ms)
15/08/21 11:18:32 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:18:32 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 72945
15/08/21 11:18:32 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4942 ms: 708.23553 rec/ms, 1416.4711 cell/ms
15/08/21 11:18:32 INFO InternalParquetRecordReader: time spent so far 1% reading (83 ms) and 98% processing (4942 ms)
15/08/21 11:18:32 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:18:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73883
15/08/21 11:18:33 INFO InternalParquetRecordReader: Assembled and processed 3500939 records from 2 columns in 5627 ms: 622.16797 rec/ms, 1244.3359 cell/ms
15/08/21 11:18:33 INFO InternalParquetRecordReader: time spent so far 1% reading (81 ms) and 98% processing (5627 ms)
15/08/21 11:18:33 INFO InternalParquetRecordReader: at row 3500939. reading next block
15/08/21 11:18:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 71904
15/08/21 11:18:33 INFO InternalParquetRecordReader: Assembled and processed 3502678 records from 2 columns in 5704 ms: 614.074 rec/ms, 1228.148 cell/ms
15/08/21 11:18:33 INFO InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (5704 ms)
15/08/21 11:18:33 INFO InternalParquetRecordReader: at row 3502678. reading next block
15/08/21 11:18:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 70255
15/08/21 11:18:33 INFO Executor: Finished task 2.0 in stage 7.0 (TID 649). 2125 bytes result sent to driver
15/08/21 11:18:33 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 663, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:33 INFO Executor: Running task 0.0 in stage 5.0 (TID 663)
15/08/21 11:18:33 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 649) in 6032 ms on localhost (1/170)
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:33 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5394 ms: 648.88763 rec/ms, 1297.7753 cell/ms
15/08/21 11:18:33 INFO InternalParquetRecordReader: time spent so far 1% reading (99 ms) and 98% processing (5394 ms)
15/08/21 11:18:33 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:18:33 INFO InternalParquetRecordReader: Assembled and processed 3500780 records from 2 columns in 5045 ms: 693.9108 rec/ms, 1387.8217 cell/ms
15/08/21 11:18:33 INFO InternalParquetRecordReader: time spent so far 1% reading (69 ms) and 98% processing (5045 ms)
15/08/21 11:18:33 INFO InternalParquetRecordReader: at row 3500780. reading next block
15/08/21 11:18:33 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 127897
15/08/21 11:18:33 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 73286
15/08/21 11:18:33 INFO Executor: Finished task 5.0 in stage 7.0 (TID 652). 2125 bytes result sent to driver
15/08/21 11:18:33 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 664, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:33 INFO Executor: Running task 1.0 in stage 5.0 (TID 664)
15/08/21 11:18:33 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 652) in 6337 ms on localhost (2/170)
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:33 INFO Executor: Finished task 4.0 in stage 7.0 (TID 651). 2125 bytes result sent to driver
15/08/21 11:18:33 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 665, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:33 INFO Executor: Running task 2.0 in stage 5.0 (TID 665)
15/08/21 11:18:33 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 651) in 6487 ms on localhost (3/170)
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 5803 ms: 603.65466 rec/ms, 1207.3093 cell/ms
15/08/21 11:18:34 INFO InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (5803 ms)
15/08/21 11:18:34 INFO InternalParquetRecordReader: at row 3503008. reading next block
15/08/21 11:18:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 70884
15/08/21 11:18:34 INFO Executor: Finished task 0.0 in stage 7.0 (TID 647). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 666, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Running task 3.0 in stage 5.0 (TID 666)
15/08/21 11:18:34 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 647) in 7105 ms on localhost (4/170)
15/08/21 11:18:34 INFO Executor: Finished task 7.0 in stage 7.0 (TID 654). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 667, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Running task 4.0 in stage 5.0 (TID 667)
15/08/21 11:18:34 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 654) in 6624 ms on localhost (5/170)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO Executor: Finished task 12.0 in stage 7.0 (TID 659). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 668, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Finished task 10.0 in stage 7.0 (TID 657). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO Executor: Running task 5.0 in stage 5.0 (TID 668)
15/08/21 11:18:34 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 669, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Running task 6.0 in stage 5.0 (TID 669)
15/08/21 11:18:34 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 659) in 6460 ms on localhost (6/170)
15/08/21 11:18:34 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6302 ms: 555.39514 rec/ms, 1110.7903 cell/ms
15/08/21 11:18:34 INFO InternalParquetRecordReader: time spent so far 1% reading (105 ms) and 98% processing (6302 ms)
15/08/21 11:18:34 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:18:34 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 657) in 6551 ms on localhost (7/170)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 72684
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:34 INFO Executor: Finished task 6.0 in stage 7.0 (TID 653). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 670, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 653) in 6893 ms on localhost (8/170)
15/08/21 11:18:34 INFO Executor: Running task 7.0 in stage 5.0 (TID 670)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:34 INFO Executor: Finished task 1.0 in stage 7.0 (TID 648). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 671, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Running task 8.0 in stage 5.0 (TID 671)
15/08/21 11:18:34 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 648) in 7318 ms on localhost (9/170)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO Executor: Finished task 8.0 in stage 7.0 (TID 655). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 672, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 655) in 6998 ms on localhost (10/170)
15/08/21 11:18:34 INFO Executor: Finished task 3.0 in stage 7.0 (TID 650). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO Executor: Running task 9.0 in stage 5.0 (TID 672)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 673, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Running task 10.0 in stage 5.0 (TID 673)
15/08/21 11:18:34 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 650) in 7360 ms on localhost (11/170)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO Executor: Finished task 11.0 in stage 7.0 (TID 658). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 674, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Finished task 15.0 in stage 7.0 (TID 662). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO Executor: Running task 11.0 in stage 5.0 (TID 674)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 675, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 658) in 6873 ms on localhost (12/170)
15/08/21 11:18:34 INFO Executor: Running task 12.0 in stage 5.0 (TID 675)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 662) in 6504 ms on localhost (13/170)
15/08/21 11:18:34 INFO Executor: Finished task 13.0 in stage 7.0 (TID 660). 2125 bytes result sent to driver
15/08/21 11:18:34 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 676, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:34 INFO Executor: Running task 13.0 in stage 5.0 (TID 676)
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:34 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 660) in 6804 ms on localhost (14/170)
15/08/21 11:18:35 INFO Executor: Finished task 9.0 in stage 7.0 (TID 656). 2125 bytes result sent to driver
15/08/21 11:18:35 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 677, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:35 INFO Executor: Running task 14.0 in stage 5.0 (TID 677)
15/08/21 11:18:35 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 656) in 7238 ms on localhost (15/170)
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:35 INFO Executor: Finished task 14.0 in stage 7.0 (TID 661). 2125 bytes result sent to driver
15/08/21 11:18:35 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 678, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:35 INFO Executor: Running task 15.0 in stage 5.0 (TID 678)
15/08/21 11:18:35 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 661) in 7469 ms on localhost (16/170)
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:39 INFO Executor: Finished task 1.0 in stage 5.0 (TID 664). 1219 bytes result sent to driver
15/08/21 11:18:39 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 679, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:39 INFO Executor: Running task 16.0 in stage 5.0 (TID 679)
15/08/21 11:18:39 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 664) in 5725 ms on localhost (1/200)
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:39 INFO Executor: Finished task 0.0 in stage 5.0 (TID 663). 1219 bytes result sent to driver
15/08/21 11:18:39 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 680, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:39 INFO Executor: Running task 17.0 in stage 5.0 (TID 680)
15/08/21 11:18:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 663) in 6475 ms on localhost (2/200)
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:39 INFO Executor: Finished task 2.0 in stage 5.0 (TID 665). 1219 bytes result sent to driver
15/08/21 11:18:39 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 681, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:39 INFO Executor: Running task 18.0 in stage 5.0 (TID 681)
15/08/21 11:18:39 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 665) in 5994 ms on localhost (3/200)
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:40 INFO Executor: Finished task 10.0 in stage 5.0 (TID 673). 1219 bytes result sent to driver
15/08/21 11:18:40 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 682, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:40 INFO Executor: Running task 19.0 in stage 5.0 (TID 682)
15/08/21 11:18:40 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 673) in 5471 ms on localhost (4/200)
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:40 INFO Executor: Finished task 8.0 in stage 5.0 (TID 671). 1219 bytes result sent to driver
15/08/21 11:18:40 INFO TaskSetManager: Starting task 20.0 in stage 5.0 (TID 683, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:40 INFO Executor: Running task 20.0 in stage 5.0 (TID 683)
15/08/21 11:18:40 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 671) in 5983 ms on localhost (5/200)
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:40 INFO Executor: Finished task 6.0 in stage 5.0 (TID 669). 1219 bytes result sent to driver
15/08/21 11:18:40 INFO TaskSetManager: Starting task 21.0 in stage 5.0 (TID 684, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:40 INFO Executor: Running task 21.0 in stage 5.0 (TID 684)
15/08/21 11:18:40 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 669) in 6214 ms on localhost (6/200)
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:40 INFO Executor: Finished task 4.0 in stage 5.0 (TID 667). 1219 bytes result sent to driver
15/08/21 11:18:40 INFO TaskSetManager: Starting task 22.0 in stage 5.0 (TID 685, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:40 INFO Executor: Running task 22.0 in stage 5.0 (TID 685)
15/08/21 11:18:40 INFO Executor: Finished task 5.0 in stage 5.0 (TID 668). 1219 bytes result sent to driver
15/08/21 11:18:40 INFO TaskSetManager: Starting task 23.0 in stage 5.0 (TID 686, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:40 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 667) in 6552 ms on localhost (7/200)
15/08/21 11:18:40 INFO Executor: Running task 23.0 in stage 5.0 (TID 686)
15/08/21 11:18:40 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 668) in 6466 ms on localhost (8/200)
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:41 INFO Executor: Finished task 7.0 in stage 5.0 (TID 670). 1219 bytes result sent to driver
15/08/21 11:18:41 INFO TaskSetManager: Starting task 24.0 in stage 5.0 (TID 687, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:41 INFO Executor: Running task 24.0 in stage 5.0 (TID 687)
15/08/21 11:18:41 INFO Executor: Finished task 3.0 in stage 5.0 (TID 666). 1219 bytes result sent to driver
15/08/21 11:18:41 INFO TaskSetManager: Starting task 25.0 in stage 5.0 (TID 688, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:41 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 670) in 6657 ms on localhost (9/200)
15/08/21 11:18:41 INFO Executor: Running task 25.0 in stage 5.0 (TID 688)
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:41 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 666) in 6787 ms on localhost (10/200)
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:41 INFO Executor: Finished task 11.0 in stage 5.0 (TID 674). 1219 bytes result sent to driver
15/08/21 11:18:41 INFO TaskSetManager: Starting task 26.0 in stage 5.0 (TID 689, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:41 INFO Executor: Running task 26.0 in stage 5.0 (TID 689)
15/08/21 11:18:41 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 674) in 6476 ms on localhost (11/200)
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:18:41 INFO Executor: Finished task 9.0 in stage 5.0 (TID 672). 1219 bytes result sent to driver
15/08/21 11:18:41 INFO TaskSetManager: Starting task 27.0 in stage 5.0 (TID 690, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:41 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 672) in 6583 ms on localhost (12/200)
15/08/21 11:18:41 INFO Executor: Running task 27.0 in stage 5.0 (TID 690)
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:41 INFO Executor: Finished task 13.0 in stage 5.0 (TID 676). 1219 bytes result sent to driver
15/08/21 11:18:41 INFO TaskSetManager: Starting task 28.0 in stage 5.0 (TID 691, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:41 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 676) in 7037 ms on localhost (13/200)
15/08/21 11:18:41 INFO Executor: Running task 28.0 in stage 5.0 (TID 691)
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:41 INFO Executor: Finished task 14.0 in stage 5.0 (TID 677). 1219 bytes result sent to driver
15/08/21 11:18:41 INFO TaskSetManager: Starting task 29.0 in stage 5.0 (TID 692, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:41 INFO Executor: Running task 29.0 in stage 5.0 (TID 692)
15/08/21 11:18:41 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 677) in 6897 ms on localhost (14/200)
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 11:18:42 INFO Executor: Finished task 12.0 in stage 5.0 (TID 675). 1219 bytes result sent to driver
15/08/21 11:18:42 INFO TaskSetManager: Starting task 30.0 in stage 5.0 (TID 693, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:42 INFO Executor: Running task 30.0 in stage 5.0 (TID 693)
15/08/21 11:18:42 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 675) in 7314 ms on localhost (15/200)
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:42 INFO Executor: Finished task 15.0 in stage 5.0 (TID 678). 1219 bytes result sent to driver
15/08/21 11:18:42 INFO TaskSetManager: Starting task 31.0 in stage 5.0 (TID 694, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:42 INFO Executor: Running task 31.0 in stage 5.0 (TID 694)
15/08/21 11:18:42 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 678) in 6954 ms on localhost (16/200)
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:45 INFO Executor: Finished task 17.0 in stage 5.0 (TID 680). 1219 bytes result sent to driver
15/08/21 11:18:45 INFO TaskSetManager: Starting task 32.0 in stage 5.0 (TID 695, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:45 INFO Executor: Running task 32.0 in stage 5.0 (TID 695)
15/08/21 11:18:45 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 680) in 5844 ms on localhost (17/200)
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:45 INFO Executor: Finished task 18.0 in stage 5.0 (TID 681). 1219 bytes result sent to driver
15/08/21 11:18:45 INFO TaskSetManager: Starting task 33.0 in stage 5.0 (TID 696, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:45 INFO Executor: Running task 33.0 in stage 5.0 (TID 696)
15/08/21 11:18:45 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 681) in 5886 ms on localhost (18/200)
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:45 INFO Executor: Finished task 16.0 in stage 5.0 (TID 679). 1219 bytes result sent to driver
15/08/21 11:18:45 INFO TaskSetManager: Starting task 34.0 in stage 5.0 (TID 697, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:45 INFO Executor: Running task 34.0 in stage 5.0 (TID 697)
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:45 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 679) in 6375 ms on localhost (19/200)
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO Executor: Finished task 20.0 in stage 5.0 (TID 683). 1219 bytes result sent to driver
15/08/21 11:18:46 INFO TaskSetManager: Starting task 35.0 in stage 5.0 (TID 698, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:46 INFO Executor: Running task 35.0 in stage 5.0 (TID 698)
15/08/21 11:18:46 INFO TaskSetManager: Finished task 20.0 in stage 5.0 (TID 683) in 5796 ms on localhost (20/200)
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO Executor: Finished task 22.0 in stage 5.0 (TID 685). 1219 bytes result sent to driver
15/08/21 11:18:46 INFO TaskSetManager: Starting task 36.0 in stage 5.0 (TID 699, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:46 INFO TaskSetManager: Finished task 22.0 in stage 5.0 (TID 685) in 5684 ms on localhost (21/200)
15/08/21 11:18:46 INFO Executor: Running task 36.0 in stage 5.0 (TID 699)
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:46 INFO Executor: Finished task 24.0 in stage 5.0 (TID 687). 1219 bytes result sent to driver
15/08/21 11:18:46 INFO TaskSetManager: Starting task 37.0 in stage 5.0 (TID 700, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:46 INFO Executor: Running task 37.0 in stage 5.0 (TID 700)
15/08/21 11:18:46 INFO TaskSetManager: Finished task 24.0 in stage 5.0 (TID 687) in 5487 ms on localhost (22/200)
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:46 INFO Executor: Finished task 21.0 in stage 5.0 (TID 684). 1219 bytes result sent to driver
15/08/21 11:18:46 INFO TaskSetManager: Starting task 38.0 in stage 5.0 (TID 701, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:46 INFO Executor: Running task 38.0 in stage 5.0 (TID 701)
15/08/21 11:18:46 INFO TaskSetManager: Finished task 21.0 in stage 5.0 (TID 684) in 6068 ms on localhost (23/200)
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO Executor: Finished task 23.0 in stage 5.0 (TID 686). 1219 bytes result sent to driver
15/08/21 11:18:46 INFO TaskSetManager: Starting task 39.0 in stage 5.0 (TID 702, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:46 INFO TaskSetManager: Finished task 23.0 in stage 5.0 (TID 686) in 5934 ms on localhost (24/200)
15/08/21 11:18:46 INFO Executor: Running task 39.0 in stage 5.0 (TID 702)
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:46 INFO Executor: Finished task 19.0 in stage 5.0 (TID 682). 1219 bytes result sent to driver
15/08/21 11:18:46 INFO TaskSetManager: Starting task 40.0 in stage 5.0 (TID 703, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:46 INFO Executor: Running task 40.0 in stage 5.0 (TID 703)
15/08/21 11:18:46 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 682) in 6815 ms on localhost (25/200)
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:47 INFO Executor: Finished task 25.0 in stage 5.0 (TID 688). 1219 bytes result sent to driver
15/08/21 11:18:47 INFO TaskSetManager: Starting task 41.0 in stage 5.0 (TID 704, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:47 INFO Executor: Running task 41.0 in stage 5.0 (TID 704)
15/08/21 11:18:47 INFO TaskSetManager: Finished task 25.0 in stage 5.0 (TID 688) in 6057 ms on localhost (26/200)
15/08/21 11:18:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:48 INFO Executor: Finished task 27.0 in stage 5.0 (TID 690). 1219 bytes result sent to driver
15/08/21 11:18:48 INFO TaskSetManager: Starting task 42.0 in stage 5.0 (TID 705, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:48 INFO Executor: Running task 42.0 in stage 5.0 (TID 705)
15/08/21 11:18:48 INFO TaskSetManager: Finished task 27.0 in stage 5.0 (TID 690) in 7157 ms on localhost (27/200)
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:48 INFO Executor: Finished task 26.0 in stage 5.0 (TID 689). 1219 bytes result sent to driver
15/08/21 11:18:48 INFO TaskSetManager: Starting task 43.0 in stage 5.0 (TID 706, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:48 INFO Executor: Running task 43.0 in stage 5.0 (TID 706)
15/08/21 11:18:48 INFO TaskSetManager: Finished task 26.0 in stage 5.0 (TID 689) in 7264 ms on localhost (28/200)
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:48 INFO Executor: Finished task 28.0 in stage 5.0 (TID 691). 1219 bytes result sent to driver
15/08/21 11:18:48 INFO TaskSetManager: Starting task 44.0 in stage 5.0 (TID 707, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:48 INFO Executor: Running task 44.0 in stage 5.0 (TID 707)
15/08/21 11:18:48 INFO TaskSetManager: Finished task 28.0 in stage 5.0 (TID 691) in 6646 ms on localhost (29/200)
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:51 INFO Executor: Finished task 30.0 in stage 5.0 (TID 693). 1219 bytes result sent to driver
15/08/21 11:18:51 INFO TaskSetManager: Starting task 45.0 in stage 5.0 (TID 708, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:51 INFO Executor: Running task 45.0 in stage 5.0 (TID 708)
15/08/21 11:18:51 INFO TaskSetManager: Finished task 30.0 in stage 5.0 (TID 693) in 8975 ms on localhost (30/200)
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:51 INFO Executor: Finished task 29.0 in stage 5.0 (TID 692). 1219 bytes result sent to driver
15/08/21 11:18:51 INFO TaskSetManager: Starting task 46.0 in stage 5.0 (TID 709, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:51 INFO TaskSetManager: Finished task 29.0 in stage 5.0 (TID 692) in 9155 ms on localhost (31/200)
15/08/21 11:18:51 INFO Executor: Running task 46.0 in stage 5.0 (TID 709)
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:51 INFO Executor: Finished task 31.0 in stage 5.0 (TID 694). 1219 bytes result sent to driver
15/08/21 11:18:51 INFO TaskSetManager: Starting task 47.0 in stage 5.0 (TID 710, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:51 INFO Executor: Running task 47.0 in stage 5.0 (TID 710)
15/08/21 11:18:51 INFO TaskSetManager: Finished task 31.0 in stage 5.0 (TID 694) in 8753 ms on localhost (32/200)
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 11:18:54 INFO Executor: Finished task 32.0 in stage 5.0 (TID 695). 1219 bytes result sent to driver
15/08/21 11:18:54 INFO TaskSetManager: Starting task 48.0 in stage 5.0 (TID 711, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:54 INFO Executor: Finished task 34.0 in stage 5.0 (TID 697). 1219 bytes result sent to driver
15/08/21 11:18:54 INFO TaskSetManager: Starting task 49.0 in stage 5.0 (TID 712, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:54 INFO Executor: Running task 49.0 in stage 5.0 (TID 712)
15/08/21 11:18:54 INFO TaskSetManager: Finished task 32.0 in stage 5.0 (TID 695) in 8789 ms on localhost (33/200)
15/08/21 11:18:54 INFO Executor: Running task 48.0 in stage 5.0 (TID 711)
15/08/21 11:18:54 INFO TaskSetManager: Finished task 34.0 in stage 5.0 (TID 697) in 8589 ms on localhost (34/200)
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO Executor: Finished task 35.0 in stage 5.0 (TID 698). 1219 bytes result sent to driver
15/08/21 11:18:54 INFO TaskSetManager: Starting task 50.0 in stage 5.0 (TID 713, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:54 INFO TaskSetManager: Finished task 35.0 in stage 5.0 (TID 698) in 8313 ms on localhost (35/200)
15/08/21 11:18:54 INFO Executor: Running task 50.0 in stage 5.0 (TID 713)
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO Executor: Finished task 37.0 in stage 5.0 (TID 700). 1219 bytes result sent to driver
15/08/21 11:18:54 INFO TaskSetManager: Starting task 51.0 in stage 5.0 (TID 714, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:54 INFO Executor: Running task 51.0 in stage 5.0 (TID 714)
15/08/21 11:18:54 INFO TaskSetManager: Finished task 37.0 in stage 5.0 (TID 700) in 8156 ms on localhost (36/200)
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:18:54 INFO Executor: Finished task 41.0 in stage 5.0 (TID 704). 1219 bytes result sent to driver
15/08/21 11:18:54 INFO TaskSetManager: Starting task 52.0 in stage 5.0 (TID 715, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:54 INFO Executor: Running task 52.0 in stage 5.0 (TID 715)
15/08/21 11:18:54 INFO TaskSetManager: Finished task 41.0 in stage 5.0 (TID 704) in 7732 ms on localhost (37/200)
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO Executor: Finished task 33.0 in stage 5.0 (TID 696). 1219 bytes result sent to driver
15/08/21 11:18:54 INFO TaskSetManager: Starting task 53.0 in stage 5.0 (TID 716, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:54 INFO Executor: Running task 53.0 in stage 5.0 (TID 716)
15/08/21 11:18:54 INFO TaskSetManager: Finished task 33.0 in stage 5.0 (TID 696) in 9157 ms on localhost (38/200)
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:54 INFO Executor: Finished task 39.0 in stage 5.0 (TID 702). 1219 bytes result sent to driver
15/08/21 11:18:54 INFO TaskSetManager: Starting task 54.0 in stage 5.0 (TID 717, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:54 INFO Executor: Running task 54.0 in stage 5.0 (TID 717)
15/08/21 11:18:55 INFO Executor: Finished task 38.0 in stage 5.0 (TID 701). 1219 bytes result sent to driver
15/08/21 11:18:55 INFO TaskSetManager: Starting task 55.0 in stage 5.0 (TID 718, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:55 INFO Executor: Running task 55.0 in stage 5.0 (TID 718)
15/08/21 11:18:55 INFO TaskSetManager: Finished task 39.0 in stage 5.0 (TID 702) in 8221 ms on localhost (39/200)
15/08/21 11:18:55 INFO TaskSetManager: Finished task 38.0 in stage 5.0 (TID 701) in 8351 ms on localhost (40/200)
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO Executor: Finished task 40.0 in stage 5.0 (TID 703). 1219 bytes result sent to driver
15/08/21 11:18:55 INFO TaskSetManager: Starting task 56.0 in stage 5.0 (TID 719, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:55 INFO Executor: Running task 56.0 in stage 5.0 (TID 719)
15/08/21 11:18:55 INFO TaskSetManager: Finished task 40.0 in stage 5.0 (TID 703) in 8155 ms on localhost (41/200)
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO Executor: Finished task 36.0 in stage 5.0 (TID 699). 1219 bytes result sent to driver
15/08/21 11:18:55 INFO TaskSetManager: Starting task 57.0 in stage 5.0 (TID 720, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:55 INFO Executor: Running task 57.0 in stage 5.0 (TID 720)
15/08/21 11:18:55 INFO TaskSetManager: Finished task 36.0 in stage 5.0 (TID 699) in 8688 ms on localhost (42/200)
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO Executor: Finished task 42.0 in stage 5.0 (TID 705). 1219 bytes result sent to driver
15/08/21 11:18:55 INFO TaskSetManager: Starting task 58.0 in stage 5.0 (TID 721, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:55 INFO Executor: Running task 58.0 in stage 5.0 (TID 721)
15/08/21 11:18:55 INFO TaskSetManager: Finished task 42.0 in stage 5.0 (TID 705) in 7439 ms on localhost (43/200)
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO Executor: Finished task 44.0 in stage 5.0 (TID 707). 1219 bytes result sent to driver
15/08/21 11:18:55 INFO TaskSetManager: Starting task 59.0 in stage 5.0 (TID 722, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:55 INFO TaskSetManager: Finished task 44.0 in stage 5.0 (TID 707) in 7403 ms on localhost (44/200)
15/08/21 11:18:55 INFO Executor: Running task 59.0 in stage 5.0 (TID 722)
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:56 INFO Executor: Finished task 43.0 in stage 5.0 (TID 706). 1219 bytes result sent to driver
15/08/21 11:18:56 INFO TaskSetManager: Starting task 60.0 in stage 5.0 (TID 723, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:56 INFO Executor: Running task 60.0 in stage 5.0 (TID 723)
15/08/21 11:18:56 INFO TaskSetManager: Finished task 43.0 in stage 5.0 (TID 706) in 7798 ms on localhost (45/200)
15/08/21 11:18:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:57 INFO Executor: Finished task 45.0 in stage 5.0 (TID 708). 1219 bytes result sent to driver
15/08/21 11:18:57 INFO TaskSetManager: Starting task 61.0 in stage 5.0 (TID 724, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:57 INFO Executor: Running task 61.0 in stage 5.0 (TID 724)
15/08/21 11:18:57 INFO TaskSetManager: Finished task 45.0 in stage 5.0 (TID 708) in 6298 ms on localhost (46/200)
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:57 INFO Executor: Finished task 46.0 in stage 5.0 (TID 709). 1219 bytes result sent to driver
15/08/21 11:18:57 INFO TaskSetManager: Starting task 62.0 in stage 5.0 (TID 725, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:57 INFO Executor: Running task 62.0 in stage 5.0 (TID 725)
15/08/21 11:18:57 INFO TaskSetManager: Finished task 46.0 in stage 5.0 (TID 709) in 6218 ms on localhost (47/200)
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:57 INFO Executor: Finished task 47.0 in stage 5.0 (TID 710). 1219 bytes result sent to driver
15/08/21 11:18:57 INFO TaskSetManager: Starting task 63.0 in stage 5.0 (TID 726, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:18:57 INFO Executor: Running task 63.0 in stage 5.0 (TID 726)
15/08/21 11:18:57 INFO TaskSetManager: Finished task 47.0 in stage 5.0 (TID 710) in 6526 ms on localhost (48/200)
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:00 INFO Executor: Finished task 48.0 in stage 5.0 (TID 711). 1219 bytes result sent to driver
15/08/21 11:19:00 INFO TaskSetManager: Starting task 64.0 in stage 5.0 (TID 727, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:00 INFO Executor: Running task 64.0 in stage 5.0 (TID 727)
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:00 INFO TaskSetManager: Finished task 48.0 in stage 5.0 (TID 711) in 5743 ms on localhost (49/200)
15/08/21 11:19:00 INFO Executor: Finished task 51.0 in stage 5.0 (TID 714). 1219 bytes result sent to driver
15/08/21 11:19:00 INFO TaskSetManager: Starting task 65.0 in stage 5.0 (TID 728, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:00 INFO Executor: Running task 65.0 in stage 5.0 (TID 728)
15/08/21 11:19:00 INFO TaskSetManager: Finished task 51.0 in stage 5.0 (TID 714) in 5727 ms on localhost (50/200)
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:01 INFO Executor: Finished task 49.0 in stage 5.0 (TID 712). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 66.0 in stage 5.0 (TID 729, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 66.0 in stage 5.0 (TID 729)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 49.0 in stage 5.0 (TID 712) in 6843 ms on localhost (51/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO Executor: Finished task 57.0 in stage 5.0 (TID 720). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 67.0 in stage 5.0 (TID 730, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 67.0 in stage 5.0 (TID 730)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 57.0 in stage 5.0 (TID 720) in 6045 ms on localhost (52/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO Executor: Finished task 50.0 in stage 5.0 (TID 713). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 68.0 in stage 5.0 (TID 731, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 68.0 in stage 5.0 (TID 731)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 50.0 in stage 5.0 (TID 713) in 6886 ms on localhost (53/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:01 INFO Executor: Finished task 55.0 in stage 5.0 (TID 718). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 69.0 in stage 5.0 (TID 732, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 69.0 in stage 5.0 (TID 732)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 55.0 in stage 5.0 (TID 718) in 6510 ms on localhost (54/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO Executor: Finished task 56.0 in stage 5.0 (TID 719). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 70.0 in stage 5.0 (TID 733, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 70.0 in stage 5.0 (TID 733)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 56.0 in stage 5.0 (TID 719) in 6443 ms on localhost (55/200)
15/08/21 11:19:01 INFO Executor: Finished task 52.0 in stage 5.0 (TID 715). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:01 INFO TaskSetManager: Starting task 71.0 in stage 5.0 (TID 734, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 71.0 in stage 5.0 (TID 734)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO TaskSetManager: Finished task 52.0 in stage 5.0 (TID 715) in 6711 ms on localhost (56/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO Executor: Finished task 54.0 in stage 5.0 (TID 717). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 72.0 in stage 5.0 (TID 735, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 72.0 in stage 5.0 (TID 735)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 54.0 in stage 5.0 (TID 717) in 6588 ms on localhost (57/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO Executor: Finished task 53.0 in stage 5.0 (TID 716). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 73.0 in stage 5.0 (TID 736, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 73.0 in stage 5.0 (TID 736)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 53.0 in stage 5.0 (TID 716) in 6779 ms on localhost (58/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO Executor: Finished task 58.0 in stage 5.0 (TID 721). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 74.0 in stage 5.0 (TID 737, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 74.0 in stage 5.0 (TID 737)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 58.0 in stage 5.0 (TID 721) in 6032 ms on localhost (59/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:01 INFO Executor: Finished task 59.0 in stage 5.0 (TID 722). 1219 bytes result sent to driver
15/08/21 11:19:01 INFO TaskSetManager: Starting task 75.0 in stage 5.0 (TID 738, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:01 INFO Executor: Running task 75.0 in stage 5.0 (TID 738)
15/08/21 11:19:01 INFO TaskSetManager: Finished task 59.0 in stage 5.0 (TID 722) in 5996 ms on localhost (60/200)
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:02 INFO Executor: Finished task 60.0 in stage 5.0 (TID 723). 1219 bytes result sent to driver
15/08/21 11:19:02 INFO TaskSetManager: Starting task 76.0 in stage 5.0 (TID 739, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:02 INFO Executor: Running task 76.0 in stage 5.0 (TID 739)
15/08/21 11:19:02 INFO TaskSetManager: Finished task 60.0 in stage 5.0 (TID 723) in 5934 ms on localhost (61/200)
15/08/21 11:19:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:05 INFO Executor: Finished task 62.0 in stage 5.0 (TID 725). 1219 bytes result sent to driver
15/08/21 11:19:05 INFO TaskSetManager: Starting task 77.0 in stage 5.0 (TID 740, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:05 INFO Executor: Running task 77.0 in stage 5.0 (TID 740)
15/08/21 11:19:05 INFO TaskSetManager: Finished task 62.0 in stage 5.0 (TID 725) in 8022 ms on localhost (62/200)
15/08/21 11:19:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:06 INFO Executor: Finished task 61.0 in stage 5.0 (TID 724). 1219 bytes result sent to driver
15/08/21 11:19:06 INFO TaskSetManager: Starting task 78.0 in stage 5.0 (TID 741, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:06 INFO Executor: Running task 78.0 in stage 5.0 (TID 741)
15/08/21 11:19:06 INFO TaskSetManager: Finished task 61.0 in stage 5.0 (TID 724) in 8864 ms on localhost (63/200)
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:06 INFO Executor: Finished task 63.0 in stage 5.0 (TID 726). 1219 bytes result sent to driver
15/08/21 11:19:06 INFO TaskSetManager: Starting task 79.0 in stage 5.0 (TID 742, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:06 INFO Executor: Running task 79.0 in stage 5.0 (TID 742)
15/08/21 11:19:06 INFO TaskSetManager: Finished task 63.0 in stage 5.0 (TID 726) in 8464 ms on localhost (64/200)
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:19:08 INFO Executor: Finished task 65.0 in stage 5.0 (TID 728). 1219 bytes result sent to driver
15/08/21 11:19:08 INFO TaskSetManager: Starting task 80.0 in stage 5.0 (TID 743, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:08 INFO Executor: Running task 80.0 in stage 5.0 (TID 743)
15/08/21 11:19:08 INFO TaskSetManager: Finished task 65.0 in stage 5.0 (TID 728) in 7629 ms on localhost (65/200)
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:08 INFO Executor: Finished task 64.0 in stage 5.0 (TID 727). 1219 bytes result sent to driver
15/08/21 11:19:08 INFO TaskSetManager: Starting task 81.0 in stage 5.0 (TID 744, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:08 INFO Executor: Running task 81.0 in stage 5.0 (TID 744)
15/08/21 11:19:08 INFO TaskSetManager: Finished task 64.0 in stage 5.0 (TID 727) in 7978 ms on localhost (66/200)
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:08 INFO Executor: Finished task 67.0 in stage 5.0 (TID 730). 1219 bytes result sent to driver
15/08/21 11:19:08 INFO TaskSetManager: Starting task 82.0 in stage 5.0 (TID 745, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:08 INFO Executor: Running task 82.0 in stage 5.0 (TID 745)
15/08/21 11:19:08 INFO TaskSetManager: Finished task 67.0 in stage 5.0 (TID 730) in 7453 ms on localhost (67/200)
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO Executor: Finished task 72.0 in stage 5.0 (TID 735). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 83.0 in stage 5.0 (TID 746, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO Executor: Running task 83.0 in stage 5.0 (TID 746)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 72.0 in stage 5.0 (TID 735) in 7822 ms on localhost (68/200)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:09 INFO Executor: Finished task 74.0 in stage 5.0 (TID 737). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 84.0 in stage 5.0 (TID 747, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO Executor: Running task 84.0 in stage 5.0 (TID 747)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 74.0 in stage 5.0 (TID 737) in 7617 ms on localhost (69/200)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO Executor: Finished task 75.0 in stage 5.0 (TID 738). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 85.0 in stage 5.0 (TID 748, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO Executor: Running task 85.0 in stage 5.0 (TID 748)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 75.0 in stage 5.0 (TID 738) in 7615 ms on localhost (70/200)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO Executor: Finished task 76.0 in stage 5.0 (TID 739). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 86.0 in stage 5.0 (TID 749, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 76.0 in stage 5.0 (TID 739) in 7381 ms on localhost (71/200)
15/08/21 11:19:09 INFO Executor: Running task 86.0 in stage 5.0 (TID 749)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO Executor: Finished task 66.0 in stage 5.0 (TID 729). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 87.0 in stage 5.0 (TID 750, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO Executor: Running task 87.0 in stage 5.0 (TID 750)
15/08/21 11:19:09 INFO Executor: Finished task 68.0 in stage 5.0 (TID 731). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 88.0 in stage 5.0 (TID 751, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 66.0 in stage 5.0 (TID 729) in 8372 ms on localhost (72/200)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 68.0 in stage 5.0 (TID 731) in 8102 ms on localhost (73/200)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:09 INFO Executor: Running task 88.0 in stage 5.0 (TID 751)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO Executor: Finished task 71.0 in stage 5.0 (TID 734). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 89.0 in stage 5.0 (TID 752, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO Executor: Running task 89.0 in stage 5.0 (TID 752)
15/08/21 11:19:09 INFO Executor: Finished task 73.0 in stage 5.0 (TID 736). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO TaskSetManager: Starting task 90.0 in stage 5.0 (TID 753, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 71.0 in stage 5.0 (TID 734) in 8046 ms on localhost (74/200)
15/08/21 11:19:09 INFO Executor: Running task 90.0 in stage 5.0 (TID 753)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO TaskSetManager: Finished task 73.0 in stage 5.0 (TID 736) in 7965 ms on localhost (75/200)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO Executor: Finished task 69.0 in stage 5.0 (TID 732). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO Executor: Finished task 70.0 in stage 5.0 (TID 733). 1219 bytes result sent to driver
15/08/21 11:19:09 INFO TaskSetManager: Starting task 91.0 in stage 5.0 (TID 754, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO TaskSetManager: Starting task 92.0 in stage 5.0 (TID 755, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:09 INFO Executor: Running task 92.0 in stage 5.0 (TID 755)
15/08/21 11:19:09 INFO Executor: Running task 91.0 in stage 5.0 (TID 754)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 69.0 in stage 5.0 (TID 732) in 8129 ms on localhost (76/200)
15/08/21 11:19:09 INFO TaskSetManager: Finished task 70.0 in stage 5.0 (TID 733) in 8085 ms on localhost (77/200)
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:11 INFO Executor: Finished task 77.0 in stage 5.0 (TID 740). 1219 bytes result sent to driver
15/08/21 11:19:11 INFO TaskSetManager: Starting task 93.0 in stage 5.0 (TID 756, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:11 INFO Executor: Running task 93.0 in stage 5.0 (TID 756)
15/08/21 11:19:11 INFO TaskSetManager: Finished task 77.0 in stage 5.0 (TID 740) in 6061 ms on localhost (78/200)
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:11 INFO Executor: Finished task 78.0 in stage 5.0 (TID 741). 1219 bytes result sent to driver
15/08/21 11:19:11 INFO TaskSetManager: Starting task 94.0 in stage 5.0 (TID 757, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:11 INFO TaskSetManager: Finished task 78.0 in stage 5.0 (TID 741) in 5418 ms on localhost (79/200)
15/08/21 11:19:11 INFO Executor: Running task 94.0 in stage 5.0 (TID 757)
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:12 INFO Executor: Finished task 79.0 in stage 5.0 (TID 742). 1219 bytes result sent to driver
15/08/21 11:19:12 INFO TaskSetManager: Starting task 95.0 in stage 5.0 (TID 758, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:12 INFO Executor: Running task 95.0 in stage 5.0 (TID 758)
15/08/21 11:19:12 INFO TaskSetManager: Finished task 79.0 in stage 5.0 (TID 742) in 5921 ms on localhost (80/200)
15/08/21 11:19:12 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:12 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:14 INFO Executor: Finished task 81.0 in stage 5.0 (TID 744). 1219 bytes result sent to driver
15/08/21 11:19:14 INFO TaskSetManager: Starting task 96.0 in stage 5.0 (TID 759, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:14 INFO Executor: Running task 96.0 in stage 5.0 (TID 759)
15/08/21 11:19:14 INFO TaskSetManager: Finished task 81.0 in stage 5.0 (TID 744) in 5989 ms on localhost (81/200)
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:14 INFO Executor: Finished task 80.0 in stage 5.0 (TID 743). 1219 bytes result sent to driver
15/08/21 11:19:14 INFO TaskSetManager: Starting task 97.0 in stage 5.0 (TID 760, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:14 INFO Executor: Running task 97.0 in stage 5.0 (TID 760)
15/08/21 11:19:14 INFO TaskSetManager: Finished task 80.0 in stage 5.0 (TID 743) in 6298 ms on localhost (82/200)
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:16 INFO Executor: Finished task 82.0 in stage 5.0 (TID 745). 1219 bytes result sent to driver
15/08/21 11:19:16 INFO TaskSetManager: Starting task 98.0 in stage 5.0 (TID 761, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:16 INFO TaskSetManager: Finished task 82.0 in stage 5.0 (TID 745) in 7341 ms on localhost (83/200)
15/08/21 11:19:16 INFO Executor: Running task 98.0 in stage 5.0 (TID 761)
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:16 INFO Executor: Finished task 91.0 in stage 5.0 (TID 754). 1219 bytes result sent to driver
15/08/21 11:19:16 INFO TaskSetManager: Starting task 99.0 in stage 5.0 (TID 762, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:16 INFO Executor: Running task 99.0 in stage 5.0 (TID 762)
15/08/21 11:19:16 INFO TaskSetManager: Finished task 91.0 in stage 5.0 (TID 754) in 7152 ms on localhost (84/200)
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO Executor: Finished task 86.0 in stage 5.0 (TID 749). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Starting task 100.0 in stage 5.0 (TID 763, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 100.0 in stage 5.0 (TID 763)
15/08/21 11:19:17 INFO TaskSetManager: Finished task 86.0 in stage 5.0 (TID 749) in 7598 ms on localhost (85/200)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO Executor: Finished task 89.0 in stage 5.0 (TID 752). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Starting task 101.0 in stage 5.0 (TID 764, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 101.0 in stage 5.0 (TID 764)
15/08/21 11:19:17 INFO TaskSetManager: Finished task 89.0 in stage 5.0 (TID 752) in 7609 ms on localhost (86/200)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO Executor: Finished task 85.0 in stage 5.0 (TID 748). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Starting task 102.0 in stage 5.0 (TID 765, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 102.0 in stage 5.0 (TID 765)
15/08/21 11:19:17 INFO TaskSetManager: Finished task 85.0 in stage 5.0 (TID 748) in 7775 ms on localhost (87/200)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:17 INFO Executor: Finished task 92.0 in stage 5.0 (TID 755). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Starting task 103.0 in stage 5.0 (TID 766, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 103.0 in stage 5.0 (TID 766)
15/08/21 11:19:17 INFO TaskSetManager: Finished task 92.0 in stage 5.0 (TID 755) in 7667 ms on localhost (88/200)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO Executor: Finished task 83.0 in stage 5.0 (TID 746). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO TaskSetManager: Starting task 104.0 in stage 5.0 (TID 767, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 104.0 in stage 5.0 (TID 767)
15/08/21 11:19:17 INFO Executor: Finished task 84.0 in stage 5.0 (TID 747). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Finished task 83.0 in stage 5.0 (TID 746) in 7908 ms on localhost (89/200)
15/08/21 11:19:17 INFO TaskSetManager: Starting task 105.0 in stage 5.0 (TID 768, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 105.0 in stage 5.0 (TID 768)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO TaskSetManager: Finished task 84.0 in stage 5.0 (TID 747) in 7848 ms on localhost (90/200)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO Executor: Finished task 87.0 in stage 5.0 (TID 750). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Starting task 106.0 in stage 5.0 (TID 769, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 106.0 in stage 5.0 (TID 769)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO TaskSetManager: Finished task 87.0 in stage 5.0 (TID 750) in 7767 ms on localhost (91/200)
15/08/21 11:19:17 INFO Executor: Finished task 88.0 in stage 5.0 (TID 751). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO Executor: Finished task 90.0 in stage 5.0 (TID 753). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Starting task 107.0 in stage 5.0 (TID 770, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 107.0 in stage 5.0 (TID 770)
15/08/21 11:19:17 INFO TaskSetManager: Starting task 108.0 in stage 5.0 (TID 771, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO TaskSetManager: Finished task 88.0 in stage 5.0 (TID 751) in 7802 ms on localhost (92/200)
15/08/21 11:19:17 INFO TaskSetManager: Finished task 90.0 in stage 5.0 (TID 753) in 7781 ms on localhost (93/200)
15/08/21 11:19:17 INFO Executor: Running task 108.0 in stage 5.0 (TID 771)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:17 INFO Executor: Finished task 93.0 in stage 5.0 (TID 756). 1219 bytes result sent to driver
15/08/21 11:19:17 INFO TaskSetManager: Starting task 109.0 in stage 5.0 (TID 772, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:17 INFO Executor: Running task 109.0 in stage 5.0 (TID 772)
15/08/21 11:19:17 INFO TaskSetManager: Finished task 93.0 in stage 5.0 (TID 756) in 6393 ms on localhost (94/200)
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:18 INFO Executor: Finished task 94.0 in stage 5.0 (TID 757). 1219 bytes result sent to driver
15/08/21 11:19:18 INFO TaskSetManager: Starting task 110.0 in stage 5.0 (TID 773, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:18 INFO Executor: Running task 110.0 in stage 5.0 (TID 773)
15/08/21 11:19:18 INFO TaskSetManager: Finished task 94.0 in stage 5.0 (TID 757) in 6474 ms on localhost (95/200)
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:18 INFO Executor: Finished task 95.0 in stage 5.0 (TID 758). 1219 bytes result sent to driver
15/08/21 11:19:18 INFO TaskSetManager: Starting task 111.0 in stage 5.0 (TID 774, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:18 INFO Executor: Running task 111.0 in stage 5.0 (TID 774)
15/08/21 11:19:18 INFO TaskSetManager: Finished task 95.0 in stage 5.0 (TID 758) in 5988 ms on localhost (96/200)
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:20 INFO Executor: Finished task 96.0 in stage 5.0 (TID 759). 1219 bytes result sent to driver
15/08/21 11:19:20 INFO TaskSetManager: Starting task 112.0 in stage 5.0 (TID 775, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:20 INFO Executor: Running task 112.0 in stage 5.0 (TID 775)
15/08/21 11:19:20 INFO TaskSetManager: Finished task 96.0 in stage 5.0 (TID 759) in 6065 ms on localhost (97/200)
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:20 INFO Executor: Finished task 97.0 in stage 5.0 (TID 760). 1219 bytes result sent to driver
15/08/21 11:19:20 INFO TaskSetManager: Starting task 113.0 in stage 5.0 (TID 776, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:20 INFO TaskSetManager: Finished task 97.0 in stage 5.0 (TID 760) in 5934 ms on localhost (98/200)
15/08/21 11:19:20 INFO Executor: Running task 113.0 in stage 5.0 (TID 776)
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:21 INFO Executor: Finished task 98.0 in stage 5.0 (TID 761). 1219 bytes result sent to driver
15/08/21 11:19:21 INFO TaskSetManager: Starting task 114.0 in stage 5.0 (TID 777, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:21 INFO Executor: Running task 114.0 in stage 5.0 (TID 777)
15/08/21 11:19:21 INFO TaskSetManager: Finished task 98.0 in stage 5.0 (TID 761) in 5709 ms on localhost (99/200)
15/08/21 11:19:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:23 INFO Executor: Finished task 103.0 in stage 5.0 (TID 766). 1219 bytes result sent to driver
15/08/21 11:19:23 INFO TaskSetManager: Starting task 115.0 in stage 5.0 (TID 778, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:23 INFO Executor: Running task 115.0 in stage 5.0 (TID 778)
15/08/21 11:19:23 INFO TaskSetManager: Finished task 103.0 in stage 5.0 (TID 766) in 5980 ms on localhost (100/200)
15/08/21 11:19:23 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:23 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:24 INFO Executor: Finished task 99.0 in stage 5.0 (TID 762). 1219 bytes result sent to driver
15/08/21 11:19:24 INFO TaskSetManager: Starting task 116.0 in stage 5.0 (TID 779, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:24 INFO Executor: Running task 116.0 in stage 5.0 (TID 779)
15/08/21 11:19:24 INFO TaskSetManager: Finished task 99.0 in stage 5.0 (TID 762) in 7788 ms on localhost (101/200)
15/08/21 11:19:24 INFO Executor: Finished task 106.0 in stage 5.0 (TID 769). 1219 bytes result sent to driver
15/08/21 11:19:24 INFO TaskSetManager: Starting task 117.0 in stage 5.0 (TID 780, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:24 INFO Executor: Running task 117.0 in stage 5.0 (TID 780)
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:24 INFO TaskSetManager: Finished task 106.0 in stage 5.0 (TID 769) in 7247 ms on localhost (102/200)
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:24 INFO Executor: Finished task 108.0 in stage 5.0 (TID 771). 1219 bytes result sent to driver
15/08/21 11:19:24 INFO TaskSetManager: Starting task 118.0 in stage 5.0 (TID 781, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:24 INFO Executor: Running task 118.0 in stage 5.0 (TID 781)
15/08/21 11:19:24 INFO TaskSetManager: Finished task 108.0 in stage 5.0 (TID 771) in 7417 ms on localhost (103/200)
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:24 INFO Executor: Finished task 105.0 in stage 5.0 (TID 768). 1219 bytes result sent to driver
15/08/21 11:19:24 INFO TaskSetManager: Starting task 119.0 in stage 5.0 (TID 782, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:24 INFO Executor: Running task 119.0 in stage 5.0 (TID 782)
15/08/21 11:19:24 INFO TaskSetManager: Finished task 105.0 in stage 5.0 (TID 768) in 7667 ms on localhost (104/200)
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO Executor: Finished task 102.0 in stage 5.0 (TID 765). 1219 bytes result sent to driver
15/08/21 11:19:25 INFO TaskSetManager: Starting task 120.0 in stage 5.0 (TID 783, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:25 INFO Executor: Running task 120.0 in stage 5.0 (TID 783)
15/08/21 11:19:25 INFO TaskSetManager: Finished task 102.0 in stage 5.0 (TID 765) in 7785 ms on localhost (105/200)
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO Executor: Finished task 101.0 in stage 5.0 (TID 764). 1219 bytes result sent to driver
15/08/21 11:19:25 INFO TaskSetManager: Starting task 121.0 in stage 5.0 (TID 784, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:25 INFO Executor: Running task 121.0 in stage 5.0 (TID 784)
15/08/21 11:19:25 INFO TaskSetManager: Finished task 101.0 in stage 5.0 (TID 764) in 7871 ms on localhost (106/200)
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO Executor: Finished task 100.0 in stage 5.0 (TID 763). 1219 bytes result sent to driver
15/08/21 11:19:25 INFO TaskSetManager: Starting task 122.0 in stage 5.0 (TID 785, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:25 INFO Executor: Running task 122.0 in stage 5.0 (TID 785)
15/08/21 11:19:25 INFO TaskSetManager: Finished task 100.0 in stage 5.0 (TID 763) in 8081 ms on localhost (107/200)
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO Executor: Finished task 104.0 in stage 5.0 (TID 767). 1219 bytes result sent to driver
15/08/21 11:19:25 INFO TaskSetManager: Starting task 123.0 in stage 5.0 (TID 786, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:25 INFO Executor: Running task 123.0 in stage 5.0 (TID 786)
15/08/21 11:19:25 INFO TaskSetManager: Finished task 104.0 in stage 5.0 (TID 767) in 8034 ms on localhost (108/200)
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO Executor: Finished task 109.0 in stage 5.0 (TID 772). 1219 bytes result sent to driver
15/08/21 11:19:25 INFO TaskSetManager: Starting task 124.0 in stage 5.0 (TID 787, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:25 INFO Executor: Running task 124.0 in stage 5.0 (TID 787)
15/08/21 11:19:25 INFO TaskSetManager: Finished task 109.0 in stage 5.0 (TID 772) in 7577 ms on localhost (109/200)
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO Executor: Finished task 107.0 in stage 5.0 (TID 770). 1219 bytes result sent to driver
15/08/21 11:19:25 INFO TaskSetManager: Starting task 125.0 in stage 5.0 (TID 788, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:25 INFO Executor: Running task 125.0 in stage 5.0 (TID 788)
15/08/21 11:19:25 INFO TaskSetManager: Finished task 107.0 in stage 5.0 (TID 770) in 7986 ms on localhost (110/200)
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO Executor: Finished task 110.0 in stage 5.0 (TID 773). 1219 bytes result sent to driver
15/08/21 11:19:25 INFO TaskSetManager: Starting task 126.0 in stage 5.0 (TID 789, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:25 INFO Executor: Running task 126.0 in stage 5.0 (TID 789)
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:25 INFO TaskSetManager: Finished task 110.0 in stage 5.0 (TID 773) in 7370 ms on localhost (111/200)
15/08/21 11:19:26 INFO Executor: Finished task 111.0 in stage 5.0 (TID 774). 1219 bytes result sent to driver
15/08/21 11:19:26 INFO TaskSetManager: Starting task 127.0 in stage 5.0 (TID 790, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:26 INFO Executor: Running task 127.0 in stage 5.0 (TID 790)
15/08/21 11:19:26 INFO TaskSetManager: Finished task 111.0 in stage 5.0 (TID 774) in 7974 ms on localhost (112/200)
15/08/21 11:19:26 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:26 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:29 INFO Executor: Finished task 112.0 in stage 5.0 (TID 775). 1219 bytes result sent to driver
15/08/21 11:19:29 INFO TaskSetManager: Starting task 128.0 in stage 5.0 (TID 791, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:29 INFO Executor: Running task 128.0 in stage 5.0 (TID 791)
15/08/21 11:19:29 INFO TaskSetManager: Finished task 112.0 in stage 5.0 (TID 775) in 9361 ms on localhost (113/200)
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:29 INFO Executor: Finished task 113.0 in stage 5.0 (TID 776). 1219 bytes result sent to driver
15/08/21 11:19:29 INFO TaskSetManager: Starting task 129.0 in stage 5.0 (TID 792, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:29 INFO Executor: Running task 129.0 in stage 5.0 (TID 792)
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:29 INFO TaskSetManager: Finished task 113.0 in stage 5.0 (TID 776) in 9294 ms on localhost (114/200)
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:30 INFO Executor: Finished task 114.0 in stage 5.0 (TID 777). 1219 bytes result sent to driver
15/08/21 11:19:30 INFO TaskSetManager: Starting task 130.0 in stage 5.0 (TID 793, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:30 INFO Executor: Running task 130.0 in stage 5.0 (TID 793)
15/08/21 11:19:30 INFO TaskSetManager: Finished task 114.0 in stage 5.0 (TID 777) in 8803 ms on localhost (115/200)
15/08/21 11:19:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:31 INFO Executor: Finished task 115.0 in stage 5.0 (TID 778). 1219 bytes result sent to driver
15/08/21 11:19:31 INFO TaskSetManager: Starting task 131.0 in stage 5.0 (TID 794, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:31 INFO Executor: Running task 131.0 in stage 5.0 (TID 794)
15/08/21 11:19:31 INFO TaskSetManager: Finished task 115.0 in stage 5.0 (TID 778) in 8639 ms on localhost (116/200)
15/08/21 11:19:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:19:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:32 INFO Executor: Finished task 116.0 in stage 5.0 (TID 779). 1219 bytes result sent to driver
15/08/21 11:19:32 INFO TaskSetManager: Starting task 132.0 in stage 5.0 (TID 795, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:32 INFO Executor: Running task 132.0 in stage 5.0 (TID 795)
15/08/21 11:19:32 INFO TaskSetManager: Finished task 116.0 in stage 5.0 (TID 779) in 7837 ms on localhost (117/200)
15/08/21 11:19:32 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:32 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:35 INFO Executor: Finished task 117.0 in stage 5.0 (TID 780). 1219 bytes result sent to driver
15/08/21 11:19:35 INFO TaskSetManager: Starting task 133.0 in stage 5.0 (TID 796, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:35 INFO Executor: Running task 133.0 in stage 5.0 (TID 796)
15/08/21 11:19:35 INFO TaskSetManager: Finished task 117.0 in stage 5.0 (TID 780) in 11360 ms on localhost (118/200)
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:35 INFO Executor: Finished task 122.0 in stage 5.0 (TID 785). 1219 bytes result sent to driver
15/08/21 11:19:35 INFO TaskSetManager: Starting task 134.0 in stage 5.0 (TID 797, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:35 INFO Executor: Running task 134.0 in stage 5.0 (TID 797)
15/08/21 11:19:35 INFO TaskSetManager: Finished task 122.0 in stage 5.0 (TID 785) in 10742 ms on localhost (119/200)
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:36 INFO Executor: Finished task 123.0 in stage 5.0 (TID 786). 1219 bytes result sent to driver
15/08/21 11:19:36 INFO TaskSetManager: Starting task 135.0 in stage 5.0 (TID 798, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:36 INFO Executor: Running task 135.0 in stage 5.0 (TID 798)
15/08/21 11:19:36 INFO TaskSetManager: Finished task 123.0 in stage 5.0 (TID 786) in 10689 ms on localhost (120/200)
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:36 INFO Executor: Finished task 119.0 in stage 5.0 (TID 782). 1219 bytes result sent to driver
15/08/21 11:19:36 INFO TaskSetManager: Starting task 136.0 in stage 5.0 (TID 799, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:36 INFO TaskSetManager: Finished task 119.0 in stage 5.0 (TID 782) in 11181 ms on localhost (121/200)
15/08/21 11:19:36 INFO Executor: Running task 136.0 in stage 5.0 (TID 799)
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:36 INFO Executor: Finished task 118.0 in stage 5.0 (TID 781). 1219 bytes result sent to driver
15/08/21 11:19:36 INFO TaskSetManager: Starting task 137.0 in stage 5.0 (TID 800, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:36 INFO Executor: Running task 137.0 in stage 5.0 (TID 800)
15/08/21 11:19:36 INFO TaskSetManager: Finished task 118.0 in stage 5.0 (TID 781) in 11555 ms on localhost (122/200)
15/08/21 11:19:36 INFO Executor: Finished task 121.0 in stage 5.0 (TID 784). 1219 bytes result sent to driver
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:36 INFO TaskSetManager: Starting task 138.0 in stage 5.0 (TID 801, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:36 INFO Executor: Running task 138.0 in stage 5.0 (TID 801)
15/08/21 11:19:36 INFO TaskSetManager: Finished task 121.0 in stage 5.0 (TID 784) in 11288 ms on localhost (123/200)
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:19:36 INFO Executor: Finished task 120.0 in stage 5.0 (TID 783). 1219 bytes result sent to driver
15/08/21 11:19:36 INFO TaskSetManager: Starting task 139.0 in stage 5.0 (TID 802, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:36 INFO TaskSetManager: Finished task 120.0 in stage 5.0 (TID 783) in 11359 ms on localhost (124/200)
15/08/21 11:19:36 INFO Executor: Running task 139.0 in stage 5.0 (TID 802)
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:36 INFO Executor: Finished task 125.0 in stage 5.0 (TID 788). 1219 bytes result sent to driver
15/08/21 11:19:36 INFO TaskSetManager: Starting task 140.0 in stage 5.0 (TID 803, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:36 INFO Executor: Running task 140.0 in stage 5.0 (TID 803)
15/08/21 11:19:36 INFO TaskSetManager: Finished task 125.0 in stage 5.0 (TID 788) in 11105 ms on localhost (125/200)
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:37 INFO Executor: Finished task 124.0 in stage 5.0 (TID 787). 1219 bytes result sent to driver
15/08/21 11:19:37 INFO TaskSetManager: Starting task 141.0 in stage 5.0 (TID 804, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:37 INFO TaskSetManager: Finished task 124.0 in stage 5.0 (TID 787) in 11835 ms on localhost (126/200)
15/08/21 11:19:37 INFO Executor: Running task 141.0 in stage 5.0 (TID 804)
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:37 INFO Executor: Finished task 126.0 in stage 5.0 (TID 789). 1219 bytes result sent to driver
15/08/21 11:19:37 INFO TaskSetManager: Starting task 142.0 in stage 5.0 (TID 805, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:37 INFO TaskSetManager: Finished task 126.0 in stage 5.0 (TID 789) in 11807 ms on localhost (127/200)
15/08/21 11:19:37 INFO Executor: Running task 142.0 in stage 5.0 (TID 805)
15/08/21 11:19:37 INFO Executor: Finished task 127.0 in stage 5.0 (TID 790). 1219 bytes result sent to driver
15/08/21 11:19:37 INFO TaskSetManager: Starting task 143.0 in stage 5.0 (TID 806, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:37 INFO Executor: Running task 143.0 in stage 5.0 (TID 806)
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:37 INFO TaskSetManager: Finished task 127.0 in stage 5.0 (TID 790) in 11031 ms on localhost (128/200)
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:39 INFO Executor: Finished task 128.0 in stage 5.0 (TID 791). 1219 bytes result sent to driver
15/08/21 11:19:39 INFO Executor: Finished task 129.0 in stage 5.0 (TID 792). 1219 bytes result sent to driver
15/08/21 11:19:39 INFO TaskSetManager: Starting task 144.0 in stage 5.0 (TID 807, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:39 INFO Executor: Running task 144.0 in stage 5.0 (TID 807)
15/08/21 11:19:39 INFO TaskSetManager: Starting task 145.0 in stage 5.0 (TID 808, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:39 INFO TaskSetManager: Finished task 128.0 in stage 5.0 (TID 791) in 9816 ms on localhost (129/200)
15/08/21 11:19:39 INFO Executor: Running task 145.0 in stage 5.0 (TID 808)
15/08/21 11:19:39 INFO TaskSetManager: Finished task 129.0 in stage 5.0 (TID 792) in 9747 ms on localhost (130/200)
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:40 INFO Executor: Finished task 130.0 in stage 5.0 (TID 793). 1219 bytes result sent to driver
15/08/21 11:19:40 INFO TaskSetManager: Starting task 146.0 in stage 5.0 (TID 809, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:40 INFO TaskSetManager: Finished task 130.0 in stage 5.0 (TID 793) in 9618 ms on localhost (131/200)
15/08/21 11:19:40 INFO Executor: Running task 146.0 in stage 5.0 (TID 809)
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:40 INFO Executor: Finished task 131.0 in stage 5.0 (TID 794). 1219 bytes result sent to driver
15/08/21 11:19:40 INFO TaskSetManager: Starting task 147.0 in stage 5.0 (TID 810, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:40 INFO TaskSetManager: Finished task 131.0 in stage 5.0 (TID 794) in 8335 ms on localhost (132/200)
15/08/21 11:19:40 INFO Executor: Running task 147.0 in stage 5.0 (TID 810)
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:41 INFO Executor: Finished task 132.0 in stage 5.0 (TID 795). 1219 bytes result sent to driver
15/08/21 11:19:41 INFO TaskSetManager: Starting task 148.0 in stage 5.0 (TID 811, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:41 INFO Executor: Running task 148.0 in stage 5.0 (TID 811)
15/08/21 11:19:41 INFO TaskSetManager: Finished task 132.0 in stage 5.0 (TID 795) in 9001 ms on localhost (133/200)
15/08/21 11:19:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:44 INFO Executor: Finished task 134.0 in stage 5.0 (TID 797). 1219 bytes result sent to driver
15/08/21 11:19:44 INFO TaskSetManager: Starting task 149.0 in stage 5.0 (TID 812, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:44 INFO Executor: Running task 149.0 in stage 5.0 (TID 812)
15/08/21 11:19:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:19:44 INFO TaskSetManager: Finished task 134.0 in stage 5.0 (TID 797) in 8677 ms on localhost (134/200)
15/08/21 11:19:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:44 INFO Executor: Finished task 133.0 in stage 5.0 (TID 796). 1219 bytes result sent to driver
15/08/21 11:19:44 INFO TaskSetManager: Starting task 150.0 in stage 5.0 (TID 813, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:44 INFO Executor: Running task 150.0 in stage 5.0 (TID 813)
15/08/21 11:19:44 INFO TaskSetManager: Finished task 133.0 in stage 5.0 (TID 796) in 9067 ms on localhost (135/200)
15/08/21 11:19:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:45 INFO Executor: Finished task 135.0 in stage 5.0 (TID 798). 1219 bytes result sent to driver
15/08/21 11:19:45 INFO TaskSetManager: Starting task 151.0 in stage 5.0 (TID 814, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:45 INFO Executor: Running task 151.0 in stage 5.0 (TID 814)
15/08/21 11:19:45 INFO TaskSetManager: Finished task 135.0 in stage 5.0 (TID 798) in 9214 ms on localhost (136/200)
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:45 INFO Executor: Finished task 136.0 in stage 5.0 (TID 799). 1219 bytes result sent to driver
15/08/21 11:19:45 INFO TaskSetManager: Starting task 152.0 in stage 5.0 (TID 815, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:45 INFO Executor: Running task 152.0 in stage 5.0 (TID 815)
15/08/21 11:19:45 INFO TaskSetManager: Finished task 136.0 in stage 5.0 (TID 799) in 9690 ms on localhost (137/200)
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:45 INFO Executor: Finished task 137.0 in stage 5.0 (TID 800). 1219 bytes result sent to driver
15/08/21 11:19:45 INFO TaskSetManager: Starting task 153.0 in stage 5.0 (TID 816, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:45 INFO Executor: Running task 153.0 in stage 5.0 (TID 816)
15/08/21 11:19:45 INFO TaskSetManager: Finished task 137.0 in stage 5.0 (TID 800) in 9646 ms on localhost (138/200)
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:46 INFO Executor: Finished task 139.0 in stage 5.0 (TID 802). 1219 bytes result sent to driver
15/08/21 11:19:46 INFO TaskSetManager: Starting task 154.0 in stage 5.0 (TID 817, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:46 INFO Executor: Running task 154.0 in stage 5.0 (TID 817)
15/08/21 11:19:46 INFO TaskSetManager: Finished task 139.0 in stage 5.0 (TID 802) in 10433 ms on localhost (139/200)
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:46 INFO Executor: Finished task 141.0 in stage 5.0 (TID 804). 1219 bytes result sent to driver
15/08/21 11:19:46 INFO TaskSetManager: Starting task 155.0 in stage 5.0 (TID 818, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:46 INFO Executor: Running task 155.0 in stage 5.0 (TID 818)
15/08/21 11:19:46 INFO TaskSetManager: Finished task 141.0 in stage 5.0 (TID 804) in 9714 ms on localhost (140/200)
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:47 INFO Executor: Finished task 138.0 in stage 5.0 (TID 801). 1219 bytes result sent to driver
15/08/21 11:19:47 INFO TaskSetManager: Starting task 156.0 in stage 5.0 (TID 819, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:47 INFO Executor: Running task 156.0 in stage 5.0 (TID 819)
15/08/21 11:19:47 INFO TaskSetManager: Finished task 138.0 in stage 5.0 (TID 801) in 10668 ms on localhost (141/200)
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:47 INFO Executor: Finished task 140.0 in stage 5.0 (TID 803). 1219 bytes result sent to driver
15/08/21 11:19:47 INFO TaskSetManager: Starting task 157.0 in stage 5.0 (TID 820, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:47 INFO Executor: Running task 157.0 in stage 5.0 (TID 820)
15/08/21 11:19:47 INFO TaskSetManager: Finished task 140.0 in stage 5.0 (TID 803) in 10907 ms on localhost (142/200)
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:47 INFO Executor: Finished task 142.0 in stage 5.0 (TID 805). 1219 bytes result sent to driver
15/08/21 11:19:47 INFO TaskSetManager: Starting task 158.0 in stage 5.0 (TID 821, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:47 INFO Executor: Running task 158.0 in stage 5.0 (TID 821)
15/08/21 11:19:47 INFO TaskSetManager: Finished task 142.0 in stage 5.0 (TID 805) in 10189 ms on localhost (143/200)
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:47 INFO Executor: Finished task 143.0 in stage 5.0 (TID 806). 1219 bytes result sent to driver
15/08/21 11:19:47 INFO TaskSetManager: Starting task 159.0 in stage 5.0 (TID 822, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:47 INFO Executor: Running task 159.0 in stage 5.0 (TID 822)
15/08/21 11:19:47 INFO TaskSetManager: Finished task 143.0 in stage 5.0 (TID 806) in 10211 ms on localhost (144/200)
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:48 INFO Executor: Finished task 144.0 in stage 5.0 (TID 807). 1219 bytes result sent to driver
15/08/21 11:19:48 INFO TaskSetManager: Starting task 160.0 in stage 5.0 (TID 823, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:48 INFO TaskSetManager: Finished task 144.0 in stage 5.0 (TID 807) in 8738 ms on localhost (145/200)
15/08/21 11:19:48 INFO Executor: Running task 160.0 in stage 5.0 (TID 823)
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:48 INFO Executor: Finished task 145.0 in stage 5.0 (TID 808). 1219 bytes result sent to driver
15/08/21 11:19:48 INFO TaskSetManager: Starting task 161.0 in stage 5.0 (TID 824, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:48 INFO Executor: Running task 161.0 in stage 5.0 (TID 824)
15/08/21 11:19:48 INFO TaskSetManager: Finished task 145.0 in stage 5.0 (TID 808) in 9353 ms on localhost (146/200)
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:48 INFO Executor: Finished task 146.0 in stage 5.0 (TID 809). 1219 bytes result sent to driver
15/08/21 11:19:48 INFO TaskSetManager: Starting task 162.0 in stage 5.0 (TID 825, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:48 INFO Executor: Running task 162.0 in stage 5.0 (TID 825)
15/08/21 11:19:48 INFO TaskSetManager: Finished task 146.0 in stage 5.0 (TID 809) in 8831 ms on localhost (147/200)
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:50 INFO Executor: Finished task 147.0 in stage 5.0 (TID 810). 1219 bytes result sent to driver
15/08/21 11:19:50 INFO TaskSetManager: Starting task 163.0 in stage 5.0 (TID 826, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:50 INFO Executor: Running task 163.0 in stage 5.0 (TID 826)
15/08/21 11:19:50 INFO TaskSetManager: Finished task 147.0 in stage 5.0 (TID 810) in 10037 ms on localhost (148/200)
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:50 INFO Executor: Finished task 148.0 in stage 5.0 (TID 811). 1219 bytes result sent to driver
15/08/21 11:19:50 INFO TaskSetManager: Starting task 164.0 in stage 5.0 (TID 827, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:50 INFO Executor: Running task 164.0 in stage 5.0 (TID 827)
15/08/21 11:19:50 INFO TaskSetManager: Finished task 148.0 in stage 5.0 (TID 811) in 8929 ms on localhost (149/200)
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:51 INFO Executor: Finished task 150.0 in stage 5.0 (TID 813). 1219 bytes result sent to driver
15/08/21 11:19:51 INFO TaskSetManager: Starting task 165.0 in stage 5.0 (TID 828, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:51 INFO Executor: Running task 165.0 in stage 5.0 (TID 828)
15/08/21 11:19:51 INFO TaskSetManager: Finished task 150.0 in stage 5.0 (TID 813) in 7010 ms on localhost (150/200)
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:52 INFO Executor: Finished task 149.0 in stage 5.0 (TID 812). 1219 bytes result sent to driver
15/08/21 11:19:52 INFO TaskSetManager: Starting task 166.0 in stage 5.0 (TID 829, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:52 INFO Executor: Running task 166.0 in stage 5.0 (TID 829)
15/08/21 11:19:52 INFO TaskSetManager: Finished task 149.0 in stage 5.0 (TID 812) in 7800 ms on localhost (151/200)
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:53 INFO Executor: Finished task 151.0 in stage 5.0 (TID 814). 1219 bytes result sent to driver
15/08/21 11:19:53 INFO Executor: Finished task 152.0 in stage 5.0 (TID 815). 1219 bytes result sent to driver
15/08/21 11:19:53 INFO TaskSetManager: Starting task 167.0 in stage 5.0 (TID 830, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:53 INFO Executor: Running task 167.0 in stage 5.0 (TID 830)
15/08/21 11:19:53 INFO TaskSetManager: Starting task 168.0 in stage 5.0 (TID 831, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:53 INFO Executor: Running task 168.0 in stage 5.0 (TID 831)
15/08/21 11:19:53 INFO TaskSetManager: Finished task 151.0 in stage 5.0 (TID 814) in 8536 ms on localhost (152/200)
15/08/21 11:19:53 INFO TaskSetManager: Finished task 152.0 in stage 5.0 (TID 815) in 7931 ms on localhost (153/200)
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 11:19:54 INFO Executor: Finished task 153.0 in stage 5.0 (TID 816). 1219 bytes result sent to driver
15/08/21 11:19:54 INFO TaskSetManager: Starting task 169.0 in stage 5.0 (TID 832, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:54 INFO Executor: Running task 169.0 in stage 5.0 (TID 832)
15/08/21 11:19:54 INFO TaskSetManager: Finished task 153.0 in stage 5.0 (TID 816) in 8887 ms on localhost (154/200)
15/08/21 11:19:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:55 INFO Executor: Finished task 154.0 in stage 5.0 (TID 817). 1219 bytes result sent to driver
15/08/21 11:19:55 INFO TaskSetManager: Starting task 170.0 in stage 5.0 (TID 833, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:55 INFO Executor: Running task 170.0 in stage 5.0 (TID 833)
15/08/21 11:19:55 INFO TaskSetManager: Finished task 154.0 in stage 5.0 (TID 817) in 8333 ms on localhost (155/200)
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:55 INFO Executor: Finished task 156.0 in stage 5.0 (TID 819). 1219 bytes result sent to driver
15/08/21 11:19:55 INFO TaskSetManager: Starting task 171.0 in stage 5.0 (TID 834, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:55 INFO Executor: Running task 171.0 in stage 5.0 (TID 834)
15/08/21 11:19:55 INFO TaskSetManager: Finished task 156.0 in stage 5.0 (TID 819) in 8569 ms on localhost (156/200)
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:55 INFO Executor: Finished task 155.0 in stage 5.0 (TID 818). 1219 bytes result sent to driver
15/08/21 11:19:55 INFO TaskSetManager: Starting task 172.0 in stage 5.0 (TID 835, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:55 INFO TaskSetManager: Finished task 155.0 in stage 5.0 (TID 818) in 8796 ms on localhost (157/200)
15/08/21 11:19:55 INFO Executor: Running task 172.0 in stage 5.0 (TID 835)
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:56 INFO Executor: Finished task 157.0 in stage 5.0 (TID 820). 1219 bytes result sent to driver
15/08/21 11:19:56 INFO TaskSetManager: Starting task 173.0 in stage 5.0 (TID 836, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:56 INFO Executor: Running task 173.0 in stage 5.0 (TID 836)
15/08/21 11:19:56 INFO TaskSetManager: Finished task 157.0 in stage 5.0 (TID 820) in 9145 ms on localhost (158/200)
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:56 INFO Executor: Finished task 160.0 in stage 5.0 (TID 823). 1219 bytes result sent to driver
15/08/21 11:19:56 INFO TaskSetManager: Starting task 174.0 in stage 5.0 (TID 837, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:56 INFO TaskSetManager: Finished task 160.0 in stage 5.0 (TID 823) in 8572 ms on localhost (159/200)
15/08/21 11:19:56 INFO Executor: Running task 174.0 in stage 5.0 (TID 837)
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:56 INFO Executor: Finished task 159.0 in stage 5.0 (TID 822). 1219 bytes result sent to driver
15/08/21 11:19:56 INFO TaskSetManager: Starting task 175.0 in stage 5.0 (TID 838, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:56 INFO Executor: Running task 175.0 in stage 5.0 (TID 838)
15/08/21 11:19:56 INFO TaskSetManager: Finished task 159.0 in stage 5.0 (TID 822) in 9211 ms on localhost (160/200)
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:56 INFO Executor: Finished task 161.0 in stage 5.0 (TID 824). 1219 bytes result sent to driver
15/08/21 11:19:56 INFO TaskSetManager: Starting task 176.0 in stage 5.0 (TID 839, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:56 INFO Executor: Running task 176.0 in stage 5.0 (TID 839)
15/08/21 11:19:56 INFO TaskSetManager: Finished task 161.0 in stage 5.0 (TID 824) in 8080 ms on localhost (161/200)
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:56 INFO Executor: Finished task 158.0 in stage 5.0 (TID 821). 1219 bytes result sent to driver
15/08/21 11:19:56 INFO TaskSetManager: Starting task 177.0 in stage 5.0 (TID 840, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:56 INFO TaskSetManager: Finished task 158.0 in stage 5.0 (TID 821) in 9343 ms on localhost (162/200)
15/08/21 11:19:56 INFO Executor: Running task 177.0 in stage 5.0 (TID 840)
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:57 INFO Executor: Finished task 162.0 in stage 5.0 (TID 825). 1219 bytes result sent to driver
15/08/21 11:19:57 INFO TaskSetManager: Starting task 178.0 in stage 5.0 (TID 841, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:57 INFO Executor: Running task 178.0 in stage 5.0 (TID 841)
15/08/21 11:19:57 INFO TaskSetManager: Finished task 162.0 in stage 5.0 (TID 825) in 8509 ms on localhost (163/200)
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:57 INFO Executor: Finished task 163.0 in stage 5.0 (TID 826). 1219 bytes result sent to driver
15/08/21 11:19:57 INFO TaskSetManager: Starting task 179.0 in stage 5.0 (TID 842, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:57 INFO Executor: Running task 179.0 in stage 5.0 (TID 842)
15/08/21 11:19:57 INFO TaskSetManager: Finished task 163.0 in stage 5.0 (TID 826) in 7587 ms on localhost (164/200)
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:58 INFO Executor: Finished task 164.0 in stage 5.0 (TID 827). 1219 bytes result sent to driver
15/08/21 11:19:58 INFO TaskSetManager: Starting task 180.0 in stage 5.0 (TID 843, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:19:58 INFO TaskSetManager: Finished task 164.0 in stage 5.0 (TID 827) in 8092 ms on localhost (165/200)
15/08/21 11:19:58 INFO Executor: Running task 180.0 in stage 5.0 (TID 843)
15/08/21 11:19:58 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:19:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:19:58 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:19:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:01 INFO Executor: Finished task 166.0 in stage 5.0 (TID 829). 1219 bytes result sent to driver
15/08/21 11:20:01 INFO TaskSetManager: Starting task 181.0 in stage 5.0 (TID 844, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:01 INFO Executor: Running task 181.0 in stage 5.0 (TID 844)
15/08/21 11:20:01 INFO TaskSetManager: Finished task 166.0 in stage 5.0 (TID 829) in 8863 ms on localhost (166/200)
15/08/21 11:20:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:02 INFO Executor: Finished task 165.0 in stage 5.0 (TID 828). 1219 bytes result sent to driver
15/08/21 11:20:02 INFO TaskSetManager: Starting task 182.0 in stage 5.0 (TID 845, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:02 INFO Executor: Running task 182.0 in stage 5.0 (TID 845)
15/08/21 11:20:02 INFO TaskSetManager: Finished task 165.0 in stage 5.0 (TID 828) in 10082 ms on localhost (167/200)
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:20:02 INFO Executor: Finished task 169.0 in stage 5.0 (TID 832). 1219 bytes result sent to driver
15/08/21 11:20:02 INFO TaskSetManager: Starting task 183.0 in stage 5.0 (TID 846, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:02 INFO Executor: Running task 183.0 in stage 5.0 (TID 846)
15/08/21 11:20:02 INFO TaskSetManager: Finished task 169.0 in stage 5.0 (TID 832) in 7724 ms on localhost (168/200)
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:02 INFO Executor: Finished task 167.0 in stage 5.0 (TID 830). 1219 bytes result sent to driver
15/08/21 11:20:02 INFO TaskSetManager: Starting task 184.0 in stage 5.0 (TID 847, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:02 INFO Executor: Running task 184.0 in stage 5.0 (TID 847)
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:02 INFO TaskSetManager: Finished task 167.0 in stage 5.0 (TID 830) in 8875 ms on localhost (169/200)
15/08/21 11:20:02 INFO Executor: Finished task 171.0 in stage 5.0 (TID 834). 1219 bytes result sent to driver
15/08/21 11:20:02 INFO TaskSetManager: Starting task 185.0 in stage 5.0 (TID 848, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:02 INFO Executor: Running task 185.0 in stage 5.0 (TID 848)
15/08/21 11:20:02 INFO TaskSetManager: Finished task 171.0 in stage 5.0 (TID 834) in 7313 ms on localhost (170/200)
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:03 INFO Executor: Finished task 168.0 in stage 5.0 (TID 831). 1219 bytes result sent to driver
15/08/21 11:20:03 INFO TaskSetManager: Starting task 186.0 in stage 5.0 (TID 849, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:03 INFO Executor: Running task 186.0 in stage 5.0 (TID 849)
15/08/21 11:20:03 INFO TaskSetManager: Finished task 168.0 in stage 5.0 (TID 831) in 9455 ms on localhost (171/200)
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:03 INFO Executor: Finished task 170.0 in stage 5.0 (TID 833). 1219 bytes result sent to driver
15/08/21 11:20:03 INFO TaskSetManager: Starting task 187.0 in stage 5.0 (TID 850, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:03 INFO Executor: Finished task 172.0 in stage 5.0 (TID 835). 1219 bytes result sent to driver
15/08/21 11:20:03 INFO Executor: Running task 187.0 in stage 5.0 (TID 850)
15/08/21 11:20:03 INFO TaskSetManager: Starting task 188.0 in stage 5.0 (TID 851, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:03 INFO TaskSetManager: Finished task 170.0 in stage 5.0 (TID 833) in 8152 ms on localhost (172/200)
15/08/21 11:20:03 INFO TaskSetManager: Finished task 172.0 in stage 5.0 (TID 835) in 7619 ms on localhost (173/200)
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:03 INFO Executor: Running task 188.0 in stage 5.0 (TID 851)
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:05 INFO Executor: Finished task 173.0 in stage 5.0 (TID 836). 1219 bytes result sent to driver
15/08/21 11:20:05 INFO TaskSetManager: Starting task 189.0 in stage 5.0 (TID 852, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:05 INFO Executor: Running task 189.0 in stage 5.0 (TID 852)
15/08/21 11:20:05 INFO TaskSetManager: Finished task 173.0 in stage 5.0 (TID 836) in 9285 ms on localhost (174/200)
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:05 INFO Executor: Finished task 179.0 in stage 5.0 (TID 842). 1219 bytes result sent to driver
15/08/21 11:20:05 INFO TaskSetManager: Starting task 190.0 in stage 5.0 (TID 853, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:05 INFO Executor: Running task 190.0 in stage 5.0 (TID 853)
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:06 INFO TaskSetManager: Finished task 179.0 in stage 5.0 (TID 842) in 8166 ms on localhost (175/200)
15/08/21 11:20:06 INFO Executor: Finished task 177.0 in stage 5.0 (TID 840). 1219 bytes result sent to driver
15/08/21 11:20:06 INFO TaskSetManager: Starting task 191.0 in stage 5.0 (TID 854, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:06 INFO Executor: Running task 191.0 in stage 5.0 (TID 854)
15/08/21 11:20:06 INFO TaskSetManager: Finished task 177.0 in stage 5.0 (TID 840) in 9289 ms on localhost (176/200)
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:06 INFO Executor: Finished task 180.0 in stage 5.0 (TID 843). 1219 bytes result sent to driver
15/08/21 11:20:06 INFO TaskSetManager: Starting task 192.0 in stage 5.0 (TID 855, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:06 INFO TaskSetManager: Finished task 180.0 in stage 5.0 (TID 843) in 7782 ms on localhost (177/200)
15/08/21 11:20:06 INFO Executor: Running task 192.0 in stage 5.0 (TID 855)
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:06 INFO Executor: Finished task 174.0 in stage 5.0 (TID 837). 1219 bytes result sent to driver
15/08/21 11:20:06 INFO TaskSetManager: Starting task 193.0 in stage 5.0 (TID 856, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:06 INFO Executor: Running task 193.0 in stage 5.0 (TID 856)
15/08/21 11:20:06 INFO TaskSetManager: Finished task 174.0 in stage 5.0 (TID 837) in 10344 ms on localhost (178/200)
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:06 INFO Executor: Finished task 176.0 in stage 5.0 (TID 839). 1219 bytes result sent to driver
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:20:06 INFO Executor: Finished task 178.0 in stage 5.0 (TID 841). 1219 bytes result sent to driver
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:06 INFO TaskSetManager: Starting task 194.0 in stage 5.0 (TID 857, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:06 INFO TaskSetManager: Starting task 195.0 in stage 5.0 (TID 858, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:06 INFO Executor: Running task 195.0 in stage 5.0 (TID 858)
15/08/21 11:20:06 INFO Executor: Running task 194.0 in stage 5.0 (TID 857)
15/08/21 11:20:06 INFO TaskSetManager: Finished task 178.0 in stage 5.0 (TID 841) in 9469 ms on localhost (179/200)
15/08/21 11:20:06 INFO TaskSetManager: Finished task 176.0 in stage 5.0 (TID 839) in 10228 ms on localhost (180/200)
15/08/21 11:20:06 INFO Executor: Finished task 175.0 in stage 5.0 (TID 838). 1219 bytes result sent to driver
15/08/21 11:20:06 INFO TaskSetManager: Starting task 196.0 in stage 5.0 (TID 859, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:06 INFO TaskSetManager: Finished task 175.0 in stage 5.0 (TID 838) in 10318 ms on localhost (181/200)
15/08/21 11:20:06 INFO Executor: Running task 196.0 in stage 5.0 (TID 859)
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:09 INFO Executor: Finished task 182.0 in stage 5.0 (TID 845). 1219 bytes result sent to driver
15/08/21 11:20:09 INFO TaskSetManager: Starting task 197.0 in stage 5.0 (TID 860, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:09 INFO Executor: Running task 197.0 in stage 5.0 (TID 860)
15/08/21 11:20:09 INFO TaskSetManager: Finished task 182.0 in stage 5.0 (TID 845) in 7597 ms on localhost (182/200)
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:09 INFO Executor: Finished task 181.0 in stage 5.0 (TID 844). 1219 bytes result sent to driver
15/08/21 11:20:09 INFO TaskSetManager: Starting task 198.0 in stage 5.0 (TID 861, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:09 INFO Executor: Running task 198.0 in stage 5.0 (TID 861)
15/08/21 11:20:09 INFO TaskSetManager: Finished task 181.0 in stage 5.0 (TID 844) in 8600 ms on localhost (183/200)
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:10 INFO Executor: Finished task 185.0 in stage 5.0 (TID 848). 1219 bytes result sent to driver
15/08/21 11:20:10 INFO TaskSetManager: Starting task 199.0 in stage 5.0 (TID 862, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 11:20:10 INFO Executor: Running task 199.0 in stage 5.0 (TID 862)
15/08/21 11:20:10 INFO TaskSetManager: Finished task 185.0 in stage 5.0 (TID 848) in 7485 ms on localhost (184/200)
15/08/21 11:20:10 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 11:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:10 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 11:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:20:10 INFO Executor: Finished task 184.0 in stage 5.0 (TID 847). 1219 bytes result sent to driver
15/08/21 11:20:10 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 863, localhost, ANY, 1759 bytes)
15/08/21 11:20:10 INFO Executor: Running task 16.0 in stage 7.0 (TID 863)
15/08/21 11:20:10 INFO TaskSetManager: Finished task 184.0 in stage 5.0 (TID 847) in 7904 ms on localhost (185/200)
15/08/21 11:20:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501183 records.
15/08/21 11:20:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:10 INFO Executor: Finished task 183.0 in stage 5.0 (TID 846). 1219 bytes result sent to driver
15/08/21 11:20:10 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 864, localhost, ANY, 1773 bytes)
15/08/21 11:20:10 INFO Executor: Running task 17.0 in stage 7.0 (TID 864)
15/08/21 11:20:10 INFO TaskSetManager: Finished task 183.0 in stage 5.0 (TID 846) in 8273 ms on localhost (186/200)
15/08/21 11:20:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 261799262 length: 127581534 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:10 INFO Executor: Finished task 186.0 in stage 5.0 (TID 849). 1219 bytes result sent to driver
15/08/21 11:20:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3689969 records.
15/08/21 11:20:10 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 865, localhost, ANY, 1762 bytes)
15/08/21 11:20:10 INFO Executor: Running task 18.0 in stage 7.0 (TID 865)
15/08/21 11:20:10 INFO TaskSetManager: Finished task 186.0 in stage 5.0 (TID 849) in 7679 ms on localhost (187/200)
15/08/21 11:20:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:10 INFO Executor: Finished task 187.0 in stage 5.0 (TID 850). 1219 bytes result sent to driver
15/08/21 11:20:10 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 866, localhost, ANY, 1773 bytes)
15/08/21 11:20:10 INFO Executor: Running task 19.0 in stage 7.0 (TID 866)
15/08/21 11:20:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:10 INFO TaskSetManager: Finished task 187.0 in stage 5.0 (TID 850) in 7643 ms on localhost (188/200)
15/08/21 11:20:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 134217728 end: 257435404 length: 123217676 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:10 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 3501351
15/08/21 11:20:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573923 records.
15/08/21 11:20:10 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
15/08/21 11:20:11 INFO Executor: Finished task 188.0 in stage 5.0 (TID 851). 1219 bytes result sent to driver
15/08/21 11:20:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:11 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 867, localhost, ANY, 1762 bytes)
15/08/21 11:20:11 INFO Executor: Running task 20.0 in stage 7.0 (TID 867)
15/08/21 11:20:11 INFO TaskSetManager: Finished task 188.0 in stage 5.0 (TID 851) in 7716 ms on localhost (189/200)
15/08/21 11:20:11 INFO InternalParquetRecordReader: block read in memory in 274 ms. row count = 3501183
15/08/21 11:20:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:11 INFO InternalParquetRecordReader: block read in memory in 52 ms. row count = 3501239
15/08/21 11:20:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:11 INFO InternalParquetRecordReader: block read in memory in 62 ms. row count = 3500100
15/08/21 11:20:12 INFO Executor: Finished task 189.0 in stage 5.0 (TID 852). 1219 bytes result sent to driver
15/08/21 11:20:12 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 868, localhost, ANY, 1775 bytes)
15/08/21 11:20:12 INFO Executor: Running task 21.0 in stage 7.0 (TID 868)
15/08/21 11:20:12 INFO TaskSetManager: Finished task 189.0 in stage 5.0 (TID 852) in 6280 ms on localhost (190/200)
15/08/21 11:20:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 134217728 end: 257347114 length: 123129386 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
15/08/21 11:20:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:12 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 3500100
15/08/21 11:20:12 INFO Executor: Finished task 190.0 in stage 5.0 (TID 853). 1219 bytes result sent to driver
15/08/21 11:20:12 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 869, localhost, ANY, 1762 bytes)
15/08/21 11:20:12 INFO Executor: Running task 22.0 in stage 7.0 (TID 869)
15/08/21 11:20:12 INFO TaskSetManager: Finished task 190.0 in stage 5.0 (TID 853) in 6476 ms on localhost (191/200)
15/08/21 11:20:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501423 records.
15/08/21 11:20:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:12 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 3501423
15/08/21 11:20:13 INFO Executor: Finished task 191.0 in stage 5.0 (TID 854). 1219 bytes result sent to driver
15/08/21 11:20:13 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 870, localhost, ANY, 1772 bytes)
15/08/21 11:20:13 INFO Executor: Running task 23.0 in stage 7.0 (TID 870)
15/08/21 11:20:13 INFO TaskSetManager: Finished task 191.0 in stage 5.0 (TID 854) in 7118 ms on localhost (192/200)
15/08/21 11:20:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 134217728 end: 257844810 length: 123627082 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573035 records.
15/08/21 11:20:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:13 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
15/08/21 11:20:13 INFO Executor: Finished task 192.0 in stage 5.0 (TID 855). 1219 bytes result sent to driver
15/08/21 11:20:13 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 871, localhost, ANY, 1762 bytes)
15/08/21 11:20:13 INFO Executor: Running task 24.0 in stage 7.0 (TID 871)
15/08/21 11:20:13 INFO TaskSetManager: Finished task 192.0 in stage 5.0 (TID 855) in 7551 ms on localhost (193/200)
15/08/21 11:20:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:13 INFO Executor: Finished task 194.0 in stage 5.0 (TID 857). 1219 bytes result sent to driver
15/08/21 11:20:13 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 872, localhost, ANY, 1771 bytes)
15/08/21 11:20:13 INFO Executor: Running task 25.0 in stage 7.0 (TID 872)
15/08/21 11:20:13 INFO TaskSetManager: Finished task 194.0 in stage 5.0 (TID 857) in 6789 ms on localhost (194/200)
15/08/21 11:20:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 257827380 length: 123609652 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:13 INFO Executor: Finished task 193.0 in stage 5.0 (TID 856). 1219 bytes result sent to driver
15/08/21 11:20:13 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 873, localhost, ANY, 1762 bytes)
15/08/21 11:20:13 INFO Executor: Running task 26.0 in stage 7.0 (TID 873)
15/08/21 11:20:13 INFO TaskSetManager: Finished task 193.0 in stage 5.0 (TID 856) in 6836 ms on localhost (195/200)
15/08/21 11:20:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573921 records.
15/08/21 11:20:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501667 records.
15/08/21 11:20:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:13 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 3501305
15/08/21 11:20:13 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
15/08/21 11:20:13 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 3501667
15/08/21 11:20:14 INFO Executor: Finished task 195.0 in stage 5.0 (TID 858). 1219 bytes result sent to driver
15/08/21 11:20:14 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 874, localhost, ANY, 1773 bytes)
15/08/21 11:20:14 INFO Executor: Running task 27.0 in stage 7.0 (TID 874)
15/08/21 11:20:14 INFO TaskSetManager: Finished task 195.0 in stage 5.0 (TID 858) in 7087 ms on localhost (196/200)
15/08/21 11:20:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 134217728 end: 257458294 length: 123240566 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572747 records.
15/08/21 11:20:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:14 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 3500100
15/08/21 11:20:14 INFO Executor: Finished task 196.0 in stage 5.0 (TID 859). 1219 bytes result sent to driver
15/08/21 11:20:14 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 875, localhost, ANY, 1762 bytes)
15/08/21 11:20:14 INFO Executor: Running task 28.0 in stage 7.0 (TID 875)
15/08/21 11:20:14 INFO TaskSetManager: Finished task 196.0 in stage 5.0 (TID 859) in 7779 ms on localhost (197/200)
15/08/21 11:20:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:14 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500100
15/08/21 11:20:16 INFO InternalParquetRecordReader: Assembled and processed 3501351 records from 2 columns in 5302 ms: 660.38306 rec/ms, 1320.7661 cell/ms
15/08/21 11:20:16 INFO InternalParquetRecordReader: time spent so far 1% reading (64 ms) and 98% processing (5302 ms)
15/08/21 11:20:16 INFO InternalParquetRecordReader: at row 3501351. reading next block
15/08/21 11:20:16 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 188618
15/08/21 11:20:17 INFO Executor: Finished task 18.0 in stage 7.0 (TID 865). 2125 bytes result sent to driver
15/08/21 11:20:17 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 876, localhost, ANY, 1773 bytes)
15/08/21 11:20:17 INFO Executor: Running task 29.0 in stage 7.0 (TID 876)
15/08/21 11:20:17 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 865) in 6588 ms on localhost (17/170)
15/08/21 11:20:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 134217728 end: 257369456 length: 123151728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
15/08/21 11:20:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:17 INFO InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500100
15/08/21 11:20:17 INFO Executor: Finished task 20.0 in stage 7.0 (TID 867). 2125 bytes result sent to driver
15/08/21 11:20:17 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 877, localhost, ANY, 1762 bytes)
15/08/21 11:20:17 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 867) in 6595 ms on localhost (18/170)
15/08/21 11:20:17 INFO Executor: Finished task 17.0 in stage 7.0 (TID 864). 2125 bytes result sent to driver
15/08/21 11:20:17 INFO Executor: Running task 30.0 in stage 7.0 (TID 877)
15/08/21 11:20:17 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 878, localhost, ANY, 1776 bytes)
15/08/21 11:20:17 INFO Executor: Running task 31.0 in stage 7.0 (TID 878)
15/08/21 11:20:17 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 864) in 6769 ms on localhost (19/170)
15/08/21 11:20:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 257340601 length: 123122873 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574247 records.
15/08/21 11:20:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:17 INFO Executor: Finished task 197.0 in stage 5.0 (TID 860). 1219 bytes result sent to driver
15/08/21 11:20:17 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 879, localhost, ANY, 1761 bytes)
15/08/21 11:20:17 INFO Executor: Running task 32.0 in stage 7.0 (TID 879)
15/08/21 11:20:17 INFO TaskSetManager: Finished task 197.0 in stage 5.0 (TID 860) in 8047 ms on localhost (198/200)
15/08/21 11:20:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:17 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
15/08/21 11:20:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:17 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 3500100
15/08/21 11:20:17 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
15/08/21 11:20:18 INFO Executor: Finished task 198.0 in stage 5.0 (TID 861). 1219 bytes result sent to driver
15/08/21 11:20:18 INFO InternalParquetRecordReader: Assembled and processed 3501239 records from 2 columns in 7080 ms: 494.52527 rec/ms, 989.05054 cell/ms
15/08/21 11:20:18 INFO InternalParquetRecordReader: time spent so far 0% reading (52 ms) and 99% processing (7080 ms)
15/08/21 11:20:18 INFO InternalParquetRecordReader: at row 3501239. reading next block
15/08/21 11:20:18 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 880, localhost, ANY, 1771 bytes)
15/08/21 11:20:18 INFO Executor: Running task 33.0 in stage 7.0 (TID 880)
15/08/21 11:20:18 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 72684
15/08/21 11:20:18 INFO TaskSetManager: Finished task 198.0 in stage 5.0 (TID 861) in 8320 ms on localhost (199/200)
15/08/21 11:20:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 259459156 length: 125241428 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627517 records.
15/08/21 11:20:18 INFO Executor: Finished task 16.0 in stage 7.0 (TID 863). 2125 bytes result sent to driver
15/08/21 11:20:18 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 881, localhost, ANY, 1762 bytes)
15/08/21 11:20:18 INFO Executor: Running task 34.0 in stage 7.0 (TID 881)
15/08/21 11:20:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:18 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 863) in 7702 ms on localhost (20/170)
15/08/21 11:20:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501180 records.
15/08/21 11:20:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:18 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3500968
15/08/21 11:20:18 INFO Executor: Finished task 199.0 in stage 5.0 (TID 862). 1219 bytes result sent to driver
15/08/21 11:20:18 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 882, localhost, ANY, 1772 bytes)
15/08/21 11:20:18 INFO Executor: Running task 35.0 in stage 7.0 (TID 882)
15/08/21 11:20:18 INFO TaskSetManager: Finished task 199.0 in stage 5.0 (TID 862) in 7966 ms on localhost (200/200)
15/08/21 11:20:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/21 11:20:18 INFO DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:423) finished in 109.930 s
15/08/21 11:20:18 INFO DAGScheduler: looking for newly runnable stages
15/08/21 11:20:18 INFO DAGScheduler: running: Set(ShuffleMapStage 7)
15/08/21 11:20:18 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 11:20:18 INFO DAGScheduler: failed: Set()
15/08/21 11:20:18 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@bde866a
15/08/21 11:20:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 134217728 end: 257580347 length: 123362619 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:18 INFO StatsReportListener: task runtime:(count: 220, mean: 7828.613636, stdev: 1432.173647, max: 11835.000000, min: 5418.000000)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	5.4 s	5.9 s	6.0 s	6.7 s	7.7 s	8.7 s	9.8 s	10.7 s	11.8 s
15/08/21 11:20:18 INFO StatsReportListener: shuffle bytes written:(count: 220, mean: 23644686.295455, stdev: 455037.180808, max: 23872475.000000, min: 21966603.000000)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	20.9 MB	21.4 MB	22.6 MB	22.7 MB	22.7 MB	22.7 MB	22.7 MB	22.7 MB	22.8 MB
15/08/21 11:20:18 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.585000, stdev: 1.443182, max: 11.000000, min: 0.000000)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	2.0 ms	11.0 ms
15/08/21 11:20:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:18 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 11:20:18 INFO StatsReportListener: task result size:(count: 220, mean: 1301.363636, stdev: 260.456687, max: 2125.000000, min: 1219.000000)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	2.1 KB	2.1 KB
15/08/21 11:20:18 INFO StatsReportListener: executor (non-fetch) time pct: (count: 220, mean: 99.629023, stdev: 0.181705, max: 99.867116, min: 98.856101)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	99 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 11:20:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573027 records.
15/08/21 11:20:18 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 7)
15/08/21 11:20:18 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.007624, stdev: 0.019537, max: 0.181488, min: 0.000000)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 11:20:18 INFO StatsReportListener: other time pct: (count: 220, mean: 0.364046, stdev: 0.181698, max: 1.143899, min: 0.125993)
15/08/21 11:20:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:20:18 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %
15/08/21 11:20:18 INFO InternalParquetRecordReader: block read in memory in 88 ms. row count = 3501180
15/08/21 11:20:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:18 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500932
15/08/21 11:20:18 INFO Executor: Finished task 22.0 in stage 7.0 (TID 869). 2125 bytes result sent to driver
15/08/21 11:20:18 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 883, localhost, ANY, 1761 bytes)
15/08/21 11:20:18 INFO Executor: Running task 36.0 in stage 7.0 (TID 883)
15/08/21 11:20:18 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 869) in 6285 ms on localhost (21/170)
15/08/21 11:20:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:18 INFO Executor: Finished task 19.0 in stage 7.0 (TID 866). 2125 bytes result sent to driver
15/08/21 11:20:18 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 884, localhost, ANY, 1773 bytes)
15/08/21 11:20:18 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 866) in 7848 ms on localhost (22/170)
15/08/21 11:20:18 INFO Executor: Running task 37.0 in stage 7.0 (TID 884)
15/08/21 11:20:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 257857124 length: 123639396 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574216 records.
15/08/21 11:20:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:18 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 3500100
15/08/21 11:20:18 INFO InternalParquetRecordReader: block read in memory in 85 ms. row count = 3501165
15/08/21 11:20:18 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6747 ms: 518.7639 rec/ms, 1037.5278 cell/ms
15/08/21 11:20:18 INFO InternalParquetRecordReader: time spent so far 0% reading (42 ms) and 99% processing (6747 ms)
15/08/21 11:20:18 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:18 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 74238
15/08/21 11:20:19 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5988 ms: 584.51904 rec/ms, 1169.0381 cell/ms
15/08/21 11:20:19 INFO InternalParquetRecordReader: time spent so far 0% reading (53 ms) and 99% processing (5988 ms)
15/08/21 11:20:19 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72935
15/08/21 11:20:19 INFO Executor: Finished task 21.0 in stage 7.0 (TID 868). 2125 bytes result sent to driver
15/08/21 11:20:19 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 885, localhost, ANY, 1761 bytes)
15/08/21 11:20:19 INFO Executor: Running task 38.0 in stage 7.0 (TID 885)
15/08/21 11:20:19 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 868) in 7284 ms on localhost (23/170)
15/08/21 11:20:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
15/08/21 11:20:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:19 INFO InternalParquetRecordReader: block read in memory in 62 ms. row count = 3501121
15/08/21 11:20:19 INFO Executor: Finished task 23.0 in stage 7.0 (TID 870). 2125 bytes result sent to driver
15/08/21 11:20:19 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 886, localhost, ANY, 1773 bytes)
15/08/21 11:20:19 INFO Executor: Running task 39.0 in stage 7.0 (TID 886)
15/08/21 11:20:19 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 870) in 6571 ms on localhost (24/170)
15/08/21 11:20:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 259199417 length: 124981689 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626725 records.
15/08/21 11:20:19 INFO InternalParquetRecordReader: Assembled and processed 3501305 records from 2 columns in 5845 ms: 599.02563 rec/ms, 1198.0513 cell/ms
15/08/21 11:20:19 INFO InternalParquetRecordReader: time spent so far 0% reading (45 ms) and 99% processing (5845 ms)
15/08/21 11:20:19 INFO InternalParquetRecordReader: at row 3501305. reading next block
15/08/21 11:20:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72616
15/08/21 11:20:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:19 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 3502670
15/08/21 11:20:20 INFO Executor: Finished task 25.0 in stage 7.0 (TID 872). 2125 bytes result sent to driver
15/08/21 11:20:20 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 887, localhost, ANY, 1761 bytes)
15/08/21 11:20:20 INFO Executor: Running task 40.0 in stage 7.0 (TID 887)
15/08/21 11:20:20 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 872) in 6375 ms on localhost (25/170)
15/08/21 11:20:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501407 records.
15/08/21 11:20:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:20 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 3501407
15/08/21 11:20:20 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6327 ms: 553.20056 rec/ms, 1106.4011 cell/ms
15/08/21 11:20:20 INFO InternalParquetRecordReader: time spent so far 1% reading (64 ms) and 98% processing (6327 ms)
15/08/21 11:20:20 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:20 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 72647
15/08/21 11:20:20 INFO Executor: Finished task 26.0 in stage 7.0 (TID 873). 2125 bytes result sent to driver
15/08/21 11:20:20 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 888, localhost, ANY, 1773 bytes)
15/08/21 11:20:20 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 873) in 6976 ms on localhost (26/170)
15/08/21 11:20:20 INFO Executor: Running task 41.0 in stage 7.0 (TID 888)
15/08/21 11:20:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 257564901 length: 123347173 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572791 records.
15/08/21 11:20:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:20 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 3500841
15/08/21 11:20:21 INFO Executor: Finished task 28.0 in stage 7.0 (TID 875). 2125 bytes result sent to driver
15/08/21 11:20:21 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 889, localhost, ANY, 1761 bytes)
15/08/21 11:20:21 INFO Executor: Running task 42.0 in stage 7.0 (TID 889)
15/08/21 11:20:21 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 875) in 6445 ms on localhost (27/170)
15/08/21 11:20:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:21 INFO Executor: Finished task 24.0 in stage 7.0 (TID 871). 2125 bytes result sent to driver
15/08/21 11:20:21 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 890, localhost, ANY, 1772 bytes)
15/08/21 11:20:21 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 871) in 7522 ms on localhost (28/170)
15/08/21 11:20:21 INFO Executor: Finished task 27.0 in stage 7.0 (TID 874). 2125 bytes result sent to driver
15/08/21 11:20:21 INFO Executor: Running task 43.0 in stage 7.0 (TID 890)
15/08/21 11:20:21 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 891, localhost, ANY, 1762 bytes)
15/08/21 11:20:21 INFO Executor: Running task 44.0 in stage 7.0 (TID 891)
15/08/21 11:20:21 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 874) in 7213 ms on localhost (29/170)
15/08/21 11:20:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501143 records.
15/08/21 11:20:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 257460000 length: 123242272 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501260 records.
15/08/21 11:20:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573047 records.
15/08/21 11:20:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:21 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 3501260
15/08/21 11:20:21 INFO InternalParquetRecordReader: block read in memory in 139 ms. row count = 3501143
15/08/21 11:20:21 INFO InternalParquetRecordReader: block read in memory in 117 ms. row count = 3500100
15/08/21 11:20:22 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4778 ms: 732.545 rec/ms, 1465.09 cell/ms
15/08/21 11:20:22 INFO InternalParquetRecordReader: time spent so far 1% reading (53 ms) and 98% processing (4778 ms)
15/08/21 11:20:22 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:22 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 74147
15/08/21 11:20:22 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5167 ms: 677.395 rec/ms, 1354.79 cell/ms
15/08/21 11:20:22 INFO InternalParquetRecordReader: time spent so far 0% reading (51 ms) and 99% processing (5167 ms)
15/08/21 11:20:22 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:22 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 74054
15/08/21 11:20:23 INFO Executor: Finished task 31.0 in stage 7.0 (TID 878). 2125 bytes result sent to driver
15/08/21 11:20:23 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 892, localhost, ANY, 1772 bytes)
15/08/21 11:20:23 INFO Executor: Running task 45.0 in stage 7.0 (TID 892)
15/08/21 11:20:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 134217728 end: 257842743 length: 123625015 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:23 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 878) in 5494 ms on localhost (30/170)
15/08/21 11:20:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573207 records.
15/08/21 11:20:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:23 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 3500100
15/08/21 11:20:23 INFO InternalParquetRecordReader: Assembled and processed 3500968 records from 2 columns in 5075 ms: 689.8459 rec/ms, 1379.6918 cell/ms
15/08/21 11:20:23 INFO InternalParquetRecordReader: time spent so far 1% reading (75 ms) and 98% processing (5075 ms)
15/08/21 11:20:23 INFO InternalParquetRecordReader: at row 3500968. reading next block
15/08/21 11:20:23 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 126549
15/08/21 11:20:23 INFO Executor: Finished task 32.0 in stage 7.0 (TID 879). 2125 bytes result sent to driver
15/08/21 11:20:23 INFO Executor: Finished task 29.0 in stage 7.0 (TID 876). 2125 bytes result sent to driver
15/08/21 11:20:23 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 893, localhost, ANY, 1762 bytes)
15/08/21 11:20:23 INFO Executor: Running task 46.0 in stage 7.0 (TID 893)
15/08/21 11:20:23 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 894, localhost, ANY, 1773 bytes)
15/08/21 11:20:23 INFO Executor: Running task 47.0 in stage 7.0 (TID 894)
15/08/21 11:20:23 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 879) in 5760 ms on localhost (31/170)
15/08/21 11:20:23 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 876) in 5993 ms on localhost (32/170)
15/08/21 11:20:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 134217728 end: 257576472 length: 123358744 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573258 records.
15/08/21 11:20:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501217 records.
15/08/21 11:20:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:23 INFO InternalParquetRecordReader: block read in memory in 300 ms. row count = 3501217
15/08/21 11:20:23 INFO InternalParquetRecordReader: block read in memory in 365 ms. row count = 3501508
15/08/21 11:20:23 INFO InternalParquetRecordReader: Assembled and processed 3500932 records from 2 columns in 5473 ms: 639.6733 rec/ms, 1279.3466 cell/ms
15/08/21 11:20:23 INFO InternalParquetRecordReader: time spent so far 1% reading (61 ms) and 98% processing (5473 ms)
15/08/21 11:20:23 INFO InternalParquetRecordReader: at row 3500932. reading next block
15/08/21 11:20:23 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 72095
15/08/21 11:20:24 INFO Executor: Finished task 30.0 in stage 7.0 (TID 877). 2125 bytes result sent to driver
15/08/21 11:20:24 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 895, localhost, ANY, 1762 bytes)
15/08/21 11:20:24 INFO Executor: Running task 48.0 in stage 7.0 (TID 895)
15/08/21 11:20:24 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 877) in 6676 ms on localhost (33/170)
15/08/21 11:20:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:24 INFO Executor: Finished task 33.0 in stage 7.0 (TID 880). 2125 bytes result sent to driver
15/08/21 11:20:24 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 896, localhost, ANY, 1773 bytes)
15/08/21 11:20:24 INFO Executor: Running task 49.0 in stage 7.0 (TID 896)
15/08/21 11:20:24 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 880) in 6167 ms on localhost (34/170)
15/08/21 11:20:24 INFO Executor: Finished task 34.0 in stage 7.0 (TID 881). 2125 bytes result sent to driver
15/08/21 11:20:24 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 897, localhost, ANY, 1761 bytes)
15/08/21 11:20:24 INFO Executor: Running task 50.0 in stage 7.0 (TID 897)
15/08/21 11:20:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 134217728 end: 256733155 length: 122515427 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:24 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 881) in 6124 ms on localhost (35/170)
15/08/21 11:20:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503050 records.
15/08/21 11:20:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3570986 records.
15/08/21 11:20:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501200 records.
15/08/21 11:20:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:24 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 3501200
15/08/21 11:20:24 INFO InternalParquetRecordReader: block read in memory in 77 ms. row count = 3503050
15/08/21 11:20:24 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 3503008
15/08/21 11:20:24 INFO Executor: Finished task 35.0 in stage 7.0 (TID 882). 2125 bytes result sent to driver
15/08/21 11:20:24 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 898, localhost, ANY, 1771 bytes)
15/08/21 11:20:24 INFO Executor: Running task 51.0 in stage 7.0 (TID 898)
15/08/21 11:20:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 134217728 end: 257568635 length: 123350907 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:24 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 882) in 6214 ms on localhost (36/170)
15/08/21 11:20:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573168 records.
15/08/21 11:20:24 INFO InternalParquetRecordReader: Assembled and processed 3501165 records from 2 columns in 5623 ms: 622.6507 rec/ms, 1245.3014 cell/ms
15/08/21 11:20:24 INFO InternalParquetRecordReader: time spent so far 1% reading (85 ms) and 98% processing (5623 ms)
15/08/21 11:20:24 INFO InternalParquetRecordReader: at row 3501165. reading next block
15/08/21 11:20:24 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 73051
15/08/21 11:20:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:24 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 3501341
15/08/21 11:20:24 INFO Executor: Finished task 36.0 in stage 7.0 (TID 883). 2125 bytes result sent to driver
15/08/21 11:20:24 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 899, localhost, ANY, 1760 bytes)
15/08/21 11:20:24 INFO Executor: Running task 52.0 in stage 7.0 (TID 899)
15/08/21 11:20:24 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 883) in 6098 ms on localhost (37/170)
15/08/21 11:20:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500823 records.
15/08/21 11:20:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:24 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 3500823
15/08/21 11:20:25 INFO Executor: Finished task 37.0 in stage 7.0 (TID 884). 2125 bytes result sent to driver
15/08/21 11:20:25 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 900, localhost, ANY, 1771 bytes)
15/08/21 11:20:25 INFO Executor: Running task 53.0 in stage 7.0 (TID 900)
15/08/21 11:20:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 257584480 length: 123366752 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:25 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 884) in 6369 ms on localhost (38/170)
15/08/21 11:20:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:25 INFO Executor: Finished task 38.0 in stage 7.0 (TID 885). 2125 bytes result sent to driver
15/08/21 11:20:25 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 901, localhost, ANY, 1760 bytes)
15/08/21 11:20:25 INFO Executor: Running task 54.0 in stage 7.0 (TID 901)
15/08/21 11:20:25 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 885) in 5828 ms on localhost (39/170)
15/08/21 11:20:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573143 records.
15/08/21 11:20:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:25 INFO InternalParquetRecordReader: Assembled and processed 3502670 records from 2 columns in 5376 ms: 651.5383 rec/ms, 1303.0767 cell/ms
15/08/21 11:20:25 INFO InternalParquetRecordReader: time spent so far 1% reading (57 ms) and 98% processing (5376 ms)
15/08/21 11:20:25 INFO InternalParquetRecordReader: at row 3502670. reading next block
15/08/21 11:20:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503144 records.
15/08/21 11:20:25 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 3501035
15/08/21 11:20:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:25 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 124055
15/08/21 11:20:25 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 3503144
15/08/21 11:20:25 INFO Executor: Finished task 39.0 in stage 7.0 (TID 886). 2125 bytes result sent to driver
15/08/21 11:20:25 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 902, localhost, ANY, 1773 bytes)
15/08/21 11:20:25 INFO Executor: Running task 55.0 in stage 7.0 (TID 902)
15/08/21 11:20:25 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 886) in 5983 ms on localhost (40/170)
15/08/21 11:20:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 259050527 length: 124832799 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3624571 records.
15/08/21 11:20:25 INFO InternalParquetRecordReader: Assembled and processed 3500841 records from 2 columns in 4868 ms: 719.1539 rec/ms, 1438.3077 cell/ms
15/08/21 11:20:25 INFO InternalParquetRecordReader: time spent so far 0% reading (41 ms) and 99% processing (4868 ms)
15/08/21 11:20:25 INFO InternalParquetRecordReader: at row 3500841. reading next block
15/08/21 11:20:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 71950
15/08/21 11:20:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:25 INFO InternalParquetRecordReader: block read in memory in 82 ms. row count = 3500100
15/08/21 11:20:26 INFO Executor: Finished task 41.0 in stage 7.0 (TID 888). 2125 bytes result sent to driver
15/08/21 11:20:26 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 903, localhost, ANY, 1761 bytes)
15/08/21 11:20:26 INFO Executor: Running task 56.0 in stage 7.0 (TID 903)
15/08/21 11:20:26 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 888) in 5464 ms on localhost (41/170)
15/08/21 11:20:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:26 INFO Executor: Finished task 40.0 in stage 7.0 (TID 887). 2125 bytes result sent to driver
15/08/21 11:20:26 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 904, localhost, ANY, 1775 bytes)
15/08/21 11:20:26 INFO Executor: Running task 57.0 in stage 7.0 (TID 904)
15/08/21 11:20:26 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 887) in 6221 ms on localhost (42/170)
15/08/21 11:20:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 257461890 length: 123244162 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573988 records.
15/08/21 11:20:26 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 3500100
15/08/21 11:20:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:26 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500824
15/08/21 11:20:26 INFO Executor: Finished task 44.0 in stage 7.0 (TID 891). 2125 bytes result sent to driver
15/08/21 11:20:26 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 905, localhost, ANY, 1762 bytes)
15/08/21 11:20:26 INFO Executor: Running task 58.0 in stage 7.0 (TID 905)
15/08/21 11:20:26 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 891) in 5470 ms on localhost (43/170)
15/08/21 11:20:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:26 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 3500100
15/08/21 11:20:26 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5585 ms: 626.69653 rec/ms, 1253.3931 cell/ms
15/08/21 11:20:26 INFO InternalParquetRecordReader: time spent so far 2% reading (117 ms) and 97% processing (5585 ms)
15/08/21 11:20:26 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72947
15/08/21 11:20:27 INFO Executor: Finished task 42.0 in stage 7.0 (TID 889). 2125 bytes result sent to driver
15/08/21 11:20:27 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 906, localhost, ANY, 1777 bytes)
15/08/21 11:20:27 INFO Executor: Running task 59.0 in stage 7.0 (TID 906)
15/08/21 11:20:27 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 889) in 6593 ms on localhost (44/170)
15/08/21 11:20:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 134217728 end: 257416343 length: 123198615 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:27 INFO Executor: Finished task 43.0 in stage 7.0 (TID 890). 2125 bytes result sent to driver
15/08/21 11:20:27 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 907, localhost, ANY, 1761 bytes)
15/08/21 11:20:27 INFO Executor: Running task 60.0 in stage 7.0 (TID 907)
15/08/21 11:20:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:27 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 890) in 6547 ms on localhost (45/170)
15/08/21 11:20:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574223 records.
15/08/21 11:20:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
15/08/21 11:20:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:27 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 3501614
15/08/21 11:20:27 INFO InternalParquetRecordReader: block read in memory in 88 ms. row count = 3501191
15/08/21 11:20:28 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4835 ms: 723.909 rec/ms, 1447.818 cell/ms
15/08/21 11:20:28 INFO InternalParquetRecordReader: time spent so far 2% reading (102 ms) and 97% processing (4835 ms)
15/08/21 11:20:28 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:28 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 73107
15/08/21 11:20:28 INFO InternalParquetRecordReader: Assembled and processed 3501508 records from 2 columns in 4427 ms: 790.9437 rec/ms, 1581.8875 cell/ms
15/08/21 11:20:28 INFO InternalParquetRecordReader: time spent so far 7% reading (365 ms) and 92% processing (4427 ms)
15/08/21 11:20:28 INFO InternalParquetRecordReader: at row 3501508. reading next block
15/08/21 11:20:28 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 71750
15/08/21 11:20:28 INFO Executor: Finished task 45.0 in stage 7.0 (TID 892). 2125 bytes result sent to driver
15/08/21 11:20:28 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 908, localhost, ANY, 1773 bytes)
15/08/21 11:20:28 INFO Executor: Running task 61.0 in stage 7.0 (TID 908)
15/08/21 11:20:28 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 892) in 5881 ms on localhost (46/170)
15/08/21 11:20:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 257883468 length: 123665740 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:29 INFO Executor: Finished task 47.0 in stage 7.0 (TID 894). 2125 bytes result sent to driver
15/08/21 11:20:29 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 909, localhost, ANY, 1761 bytes)
15/08/21 11:20:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573362 records.
15/08/21 11:20:29 INFO Executor: Running task 62.0 in stage 7.0 (TID 909)
15/08/21 11:20:29 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 894) in 5580 ms on localhost (47/170)
15/08/21 11:20:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:29 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
15/08/21 11:20:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501059 records.
15/08/21 11:20:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:29 INFO InternalParquetRecordReader: block read in memory in 96 ms. row count = 3501059
15/08/21 11:20:29 INFO InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 5146 ms: 680.7244 rec/ms, 1361.4489 cell/ms
15/08/21 11:20:29 INFO InternalParquetRecordReader: time spent so far 1% reading (73 ms) and 98% processing (5146 ms)
15/08/21 11:20:29 INFO InternalParquetRecordReader: at row 3503008. reading next block
15/08/21 11:20:29 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 67978
15/08/21 11:20:30 INFO Executor: Finished task 50.0 in stage 7.0 (TID 897). 2125 bytes result sent to driver
15/08/21 11:20:30 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 910, localhost, ANY, 1775 bytes)
15/08/21 11:20:30 INFO Executor: Running task 63.0 in stage 7.0 (TID 910)
15/08/21 11:20:30 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 897) in 5724 ms on localhost (48/170)
15/08/21 11:20:30 INFO Executor: Finished task 46.0 in stage 7.0 (TID 893). 2125 bytes result sent to driver
15/08/21 11:20:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 134217728 end: 257592803 length: 123375075 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:30 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 911, localhost, ANY, 1761 bytes)
15/08/21 11:20:30 INFO Executor: Running task 64.0 in stage 7.0 (TID 911)
15/08/21 11:20:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:30 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 893) in 6615 ms on localhost (49/170)
15/08/21 11:20:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573291 records.
15/08/21 11:20:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:30 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3501221
15/08/21 11:20:30 INFO InternalParquetRecordReader: Assembled and processed 3501341 records from 2 columns in 5508 ms: 635.6828 rec/ms, 1271.3656 cell/ms
15/08/21 11:20:30 INFO InternalParquetRecordReader: time spent so far 1% reading (58 ms) and 98% processing (5508 ms)
15/08/21 11:20:30 INFO InternalParquetRecordReader: at row 3501341. reading next block
15/08/21 11:20:30 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 71827
15/08/21 11:20:30 INFO InternalParquetRecordReader: block read in memory in 116 ms. row count = 3500100
15/08/21 11:20:30 INFO InternalParquetRecordReader: Assembled and processed 3501035 records from 2 columns in 5320 ms: 658.0893 rec/ms, 1316.1786 cell/ms
15/08/21 11:20:30 INFO InternalParquetRecordReader: time spent so far 0% reading (33 ms) and 99% processing (5320 ms)
15/08/21 11:20:30 INFO InternalParquetRecordReader: at row 3501035. reading next block
15/08/21 11:20:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 72108
15/08/21 11:20:30 INFO Executor: Finished task 48.0 in stage 7.0 (TID 895). 2125 bytes result sent to driver
15/08/21 11:20:30 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 912, localhost, ANY, 1773 bytes)
15/08/21 11:20:30 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 895) in 6326 ms on localhost (50/170)
15/08/21 11:20:30 INFO Executor: Running task 65.0 in stage 7.0 (TID 912)
15/08/21 11:20:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258145407 length: 123927679 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574053 records.
15/08/21 11:20:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:30 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 3500100
15/08/21 11:20:30 INFO Executor: Finished task 49.0 in stage 7.0 (TID 896). 2125 bytes result sent to driver
15/08/21 11:20:30 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 913, localhost, ANY, 1760 bytes)
15/08/21 11:20:30 INFO Executor: Running task 66.0 in stage 7.0 (TID 913)
15/08/21 11:20:30 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 896) in 6673 ms on localhost (51/170)
15/08/21 11:20:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:31 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5279 ms: 663.0233 rec/ms, 1326.0466 cell/ms
15/08/21 11:20:31 INFO InternalParquetRecordReader: time spent so far 1% reading (82 ms) and 98% processing (5279 ms)
15/08/21 11:20:31 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:31 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 124471
15/08/21 11:20:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:31 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 3500100
15/08/21 11:20:31 INFO Executor: Finished task 52.0 in stage 7.0 (TID 899). 2125 bytes result sent to driver
15/08/21 11:20:31 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 914, localhost, ANY, 1773 bytes)
15/08/21 11:20:31 INFO Executor: Running task 67.0 in stage 7.0 (TID 914)
15/08/21 11:20:31 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 899) in 6977 ms on localhost (52/170)
15/08/21 11:20:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259344407 length: 125126679 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627630 records.
15/08/21 11:20:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:31 INFO Executor: Finished task 54.0 in stage 7.0 (TID 901). 2125 bytes result sent to driver
15/08/21 11:20:31 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 915, localhost, ANY, 1762 bytes)
15/08/21 11:20:31 INFO Executor: Running task 68.0 in stage 7.0 (TID 915)
15/08/21 11:20:31 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 901) in 6698 ms on localhost (53/170)
15/08/21 11:20:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:31 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
15/08/21 11:20:31 INFO Executor: Finished task 53.0 in stage 7.0 (TID 900). 2125 bytes result sent to driver
15/08/21 11:20:31 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 916, localhost, ANY, 1773 bytes)
15/08/21 11:20:31 INFO Executor: Finished task 51.0 in stage 7.0 (TID 898). 2125 bytes result sent to driver
15/08/21 11:20:31 INFO Executor: Running task 69.0 in stage 7.0 (TID 916)
15/08/21 11:20:31 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 917, localhost, ANY, 1761 bytes)
15/08/21 11:20:31 INFO Executor: Running task 70.0 in stage 7.0 (TID 917)
15/08/21 11:20:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 134217728 end: 257768338 length: 123550610 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:31 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 900) in 6775 ms on localhost (54/170)
15/08/21 11:20:31 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 898) in 7398 ms on localhost (55/170)
15/08/21 11:20:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574075 records.
15/08/21 11:20:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502727 records.
15/08/21 11:20:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:31 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500100
15/08/21 11:20:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:32 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 3500100
15/08/21 11:20:32 INFO Executor: Finished task 56.0 in stage 7.0 (TID 903). 2125 bytes result sent to driver
15/08/21 11:20:32 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 918, localhost, ANY, 1775 bytes)
15/08/21 11:20:32 INFO Executor: Running task 71.0 in stage 7.0 (TID 918)
15/08/21 11:20:32 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 903) in 5838 ms on localhost (56/170)
15/08/21 11:20:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 134217728 end: 257102581 length: 122884853 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571405 records.
15/08/21 11:20:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:32 INFO Executor: Finished task 55.0 in stage 7.0 (TID 902). 2125 bytes result sent to driver
15/08/21 11:20:32 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 3502727
15/08/21 11:20:32 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 919, localhost, ANY, 1762 bytes)
15/08/21 11:20:32 INFO Executor: Running task 72.0 in stage 7.0 (TID 919)
15/08/21 11:20:32 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 902) in 6381 ms on localhost (57/170)
15/08/21 11:20:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501229 records.
15/08/21 11:20:32 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 3500100
15/08/21 11:20:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:32 INFO InternalParquetRecordReader: Assembled and processed 3500824 records from 2 columns in 5661 ms: 618.4109 rec/ms, 1236.8218 cell/ms
15/08/21 11:20:32 INFO InternalParquetRecordReader: time spent so far 1% reading (68 ms) and 98% processing (5661 ms)
15/08/21 11:20:32 INFO InternalParquetRecordReader: at row 3500824. reading next block
15/08/21 11:20:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 73164
15/08/21 11:20:32 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 3501229
15/08/21 11:20:32 INFO Executor: Finished task 58.0 in stage 7.0 (TID 905). 2125 bytes result sent to driver
15/08/21 11:20:32 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 920, localhost, ANY, 1775 bytes)
15/08/21 11:20:32 INFO Executor: Running task 73.0 in stage 7.0 (TID 920)
15/08/21 11:20:32 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 905) in 5819 ms on localhost (58/170)
15/08/21 11:20:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 134217728 end: 257873629 length: 123655901 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572739 records.
15/08/21 11:20:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:32 INFO Executor: Finished task 57.0 in stage 7.0 (TID 904). 2125 bytes result sent to driver
15/08/21 11:20:32 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 921, localhost, ANY, 1760 bytes)
15/08/21 11:20:32 INFO Executor: Running task 74.0 in stage 7.0 (TID 921)
15/08/21 11:20:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:32 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 904) in 6322 ms on localhost (59/170)
15/08/21 11:20:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:32 INFO InternalParquetRecordReader: block read in memory in 96 ms. row count = 3500100
15/08/21 11:20:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:32 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
15/08/21 11:20:33 INFO InternalParquetRecordReader: Assembled and processed 3501614 records from 2 columns in 5187 ms: 675.075 rec/ms, 1350.15 cell/ms
15/08/21 11:20:33 INFO InternalParquetRecordReader: time spent so far 1% reading (89 ms) and 98% processing (5187 ms)
15/08/21 11:20:33 INFO InternalParquetRecordReader: at row 3501614. reading next block
15/08/21 11:20:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 72609
15/08/21 11:20:33 INFO Executor: Finished task 59.0 in stage 7.0 (TID 906). 2125 bytes result sent to driver
15/08/21 11:20:33 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 922, localhost, ANY, 1770 bytes)
15/08/21 11:20:33 INFO Executor: Running task 75.0 in stage 7.0 (TID 922)
15/08/21 11:20:33 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 906) in 5657 ms on localhost (60/170)
15/08/21 11:20:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 259470450 length: 125252722 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627768 records.
15/08/21 11:20:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:33 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 3501462
15/08/21 11:20:33 INFO Executor: Finished task 60.0 in stage 7.0 (TID 907). 2125 bytes result sent to driver
15/08/21 11:20:33 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 923, localhost, ANY, 1761 bytes)
15/08/21 11:20:33 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 907) in 5800 ms on localhost (61/170)
15/08/21 11:20:33 INFO Executor: Running task 76.0 in stage 7.0 (TID 923)
15/08/21 11:20:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:33 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3500100
15/08/21 11:20:34 INFO Executor: Finished task 62.0 in stage 7.0 (TID 909). 2125 bytes result sent to driver
15/08/21 11:20:34 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 924, localhost, ANY, 1774 bytes)
15/08/21 11:20:34 INFO Executor: Running task 77.0 in stage 7.0 (TID 924)
15/08/21 11:20:34 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 909) in 5296 ms on localhost (62/170)
15/08/21 11:20:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 259842205 length: 125624477 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627712 records.
15/08/21 11:20:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:34 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3501302
15/08/21 11:20:34 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5525 ms: 633.50226 rec/ms, 1267.0045 cell/ms
15/08/21 11:20:34 INFO InternalParquetRecordReader: time spent so far 1% reading (70 ms) and 98% processing (5525 ms)
15/08/21 11:20:34 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:34 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 73262
15/08/21 11:20:35 INFO Executor: Finished task 64.0 in stage 7.0 (TID 911). 2125 bytes result sent to driver
15/08/21 11:20:35 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 925, localhost, ANY, 1761 bytes)
15/08/21 11:20:35 INFO Executor: Running task 78.0 in stage 7.0 (TID 925)
15/08/21 11:20:35 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 911) in 5076 ms on localhost (63/170)
15/08/21 11:20:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:35 INFO Executor: Finished task 61.0 in stage 7.0 (TID 908). 2125 bytes result sent to driver
15/08/21 11:20:35 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 926, localhost, ANY, 1776 bytes)
15/08/21 11:20:35 INFO Executor: Running task 79.0 in stage 7.0 (TID 926)
15/08/21 11:20:35 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 908) in 6204 ms on localhost (64/170)
15/08/21 11:20:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 257071082 length: 122853354 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503221 records.
15/08/21 11:20:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572089 records.
15/08/21 11:20:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:35 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3503221
15/08/21 11:20:35 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
15/08/21 11:20:35 INFO InternalParquetRecordReader: Assembled and processed 3501221 records from 2 columns in 5090 ms: 687.8627 rec/ms, 1375.7253 cell/ms
15/08/21 11:20:35 INFO InternalParquetRecordReader: time spent so far 1% reading (65 ms) and 98% processing (5090 ms)
15/08/21 11:20:35 INFO InternalParquetRecordReader: at row 3501221. reading next block
15/08/21 11:20:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72070
15/08/21 11:20:35 INFO Executor: Finished task 63.0 in stage 7.0 (TID 910). 2125 bytes result sent to driver
15/08/21 11:20:35 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 927, localhost, ANY, 1762 bytes)
15/08/21 11:20:35 INFO Executor: Running task 80.0 in stage 7.0 (TID 927)
15/08/21 11:20:35 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 910) in 5662 ms on localhost (65/170)
15/08/21 11:20:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:35 INFO InternalParquetRecordReader: block read in memory in 62 ms. row count = 3500100
15/08/21 11:20:36 INFO Executor: Finished task 66.0 in stage 7.0 (TID 913). 2125 bytes result sent to driver
15/08/21 11:20:36 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 928, localhost, ANY, 1774 bytes)
15/08/21 11:20:36 INFO Executor: Running task 81.0 in stage 7.0 (TID 928)
15/08/21 11:20:36 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 913) in 5121 ms on localhost (66/170)
15/08/21 11:20:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 134217728 end: 257075001 length: 122857273 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574581 records.
15/08/21 11:20:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:36 INFO InternalParquetRecordReader: block read in memory in 528 ms. row count = 3503231
15/08/21 11:20:36 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6047 ms: 578.8159 rec/ms, 1157.6318 cell/ms
15/08/21 11:20:36 INFO InternalParquetRecordReader: time spent so far 1% reading (84 ms) and 98% processing (6047 ms)
15/08/21 11:20:36 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 73953
15/08/21 11:20:36 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4274 ms: 818.9284 rec/ms, 1637.8568 cell/ms
15/08/21 11:20:36 INFO InternalParquetRecordReader: time spent so far 2% reading (96 ms) and 97% processing (4274 ms)
15/08/21 11:20:36 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:36 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 72639
15/08/21 11:20:37 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5423 ms: 645.41766 rec/ms, 1290.8353 cell/ms
15/08/21 11:20:37 INFO InternalParquetRecordReader: time spent so far 1% reading (68 ms) and 98% processing (5423 ms)
15/08/21 11:20:37 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:37 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 127530
15/08/21 11:20:37 INFO Executor: Finished task 65.0 in stage 7.0 (TID 912). 2125 bytes result sent to driver
15/08/21 11:20:37 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 929, localhost, ANY, 1761 bytes)
15/08/21 11:20:37 INFO Executor: Running task 82.0 in stage 7.0 (TID 929)
15/08/21 11:20:37 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 912) in 6867 ms on localhost (67/170)
15/08/21 11:20:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:37 INFO Executor: Finished task 73.0 in stage 7.0 (TID 920). 2125 bytes result sent to driver
15/08/21 11:20:37 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 930, localhost, ANY, 1773 bytes)
15/08/21 11:20:37 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 920) in 5117 ms on localhost (68/170)
15/08/21 11:20:37 INFO Executor: Running task 83.0 in stage 7.0 (TID 930)
15/08/21 11:20:37 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 3500100
15/08/21 11:20:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 134217728 end: 257427527 length: 123209799 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574316 records.
15/08/21 11:20:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:37 INFO InternalParquetRecordReader: block read in memory in 113 ms. row count = 3503274
15/08/21 11:20:37 INFO Executor: Finished task 72.0 in stage 7.0 (TID 919). 2125 bytes result sent to driver
15/08/21 11:20:37 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 931, localhost, ANY, 1761 bytes)
15/08/21 11:20:37 INFO Executor: Running task 84.0 in stage 7.0 (TID 931)
15/08/21 11:20:37 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 919) in 5760 ms on localhost (69/170)
15/08/21 11:20:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 92 ms. row count = 3500100
15/08/21 11:20:38 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6197 ms: 564.80554 rec/ms, 1129.6111 cell/ms
15/08/21 11:20:38 INFO InternalParquetRecordReader: time spent so far 0% reading (46 ms) and 99% processing (6197 ms)
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 71305
15/08/21 11:20:38 INFO Executor: Finished task 67.0 in stage 7.0 (TID 914). 2125 bytes result sent to driver
15/08/21 11:20:38 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 932, localhost, ANY, 1774 bytes)
15/08/21 11:20:38 INFO Executor: Running task 85.0 in stage 7.0 (TID 932)
15/08/21 11:20:38 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 914) in 6571 ms on localhost (70/170)
15/08/21 11:20:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 257349334 length: 123131606 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574256 records.
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:38 INFO Executor: Finished task 68.0 in stage 7.0 (TID 915). 2125 bytes result sent to driver
15/08/21 11:20:38 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 933, localhost, ANY, 1761 bytes)
15/08/21 11:20:38 INFO Executor: Running task 86.0 in stage 7.0 (TID 933)
15/08/21 11:20:38 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 915) in 6579 ms on localhost (71/170)
15/08/21 11:20:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:38 INFO Executor: Finished task 70.0 in stage 7.0 (TID 917). 2125 bytes result sent to driver
15/08/21 11:20:38 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 934, localhost, ANY, 1773 bytes)
15/08/21 11:20:38 INFO Executor: Running task 87.0 in stage 7.0 (TID 934)
15/08/21 11:20:38 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 917) in 6544 ms on localhost (72/170)
15/08/21 11:20:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 257709471 length: 123491743 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
15/08/21 11:20:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
15/08/21 11:20:38 INFO Executor: Finished task 71.0 in stage 7.0 (TID 918). 2125 bytes result sent to driver
15/08/21 11:20:38 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 935, localhost, ANY, 1760 bytes)
15/08/21 11:20:38 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 918) in 6729 ms on localhost (73/170)
15/08/21 11:20:38 INFO InternalParquetRecordReader: Assembled and processed 3501462 records from 2 columns in 5191 ms: 674.5255 rec/ms, 1349.051 cell/ms
15/08/21 11:20:38 INFO InternalParquetRecordReader: time spent so far 1% reading (89 ms) and 98% processing (5191 ms)
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 3501462. reading next block
15/08/21 11:20:38 INFO Executor: Running task 88.0 in stage 7.0 (TID 935)
15/08/21 11:20:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 126306
15/08/21 11:20:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:38 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6757 ms: 517.99615 rec/ms, 1035.9923 cell/ms
15/08/21 11:20:38 INFO InternalParquetRecordReader: time spent so far 1% reading (71 ms) and 98% processing (6757 ms)
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 73975
15/08/21 11:20:38 INFO Executor: Finished task 74.0 in stage 7.0 (TID 921). 2125 bytes result sent to driver
15/08/21 11:20:38 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 936, localhost, ANY, 1772 bytes)
15/08/21 11:20:38 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 921) in 6215 ms on localhost (74/170)
15/08/21 11:20:38 INFO Executor: Running task 89.0 in stage 7.0 (TID 936)
15/08/21 11:20:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 259741355 length: 125523627 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 3500100
15/08/21 11:20:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627697 records.
15/08/21 11:20:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:38 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 3500100
15/08/21 11:20:39 INFO Executor: Finished task 69.0 in stage 7.0 (TID 916). 2125 bytes result sent to driver
15/08/21 11:20:39 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 937, localhost, ANY, 1761 bytes)
15/08/21 11:20:39 INFO Executor: Running task 90.0 in stage 7.0 (TID 937)
15/08/21 11:20:39 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 916) in 7433 ms on localhost (75/170)
15/08/21 11:20:39 INFO Executor: Finished task 75.0 in stage 7.0 (TID 922). 2125 bytes result sent to driver
15/08/21 11:20:39 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 938, localhost, ANY, 1776 bytes)
15/08/21 11:20:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:39 INFO Executor: Running task 91.0 in stage 7.0 (TID 938)
15/08/21 11:20:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:39 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 922) in 5938 ms on localhost (76/170)
15/08/21 11:20:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 134217728 end: 257333395 length: 123115667 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573967 records.
15/08/21 11:20:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:39 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 3500100
15/08/21 11:20:39 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
15/08/21 11:20:39 INFO Executor: Finished task 76.0 in stage 7.0 (TID 923). 2125 bytes result sent to driver
15/08/21 11:20:39 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 939, localhost, ANY, 1761 bytes)
15/08/21 11:20:39 INFO Executor: Running task 92.0 in stage 7.0 (TID 939)
15/08/21 11:20:39 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 923) in 6291 ms on localhost (77/170)
15/08/21 11:20:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:39 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 3500100
15/08/21 11:20:40 INFO InternalParquetRecordReader: Assembled and processed 3501302 records from 2 columns in 5826 ms: 600.9787 rec/ms, 1201.9574 cell/ms
15/08/21 11:20:40 INFO InternalParquetRecordReader: time spent so far 1% reading (65 ms) and 98% processing (5826 ms)
15/08/21 11:20:40 INFO InternalParquetRecordReader: at row 3501302. reading next block
15/08/21 11:20:40 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 126410
15/08/21 11:20:40 INFO Executor: Finished task 77.0 in stage 7.0 (TID 924). 2125 bytes result sent to driver
15/08/21 11:20:40 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 940, localhost, ANY, 1776 bytes)
15/08/21 11:20:40 INFO Executor: Running task 93.0 in stage 7.0 (TID 940)
15/08/21 11:20:40 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 924) in 6345 ms on localhost (78/170)
15/08/21 11:20:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 257330329 length: 123112601 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574008 records.
15/08/21 11:20:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:40 INFO InternalParquetRecordReader: block read in memory in 120 ms. row count = 3500100
15/08/21 11:20:40 INFO Executor: Finished task 78.0 in stage 7.0 (TID 925). 2125 bytes result sent to driver
15/08/21 11:20:40 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 941, localhost, ANY, 1762 bytes)
15/08/21 11:20:40 INFO Executor: Running task 94.0 in stage 7.0 (TID 941)
15/08/21 11:20:40 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 925) in 5791 ms on localhost (79/170)
15/08/21 11:20:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:41 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 3500100
15/08/21 11:20:41 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5835 ms: 599.84576 rec/ms, 1199.6915 cell/ms
15/08/21 11:20:41 INFO InternalParquetRecordReader: time spent so far 1% reading (68 ms) and 98% processing (5835 ms)
15/08/21 11:20:41 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:41 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 71989
15/08/21 11:20:41 INFO Executor: Finished task 80.0 in stage 7.0 (TID 927). 2125 bytes result sent to driver
15/08/21 11:20:41 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 942, localhost, ANY, 1772 bytes)
15/08/21 11:20:41 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 927) in 5852 ms on localhost (80/170)
15/08/21 11:20:41 INFO Executor: Running task 95.0 in stage 7.0 (TID 942)
15/08/21 11:20:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 134217728 end: 257756022 length: 123538294 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574308 records.
15/08/21 11:20:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:41 INFO Executor: Finished task 79.0 in stage 7.0 (TID 926). 2125 bytes result sent to driver
15/08/21 11:20:41 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 943, localhost, ANY, 1761 bytes)
15/08/21 11:20:41 INFO Executor: Running task 96.0 in stage 7.0 (TID 943)
15/08/21 11:20:41 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 926) in 6532 ms on localhost (81/170)
15/08/21 11:20:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:41 INFO InternalParquetRecordReader: block read in memory in 106 ms. row count = 3500100
15/08/21 11:20:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:41 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
15/08/21 11:20:42 INFO InternalParquetRecordReader: Assembled and processed 3503231 records from 2 columns in 5296 ms: 661.4862 rec/ms, 1322.9724 cell/ms
15/08/21 11:20:42 INFO InternalParquetRecordReader: time spent so far 9% reading (528 ms) and 90% processing (5296 ms)
15/08/21 11:20:42 INFO InternalParquetRecordReader: at row 3503231. reading next block
15/08/21 11:20:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 71350
15/08/21 11:20:42 INFO InternalParquetRecordReader: Assembled and processed 3503274 records from 2 columns in 4276 ms: 819.28766 rec/ms, 1638.5753 cell/ms
15/08/21 11:20:42 INFO InternalParquetRecordReader: time spent so far 2% reading (113 ms) and 97% processing (4276 ms)
15/08/21 11:20:42 INFO InternalParquetRecordReader: at row 3503274. reading next block
15/08/21 11:20:42 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 71042
15/08/21 11:20:42 INFO Executor: Finished task 81.0 in stage 7.0 (TID 928). 2125 bytes result sent to driver
15/08/21 11:20:42 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 944, localhost, ANY, 1774 bytes)
15/08/21 11:20:42 INFO Executor: Running task 97.0 in stage 7.0 (TID 944)
15/08/21 11:20:42 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 928) in 6361 ms on localhost (82/170)
15/08/21 11:20:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 257832393 length: 123614665 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573690 records.
15/08/21 11:20:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:42 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500612
15/08/21 11:20:42 INFO Executor: Finished task 83.0 in stage 7.0 (TID 930). 2125 bytes result sent to driver
15/08/21 11:20:42 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 945, localhost, ANY, 1762 bytes)
15/08/21 11:20:42 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 930) in 4988 ms on localhost (83/170)
15/08/21 11:20:42 INFO Executor: Running task 98.0 in stage 7.0 (TID 945)
15/08/21 11:20:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501124 records.
15/08/21 11:20:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:42 INFO Executor: Finished task 84.0 in stage 7.0 (TID 931). 2125 bytes result sent to driver
15/08/21 11:20:42 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 946, localhost, ANY, 1775 bytes)
15/08/21 11:20:42 INFO Executor: Running task 99.0 in stage 7.0 (TID 946)
15/08/21 11:20:42 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 931) in 4855 ms on localhost (84/170)
15/08/21 11:20:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 134217728 end: 257486081 length: 123268353 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573157 records.
15/08/21 11:20:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:42 INFO InternalParquetRecordReader: block read in memory in 52 ms. row count = 3501124
15/08/21 11:20:43 INFO InternalParquetRecordReader: block read in memory in 361 ms. row count = 3500100
15/08/21 11:20:43 INFO Executor: Finished task 82.0 in stage 7.0 (TID 929). 2125 bytes result sent to driver
15/08/21 11:20:43 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 947, localhost, ANY, 1761 bytes)
15/08/21 11:20:43 INFO Executor: Running task 100.0 in stage 7.0 (TID 947)
15/08/21 11:20:43 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 929) in 5814 ms on localhost (85/170)
15/08/21 11:20:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:43 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4748 ms: 737.1735 rec/ms, 1474.347 cell/ms
15/08/21 11:20:43 INFO InternalParquetRecordReader: time spent so far 1% reading (63 ms) and 98% processing (4748 ms)
15/08/21 11:20:43 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:43 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 74054
15/08/21 11:20:43 INFO InternalParquetRecordReader: block read in memory in 76 ms. row count = 3500100
15/08/21 11:20:43 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4942 ms: 708.23553 rec/ms, 1416.4711 cell/ms
15/08/21 11:20:43 INFO InternalParquetRecordReader: time spent so far 1% reading (69 ms) and 98% processing (4942 ms)
15/08/21 11:20:43 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:43 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 74156
15/08/21 11:20:43 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4728 ms: 740.2919 rec/ms, 1480.5837 cell/ms
15/08/21 11:20:43 INFO InternalParquetRecordReader: time spent so far 0% reading (43 ms) and 99% processing (4728 ms)
15/08/21 11:20:43 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:43 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 127597
15/08/21 11:20:44 INFO Executor: Finished task 86.0 in stage 7.0 (TID 933). 2125 bytes result sent to driver
15/08/21 11:20:44 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 948, localhost, ANY, 1773 bytes)
15/08/21 11:20:44 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 933) in 6279 ms on localhost (86/170)
15/08/21 11:20:44 INFO Executor: Running task 101.0 in stage 7.0 (TID 948)
15/08/21 11:20:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 134217728 end: 257368738 length: 123151010 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
15/08/21 11:20:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:44 INFO Executor: Finished task 87.0 in stage 7.0 (TID 934). 2125 bytes result sent to driver
15/08/21 11:20:44 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 949, localhost, ANY, 1762 bytes)
15/08/21 11:20:44 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 934) in 6334 ms on localhost (87/170)
15/08/21 11:20:44 INFO Executor: Running task 102.0 in stage 7.0 (TID 949)
15/08/21 11:20:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:44 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 3500100
15/08/21 11:20:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:44 INFO Executor: Finished task 85.0 in stage 7.0 (TID 932). 2125 bytes result sent to driver
15/08/21 11:20:44 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 950, localhost, ANY, 1774 bytes)
15/08/21 11:20:44 INFO Executor: Running task 103.0 in stage 7.0 (TID 950)
15/08/21 11:20:44 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 932) in 6528 ms on localhost (88/170)
15/08/21 11:20:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 134217728 end: 257317174 length: 123099446 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:44 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 3500100
15/08/21 11:20:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573826 records.
15/08/21 11:20:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:44 INFO Executor: Finished task 89.0 in stage 7.0 (TID 936). 2125 bytes result sent to driver
15/08/21 11:20:44 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 951, localhost, ANY, 1761 bytes)
15/08/21 11:20:44 INFO Executor: Running task 104.0 in stage 7.0 (TID 951)
15/08/21 11:20:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:44 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 936) in 6123 ms on localhost (89/170)
15/08/21 11:20:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500786 records.
15/08/21 11:20:45 INFO InternalParquetRecordReader: block read in memory in 85 ms. row count = 3500100
15/08/21 11:20:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:45 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500786
15/08/21 11:20:45 INFO Executor: Finished task 88.0 in stage 7.0 (TID 935). 2125 bytes result sent to driver
15/08/21 11:20:45 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 952, localhost, ANY, 1773 bytes)
15/08/21 11:20:45 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 935) in 6384 ms on localhost (90/170)
15/08/21 11:20:45 INFO Executor: Running task 105.0 in stage 7.0 (TID 952)
15/08/21 11:20:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 257566042 length: 123348314 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573221 records.
15/08/21 11:20:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:45 INFO Executor: Finished task 90.0 in stage 7.0 (TID 937). 2125 bytes result sent to driver
15/08/21 11:20:45 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 953, localhost, ANY, 1762 bytes)
15/08/21 11:20:45 INFO Executor: Running task 106.0 in stage 7.0 (TID 953)
15/08/21 11:20:45 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 937) in 5860 ms on localhost (91/170)
15/08/21 11:20:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502806 records.
15/08/21 11:20:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:45 INFO InternalParquetRecordReader: block read in memory in 93 ms. row count = 3501171
15/08/21 11:20:45 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 3502806
15/08/21 11:20:45 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5896 ms: 593.6398 rec/ms, 1187.2795 cell/ms
15/08/21 11:20:45 INFO InternalParquetRecordReader: time spent so far 1% reading (73 ms) and 98% processing (5896 ms)
15/08/21 11:20:45 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:45 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73867
15/08/21 11:20:45 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4583 ms: 763.71375 rec/ms, 1527.4275 cell/ms
15/08/21 11:20:45 INFO InternalParquetRecordReader: time spent so far 2% reading (120 ms) and 97% processing (4583 ms)
15/08/21 11:20:45 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:45 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 73908
15/08/21 11:20:45 INFO Executor: Finished task 92.0 in stage 7.0 (TID 939). 2125 bytes result sent to driver
15/08/21 11:20:45 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 954, localhost, ANY, 1775 bytes)
15/08/21 11:20:45 INFO Executor: Running task 107.0 in stage 7.0 (TID 954)
15/08/21 11:20:45 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 939) in 5872 ms on localhost (92/170)
15/08/21 11:20:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 134217728 end: 257458240 length: 123240512 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571385 records.
15/08/21 11:20:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:45 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 3500100
15/08/21 11:20:45 INFO Executor: Finished task 93.0 in stage 7.0 (TID 940). 2125 bytes result sent to driver
15/08/21 11:20:45 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 955, localhost, ANY, 1762 bytes)
15/08/21 11:20:45 INFO Executor: Running task 108.0 in stage 7.0 (TID 955)
15/08/21 11:20:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:46 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 940) in 5354 ms on localhost (93/170)
15/08/21 11:20:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:46 INFO Executor: Finished task 91.0 in stage 7.0 (TID 938). 2125 bytes result sent to driver
15/08/21 11:20:46 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 956, localhost, ANY, 1773 bytes)
15/08/21 11:20:46 INFO Executor: Running task 109.0 in stage 7.0 (TID 956)
15/08/21 11:20:46 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 938) in 6674 ms on localhost (94/170)
15/08/21 11:20:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 134217728 end: 257458739 length: 123241011 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574219 records.
15/08/21 11:20:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:46 INFO InternalParquetRecordReader: block read in memory in 76 ms. row count = 3501364
15/08/21 11:20:46 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 3500100
15/08/21 11:20:46 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5178 ms: 675.956 rec/ms, 1351.912 cell/ms
15/08/21 11:20:46 INFO InternalParquetRecordReader: time spent so far 2% reading (106 ms) and 97% processing (5178 ms)
15/08/21 11:20:46 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:46 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 74208
15/08/21 11:20:47 INFO InternalParquetRecordReader: Assembled and processed 3500612 records from 2 columns in 5046 ms: 693.74 rec/ms, 1387.48 cell/ms
15/08/21 11:20:47 INFO InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (5046 ms)
15/08/21 11:20:47 INFO InternalParquetRecordReader: at row 3500612. reading next block
15/08/21 11:20:47 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73078
15/08/21 11:20:47 INFO Executor: Finished task 96.0 in stage 7.0 (TID 943). 2125 bytes result sent to driver
15/08/21 11:20:47 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 957, localhost, ANY, 1762 bytes)
15/08/21 11:20:47 INFO Executor: Running task 110.0 in stage 7.0 (TID 957)
15/08/21 11:20:47 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 943) in 5995 ms on localhost (95/170)
15/08/21 11:20:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:47 INFO Executor: Finished task 95.0 in stage 7.0 (TID 942). 2125 bytes result sent to driver
15/08/21 11:20:47 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 958, localhost, ANY, 1774 bytes)
15/08/21 11:20:47 INFO Executor: Running task 111.0 in stage 7.0 (TID 958)
15/08/21 11:20:47 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 942) in 6195 ms on localhost (96/170)
15/08/21 11:20:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 134217728 end: 257439181 length: 123221453 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501448 records.
15/08/21 11:20:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572926 records.
15/08/21 11:20:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:47 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 3501448
15/08/21 11:20:47 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3500100
15/08/21 11:20:48 INFO Executor: Finished task 98.0 in stage 7.0 (TID 945). 2125 bytes result sent to driver
15/08/21 11:20:48 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 959, localhost, ANY, 1762 bytes)
15/08/21 11:20:48 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 945) in 5589 ms on localhost (97/170)
15/08/21 11:20:48 INFO Executor: Running task 112.0 in stage 7.0 (TID 959)
15/08/21 11:20:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:48 INFO Executor: Finished task 97.0 in stage 7.0 (TID 944). 2125 bytes result sent to driver
15/08/21 11:20:48 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 960, localhost, ANY, 1773 bytes)
15/08/21 11:20:48 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 944) in 5738 ms on localhost (98/170)
15/08/21 11:20:48 INFO Executor: Running task 113.0 in stage 7.0 (TID 960)
15/08/21 11:20:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 134217728 end: 181459518 length: 47241790 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1466882 records.
15/08/21 11:20:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:48 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 1466882
15/08/21 11:20:48 INFO InternalParquetRecordReader: block read in memory in 115 ms. row count = 3500100
15/08/21 11:20:48 INFO Executor: Finished task 94.0 in stage 7.0 (TID 941). 2125 bytes result sent to driver
15/08/21 11:20:48 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 961, localhost, ANY, 1761 bytes)
15/08/21 11:20:48 INFO Executor: Running task 114.0 in stage 7.0 (TID 961)
15/08/21 11:20:48 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 941) in 7563 ms on localhost (99/170)
15/08/21 11:20:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:48 INFO InternalParquetRecordReader: block read in memory in 97 ms. row count = 3500100
15/08/21 11:20:48 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5749 ms: 608.8189 rec/ms, 1217.6378 cell/ms
15/08/21 11:20:48 INFO InternalParquetRecordReader: time spent so far 5% reading (361 ms) and 94% processing (5749 ms)
15/08/21 11:20:48 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:48 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 73057
15/08/21 11:20:49 INFO Executor: Finished task 100.0 in stage 7.0 (TID 947). 2125 bytes result sent to driver
15/08/21 11:20:49 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 962, localhost, ANY, 1772 bytes)
15/08/21 11:20:49 INFO Executor: Running task 115.0 in stage 7.0 (TID 962)
15/08/21 11:20:49 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 947) in 5809 ms on localhost (100/170)
15/08/21 11:20:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 259458210 length: 125240482 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627682 records.
15/08/21 11:20:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:49 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 3503132
15/08/21 11:20:49 INFO Executor: Finished task 99.0 in stage 7.0 (TID 946). 2125 bytes result sent to driver
15/08/21 11:20:49 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 963, localhost, ANY, 1761 bytes)
15/08/21 11:20:49 INFO Executor: Running task 116.0 in stage 7.0 (TID 963)
15/08/21 11:20:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:49 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 946) in 6769 ms on localhost (101/170)
15/08/21 11:20:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:49 INFO InternalParquetRecordReader: block read in memory in 113 ms. row count = 3500100
15/08/21 11:20:49 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5018 ms: 697.509 rec/ms, 1395.018 cell/ms
15/08/21 11:20:49 INFO InternalParquetRecordReader: time spent so far 0% reading (34 ms) and 99% processing (5018 ms)
15/08/21 11:20:49 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:49 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 73871
15/08/21 11:20:51 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6168 ms: 567.46106 rec/ms, 1134.9221 cell/ms
15/08/21 11:20:51 INFO InternalParquetRecordReader: time spent so far 1% reading (85 ms) and 98% processing (6168 ms)
15/08/21 11:20:51 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:51 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 73726
15/08/21 11:20:51 INFO InternalParquetRecordReader: Assembled and processed 3501171 records from 2 columns in 6308 ms: 555.0366 rec/ms, 1110.0732 cell/ms
15/08/21 11:20:51 INFO InternalParquetRecordReader: time spent so far 1% reading (93 ms) and 98% processing (6308 ms)
15/08/21 11:20:51 INFO InternalParquetRecordReader: at row 3501171. reading next block
15/08/21 11:20:51 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 72050
15/08/21 11:20:51 INFO Executor: Finished task 102.0 in stage 7.0 (TID 949). 2125 bytes result sent to driver
15/08/21 11:20:51 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 964, localhost, ANY, 1772 bytes)
15/08/21 11:20:51 INFO Executor: Running task 117.0 in stage 7.0 (TID 964)
15/08/21 11:20:51 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 949) in 6897 ms on localhost (102/170)
15/08/21 11:20:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 257467186 length: 123249458 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:51 INFO Executor: Finished task 101.0 in stage 7.0 (TID 948). 2125 bytes result sent to driver
15/08/21 11:20:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:51 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 948) in 6985 ms on localhost (103/170)
15/08/21 11:20:51 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 965, localhost, ANY, 1760 bytes)
15/08/21 11:20:51 INFO Executor: Running task 118.0 in stage 7.0 (TID 965)
15/08/21 11:20:51 INFO Executor: Finished task 104.0 in stage 7.0 (TID 951). 2125 bytes result sent to driver
15/08/21 11:20:51 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 966, localhost, ANY, 1772 bytes)
15/08/21 11:20:51 INFO Executor: Running task 119.0 in stage 7.0 (TID 966)
15/08/21 11:20:51 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 951) in 6769 ms on localhost (104/170)
15/08/21 11:20:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 260141700 length: 125923972 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574192 records.
15/08/21 11:20:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3648307 records.
15/08/21 11:20:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500631 records.
15/08/21 11:20:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:51 INFO InternalParquetRecordReader: block read in memory in 94 ms. row count = 3501130
15/08/21 11:20:51 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3500631
15/08/21 11:20:51 INFO Executor: Finished task 113.0 in stage 7.0 (TID 960). 2125 bytes result sent to driver
15/08/21 11:20:51 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 967, localhost, ANY, 1760 bytes)
15/08/21 11:20:51 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 960) in 3729 ms on localhost (105/170)
15/08/21 11:20:51 INFO Executor: Running task 120.0 in stage 7.0 (TID 967)
15/08/21 11:20:51 INFO InternalParquetRecordReader: block read in memory in 117 ms. row count = 3500100
15/08/21 11:20:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501115 records.
15/08/21 11:20:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:52 INFO Executor: Finished task 106.0 in stage 7.0 (TID 953). 2125 bytes result sent to driver
15/08/21 11:20:52 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 968, localhost, ANY, 1772 bytes)
15/08/21 11:20:52 INFO Executor: Running task 121.0 in stage 7.0 (TID 968)
15/08/21 11:20:52 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 953) in 6848 ms on localhost (106/170)
15/08/21 11:20:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 259582440 length: 125364712 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:52 INFO InternalParquetRecordReader: block read in memory in 51 ms. row count = 3501115
15/08/21 11:20:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626900 records.
15/08/21 11:20:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:52 INFO Executor: Finished task 103.0 in stage 7.0 (TID 950). 2125 bytes result sent to driver
15/08/21 11:20:52 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 969, localhost, ANY, 1761 bytes)
15/08/21 11:20:52 INFO Executor: Running task 122.0 in stage 7.0 (TID 969)
15/08/21 11:20:52 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 950) in 7230 ms on localhost (107/170)
15/08/21 11:20:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:52 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 3501235
15/08/21 11:20:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:52 INFO Executor: Finished task 105.0 in stage 7.0 (TID 952). 2125 bytes result sent to driver
15/08/21 11:20:52 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 970, localhost, ANY, 1774 bytes)
15/08/21 11:20:52 INFO Executor: Running task 123.0 in stage 7.0 (TID 970)
15/08/21 11:20:52 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 952) in 7142 ms on localhost (108/170)
15/08/21 11:20:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 257838232 length: 123620504 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574036 records.
15/08/21 11:20:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:52 INFO InternalParquetRecordReader: block read in memory in 113 ms. row count = 3500100
15/08/21 11:20:52 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501487
15/08/21 11:20:52 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6524 ms: 536.49603 rec/ms, 1072.9921 cell/ms
15/08/21 11:20:52 INFO InternalParquetRecordReader: time spent so far 1% reading (72 ms) and 98% processing (6524 ms)
15/08/21 11:20:52 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:52 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 71285
15/08/21 11:20:52 INFO Executor: Finished task 107.0 in stage 7.0 (TID 954). 2125 bytes result sent to driver
15/08/21 11:20:52 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 971, localhost, ANY, 1761 bytes)
15/08/21 11:20:52 INFO Executor: Running task 124.0 in stage 7.0 (TID 971)
15/08/21 11:20:52 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 954) in 7178 ms on localhost (109/170)
15/08/21 11:20:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501235 records.
15/08/21 11:20:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:52 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 3501235
15/08/21 11:20:54 INFO Executor: Finished task 108.0 in stage 7.0 (TID 955). 2125 bytes result sent to driver
15/08/21 11:20:54 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 972, localhost, ANY, 1773 bytes)
15/08/21 11:20:54 INFO Executor: Running task 125.0 in stage 7.0 (TID 972)
15/08/21 11:20:54 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 955) in 8478 ms on localhost (110/170)
15/08/21 11:20:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 257849235 length: 123631507 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573259 records.
15/08/21 11:20:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:54 INFO InternalParquetRecordReader: Assembled and processed 3501364 records from 2 columns in 8329 ms: 420.3823 rec/ms, 840.7646 cell/ms
15/08/21 11:20:54 INFO InternalParquetRecordReader: time spent so far 0% reading (76 ms) and 99% processing (8329 ms)
15/08/21 11:20:54 INFO InternalParquetRecordReader: at row 3501364. reading next block
15/08/21 11:20:54 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 72855
15/08/21 11:20:54 INFO InternalParquetRecordReader: block read in memory in 149 ms. row count = 3500100
15/08/21 11:20:55 INFO Executor: Finished task 109.0 in stage 7.0 (TID 956). 2125 bytes result sent to driver
15/08/21 11:20:55 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 973, localhost, ANY, 1761 bytes)
15/08/21 11:20:55 INFO Executor: Running task 126.0 in stage 7.0 (TID 973)
15/08/21 11:20:55 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 956) in 9173 ms on localhost (111/170)
15/08/21 11:20:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502933 records.
15/08/21 11:20:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:55 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 3502933
15/08/21 11:20:55 INFO Executor: Finished task 110.0 in stage 7.0 (TID 957). 2125 bytes result sent to driver
15/08/21 11:20:55 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 974, localhost, ANY, 1774 bytes)
15/08/21 11:20:55 INFO Executor: Running task 127.0 in stage 7.0 (TID 974)
15/08/21 11:20:55 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 957) in 7778 ms on localhost (112/170)
15/08/21 11:20:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 134217728 end: 257181348 length: 122963620 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571471 records.
15/08/21 11:20:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:55 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3501332
15/08/21 11:20:55 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 8044 ms: 435.11935 rec/ms, 870.2387 cell/ms
15/08/21 11:20:55 INFO InternalParquetRecordReader: time spent so far 0% reading (75 ms) and 99% processing (8044 ms)
15/08/21 11:20:55 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:55 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72826
15/08/21 11:20:55 INFO Executor: Finished task 112.0 in stage 7.0 (TID 959). 2125 bytes result sent to driver
15/08/21 11:20:55 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 975, localhost, ANY, 1760 bytes)
15/08/21 11:20:55 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 959) in 7830 ms on localhost (113/170)
15/08/21 11:20:55 INFO Executor: Running task 128.0 in stage 7.0 (TID 975)
15/08/21 11:20:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500833 records.
15/08/21 11:20:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:56 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 3500833
15/08/21 11:20:56 INFO InternalParquetRecordReader: Assembled and processed 3503132 records from 2 columns in 7192 ms: 487.0873 rec/ms, 974.1746 cell/ms
15/08/21 11:20:56 INFO InternalParquetRecordReader: time spent so far 0% reading (72 ms) and 99% processing (7192 ms)
15/08/21 11:20:56 INFO InternalParquetRecordReader: at row 3503132. reading next block
15/08/21 11:20:56 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 124550
15/08/21 11:20:56 INFO Executor: Finished task 111.0 in stage 7.0 (TID 958). 2125 bytes result sent to driver
15/08/21 11:20:56 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 976, localhost, ANY, 1772 bytes)
15/08/21 11:20:56 INFO Executor: Running task 129.0 in stage 7.0 (TID 976)
15/08/21 11:20:56 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 958) in 8750 ms on localhost (114/170)
15/08/21 11:20:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 257473792 length: 123256064 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573329 records.
15/08/21 11:20:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:56 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 3500100
15/08/21 11:20:57 INFO Executor: Finished task 115.0 in stage 7.0 (TID 962). 2125 bytes result sent to driver
15/08/21 11:20:57 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 977, localhost, ANY, 1761 bytes)
15/08/21 11:20:57 INFO Executor: Running task 130.0 in stage 7.0 (TID 977)
15/08/21 11:20:57 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 962) in 8045 ms on localhost (115/170)
15/08/21 11:20:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502559 records.
15/08/21 11:20:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:57 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 3502559
15/08/21 11:20:57 INFO Executor: Finished task 114.0 in stage 7.0 (TID 961). 2125 bytes result sent to driver
15/08/21 11:20:57 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 978, localhost, ANY, 1774 bytes)
15/08/21 11:20:57 INFO Executor: Running task 131.0 in stage 7.0 (TID 978)
15/08/21 11:20:57 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 961) in 9034 ms on localhost (116/170)
15/08/21 11:20:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 257455806 length: 123238078 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571559 records.
15/08/21 11:20:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:57 INFO Executor: Finished task 116.0 in stage 7.0 (TID 963). 2125 bytes result sent to driver
15/08/21 11:20:57 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 979, localhost, ANY, 1762 bytes)
15/08/21 11:20:57 INFO Executor: Running task 132.0 in stage 7.0 (TID 979)
15/08/21 11:20:57 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 963) in 8205 ms on localhost (117/170)
15/08/21 11:20:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:57 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5673 ms: 616.97516 rec/ms, 1233.9503 cell/ms
15/08/21 11:20:57 INFO InternalParquetRecordReader: time spent so far 2% reading (117 ms) and 97% processing (5673 ms)
15/08/21 11:20:57 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:20:57 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 148207
15/08/21 11:20:57 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
15/08/21 11:20:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501043 records.
15/08/21 11:20:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:57 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 3501043
15/08/21 11:20:58 INFO InternalParquetRecordReader: Assembled and processed 3501235 records from 2 columns in 5869 ms: 596.56415 rec/ms, 1193.1283 cell/ms
15/08/21 11:20:58 INFO InternalParquetRecordReader: time spent so far 1% reading (60 ms) and 98% processing (5869 ms)
15/08/21 11:20:58 INFO InternalParquetRecordReader: at row 3501235. reading next block
15/08/21 11:20:58 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 125665
15/08/21 11:20:58 INFO InternalParquetRecordReader: Assembled and processed 3501130 records from 2 columns in 6315 ms: 554.41486 rec/ms, 1108.8297 cell/ms
15/08/21 11:20:58 INFO InternalParquetRecordReader: time spent so far 1% reading (94 ms) and 98% processing (6315 ms)
15/08/21 11:20:58 INFO InternalParquetRecordReader: at row 3501130. reading next block
15/08/21 11:20:58 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73062
15/08/21 11:20:58 INFO Executor: Finished task 119.0 in stage 7.0 (TID 966). 2125 bytes result sent to driver
15/08/21 11:20:58 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 980, localhost, ANY, 1776 bytes)
15/08/21 11:20:58 INFO Executor: Running task 133.0 in stage 7.0 (TID 980)
15/08/21 11:20:58 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 966) in 6625 ms on localhost (118/170)
15/08/21 11:20:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 134217728 end: 257547934 length: 123330206 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573366 records.
15/08/21 11:20:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:58 INFO Executor: Finished task 120.0 in stage 7.0 (TID 967). 2125 bytes result sent to driver
15/08/21 11:20:58 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 981, localhost, ANY, 1762 bytes)
15/08/21 11:20:58 INFO Executor: Running task 134.0 in stage 7.0 (TID 981)
15/08/21 11:20:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:58 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 967) in 6618 ms on localhost (119/170)
15/08/21 11:20:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:58 INFO InternalParquetRecordReader: block read in memory in 119 ms. row count = 3501786
15/08/21 11:20:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:20:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:58 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
15/08/21 11:20:58 INFO InternalParquetRecordReader: Assembled and processed 3501487 records from 2 columns in 6429 ms: 544.63947 rec/ms, 1089.2789 cell/ms
15/08/21 11:20:58 INFO InternalParquetRecordReader: time spent so far 0% reading (46 ms) and 99% processing (6429 ms)
15/08/21 11:20:58 INFO InternalParquetRecordReader: at row 3501487. reading next block
15/08/21 11:20:58 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 72549
15/08/21 11:20:59 INFO Executor: Finished task 117.0 in stage 7.0 (TID 964). 2125 bytes result sent to driver
15/08/21 11:20:59 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 982, localhost, ANY, 1773 bytes)
15/08/21 11:20:59 INFO Executor: Running task 135.0 in stage 7.0 (TID 982)
15/08/21 11:20:59 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 964) in 7416 ms on localhost (120/170)
15/08/21 11:20:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 257790576 length: 123572848 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574145 records.
15/08/21 11:20:59 INFO Executor: Finished task 121.0 in stage 7.0 (TID 968). 2125 bytes result sent to driver
15/08/21 11:20:59 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 983, localhost, ANY, 1761 bytes)
15/08/21 11:20:59 INFO Executor: Running task 136.0 in stage 7.0 (TID 983)
15/08/21 11:20:59 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 968) in 7099 ms on localhost (121/170)
15/08/21 11:20:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501169 records.
15/08/21 11:20:59 INFO InternalParquetRecordReader: block read in memory in 119 ms. row count = 3500100
15/08/21 11:20:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:59 INFO InternalParquetRecordReader: block read in memory in 107 ms. row count = 3501169
15/08/21 11:20:59 INFO Executor: Finished task 118.0 in stage 7.0 (TID 965). 2125 bytes result sent to driver
15/08/21 11:20:59 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 984, localhost, ANY, 1773 bytes)
15/08/21 11:20:59 INFO Executor: Running task 137.0 in stage 7.0 (TID 984)
15/08/21 11:20:59 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 965) in 7737 ms on localhost (122/170)
15/08/21 11:20:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 134217728 end: 257571649 length: 123353921 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573313 records.
15/08/21 11:20:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:59 INFO Executor: Finished task 123.0 in stage 7.0 (TID 970). 2125 bytes result sent to driver
15/08/21 11:20:59 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 985, localhost, ANY, 1761 bytes)
15/08/21 11:20:59 INFO Executor: Running task 138.0 in stage 7.0 (TID 985)
15/08/21 11:20:59 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 970) in 7279 ms on localhost (123/170)
15/08/21 11:20:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501195 records.
15/08/21 11:20:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:59 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 3501584
15/08/21 11:20:59 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 3501195
15/08/21 11:20:59 INFO Executor: Finished task 124.0 in stage 7.0 (TID 971). 2125 bytes result sent to driver
15/08/21 11:20:59 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 986, localhost, ANY, 1774 bytes)
15/08/21 11:20:59 INFO Executor: Running task 139.0 in stage 7.0 (TID 986)
15/08/21 11:20:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 257573201 length: 123355473 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:20:59 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 971) in 6755 ms on localhost (124/170)
15/08/21 11:20:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:20:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572913 records.
15/08/21 11:20:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:20:59 INFO InternalParquetRecordReader: block read in memory in 109 ms. row count = 3500728
15/08/21 11:21:00 INFO Executor: Finished task 122.0 in stage 7.0 (TID 969). 2125 bytes result sent to driver
15/08/21 11:21:00 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 987, localhost, ANY, 1761 bytes)
15/08/21 11:21:00 INFO Executor: Running task 140.0 in stage 7.0 (TID 987)
15/08/21 11:21:00 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 969) in 8447 ms on localhost (125/170)
15/08/21 11:21:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501339 records.
15/08/21 11:21:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:00 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 3501339
15/08/21 11:21:01 INFO Executor: Finished task 126.0 in stage 7.0 (TID 973). 2125 bytes result sent to driver
15/08/21 11:21:01 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 988, localhost, ANY, 1773 bytes)
15/08/21 11:21:01 INFO Executor: Running task 141.0 in stage 7.0 (TID 988)
15/08/21 11:21:01 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 973) in 6291 ms on localhost (126/170)
15/08/21 11:21:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 257466118 length: 123248390 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572985 records.
15/08/21 11:21:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:01 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500100
15/08/21 11:21:01 INFO InternalParquetRecordReader: Assembled and processed 3501332 records from 2 columns in 6181 ms: 566.4669 rec/ms, 1132.9338 cell/ms
15/08/21 11:21:01 INFO InternalParquetRecordReader: time spent so far 1% reading (65 ms) and 98% processing (6181 ms)
15/08/21 11:21:01 INFO InternalParquetRecordReader: at row 3501332. reading next block
15/08/21 11:21:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 70139
15/08/21 11:21:02 INFO Executor: Finished task 127.0 in stage 7.0 (TID 974). 2125 bytes result sent to driver
15/08/21 11:21:02 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 989, localhost, ANY, 1761 bytes)
15/08/21 11:21:02 INFO Executor: Running task 142.0 in stage 7.0 (TID 989)
15/08/21 11:21:02 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 974) in 6826 ms on localhost (127/170)
15/08/21 11:21:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:21:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:02 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 3500100
15/08/21 11:21:02 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 7910 ms: 442.4905 rec/ms, 884.981 cell/ms
15/08/21 11:21:02 INFO InternalParquetRecordReader: time spent so far 1% reading (149 ms) and 98% processing (7910 ms)
15/08/21 11:21:02 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:02 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 73159
15/08/21 11:21:03 INFO Executor: Finished task 130.0 in stage 7.0 (TID 977). 2125 bytes result sent to driver
15/08/21 11:21:03 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 990, localhost, ANY, 1773 bytes)
15/08/21 11:21:03 INFO Executor: Running task 143.0 in stage 7.0 (TID 990)
15/08/21 11:21:03 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 977) in 6343 ms on localhost (128/170)
15/08/21 11:21:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 257888240 length: 123670512 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
15/08/21 11:21:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:03 INFO Executor: Finished task 125.0 in stage 7.0 (TID 972). 2125 bytes result sent to driver
15/08/21 11:21:03 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 991, localhost, ANY, 1761 bytes)
15/08/21 11:21:03 INFO Executor: Running task 144.0 in stage 7.0 (TID 991)
15/08/21 11:21:03 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 3501076
15/08/21 11:21:03 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 972) in 9058 ms on localhost (129/170)
15/08/21 11:21:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:21:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:03 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3500100
15/08/21 11:21:03 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 7104 ms: 492.69424 rec/ms, 985.3885 cell/ms
15/08/21 11:21:03 INFO InternalParquetRecordReader: time spent so far 1% reading (89 ms) and 98% processing (7104 ms)
15/08/21 11:21:03 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:03 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 73229
15/08/21 11:21:04 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6546 ms: 534.69293 rec/ms, 1069.3859 cell/ms
15/08/21 11:21:04 INFO InternalParquetRecordReader: time spent so far 1% reading (70 ms) and 98% processing (6546 ms)
15/08/21 11:21:04 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:04 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 71459
15/08/21 11:21:04 INFO Executor: Finished task 128.0 in stage 7.0 (TID 975). 2125 bytes result sent to driver
15/08/21 11:21:04 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 992, localhost, ANY, 1772 bytes)
15/08/21 11:21:04 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 975) in 8382 ms on localhost (130/170)
15/08/21 11:21:04 INFO Executor: Running task 145.0 in stage 7.0 (TID 992)
15/08/21 11:21:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 134217728 end: 258178393 length: 123960665 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573846 records.
15/08/21 11:21:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:04 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
15/08/21 11:21:04 INFO InternalParquetRecordReader: Assembled and processed 3501786 records from 2 columns in 5970 ms: 586.56384 rec/ms, 1173.1277 cell/ms
15/08/21 11:21:04 INFO InternalParquetRecordReader: time spent so far 1% reading (119 ms) and 98% processing (5970 ms)
15/08/21 11:21:04 INFO InternalParquetRecordReader: at row 3501786. reading next block
15/08/21 11:21:04 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 71580
15/08/21 11:21:04 INFO InternalParquetRecordReader: Assembled and processed 3501584 records from 2 columns in 5179 ms: 676.112 rec/ms, 1352.224 cell/ms
15/08/21 11:21:04 INFO InternalParquetRecordReader: time spent so far 1% reading (95 ms) and 98% processing (5179 ms)
15/08/21 11:21:04 INFO InternalParquetRecordReader: at row 3501584. reading next block
15/08/21 11:21:04 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 71729
15/08/21 11:21:04 INFO Executor: Finished task 129.0 in stage 7.0 (TID 976). 2125 bytes result sent to driver
15/08/21 11:21:04 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 993, localhost, ANY, 1762 bytes)
15/08/21 11:21:04 INFO Executor: Running task 146.0 in stage 7.0 (TID 993)
15/08/21 11:21:04 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 976) in 8396 ms on localhost (131/170)
15/08/21 11:21:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501395 records.
15/08/21 11:21:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:04 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 3501395
15/08/21 11:21:04 INFO Executor: Finished task 134.0 in stage 7.0 (TID 981). 2125 bytes result sent to driver
15/08/21 11:21:04 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 994, localhost, ANY, 1775 bytes)
15/08/21 11:21:04 INFO Executor: Running task 147.0 in stage 7.0 (TID 994)
15/08/21 11:21:04 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 981) in 6466 ms on localhost (132/170)
15/08/21 11:21:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 134217728 end: 257798680 length: 123580952 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572381 records.
15/08/21 11:21:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:05 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500100
15/08/21 11:21:05 INFO InternalParquetRecordReader: Assembled and processed 3500728 records from 2 columns in 4935 ms: 709.3674 rec/ms, 1418.7347 cell/ms
15/08/21 11:21:05 INFO InternalParquetRecordReader: time spent so far 2% reading (109 ms) and 97% processing (4935 ms)
15/08/21 11:21:05 INFO InternalParquetRecordReader: at row 3500728. reading next block
15/08/21 11:21:05 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 72185
15/08/21 11:21:05 INFO Executor: Finished task 131.0 in stage 7.0 (TID 978). 2125 bytes result sent to driver
15/08/21 11:21:05 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 995, localhost, ANY, 1761 bytes)
15/08/21 11:21:05 INFO Executor: Running task 148.0 in stage 7.0 (TID 995)
15/08/21 11:21:05 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 978) in 7868 ms on localhost (133/170)
15/08/21 11:21:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
15/08/21 11:21:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:05 INFO InternalParquetRecordReader: block read in memory in 130 ms. row count = 3501121
15/08/21 11:21:05 INFO Executor: Finished task 132.0 in stage 7.0 (TID 979). 2125 bytes result sent to driver
15/08/21 11:21:05 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 996, localhost, ANY, 1774 bytes)
15/08/21 11:21:05 INFO Executor: Running task 149.0 in stage 7.0 (TID 996)
15/08/21 11:21:05 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 979) in 8029 ms on localhost (134/170)
15/08/21 11:21:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 134217728 end: 257837778 length: 123620050 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573120 records.
15/08/21 11:21:05 INFO Executor: Finished task 133.0 in stage 7.0 (TID 980). 2125 bytes result sent to driver
15/08/21 11:21:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:05 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 997, localhost, ANY, 1762 bytes)
15/08/21 11:21:05 INFO Executor: Running task 150.0 in stage 7.0 (TID 997)
15/08/21 11:21:05 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 980) in 7356 ms on localhost (135/170)
15/08/21 11:21:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:05 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 3500100
15/08/21 11:21:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:21:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:05 INFO Executor: Finished task 137.0 in stage 7.0 (TID 984). 2125 bytes result sent to driver
15/08/21 11:21:05 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 998, localhost, ANY, 1775 bytes)
15/08/21 11:21:05 INFO Executor: Running task 151.0 in stage 7.0 (TID 998)
15/08/21 11:21:05 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 984) in 6377 ms on localhost (136/170)
15/08/21 11:21:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 134217728 end: 257748250 length: 123530522 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:05 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 3500100
15/08/21 11:21:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573993 records.
15/08/21 11:21:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:05 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6552 ms: 534.2033 rec/ms, 1068.4066 cell/ms
15/08/21 11:21:05 INFO InternalParquetRecordReader: time spent so far 1% reading (119 ms) and 98% processing (6552 ms)
15/08/21 11:21:05 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:05 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 74045
15/08/21 11:21:05 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
15/08/21 11:21:05 INFO Executor: Finished task 139.0 in stage 7.0 (TID 986). 2125 bytes result sent to driver
15/08/21 11:21:05 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 999, localhost, ANY, 1761 bytes)
15/08/21 11:21:05 INFO Executor: Running task 152.0 in stage 7.0 (TID 999)
15/08/21 11:21:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:05 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 986) in 6349 ms on localhost (137/170)
15/08/21 11:21:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:05 INFO Executor: Finished task 140.0 in stage 7.0 (TID 987). 2125 bytes result sent to driver
15/08/21 11:21:05 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 1000, localhost, ANY, 1776 bytes)
15/08/21 11:21:05 INFO Executor: Running task 153.0 in stage 7.0 (TID 1000)
15/08/21 11:21:05 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 987) in 5448 ms on localhost (138/170)
15/08/21 11:21:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 257331238 length: 123113510 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:21:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
15/08/21 11:21:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:06 INFO Executor: Finished task 136.0 in stage 7.0 (TID 983). 2125 bytes result sent to driver
15/08/21 11:21:06 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 1001, localhost, ANY, 1762 bytes)
15/08/21 11:21:06 INFO Executor: Running task 154.0 in stage 7.0 (TID 1001)
15/08/21 11:21:06 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 983) in 6921 ms on localhost (139/170)
15/08/21 11:21:06 INFO Executor: Finished task 138.0 in stage 7.0 (TID 985). 2125 bytes result sent to driver
15/08/21 11:21:06 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 1002, localhost, ANY, 1775 bytes)
15/08/21 11:21:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:06 INFO Executor: Running task 155.0 in stage 7.0 (TID 1002)
15/08/21 11:21:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 134217728 end: 257176539 length: 122958811 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:06 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 985) in 6522 ms on localhost (140/170)
15/08/21 11:21:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:06 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 3500100
15/08/21 11:21:06 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 3500100
15/08/21 11:21:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503161 records.
15/08/21 11:21:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571357 records.
15/08/21 11:21:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:06 INFO InternalParquetRecordReader: block read in memory in 47 ms. row count = 3501317
15/08/21 11:21:06 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3503161
15/08/21 11:21:06 INFO Executor: Finished task 135.0 in stage 7.0 (TID 982). 2125 bytes result sent to driver
15/08/21 11:21:06 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 1003, localhost, ANY, 1761 bytes)
15/08/21 11:21:06 INFO Executor: Running task 156.0 in stage 7.0 (TID 1003)
15/08/21 11:21:06 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 982) in 7167 ms on localhost (141/170)
15/08/21 11:21:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501689 records.
15/08/21 11:21:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:06 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3501689
15/08/21 11:21:07 INFO Executor: Finished task 142.0 in stage 7.0 (TID 989). 2125 bytes result sent to driver
15/08/21 11:21:07 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 1004, localhost, ANY, 1774 bytes)
15/08/21 11:21:07 INFO Executor: Running task 157.0 in stage 7.0 (TID 1004)
15/08/21 11:21:07 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 989) in 5338 ms on localhost (142/170)
15/08/21 11:21:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 134217728 end: 257446174 length: 123228446 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572541 records.
15/08/21 11:21:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:07 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 3500100
15/08/21 11:21:07 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6148 ms: 569.30707 rec/ms, 1138.6141 cell/ms
15/08/21 11:21:07 INFO InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (6148 ms)
15/08/21 11:21:07 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:07 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 72885
15/08/21 11:21:08 INFO Executor: Finished task 141.0 in stage 7.0 (TID 988). 2125 bytes result sent to driver
15/08/21 11:21:08 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 1005, localhost, ANY, 1760 bytes)
15/08/21 11:21:08 INFO Executor: Running task 158.0 in stage 7.0 (TID 1005)
15/08/21 11:21:08 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 988) in 6638 ms on localhost (143/170)
15/08/21 11:21:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501046 records.
15/08/21 11:21:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:08 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3501046
15/08/21 11:21:08 INFO InternalParquetRecordReader: Assembled and processed 3501076 records from 2 columns in 4854 ms: 721.2765 rec/ms, 1442.553 cell/ms
15/08/21 11:21:08 INFO InternalParquetRecordReader: time spent so far 1% reading (50 ms) and 98% processing (4854 ms)
15/08/21 11:21:08 INFO InternalParquetRecordReader: at row 3501076. reading next block
15/08/21 11:21:08 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 73262
15/08/21 11:21:08 INFO Executor: Finished task 144.0 in stage 7.0 (TID 991). 2125 bytes result sent to driver
15/08/21 11:21:08 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 1006, localhost, ANY, 1772 bytes)
15/08/21 11:21:08 INFO Executor: Running task 159.0 in stage 7.0 (TID 1006)
15/08/21 11:21:08 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 991) in 4963 ms on localhost (144/170)
15/08/21 11:21:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 259884384 length: 125666656 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626733 records.
15/08/21 11:21:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:08 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 3500100
15/08/21 11:21:08 INFO Executor: Finished task 143.0 in stage 7.0 (TID 990). 2125 bytes result sent to driver
15/08/21 11:21:08 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 1007, localhost, ANY, 1760 bytes)
15/08/21 11:21:08 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 990) in 5355 ms on localhost (145/170)
15/08/21 11:21:08 INFO Executor: Running task 160.0 in stage 7.0 (TID 1007)
15/08/21 11:21:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501221 records.
15/08/21 11:21:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:08 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501221
15/08/21 11:21:08 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4375 ms: 800.0228 rec/ms, 1600.0457 cell/ms
15/08/21 11:21:08 INFO InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (4375 ms)
15/08/21 11:21:08 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:08 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 73746
15/08/21 11:21:09 INFO Executor: Finished task 145.0 in stage 7.0 (TID 992). 2125 bytes result sent to driver
15/08/21 11:21:09 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 1008, localhost, ANY, 1774 bytes)
15/08/21 11:21:09 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 992) in 4968 ms on localhost (146/170)
15/08/21 11:21:09 INFO Executor: Running task 161.0 in stage 7.0 (TID 1008)
15/08/21 11:21:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259183553 length: 124965825 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627071 records.
15/08/21 11:21:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:09 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 3503161
15/08/21 11:21:09 INFO Executor: Finished task 146.0 in stage 7.0 (TID 993). 2125 bytes result sent to driver
15/08/21 11:21:09 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 1009, localhost, ANY, 1761 bytes)
15/08/21 11:21:09 INFO Executor: Running task 162.0 in stage 7.0 (TID 1009)
15/08/21 11:21:09 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 993) in 4778 ms on localhost (147/170)
15/08/21 11:21:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:21:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:09 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 3500100
15/08/21 11:21:09 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4786 ms: 731.3205 rec/ms, 1462.641 cell/ms
15/08/21 11:21:09 INFO InternalParquetRecordReader: time spent so far 0% reading (39 ms) and 99% processing (4786 ms)
15/08/21 11:21:09 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:09 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 72281
15/08/21 11:21:09 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4204 ms: 832.5642 rec/ms, 1665.1284 cell/ms
15/08/21 11:21:09 INFO InternalParquetRecordReader: time spent so far 0% reading (40 ms) and 99% processing (4204 ms)
15/08/21 11:21:09 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:09 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 73020
15/08/21 11:21:10 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4154 ms: 842.58545 rec/ms, 1685.1709 cell/ms
15/08/21 11:21:10 INFO InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (4154 ms)
15/08/21 11:21:10 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 73893
15/08/21 11:21:10 INFO InternalParquetRecordReader: Assembled and processed 3501317 records from 2 columns in 4092 ms: 855.6493 rec/ms, 1711.2986 cell/ms
15/08/21 11:21:10 INFO InternalParquetRecordReader: time spent so far 1% reading (47 ms) and 98% processing (4092 ms)
15/08/21 11:21:10 INFO InternalParquetRecordReader: at row 3501317. reading next block
15/08/21 11:21:10 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 70040
15/08/21 11:21:10 INFO Executor: Finished task 147.0 in stage 7.0 (TID 994). 2125 bytes result sent to driver
15/08/21 11:21:10 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 1010, localhost, ANY, 1774 bytes)
15/08/21 11:21:10 INFO Executor: Running task 163.0 in stage 7.0 (TID 1010)
15/08/21 11:21:10 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 994) in 5437 ms on localhost (148/170)
15/08/21 11:21:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 257504450 length: 123286722 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573920 records.
15/08/21 11:21:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:10 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 3500779
15/08/21 11:21:10 INFO Executor: Finished task 149.0 in stage 7.0 (TID 996). 2125 bytes result sent to driver
15/08/21 11:21:10 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 1011, localhost, ANY, 1761 bytes)
15/08/21 11:21:10 INFO Executor: Running task 164.0 in stage 7.0 (TID 1011)
15/08/21 11:21:10 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 996) in 5162 ms on localhost (149/170)
15/08/21 11:21:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501110 records.
15/08/21 11:21:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:10 INFO InternalParquetRecordReader: block read in memory in 51 ms. row count = 3501110
15/08/21 11:21:10 INFO Executor: Finished task 151.0 in stage 7.0 (TID 998). 2125 bytes result sent to driver
15/08/21 11:21:10 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 1012, localhost, ANY, 1773 bytes)
15/08/21 11:21:10 INFO Executor: Running task 165.0 in stage 7.0 (TID 1012)
15/08/21 11:21:10 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 998) in 5125 ms on localhost (150/170)
15/08/21 11:21:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 134217728 end: 257173847 length: 122956119 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573142 records.
15/08/21 11:21:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:11 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 3502917
15/08/21 11:21:11 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5067 ms: 690.7638 rec/ms, 1381.5276 cell/ms
15/08/21 11:21:11 INFO InternalParquetRecordReader: time spent so far 0% reading (40 ms) and 99% processing (5067 ms)
15/08/21 11:21:11 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:11 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73871
15/08/21 11:21:11 INFO Executor: Finished task 152.0 in stage 7.0 (TID 999). 2125 bytes result sent to driver
15/08/21 11:21:11 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 1013, localhost, ANY, 1761 bytes)
15/08/21 11:21:11 INFO Executor: Running task 166.0 in stage 7.0 (TID 1013)
15/08/21 11:21:11 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 999) in 5163 ms on localhost (151/170)
15/08/21 11:21:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:11 INFO Executor: Finished task 155.0 in stage 7.0 (TID 1002). 2125 bytes result sent to driver
15/08/21 11:21:11 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 1014, localhost, ANY, 1774 bytes)
15/08/21 11:21:11 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 1002) in 5144 ms on localhost (152/170)
15/08/21 11:21:11 INFO Executor: Running task 167.0 in stage 7.0 (TID 1014)
15/08/21 11:21:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 257494956 length: 123277228 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500519 records.
15/08/21 11:21:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573577 records.
15/08/21 11:21:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:11 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
15/08/21 11:21:11 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 3500519
15/08/21 11:21:11 INFO Executor: Finished task 154.0 in stage 7.0 (TID 1001). 2125 bytes result sent to driver
15/08/21 11:21:11 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 1015, localhost, ANY, 1762 bytes)
15/08/21 11:21:11 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 1001) in 5299 ms on localhost (153/170)
15/08/21 11:21:11 INFO Executor: Running task 168.0 in stage 7.0 (TID 1015)
15/08/21 11:21:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:11 INFO Executor: Finished task 156.0 in stage 7.0 (TID 1003). 2125 bytes result sent to driver
15/08/21 11:21:11 INFO Executor: Finished task 150.0 in stage 7.0 (TID 997). 2125 bytes result sent to driver
15/08/21 11:21:11 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 1016, localhost, ANY, 1775 bytes)
15/08/21 11:21:11 INFO Executor: Running task 169.0 in stage 7.0 (TID 1016)
15/08/21 11:21:11 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 1003) in 5149 ms on localhost (154/170)
15/08/21 11:21:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 134217728 end: 257035718 length: 122817990 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 11:21:11 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 997) in 5700 ms on localhost (155/170)
15/08/21 11:21:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 11:21:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 11:21:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574089 records.
15/08/21 11:21:11 INFO Executor: Finished task 148.0 in stage 7.0 (TID 995). 2125 bytes result sent to driver
15/08/21 11:21:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 11:21:11 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 3500100
15/08/21 11:21:11 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 995) in 6149 ms on localhost (156/170)
15/08/21 11:21:11 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 3503378
15/08/21 11:21:11 INFO Executor: Finished task 153.0 in stage 7.0 (TID 1000). 2125 bytes result sent to driver
15/08/21 11:21:11 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 1000) in 5636 ms on localhost (157/170)
15/08/21 11:21:11 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4224 ms: 828.62213 rec/ms, 1657.2443 cell/ms
15/08/21 11:21:11 INFO InternalParquetRecordReader: time spent so far 0% reading (33 ms) and 99% processing (4224 ms)
15/08/21 11:21:11 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 72441
15/08/21 11:21:12 INFO Executor: Finished task 157.0 in stage 7.0 (TID 1004). 2125 bytes result sent to driver
15/08/21 11:21:12 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 1004) in 4670 ms on localhost (158/170)
15/08/21 11:21:12 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4271 ms: 819.5036 rec/ms, 1639.0072 cell/ms
15/08/21 11:21:12 INFO InternalParquetRecordReader: time spent so far 1% reading (84 ms) and 98% processing (4271 ms)
15/08/21 11:21:12 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:12 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 126633
15/08/21 11:21:12 INFO Executor: Finished task 158.0 in stage 7.0 (TID 1005). 2125 bytes result sent to driver
15/08/21 11:21:12 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 1005) in 4895 ms on localhost (159/170)
15/08/21 11:21:13 INFO Executor: Finished task 159.0 in stage 7.0 (TID 1006). 2125 bytes result sent to driver
15/08/21 11:21:13 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 1006) in 4859 ms on localhost (160/170)
15/08/21 11:21:13 INFO Executor: Finished task 160.0 in stage 7.0 (TID 1007). 2125 bytes result sent to driver
15/08/21 11:21:13 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 1007) in 4779 ms on localhost (161/170)
15/08/21 11:21:14 INFO InternalParquetRecordReader: Assembled and processed 3503161 records from 2 columns in 4830 ms: 725.2921 rec/ms, 1450.5842 cell/ms
15/08/21 11:21:14 INFO InternalParquetRecordReader: time spent so far 1% reading (73 ms) and 98% processing (4830 ms)
15/08/21 11:21:14 INFO InternalParquetRecordReader: at row 3503161. reading next block
15/08/21 11:21:14 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 123910
15/08/21 11:21:14 INFO Executor: Finished task 162.0 in stage 7.0 (TID 1009). 2125 bytes result sent to driver
15/08/21 11:21:14 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 1009) in 5032 ms on localhost (162/170)
15/08/21 11:21:15 INFO Executor: Finished task 161.0 in stage 7.0 (TID 1008). 2125 bytes result sent to driver
15/08/21 11:21:15 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 1008) in 5698 ms on localhost (163/170)
15/08/21 11:21:15 INFO InternalParquetRecordReader: Assembled and processed 3500779 records from 2 columns in 4854 ms: 721.2153 rec/ms, 1442.4305 cell/ms
15/08/21 11:21:15 INFO InternalParquetRecordReader: time spent so far 1% reading (58 ms) and 98% processing (4854 ms)
15/08/21 11:21:15 INFO InternalParquetRecordReader: at row 3500779. reading next block
15/08/21 11:21:15 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 73141
15/08/21 11:21:15 INFO Executor: Finished task 164.0 in stage 7.0 (TID 1011). 2125 bytes result sent to driver
15/08/21 11:21:15 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 1011) in 4693 ms on localhost (164/170)
15/08/21 11:21:15 INFO InternalParquetRecordReader: Assembled and processed 3502917 records from 2 columns in 4484 ms: 781.2036 rec/ms, 1562.4072 cell/ms
15/08/21 11:21:15 INFO InternalParquetRecordReader: time spent so far 1% reading (64 ms) and 98% processing (4484 ms)
15/08/21 11:21:15 INFO InternalParquetRecordReader: at row 3502917. reading next block
15/08/21 11:21:15 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 70225
15/08/21 11:21:15 INFO InternalParquetRecordReader: Assembled and processed 3503378 records from 2 columns in 4305 ms: 813.7928 rec/ms, 1627.5856 cell/ms
15/08/21 11:21:15 INFO InternalParquetRecordReader: time spent so far 1% reading (60 ms) and 98% processing (4305 ms)
15/08/21 11:21:15 INFO InternalParquetRecordReader: at row 3503378. reading next block
15/08/21 11:21:15 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 70711
15/08/21 11:21:15 INFO Executor: Finished task 163.0 in stage 7.0 (TID 1010). 2125 bytes result sent to driver
15/08/21 11:21:15 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 1010) in 5498 ms on localhost (165/170)
15/08/21 11:21:15 INFO Executor: Finished task 166.0 in stage 7.0 (TID 1013). 2125 bytes result sent to driver
15/08/21 11:21:15 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 1013) in 4800 ms on localhost (166/170)
15/08/21 11:21:15 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4652 ms: 752.38605 rec/ms, 1504.7721 cell/ms
15/08/21 11:21:15 INFO InternalParquetRecordReader: time spent so far 1% reading (50 ms) and 98% processing (4652 ms)
15/08/21 11:21:15 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 11:21:15 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 73477
15/08/21 11:21:15 INFO Executor: Finished task 165.0 in stage 7.0 (TID 1012). 2125 bytes result sent to driver
15/08/21 11:21:15 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 1012) in 5075 ms on localhost (167/170)
15/08/21 11:21:16 INFO Executor: Finished task 169.0 in stage 7.0 (TID 1016). 2125 bytes result sent to driver
15/08/21 11:21:16 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 1016) in 4881 ms on localhost (168/170)
15/08/21 11:21:16 INFO Executor: Finished task 168.0 in stage 7.0 (TID 1015). 2125 bytes result sent to driver
15/08/21 11:21:16 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 1015) in 4953 ms on localhost (169/170)
15/08/21 11:21:16 INFO Executor: Finished task 167.0 in stage 7.0 (TID 1014). 2125 bytes result sent to driver
15/08/21 11:21:16 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 1014) in 5121 ms on localhost (170/170)
15/08/21 11:21:16 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/21 11:21:16 INFO DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:423) finished in 215.576 s
15/08/21 11:21:16 INFO DAGScheduler: looking for newly runnable stages
15/08/21 11:21:16 INFO DAGScheduler: running: Set()
15/08/21 11:21:16 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 11:21:16 INFO DAGScheduler: failed: Set()
15/08/21 11:21:16 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@14e478b
15/08/21 11:21:16 INFO StatsReportListener: task runtime:(count: 150, mean: 6305.300000, stdev: 1014.015725, max: 9173.000000, min: 3729.000000)
15/08/21 11:21:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:21:16 INFO StatsReportListener: 	3.7 s	4.9 s	5.1 s	5.6 s	6.3 s	6.8 s	7.8 s	8.4 s	9.2 s
15/08/21 11:21:16 INFO StatsReportListener: shuffle bytes written:(count: 150, mean: 22149727.846667, stdev: 1093586.884560, max: 22897011.000000, min: 9212204.000000)
15/08/21 11:21:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:21:16 INFO StatsReportListener: 	8.8 MB	20.9 MB	20.9 MB	21.0 MB	21.4 MB	21.4 MB	21.4 MB	21.7 MB	21.8 MB
15/08/21 11:21:16 INFO StatsReportListener: task result size:(count: 150, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 11:21:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:21:16 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 11:21:16 INFO StatsReportListener: executor (non-fetch) time pct: (count: 150, mean: 99.575943, stdev: 0.207384, max: 99.806919, min: 98.149638)
15/08/21 11:21:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:21:16 INFO StatsReportListener: 	98 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 11:21:16 INFO DAGScheduler: Missing parents for ResultStage 8: List()
15/08/21 11:21:16 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 11:21:16 INFO StatsReportListener: other time pct: (count: 150, mean: 0.424057, stdev: 0.207384, max: 1.850362, min: 0.193081)
15/08/21 11:21:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:21:16 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 2 %
15/08/21 11:21:16 INFO MemoryStore: ensureFreeSpace(16720) called with curMem=1454842, maxMem=22226833244
15/08/21 11:21:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.3 KB, free 20.7 GB)
15/08/21 11:21:16 INFO MemoryStore: ensureFreeSpace(7754) called with curMem=1471562, maxMem=22226833244
15/08/21 11:21:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KB, free 20.7 GB)
15/08/21 11:21:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:53620 (size: 7.6 KB, free: 20.7 GB)
15/08/21 11:21:16 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/21 11:21:16 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423)
15/08/21 11:21:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/21 11:21:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 1017, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 1018, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 1019, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 1020, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 1021, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 1022, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 1023, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 1024, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 1025, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 1026, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 1027, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 1028, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 1029, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 1030, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 1031, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 1032, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:16 INFO Executor: Running task 0.0 in stage 8.0 (TID 1017)
15/08/21 11:21:16 INFO Executor: Running task 4.0 in stage 8.0 (TID 1021)
15/08/21 11:21:16 INFO Executor: Running task 5.0 in stage 8.0 (TID 1022)
15/08/21 11:21:16 INFO Executor: Running task 3.0 in stage 8.0 (TID 1020)
15/08/21 11:21:16 INFO Executor: Running task 1.0 in stage 8.0 (TID 1018)
15/08/21 11:21:16 INFO Executor: Running task 2.0 in stage 8.0 (TID 1019)
15/08/21 11:21:16 INFO Executor: Running task 8.0 in stage 8.0 (TID 1025)
15/08/21 11:21:16 INFO Executor: Running task 6.0 in stage 8.0 (TID 1023)
15/08/21 11:21:16 INFO Executor: Running task 9.0 in stage 8.0 (TID 1026)
15/08/21 11:21:16 INFO Executor: Running task 14.0 in stage 8.0 (TID 1031)
15/08/21 11:21:16 INFO Executor: Running task 13.0 in stage 8.0 (TID 1030)
15/08/21 11:21:16 INFO Executor: Running task 7.0 in stage 8.0 (TID 1024)
15/08/21 11:21:16 INFO Executor: Running task 12.0 in stage 8.0 (TID 1029)
15/08/21 11:21:16 INFO Executor: Running task 10.0 in stage 8.0 (TID 1027)
15/08/21 11:21:16 INFO Executor: Running task 11.0 in stage 8.0 (TID 1028)
15/08/21 11:21:16 INFO Executor: Running task 15.0 in stage 8.0 (TID 1032)
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:21:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:21:40 INFO Executor: Finished task 6.0 in stage 8.0 (TID 1023). 1802 bytes result sent to driver
15/08/21 11:21:40 INFO Executor: Finished task 1.0 in stage 8.0 (TID 1018). 1602 bytes result sent to driver
15/08/21 11:21:40 INFO Executor: Finished task 8.0 in stage 8.0 (TID 1025). 1941 bytes result sent to driver
15/08/21 11:21:40 INFO Executor: Finished task 9.0 in stage 8.0 (TID 1026). 1738 bytes result sent to driver
15/08/21 11:21:40 INFO Executor: Finished task 3.0 in stage 8.0 (TID 1020). 1668 bytes result sent to driver
15/08/21 11:21:40 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 1033, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 16.0 in stage 8.0 (TID 1033)
15/08/21 11:21:40 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 1034, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 17.0 in stage 8.0 (TID 1034)
15/08/21 11:21:40 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 1035, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 18.0 in stage 8.0 (TID 1035)
15/08/21 11:21:40 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 1036, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 19.0 in stage 8.0 (TID 1036)
15/08/21 11:21:40 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 1037, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 20.0 in stage 8.0 (TID 1037)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO Executor: Finished task 2.0 in stage 8.0 (TID 1019). 1738 bytes result sent to driver
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 1038, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 21.0 in stage 8.0 (TID 1038)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 1026) in 24050 ms on localhost (1/200)
15/08/21 11:21:40 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 1025) in 24057 ms on localhost (2/200)
15/08/21 11:21:40 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 1023) in 24058 ms on localhost (3/200)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO Executor: Finished task 7.0 in stage 8.0 (TID 1024). 1805 bytes result sent to driver
15/08/21 11:21:40 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 1039, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 22.0 in stage 8.0 (TID 1039)
15/08/21 11:21:40 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 1018) in 24113 ms on localhost (4/200)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 1020) in 24122 ms on localhost (5/200)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 1019) in 24126 ms on localhost (6/200)
15/08/21 11:21:40 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 1024) in 24125 ms on localhost (7/200)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO Executor: Finished task 15.0 in stage 8.0 (TID 1032). 1738 bytes result sent to driver
15/08/21 11:21:40 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 1040, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 23.0 in stage 8.0 (TID 1040)
15/08/21 11:21:40 INFO Executor: Finished task 14.0 in stage 8.0 (TID 1031). 1874 bytes result sent to driver
15/08/21 11:21:40 INFO Executor: Finished task 13.0 in stage 8.0 (TID 1030). 1737 bytes result sent to driver
15/08/21 11:21:40 INFO Executor: Finished task 10.0 in stage 8.0 (TID 1027). 1943 bytes result sent to driver
15/08/21 11:21:40 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 1041, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 1042, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 1043, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 26.0 in stage 8.0 (TID 1043)
15/08/21 11:21:40 INFO Executor: Running task 25.0 in stage 8.0 (TID 1042)
15/08/21 11:21:40 INFO Executor: Running task 24.0 in stage 8.0 (TID 1041)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 1032) in 24289 ms on localhost (8/200)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 1027) in 24301 ms on localhost (9/200)
15/08/21 11:21:40 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 1030) in 24319 ms on localhost (10/200)
15/08/21 11:21:40 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 1031) in 24320 ms on localhost (11/200)
15/08/21 11:21:40 INFO Executor: Finished task 0.0 in stage 8.0 (TID 1017). 1805 bytes result sent to driver
15/08/21 11:21:40 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 1044, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 27.0 in stage 8.0 (TID 1044)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO Executor: Finished task 11.0 in stage 8.0 (TID 1028). 1737 bytes result sent to driver
15/08/21 11:21:40 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 1045, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 28.0 in stage 8.0 (TID 1045)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 1028) in 24354 ms on localhost (12/200)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 1017) in 24367 ms on localhost (13/200)
15/08/21 11:21:40 INFO Executor: Finished task 5.0 in stage 8.0 (TID 1022). 1806 bytes result sent to driver
15/08/21 11:21:40 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 1046, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 29.0 in stage 8.0 (TID 1046)
15/08/21 11:21:40 INFO Executor: Finished task 4.0 in stage 8.0 (TID 1021). 1602 bytes result sent to driver
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 1022) in 24401 ms on localhost (14/200)
15/08/21 11:21:40 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 1047, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 30.0 in stage 8.0 (TID 1047)
15/08/21 11:21:40 INFO Executor: Finished task 12.0 in stage 8.0 (TID 1029). 1737 bytes result sent to driver
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 1048, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:21:40 INFO Executor: Running task 31.0 in stage 8.0 (TID 1048)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:21:40 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 1029) in 24423 ms on localhost (15/200)
15/08/21 11:21:40 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 1021) in 24444 ms on localhost (16/200)
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:21:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:06 INFO Executor: Finished task 16.0 in stage 8.0 (TID 1033). 2077 bytes result sent to driver
15/08/21 11:22:06 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 1049, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:06 INFO Executor: Running task 32.0 in stage 8.0 (TID 1049)
15/08/21 11:22:06 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:06 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 1033) in 26605 ms on localhost (17/200)
15/08/21 11:22:06 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:07 INFO Executor: Finished task 17.0 in stage 8.0 (TID 1034). 1426 bytes result sent to driver
15/08/21 11:22:07 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 1050, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:07 INFO Executor: Running task 33.0 in stage 8.0 (TID 1050)
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:07 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 1034) in 26824 ms on localhost (18/200)
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:07 INFO Executor: Finished task 18.0 in stage 8.0 (TID 1035). 1806 bytes result sent to driver
15/08/21 11:22:07 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 1051, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:07 INFO Executor: Running task 34.0 in stage 8.0 (TID 1051)
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:07 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 1035) in 27375 ms on localhost (19/200)
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:22:09 INFO Executor: Finished task 20.0 in stage 8.0 (TID 1037). 1602 bytes result sent to driver
15/08/21 11:22:09 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 1052, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:09 INFO Executor: Running task 35.0 in stage 8.0 (TID 1052)
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/21 11:22:09 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 1037) in 29334 ms on localhost (20/200)
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:09 INFO Executor: Finished task 19.0 in stage 8.0 (TID 1036). 1670 bytes result sent to driver
15/08/21 11:22:09 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 1053, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:09 INFO Executor: Running task 36.0 in stage 8.0 (TID 1053)
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:09 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 1036) in 29485 ms on localhost (21/200)
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:09 INFO Executor: Finished task 21.0 in stage 8.0 (TID 1038). 1426 bytes result sent to driver
15/08/21 11:22:09 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 1054, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:09 INFO Executor: Running task 37.0 in stage 8.0 (TID 1054)
15/08/21 11:22:09 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 1038) in 29507 ms on localhost (22/200)
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:09 INFO Executor: Finished task 23.0 in stage 8.0 (TID 1040). 1669 bytes result sent to driver
15/08/21 11:22:09 INFO Executor: Finished task 24.0 in stage 8.0 (TID 1041). 1738 bytes result sent to driver
15/08/21 11:22:09 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 1055, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:09 INFO Executor: Running task 38.0 in stage 8.0 (TID 1055)
15/08/21 11:22:09 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 1056, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:09 INFO Executor: Running task 39.0 in stage 8.0 (TID 1056)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:10 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 1041) in 29363 ms on localhost (23/200)
15/08/21 11:22:10 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 1040) in 29374 ms on localhost (24/200)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:22:10 INFO Executor: Finished task 22.0 in stage 8.0 (TID 1039). 1602 bytes result sent to driver
15/08/21 11:22:10 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 1057, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:10 INFO Executor: Running task 40.0 in stage 8.0 (TID 1057)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 1039) in 29568 ms on localhost (25/200)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO Executor: Finished task 25.0 in stage 8.0 (TID 1042). 1426 bytes result sent to driver
15/08/21 11:22:10 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 1058, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:10 INFO Executor: Finished task 26.0 in stage 8.0 (TID 1043). 1874 bytes result sent to driver
15/08/21 11:22:10 INFO Executor: Running task 41.0 in stage 8.0 (TID 1058)
15/08/21 11:22:10 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 1059, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:10 INFO Executor: Running task 42.0 in stage 8.0 (TID 1059)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 1042) in 29587 ms on localhost (26/200)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:10 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 1043) in 29591 ms on localhost (27/200)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:10 INFO Executor: Finished task 27.0 in stage 8.0 (TID 1044). 1601 bytes result sent to driver
15/08/21 11:22:10 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 1060, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:10 INFO Executor: Running task 43.0 in stage 8.0 (TID 1060)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:10 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 1044) in 29713 ms on localhost (28/200)
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:12 INFO Executor: Finished task 28.0 in stage 8.0 (TID 1045). 1875 bytes result sent to driver
15/08/21 11:22:12 INFO Executor: Finished task 29.0 in stage 8.0 (TID 1046). 1806 bytes result sent to driver
15/08/21 11:22:12 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 1061, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:12 INFO Executor: Running task 44.0 in stage 8.0 (TID 1061)
15/08/21 11:22:12 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 1062, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:12 INFO Executor: Running task 45.0 in stage 8.0 (TID 1062)
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 1046) in 31279 ms on localhost (29/200)
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:12 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 1045) in 31327 ms on localhost (30/200)
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:22:12 INFO Executor: Finished task 30.0 in stage 8.0 (TID 1047). 1601 bytes result sent to driver
15/08/21 11:22:12 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 1063, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:12 INFO Executor: Running task 46.0 in stage 8.0 (TID 1063)
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:12 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 1047) in 31355 ms on localhost (31/200)
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:12 INFO Executor: Finished task 31.0 in stage 8.0 (TID 1048). 1601 bytes result sent to driver
15/08/21 11:22:12 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 1064, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:12 INFO Executor: Running task 47.0 in stage 8.0 (TID 1064)
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:12 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 1048) in 31400 ms on localhost (32/200)
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:22 INFO Executor: Finished task 32.0 in stage 8.0 (TID 1049). 1670 bytes result sent to driver
15/08/21 11:22:22 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 1065, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:22 INFO Executor: Running task 48.0 in stage 8.0 (TID 1065)
15/08/21 11:22:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:22 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:22 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 1049) in 15634 ms on localhost (33/200)
15/08/21 11:22:22 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:23 INFO Executor: Finished task 33.0 in stage 8.0 (TID 1050). 1874 bytes result sent to driver
15/08/21 11:22:23 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 1066, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:23 INFO Executor: Running task 49.0 in stage 8.0 (TID 1066)
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:22:23 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 1050) in 16017 ms on localhost (34/200)
15/08/21 11:22:23 INFO Executor: Finished task 34.0 in stage 8.0 (TID 1051). 1738 bytes result sent to driver
15/08/21 11:22:23 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 1067, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:23 INFO Executor: Running task 50.0 in stage 8.0 (TID 1067)
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:23 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 1051) in 15548 ms on localhost (35/200)
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:28 INFO Executor: Finished task 35.0 in stage 8.0 (TID 1052). 1873 bytes result sent to driver
15/08/21 11:22:28 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 1068, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:28 INFO Executor: Running task 51.0 in stage 8.0 (TID 1068)
15/08/21 11:22:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:28 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 1052) in 18700 ms on localhost (36/200)
15/08/21 11:22:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:29 INFO Executor: Finished task 36.0 in stage 8.0 (TID 1053). 1804 bytes result sent to driver
15/08/21 11:22:29 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 1069, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:29 INFO Executor: Running task 52.0 in stage 8.0 (TID 1069)
15/08/21 11:22:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:29 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 1053) in 20044 ms on localhost (37/200)
15/08/21 11:22:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:30 INFO Executor: Finished task 37.0 in stage 8.0 (TID 1054). 1602 bytes result sent to driver
15/08/21 11:22:30 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 1070, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:30 INFO Executor: Running task 53.0 in stage 8.0 (TID 1070)
15/08/21 11:22:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:30 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 1054) in 20271 ms on localhost (38/200)
15/08/21 11:22:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO Executor: Finished task 38.0 in stage 8.0 (TID 1055). 1670 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 1071, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 54.0 in stage 8.0 (TID 1071)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 1055) in 26077 ms on localhost (39/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO Executor: Finished task 43.0 in stage 8.0 (TID 1060). 1806 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 1072, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 55.0 in stage 8.0 (TID 1072)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO Executor: Finished task 45.0 in stage 8.0 (TID 1062). 1738 bytes result sent to driver
15/08/21 11:22:36 INFO Executor: Finished task 46.0 in stage 8.0 (TID 1063). 1602 bytes result sent to driver
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 1073, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO Executor: Running task 56.0 in stage 8.0 (TID 1073)
15/08/21 11:22:36 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 1060) in 26053 ms on localhost (40/200)
15/08/21 11:22:36 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 1074, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 57.0 in stage 8.0 (TID 1074)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 1062) in 24435 ms on localhost (41/200)
15/08/21 11:22:36 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 1063) in 24357 ms on localhost (42/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO Executor: Finished task 44.0 in stage 8.0 (TID 1061). 1943 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 1075, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 58.0 in stage 8.0 (TID 1075)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 1061) in 24519 ms on localhost (43/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO Executor: Finished task 39.0 in stage 8.0 (TID 1056). 1736 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 1076, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 59.0 in stage 8.0 (TID 1076)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 1056) in 26701 ms on localhost (44/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:22:36 INFO Executor: Finished task 42.0 in stage 8.0 (TID 1059). 1670 bytes result sent to driver
15/08/21 11:22:36 INFO Executor: Finished task 41.0 in stage 8.0 (TID 1058). 1873 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 1077, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 1078, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 60.0 in stage 8.0 (TID 1077)
15/08/21 11:22:36 INFO Executor: Running task 61.0 in stage 8.0 (TID 1078)
15/08/21 11:22:36 INFO Executor: Finished task 47.0 in stage 8.0 (TID 1064). 1806 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 1079, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO Executor: Running task 62.0 in stage 8.0 (TID 1079)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 1059) in 26675 ms on localhost (45/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 1058) in 26680 ms on localhost (46/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 1064) in 24735 ms on localhost (47/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO Executor: Finished task 40.0 in stage 8.0 (TID 1057). 1737 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 1080, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 63.0 in stage 8.0 (TID 1080)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:36 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 1057) in 26943 ms on localhost (48/200)
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:36 INFO Executor: Finished task 48.0 in stage 8.0 (TID 1065). 1943 bytes result sent to driver
15/08/21 11:22:36 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 1081, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:36 INFO Executor: Running task 64.0 in stage 8.0 (TID 1081)
15/08/21 11:22:37 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 1065) in 14482 ms on localhost (49/200)
15/08/21 11:22:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:40 INFO Executor: Finished task 49.0 in stage 8.0 (TID 1066). 1602 bytes result sent to driver
15/08/21 11:22:40 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 1082, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:40 INFO Executor: Running task 65.0 in stage 8.0 (TID 1082)
15/08/21 11:22:41 INFO Executor: Finished task 50.0 in stage 8.0 (TID 1067). 1670 bytes result sent to driver
15/08/21 11:22:41 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 1083, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:41 INFO Executor: Running task 66.0 in stage 8.0 (TID 1083)
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:41 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 1067) in 17771 ms on localhost (50/200)
15/08/21 11:22:41 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 1066) in 17877 ms on localhost (51/200)
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:46 INFO Executor: Finished task 51.0 in stage 8.0 (TID 1068). 1669 bytes result sent to driver
15/08/21 11:22:46 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 1084, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:46 INFO Executor: Running task 67.0 in stage 8.0 (TID 1084)
15/08/21 11:22:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:46 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 1068) in 18693 ms on localhost (52/200)
15/08/21 11:22:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:53 INFO Executor: Finished task 52.0 in stage 8.0 (TID 1069). 1805 bytes result sent to driver
15/08/21 11:22:53 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 1085, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:53 INFO Executor: Running task 68.0 in stage 8.0 (TID 1085)
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:53 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 1069) in 23400 ms on localhost (53/200)
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:53 INFO Executor: Finished task 53.0 in stage 8.0 (TID 1070). 1805 bytes result sent to driver
15/08/21 11:22:53 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 1086, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:53 INFO Executor: Running task 69.0 in stage 8.0 (TID 1086)
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:53 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 1070) in 23226 ms on localhost (54/200)
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:54 INFO Executor: Finished task 54.0 in stage 8.0 (TID 1071). 1669 bytes result sent to driver
15/08/21 11:22:54 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 1087, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:54 INFO Executor: Running task 70.0 in stage 8.0 (TID 1087)
15/08/21 11:22:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:22:54 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 1071) in 18200 ms on localhost (55/200)
15/08/21 11:22:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:55 INFO Executor: Finished task 55.0 in stage 8.0 (TID 1072). 1804 bytes result sent to driver
15/08/21 11:22:55 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 1088, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:22:55 INFO Executor: Running task 71.0 in stage 8.0 (TID 1088)
15/08/21 11:22:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:22:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:55 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 1072) in 19342 ms on localhost (56/200)
15/08/21 11:22:55 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:22:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:22:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:22:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:01 INFO Executor: Finished task 56.0 in stage 8.0 (TID 1073). 1805 bytes result sent to driver
15/08/21 11:23:01 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 1089, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:01 INFO Executor: Running task 72.0 in stage 8.0 (TID 1089)
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:01 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 1073) in 24664 ms on localhost (57/200)
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:01 INFO Executor: Finished task 57.0 in stage 8.0 (TID 1074). 1737 bytes result sent to driver
15/08/21 11:23:01 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 1090, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:01 INFO Executor: Running task 73.0 in stage 8.0 (TID 1090)
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:01 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 1074) in 24747 ms on localhost (58/200)
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:01 INFO Executor: Finished task 58.0 in stage 8.0 (TID 1075). 1668 bytes result sent to driver
15/08/21 11:23:01 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 1091, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:01 INFO Executor: Running task 74.0 in stage 8.0 (TID 1091)
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:02 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 1075) in 25487 ms on localhost (59/200)
15/08/21 11:23:02 INFO Executor: Finished task 59.0 in stage 8.0 (TID 1076). 1875 bytes result sent to driver
15/08/21 11:23:02 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 1092, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:02 INFO Executor: Running task 75.0 in stage 8.0 (TID 1092)
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:02 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 1076) in 26026 ms on localhost (60/200)
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:02 INFO Executor: Finished task 60.0 in stage 8.0 (TID 1077). 1805 bytes result sent to driver
15/08/21 11:23:02 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 1093, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:02 INFO Executor: Running task 76.0 in stage 8.0 (TID 1093)
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:02 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 1077) in 25958 ms on localhost (61/200)
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:18 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:53620 in memory (size: 4.9 KB, free: 20.7 GB)
15/08/21 11:23:18 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:53620 in memory (size: 3.7 KB, free: 20.7 GB)
15/08/21 11:23:18 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:53620 in memory (size: 3.8 KB, free: 20.7 GB)
15/08/21 11:23:18 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:53620 in memory (size: 3.6 KB, free: 20.7 GB)
15/08/21 11:23:18 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:53620 in memory (size: 3.5 KB, free: 20.7 GB)
15/08/21 11:23:18 INFO Executor: Finished task 61.0 in stage 8.0 (TID 1078). 2009 bytes result sent to driver
15/08/21 11:23:18 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 1094, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:18 INFO Executor: Running task 77.0 in stage 8.0 (TID 1094)
15/08/21 11:23:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:18 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 1078) in 41537 ms on localhost (62/200)
15/08/21 11:23:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:19 INFO Executor: Finished task 62.0 in stage 8.0 (TID 1079). 1737 bytes result sent to driver
15/08/21 11:23:19 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 1095, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:19 INFO Executor: Running task 78.0 in stage 8.0 (TID 1095)
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:19 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 1079) in 42472 ms on localhost (63/200)
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:19 INFO Executor: Finished task 63.0 in stage 8.0 (TID 1080). 1804 bytes result sent to driver
15/08/21 11:23:19 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 1096, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:19 INFO Executor: Running task 79.0 in stage 8.0 (TID 1096)
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:19 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 1080) in 42584 ms on localhost (64/200)
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:19 INFO Executor: Finished task 64.0 in stage 8.0 (TID 1081). 1873 bytes result sent to driver
15/08/21 11:23:19 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 1097, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:19 INFO Executor: Running task 80.0 in stage 8.0 (TID 1097)
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:19 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 1081) in 42623 ms on localhost (65/200)
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO Executor: Finished task 65.0 in stage 8.0 (TID 1082). 1803 bytes result sent to driver
15/08/21 11:23:20 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 1098, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:20 INFO Executor: Running task 81.0 in stage 8.0 (TID 1098)
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:20 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 1082) in 39025 ms on localhost (66/200)
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO Executor: Finished task 66.0 in stage 8.0 (TID 1083). 1876 bytes result sent to driver
15/08/21 11:23:20 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 1099, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:20 INFO Executor: Running task 82.0 in stage 8.0 (TID 1099)
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:20 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 1083) in 39112 ms on localhost (67/200)
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO Executor: Finished task 67.0 in stage 8.0 (TID 1084). 2078 bytes result sent to driver
15/08/21 11:23:20 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 1100, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:20 INFO Executor: Running task 83.0 in stage 8.0 (TID 1100)
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 1084) in 33335 ms on localhost (68/200)
15/08/21 11:23:20 INFO Executor: Finished task 68.0 in stage 8.0 (TID 1085). 1670 bytes result sent to driver
15/08/21 11:23:20 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 1101, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:20 INFO Executor: Running task 84.0 in stage 8.0 (TID 1101)
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 1085) in 27572 ms on localhost (69/200)
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:23 INFO Executor: Finished task 69.0 in stage 8.0 (TID 1086). 1426 bytes result sent to driver
15/08/21 11:23:23 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 1102, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:23 INFO Executor: Running task 85.0 in stage 8.0 (TID 1102)
15/08/21 11:23:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:23 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 1086) in 29927 ms on localhost (70/200)
15/08/21 11:23:24 INFO Executor: Finished task 70.0 in stage 8.0 (TID 1087). 1426 bytes result sent to driver
15/08/21 11:23:24 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 1103, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:24 INFO Executor: Running task 86.0 in stage 8.0 (TID 1103)
15/08/21 11:23:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:24 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 1087) in 30291 ms on localhost (71/200)
15/08/21 11:23:24 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:29 INFO Executor: Finished task 71.0 in stage 8.0 (TID 1088). 1805 bytes result sent to driver
15/08/21 11:23:29 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 1104, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:29 INFO Executor: Running task 87.0 in stage 8.0 (TID 1104)
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:29 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 1088) in 33530 ms on localhost (72/200)
15/08/21 11:23:29 INFO Executor: Finished task 72.0 in stage 8.0 (TID 1089). 1426 bytes result sent to driver
15/08/21 11:23:29 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 1105, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:29 INFO Executor: Running task 88.0 in stage 8.0 (TID 1105)
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:29 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 1089) in 28706 ms on localhost (73/200)
15/08/21 11:23:37 INFO Executor: Finished task 73.0 in stage 8.0 (TID 1090). 1942 bytes result sent to driver
15/08/21 11:23:37 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 1106, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:37 INFO Executor: Running task 89.0 in stage 8.0 (TID 1106)
15/08/21 11:23:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:37 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 1090) in 35933 ms on localhost (74/200)
15/08/21 11:23:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:38 INFO Executor: Finished task 74.0 in stage 8.0 (TID 1091). 2150 bytes result sent to driver
15/08/21 11:23:38 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 1107, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:38 INFO Executor: Running task 90.0 in stage 8.0 (TID 1107)
15/08/21 11:23:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:38 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 1091) in 36242 ms on localhost (75/200)
15/08/21 11:23:38 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:45 INFO Executor: Finished task 75.0 in stage 8.0 (TID 1092). 1876 bytes result sent to driver
15/08/21 11:23:45 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 1108, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:45 INFO Executor: Running task 91.0 in stage 8.0 (TID 1108)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:23:45 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 1092) in 42762 ms on localhost (76/200)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:45 INFO Executor: Finished task 76.0 in stage 8.0 (TID 1093). 1805 bytes result sent to driver
15/08/21 11:23:45 INFO Executor: Finished task 77.0 in stage 8.0 (TID 1094). 2010 bytes result sent to driver
15/08/21 11:23:45 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 1109, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:45 INFO Executor: Running task 92.0 in stage 8.0 (TID 1109)
15/08/21 11:23:45 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 1110, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:45 INFO Executor: Running task 93.0 in stage 8.0 (TID 1110)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:23:45 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 1094) in 27280 ms on localhost (77/200)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 1093) in 42857 ms on localhost (78/200)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:45 INFO Executor: Finished task 78.0 in stage 8.0 (TID 1095). 1669 bytes result sent to driver
15/08/21 11:23:45 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 1111, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:45 INFO Executor: Running task 94.0 in stage 8.0 (TID 1111)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:45 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 1095) in 26474 ms on localhost (79/200)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:45 INFO Executor: Finished task 79.0 in stage 8.0 (TID 1096). 1939 bytes result sent to driver
15/08/21 11:23:45 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 1112, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:45 INFO Executor: Running task 95.0 in stage 8.0 (TID 1112)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:45 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 1096) in 26397 ms on localhost (80/200)
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO Executor: Finished task 80.0 in stage 8.0 (TID 1097). 1738 bytes result sent to driver
15/08/21 11:23:46 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 1113, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:46 INFO Executor: Running task 96.0 in stage 8.0 (TID 1113)
15/08/21 11:23:46 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 1097) in 26437 ms on localhost (81/200)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO Executor: Finished task 81.0 in stage 8.0 (TID 1098). 1736 bytes result sent to driver
15/08/21 11:23:46 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 1114, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:46 INFO Executor: Running task 97.0 in stage 8.0 (TID 1114)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:46 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 1098) in 26142 ms on localhost (82/200)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO Executor: Finished task 82.0 in stage 8.0 (TID 1099). 1804 bytes result sent to driver
15/08/21 11:23:46 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 1115, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:46 INFO Executor: Running task 98.0 in stage 8.0 (TID 1115)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO Executor: Finished task 83.0 in stage 8.0 (TID 1100). 1426 bytes result sent to driver
15/08/21 11:23:46 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 1116, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:46 INFO Executor: Running task 99.0 in stage 8.0 (TID 1116)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 1099) in 26596 ms on localhost (83/200)
15/08/21 11:23:46 INFO Executor: Finished task 84.0 in stage 8.0 (TID 1101). 1738 bytes result sent to driver
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 1117, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:46 INFO Executor: Running task 100.0 in stage 8.0 (TID 1117)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 1100) in 26431 ms on localhost (84/200)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:46 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 1101) in 25921 ms on localhost (85/200)
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:47 INFO Executor: Finished task 85.0 in stage 8.0 (TID 1102). 1602 bytes result sent to driver
15/08/21 11:23:47 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 1118, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:47 INFO Executor: Running task 101.0 in stage 8.0 (TID 1118)
15/08/21 11:23:47 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 1102) in 23866 ms on localhost (86/200)
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:47 INFO Executor: Finished task 86.0 in stage 8.0 (TID 1103). 1805 bytes result sent to driver
15/08/21 11:23:47 INFO Executor: Finished task 87.0 in stage 8.0 (TID 1104). 1669 bytes result sent to driver
15/08/21 11:23:47 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 1119, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:47 INFO Executor: Running task 102.0 in stage 8.0 (TID 1119)
15/08/21 11:23:47 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 1120, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:47 INFO Executor: Running task 103.0 in stage 8.0 (TID 1120)
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:47 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 1103) in 22861 ms on localhost (87/200)
15/08/21 11:23:47 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 1104) in 18135 ms on localhost (88/200)
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:51 INFO Executor: Finished task 88.0 in stage 8.0 (TID 1105). 2009 bytes result sent to driver
15/08/21 11:23:51 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 1121, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:51 INFO Executor: Running task 104.0 in stage 8.0 (TID 1121)
15/08/21 11:23:51 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 1105) in 21522 ms on localhost (89/200)
15/08/21 11:23:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:23:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:56 INFO Executor: Finished task 89.0 in stage 8.0 (TID 1106). 1670 bytes result sent to driver
15/08/21 11:23:56 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 1122, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:56 INFO Executor: Running task 105.0 in stage 8.0 (TID 1122)
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:56 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 1106) in 19437 ms on localhost (90/200)
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:56 INFO Executor: Finished task 90.0 in stage 8.0 (TID 1107). 1941 bytes result sent to driver
15/08/21 11:23:56 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 1123, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:23:56 INFO Executor: Running task 106.0 in stage 8.0 (TID 1123)
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:56 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 1107) in 18612 ms on localhost (91/200)
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:23:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:03 INFO Executor: Finished task 91.0 in stage 8.0 (TID 1108). 2009 bytes result sent to driver
15/08/21 11:24:03 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 1124, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:03 INFO Executor: Running task 107.0 in stage 8.0 (TID 1124)
15/08/21 11:24:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:03 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 1108) in 18260 ms on localhost (92/200)
15/08/21 11:24:03 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:11 INFO Executor: Finished task 92.0 in stage 8.0 (TID 1109). 1670 bytes result sent to driver
15/08/21 11:24:11 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 1125, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:11 INFO Executor: Running task 108.0 in stage 8.0 (TID 1125)
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:11 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 1109) in 25935 ms on localhost (93/200)
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:11 INFO Executor: Finished task 94.0 in stage 8.0 (TID 1111). 1738 bytes result sent to driver
15/08/21 11:24:11 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 1126, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:11 INFO Executor: Running task 109.0 in stage 8.0 (TID 1126)
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:11 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 1111) in 26176 ms on localhost (94/200)
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO Executor: Finished task 95.0 in stage 8.0 (TID 1112). 1805 bytes result sent to driver
15/08/21 11:24:12 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 1127, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:12 INFO Executor: Running task 110.0 in stage 8.0 (TID 1127)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 1112) in 26139 ms on localhost (95/200)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO Executor: Finished task 96.0 in stage 8.0 (TID 1113). 1738 bytes result sent to driver
15/08/21 11:24:12 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 1128, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:12 INFO Executor: Running task 111.0 in stage 8.0 (TID 1128)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:12 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 1113) in 26081 ms on localhost (96/200)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO Executor: Finished task 93.0 in stage 8.0 (TID 1110). 1736 bytes result sent to driver
15/08/21 11:24:12 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 1129, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:12 INFO Executor: Running task 112.0 in stage 8.0 (TID 1129)
15/08/21 11:24:12 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 1110) in 26470 ms on localhost (97/200)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO Executor: Finished task 97.0 in stage 8.0 (TID 1114). 1876 bytes result sent to driver
15/08/21 11:24:12 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 1130, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:12 INFO Executor: Running task 113.0 in stage 8.0 (TID 1130)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:12 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 1114) in 26199 ms on localhost (98/200)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO Executor: Finished task 98.0 in stage 8.0 (TID 1115). 1668 bytes result sent to driver
15/08/21 11:24:12 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 1131, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:12 INFO Executor: Running task 114.0 in stage 8.0 (TID 1131)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:12 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 1115) in 25808 ms on localhost (99/200)
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:13 INFO Executor: Finished task 99.0 in stage 8.0 (TID 1116). 1737 bytes result sent to driver
15/08/21 11:24:13 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 1132, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:13 INFO Executor: Running task 115.0 in stage 8.0 (TID 1132)
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:13 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 1116) in 26391 ms on localhost (100/200)
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:13 INFO Executor: Finished task 100.0 in stage 8.0 (TID 1117). 1669 bytes result sent to driver
15/08/21 11:24:13 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 1133, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:13 INFO Executor: Running task 116.0 in stage 8.0 (TID 1133)
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:13 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 1117) in 26596 ms on localhost (101/200)
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:26 INFO Executor: Finished task 101.0 in stage 8.0 (TID 1118). 1670 bytes result sent to driver
15/08/21 11:24:26 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 1134, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:26 INFO Executor: Running task 117.0 in stage 8.0 (TID 1134)
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:26 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 1118) in 39425 ms on localhost (102/200)
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:26 INFO Executor: Finished task 102.0 in stage 8.0 (TID 1119). 1602 bytes result sent to driver
15/08/21 11:24:26 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 1135, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:26 INFO Executor: Running task 118.0 in stage 8.0 (TID 1135)
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:26 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 1119) in 39626 ms on localhost (103/200)
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:27 INFO Executor: Finished task 103.0 in stage 8.0 (TID 1120). 1426 bytes result sent to driver
15/08/21 11:24:27 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 1136, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:27 INFO Executor: Running task 119.0 in stage 8.0 (TID 1136)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:27 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 1120) in 39732 ms on localhost (104/200)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:27 INFO Executor: Finished task 104.0 in stage 8.0 (TID 1121). 1602 bytes result sent to driver
15/08/21 11:24:27 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 1137, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:27 INFO Executor: Running task 120.0 in stage 8.0 (TID 1137)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:27 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 1121) in 36130 ms on localhost (105/200)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:27 INFO Executor: Finished task 105.0 in stage 8.0 (TID 1122). 1426 bytes result sent to driver
15/08/21 11:24:27 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 1138, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:27 INFO Executor: Running task 121.0 in stage 8.0 (TID 1138)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:27 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 1122) in 31088 ms on localhost (106/200)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:27 INFO Executor: Finished task 106.0 in stage 8.0 (TID 1123). 1670 bytes result sent to driver
15/08/21 11:24:27 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 1139, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:27 INFO Executor: Running task 122.0 in stage 8.0 (TID 1139)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:27 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 1123) in 31055 ms on localhost (107/200)
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:30 INFO Executor: Finished task 107.0 in stage 8.0 (TID 1124). 1738 bytes result sent to driver
15/08/21 11:24:30 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 1140, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:30 INFO Executor: Running task 123.0 in stage 8.0 (TID 1140)
15/08/21 11:24:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 11:24:30 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 1124) in 27253 ms on localhost (108/200)
15/08/21 11:24:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:38 INFO Executor: Finished task 108.0 in stage 8.0 (TID 1125). 1806 bytes result sent to driver
15/08/21 11:24:38 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 1141, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:38 INFO Executor: Running task 124.0 in stage 8.0 (TID 1141)
15/08/21 11:24:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:38 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 1125) in 27304 ms on localhost (109/200)
15/08/21 11:24:38 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:43 INFO Executor: Finished task 109.0 in stage 8.0 (TID 1126). 1669 bytes result sent to driver
15/08/21 11:24:43 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 1142, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:43 INFO Executor: Running task 125.0 in stage 8.0 (TID 1142)
15/08/21 11:24:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:43 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 1126) in 31888 ms on localhost (110/200)
15/08/21 11:24:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:44 INFO Executor: Finished task 110.0 in stage 8.0 (TID 1127). 1426 bytes result sent to driver
15/08/21 11:24:44 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 1143, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:44 INFO Executor: Running task 126.0 in stage 8.0 (TID 1143)
15/08/21 11:24:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:44 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 1127) in 32389 ms on localhost (111/200)
15/08/21 11:24:44 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:46 INFO Executor: Finished task 111.0 in stage 8.0 (TID 1128). 1738 bytes result sent to driver
15/08/21 11:24:46 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 1144, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:46 INFO Executor: Running task 127.0 in stage 8.0 (TID 1144)
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:46 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 1128) in 34144 ms on localhost (112/200)
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:46 INFO Executor: Finished task 112.0 in stage 8.0 (TID 1129). 1874 bytes result sent to driver
15/08/21 11:24:46 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 1145, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:46 INFO Executor: Running task 128.0 in stage 8.0 (TID 1145)
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:46 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 1129) in 34131 ms on localhost (113/200)
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:46 INFO Executor: Finished task 113.0 in stage 8.0 (TID 1130). 1737 bytes result sent to driver
15/08/21 11:24:46 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 1146, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:46 INFO Executor: Running task 129.0 in stage 8.0 (TID 1146)
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:46 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 1130) in 34314 ms on localhost (114/200)
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:52 INFO Executor: Finished task 114.0 in stage 8.0 (TID 1131). 1806 bytes result sent to driver
15/08/21 11:24:52 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 1147, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:52 INFO Executor: Running task 130.0 in stage 8.0 (TID 1147)
15/08/21 11:24:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 11:24:52 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 1131) in 39588 ms on localhost (115/200)
15/08/21 11:24:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:53 INFO Executor: Finished task 115.0 in stage 8.0 (TID 1132). 1670 bytes result sent to driver
15/08/21 11:24:53 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 1148, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:53 INFO Executor: Running task 131.0 in stage 8.0 (TID 1148)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:53 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 1132) in 40081 ms on localhost (116/200)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:53 INFO Executor: Finished task 116.0 in stage 8.0 (TID 1133). 1737 bytes result sent to driver
15/08/21 11:24:53 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 1149, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:53 INFO Executor: Running task 132.0 in stage 8.0 (TID 1149)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 1133) in 39917 ms on localhost (117/200)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO Executor: Finished task 117.0 in stage 8.0 (TID 1134). 1737 bytes result sent to driver
15/08/21 11:24:53 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 1150, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:53 INFO Executor: Running task 133.0 in stage 8.0 (TID 1150)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO Executor: Finished task 118.0 in stage 8.0 (TID 1135). 1736 bytes result sent to driver
15/08/21 11:24:53 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 1151, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:53 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 1134) in 27346 ms on localhost (118/200)
15/08/21 11:24:53 INFO Executor: Running task 134.0 in stage 8.0 (TID 1151)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:53 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 1135) in 26803 ms on localhost (119/200)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:53 INFO Executor: Finished task 119.0 in stage 8.0 (TID 1136). 1738 bytes result sent to driver
15/08/21 11:24:53 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 1152, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:53 INFO Executor: Running task 135.0 in stage 8.0 (TID 1152)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 1136) in 26864 ms on localhost (120/200)
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO Executor: Finished task 120.0 in stage 8.0 (TID 1137). 1670 bytes result sent to driver
15/08/21 11:24:54 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 1153, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:54 INFO Executor: Running task 136.0 in stage 8.0 (TID 1153)
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 1137) in 26968 ms on localhost (121/200)
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO Executor: Finished task 121.0 in stage 8.0 (TID 1138). 1737 bytes result sent to driver
15/08/21 11:24:54 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 1154, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:54 INFO Executor: Running task 137.0 in stage 8.0 (TID 1154)
15/08/21 11:24:54 INFO Executor: Finished task 122.0 in stage 8.0 (TID 1139). 1804 bytes result sent to driver
15/08/21 11:24:54 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 1155, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:54 INFO Executor: Running task 138.0 in stage 8.0 (TID 1155)
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 1138) in 26924 ms on localhost (122/200)
15/08/21 11:24:54 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 1139) in 26657 ms on localhost (123/200)
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO Executor: Finished task 123.0 in stage 8.0 (TID 1140). 1737 bytes result sent to driver
15/08/21 11:24:54 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 1156, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:54 INFO Executor: Running task 139.0 in stage 8.0 (TID 1156)
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:24:54 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 1140) in 23615 ms on localhost (124/200)
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:58 INFO Executor: Finished task 124.0 in stage 8.0 (TID 1141). 1805 bytes result sent to driver
15/08/21 11:24:58 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 1157, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:24:58 INFO Executor: Running task 140.0 in stage 8.0 (TID 1157)
15/08/21 11:24:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:24:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:58 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 1141) in 19267 ms on localhost (125/200)
15/08/21 11:24:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:24:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:24:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:24:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:08 INFO Executor: Finished task 125.0 in stage 8.0 (TID 1142). 1806 bytes result sent to driver
15/08/21 11:25:08 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 1158, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:08 INFO Executor: Running task 141.0 in stage 8.0 (TID 1158)
15/08/21 11:25:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:08 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 1142) in 25154 ms on localhost (126/200)
15/08/21 11:25:08 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:09 INFO Executor: Finished task 126.0 in stage 8.0 (TID 1143). 1670 bytes result sent to driver
15/08/21 11:25:09 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 1159, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:09 INFO Executor: Running task 142.0 in stage 8.0 (TID 1159)
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:09 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 1143) in 24966 ms on localhost (127/200)
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:09 INFO Executor: Finished task 127.0 in stage 8.0 (TID 1144). 1670 bytes result sent to driver
15/08/21 11:25:09 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 1160, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:09 INFO Executor: Running task 143.0 in stage 8.0 (TID 1160)
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:09 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 1144) in 23476 ms on localhost (128/200)
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:09 INFO Executor: Finished task 128.0 in stage 8.0 (TID 1145). 1669 bytes result sent to driver
15/08/21 11:25:09 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 1161, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:09 INFO Executor: Running task 144.0 in stage 8.0 (TID 1161)
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:09 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 1145) in 23669 ms on localhost (129/200)
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:10 INFO Executor: Finished task 129.0 in stage 8.0 (TID 1146). 1426 bytes result sent to driver
15/08/21 11:25:10 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 1162, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:10 INFO Executor: Running task 145.0 in stage 8.0 (TID 1162)
15/08/21 11:25:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:10 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 1146) in 24018 ms on localhost (130/200)
15/08/21 11:25:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:11 INFO Executor: Finished task 130.0 in stage 8.0 (TID 1147). 1669 bytes result sent to driver
15/08/21 11:25:11 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 1163, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:11 INFO Executor: Running task 146.0 in stage 8.0 (TID 1163)
15/08/21 11:25:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:11 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 1147) in 19080 ms on localhost (131/200)
15/08/21 11:25:11 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:12 INFO Executor: Finished task 131.0 in stage 8.0 (TID 1148). 1738 bytes result sent to driver
15/08/21 11:25:12 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 1164, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:12 INFO Executor: Running task 147.0 in stage 8.0 (TID 1164)
15/08/21 11:25:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:12 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 1148) in 19225 ms on localhost (132/200)
15/08/21 11:25:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:17 INFO Executor: Finished task 133.0 in stage 8.0 (TID 1150). 1806 bytes result sent to driver
15/08/21 11:25:17 INFO Executor: Finished task 134.0 in stage 8.0 (TID 1151). 1602 bytes result sent to driver
15/08/21 11:25:17 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 1165, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:17 INFO Executor: Running task 148.0 in stage 8.0 (TID 1165)
15/08/21 11:25:17 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 1166, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:17 INFO Executor: Running task 149.0 in stage 8.0 (TID 1166)
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:17 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 1151) in 23828 ms on localhost (133/200)
15/08/21 11:25:17 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 1150) in 23842 ms on localhost (134/200)
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:17 INFO Executor: Finished task 132.0 in stage 8.0 (TID 1149). 1602 bytes result sent to driver
15/08/21 11:25:17 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 1167, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:17 INFO Executor: Running task 150.0 in stage 8.0 (TID 1167)
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:17 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 1149) in 24552 ms on localhost (135/200)
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:18 INFO Executor: Finished task 135.0 in stage 8.0 (TID 1152). 1670 bytes result sent to driver
15/08/21 11:25:18 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 1168, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:18 INFO Executor: Running task 151.0 in stage 8.0 (TID 1168)
15/08/21 11:25:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:18 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 1152) in 25045 ms on localhost (136/200)
15/08/21 11:25:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:19 INFO Executor: Finished task 136.0 in stage 8.0 (TID 1153). 1874 bytes result sent to driver
15/08/21 11:25:19 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 1169, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:19 INFO Executor: Running task 152.0 in stage 8.0 (TID 1169)
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:19 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 1153) in 24862 ms on localhost (137/200)
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:19 INFO Executor: Finished task 137.0 in stage 8.0 (TID 1154). 1737 bytes result sent to driver
15/08/21 11:25:19 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 1170, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:19 INFO Executor: Running task 153.0 in stage 8.0 (TID 1170)
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:19 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 1154) in 24881 ms on localhost (138/200)
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:37 INFO Executor: Finished task 138.0 in stage 8.0 (TID 1155). 1738 bytes result sent to driver
15/08/21 11:25:37 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 1171, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:37 INFO Executor: Running task 154.0 in stage 8.0 (TID 1171)
15/08/21 11:25:37 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 1155) in 43294 ms on localhost (139/200)
15/08/21 11:25:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:38 INFO Executor: Finished task 139.0 in stage 8.0 (TID 1156). 1670 bytes result sent to driver
15/08/21 11:25:38 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 1172, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:38 INFO Executor: Running task 155.0 in stage 8.0 (TID 1172)
15/08/21 11:25:38 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 1156) in 43548 ms on localhost (140/200)
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:38 INFO Executor: Finished task 140.0 in stage 8.0 (TID 1157). 1737 bytes result sent to driver
15/08/21 11:25:38 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 1173, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:38 INFO Executor: Running task 156.0 in stage 8.0 (TID 1173)
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:38 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 1157) in 40572 ms on localhost (141/200)
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:42 INFO Executor: Finished task 141.0 in stage 8.0 (TID 1158). 1670 bytes result sent to driver
15/08/21 11:25:42 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 1174, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:42 INFO Executor: Running task 157.0 in stage 8.0 (TID 1174)
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:42 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 1158) in 33114 ms on localhost (142/200)
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:42 INFO Executor: Finished task 142.0 in stage 8.0 (TID 1159). 1736 bytes result sent to driver
15/08/21 11:25:42 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 1175, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:42 INFO Executor: Running task 158.0 in stage 8.0 (TID 1175)
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:42 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 1159) in 33010 ms on localhost (143/200)
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:43 INFO Executor: Finished task 143.0 in stage 8.0 (TID 1160). 1737 bytes result sent to driver
15/08/21 11:25:43 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 1176, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:43 INFO Executor: Running task 159.0 in stage 8.0 (TID 1176)
15/08/21 11:25:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:43 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 1160) in 33982 ms on localhost (144/200)
15/08/21 11:25:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:48 INFO Executor: Finished task 144.0 in stage 8.0 (TID 1161). 1805 bytes result sent to driver
15/08/21 11:25:48 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 1177, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:48 INFO Executor: Running task 160.0 in stage 8.0 (TID 1177)
15/08/21 11:25:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:48 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 1161) in 38809 ms on localhost (145/200)
15/08/21 11:25:48 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:50 INFO Executor: Finished task 145.0 in stage 8.0 (TID 1162). 1874 bytes result sent to driver
15/08/21 11:25:50 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 1178, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:50 INFO Executor: Running task 161.0 in stage 8.0 (TID 1178)
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:50 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 1162) in 40138 ms on localhost (146/200)
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:50 INFO Executor: Finished task 146.0 in stage 8.0 (TID 1163). 1601 bytes result sent to driver
15/08/21 11:25:50 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 1179, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:50 INFO Executor: Running task 162.0 in stage 8.0 (TID 1179)
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:50 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 1163) in 39882 ms on localhost (147/200)
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:51 INFO Executor: Finished task 147.0 in stage 8.0 (TID 1164). 1669 bytes result sent to driver
15/08/21 11:25:51 INFO Executor: Finished task 148.0 in stage 8.0 (TID 1165). 1942 bytes result sent to driver
15/08/21 11:25:51 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 1180, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:51 INFO Executor: Running task 163.0 in stage 8.0 (TID 1180)
15/08/21 11:25:51 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 1181, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:51 INFO Executor: Running task 164.0 in stage 8.0 (TID 1181)
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:51 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 1164) in 39116 ms on localhost (148/200)
15/08/21 11:25:51 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 1165) in 33878 ms on localhost (149/200)
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:55 INFO Executor: Finished task 149.0 in stage 8.0 (TID 1166). 2010 bytes result sent to driver
15/08/21 11:25:55 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 1182, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:55 INFO Executor: Running task 165.0 in stage 8.0 (TID 1182)
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:55 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 1166) in 37834 ms on localhost (150/200)
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:55 INFO Executor: Finished task 150.0 in stage 8.0 (TID 1167). 1941 bytes result sent to driver
15/08/21 11:25:55 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 1183, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:55 INFO Executor: Running task 166.0 in stage 8.0 (TID 1183)
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:25:55 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 1167) in 37999 ms on localhost (151/200)
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:55 INFO Executor: Finished task 151.0 in stage 8.0 (TID 1168). 1805 bytes result sent to driver
15/08/21 11:25:55 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 1184, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:55 INFO Executor: Running task 167.0 in stage 8.0 (TID 1184)
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:55 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 1168) in 36816 ms on localhost (152/200)
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:58 INFO Executor: Finished task 152.0 in stage 8.0 (TID 1169). 1738 bytes result sent to driver
15/08/21 11:25:58 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 1185, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:58 INFO Executor: Running task 168.0 in stage 8.0 (TID 1185)
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:58 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 1169) in 39029 ms on localhost (153/200)
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:58 INFO Executor: Finished task 153.0 in stage 8.0 (TID 1170). 1737 bytes result sent to driver
15/08/21 11:25:58 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 1186, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:25:58 INFO Executor: Running task 169.0 in stage 8.0 (TID 1186)
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:25:58 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 1170) in 38973 ms on localhost (154/200)
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:25:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:02 INFO Executor: Finished task 154.0 in stage 8.0 (TID 1171). 1876 bytes result sent to driver
15/08/21 11:26:02 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 1187, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:02 INFO Executor: Running task 170.0 in stage 8.0 (TID 1187)
15/08/21 11:26:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:02 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 1171) in 25077 ms on localhost (155/200)
15/08/21 11:26:02 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:03 INFO Executor: Finished task 155.0 in stage 8.0 (TID 1172). 1601 bytes result sent to driver
15/08/21 11:26:03 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 1188, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:03 INFO Executor: Running task 171.0 in stage 8.0 (TID 1188)
15/08/21 11:26:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:03 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 1172) in 25559 ms on localhost (156/200)
15/08/21 11:26:03 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:04 INFO Executor: Finished task 156.0 in stage 8.0 (TID 1173). 1737 bytes result sent to driver
15/08/21 11:26:04 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 1189, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:04 INFO Executor: Running task 172.0 in stage 8.0 (TID 1189)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:04 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 1173) in 25545 ms on localhost (157/200)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO Executor: Finished task 157.0 in stage 8.0 (TID 1174). 1805 bytes result sent to driver
15/08/21 11:26:04 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 1190, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:04 INFO Executor: Running task 173.0 in stage 8.0 (TID 1190)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 1174) in 22359 ms on localhost (158/200)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO Executor: Finished task 158.0 in stage 8.0 (TID 1175). 1940 bytes result sent to driver
15/08/21 11:26:04 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 1191, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:04 INFO Executor: Running task 174.0 in stage 8.0 (TID 1191)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 1175) in 22097 ms on localhost (159/200)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO Executor: Finished task 159.0 in stage 8.0 (TID 1176). 1670 bytes result sent to driver
15/08/21 11:26:04 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 1192, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:04 INFO Executor: Running task 175.0 in stage 8.0 (TID 1192)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 1176) in 21005 ms on localhost (160/200)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:04 INFO Executor: Finished task 160.0 in stage 8.0 (TID 1177). 1602 bytes result sent to driver
15/08/21 11:26:04 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 1193, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:04 INFO Executor: Running task 176.0 in stage 8.0 (TID 1193)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:04 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 1177) in 16201 ms on localhost (161/200)
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:11 INFO Executor: Finished task 161.0 in stage 8.0 (TID 1178). 1670 bytes result sent to driver
15/08/21 11:26:11 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 1194, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:11 INFO Executor: Running task 177.0 in stage 8.0 (TID 1194)
15/08/21 11:26:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:11 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 1178) in 21066 ms on localhost (162/200)
15/08/21 11:26:11 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:12 INFO Executor: Finished task 162.0 in stage 8.0 (TID 1179). 1738 bytes result sent to driver
15/08/21 11:26:12 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 1195, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:12 INFO Executor: Running task 178.0 in stage 8.0 (TID 1195)
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:12 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 1179) in 21114 ms on localhost (163/200)
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:12 INFO Executor: Finished task 163.0 in stage 8.0 (TID 1180). 1670 bytes result sent to driver
15/08/21 11:26:12 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 1196, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:12 INFO Executor: Running task 179.0 in stage 8.0 (TID 1196)
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:12 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 1180) in 20729 ms on localhost (164/200)
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:17 INFO Executor: Finished task 164.0 in stage 8.0 (TID 1181). 1602 bytes result sent to driver
15/08/21 11:26:17 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 1197, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:17 INFO Executor: Running task 180.0 in stage 8.0 (TID 1197)
15/08/21 11:26:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:17 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 1181) in 26093 ms on localhost (165/200)
15/08/21 11:26:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:18 INFO Executor: Finished task 165.0 in stage 8.0 (TID 1182). 1669 bytes result sent to driver
15/08/21 11:26:18 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 1198, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:18 INFO Executor: Running task 181.0 in stage 8.0 (TID 1198)
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:18 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 1182) in 23160 ms on localhost (166/200)
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:18 INFO Executor: Finished task 166.0 in stage 8.0 (TID 1183). 1737 bytes result sent to driver
15/08/21 11:26:18 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 1199, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:18 INFO Executor: Running task 182.0 in stage 8.0 (TID 1199)
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:18 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 1183) in 23089 ms on localhost (167/200)
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:19 INFO Executor: Finished task 167.0 in stage 8.0 (TID 1184). 1738 bytes result sent to driver
15/08/21 11:26:19 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 1200, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:19 INFO Executor: Running task 183.0 in stage 8.0 (TID 1200)
15/08/21 11:26:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:19 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 1184) in 23331 ms on localhost (168/200)
15/08/21 11:26:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:20 INFO Executor: Finished task 168.0 in stage 8.0 (TID 1185). 1668 bytes result sent to driver
15/08/21 11:26:20 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 1201, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:20 INFO Executor: Running task 184.0 in stage 8.0 (TID 1201)
15/08/21 11:26:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:20 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 1185) in 22403 ms on localhost (169/200)
15/08/21 11:26:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:27 INFO Executor: Finished task 169.0 in stage 8.0 (TID 1186). 1942 bytes result sent to driver
15/08/21 11:26:27 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 1202, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:27 INFO Executor: Running task 185.0 in stage 8.0 (TID 1202)
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:27 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 1186) in 28753 ms on localhost (170/200)
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:27 INFO Executor: Finished task 170.0 in stage 8.0 (TID 1187). 1736 bytes result sent to driver
15/08/21 11:26:27 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 1203, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:27 INFO Executor: Running task 186.0 in stage 8.0 (TID 1203)
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:27 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 1187) in 24900 ms on localhost (171/200)
15/08/21 11:26:28 INFO Executor: Finished task 171.0 in stage 8.0 (TID 1188). 1804 bytes result sent to driver
15/08/21 11:26:28 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 1204, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:28 INFO Executor: Running task 187.0 in stage 8.0 (TID 1204)
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:28 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 1188) in 24679 ms on localhost (172/200)
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 11:26:28 INFO Executor: Finished task 172.0 in stage 8.0 (TID 1189). 1736 bytes result sent to driver
15/08/21 11:26:28 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 1205, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:28 INFO Executor: Running task 188.0 in stage 8.0 (TID 1205)
15/08/21 11:26:28 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 1189) in 24178 ms on localhost (173/200)
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:28 INFO Executor: Finished task 173.0 in stage 8.0 (TID 1190). 1738 bytes result sent to driver
15/08/21 11:26:28 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 1206, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:28 INFO Executor: Running task 189.0 in stage 8.0 (TID 1206)
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:28 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 1190) in 24127 ms on localhost (174/200)
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:34 INFO Executor: Finished task 174.0 in stage 8.0 (TID 1191). 1875 bytes result sent to driver
15/08/21 11:26:34 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 1207, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:34 INFO Executor: Running task 190.0 in stage 8.0 (TID 1207)
15/08/21 11:26:34 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 1191) in 30184 ms on localhost (175/200)
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:34 INFO Executor: Finished task 175.0 in stage 8.0 (TID 1192). 2011 bytes result sent to driver
15/08/21 11:26:34 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 1208, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:34 INFO Executor: Running task 191.0 in stage 8.0 (TID 1208)
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:34 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 1192) in 30379 ms on localhost (176/200)
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:35 INFO Executor: Finished task 176.0 in stage 8.0 (TID 1193). 1602 bytes result sent to driver
15/08/21 11:26:35 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 1209, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:35 INFO Executor: Running task 192.0 in stage 8.0 (TID 1209)
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:35 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 1193) in 30420 ms on localhost (177/200)
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:35 INFO Executor: Finished task 177.0 in stage 8.0 (TID 1194). 1670 bytes result sent to driver
15/08/21 11:26:35 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 1210, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:35 INFO Executor: Running task 193.0 in stage 8.0 (TID 1210)
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:35 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 1194) in 23981 ms on localhost (178/200)
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:35 INFO Executor: Finished task 178.0 in stage 8.0 (TID 1195). 1806 bytes result sent to driver
15/08/21 11:26:35 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 1211, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:35 INFO Executor: Running task 194.0 in stage 8.0 (TID 1211)
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:36 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 1195) in 23922 ms on localhost (179/200)
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:36 INFO Executor: Finished task 179.0 in stage 8.0 (TID 1196). 1738 bytes result sent to driver
15/08/21 11:26:36 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 1212, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:36 INFO Executor: Running task 195.0 in stage 8.0 (TID 1212)
15/08/21 11:26:36 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 1196) in 24290 ms on localhost (180/200)
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:37 INFO Executor: Finished task 180.0 in stage 8.0 (TID 1197). 1737 bytes result sent to driver
15/08/21 11:26:37 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 1213, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:37 INFO Executor: Running task 196.0 in stage 8.0 (TID 1213)
15/08/21 11:26:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:37 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 1197) in 19910 ms on localhost (181/200)
15/08/21 11:26:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 11:26:54 INFO Executor: Finished task 181.0 in stage 8.0 (TID 1198). 1736 bytes result sent to driver
15/08/21 11:26:54 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 1214, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:54 INFO Executor: Running task 197.0 in stage 8.0 (TID 1214)
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:54 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 1198) in 35897 ms on localhost (182/200)
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:54 INFO Executor: Finished task 182.0 in stage 8.0 (TID 1199). 1802 bytes result sent to driver
15/08/21 11:26:54 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 1215, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:54 INFO Executor: Running task 198.0 in stage 8.0 (TID 1215)
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:54 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 1199) in 35675 ms on localhost (183/200)
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:54 INFO Executor: Finished task 183.0 in stage 8.0 (TID 1200). 1600 bytes result sent to driver
15/08/21 11:26:54 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 1216, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 11:26:54 INFO Executor: Running task 199.0 in stage 8.0 (TID 1216)
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:54 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 1200) in 35433 ms on localhost (184/200)
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 11:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 11:26:54 INFO Executor: Finished task 184.0 in stage 8.0 (TID 1201). 1737 bytes result sent to driver
15/08/21 11:26:54 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 1201) in 34278 ms on localhost (185/200)
15/08/21 11:26:59 INFO Executor: Finished task 185.0 in stage 8.0 (TID 1202). 1602 bytes result sent to driver
15/08/21 11:26:59 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 1202) in 32380 ms on localhost (186/200)
15/08/21 11:26:59 INFO Executor: Finished task 186.0 in stage 8.0 (TID 1203). 1670 bytes result sent to driver
15/08/21 11:26:59 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 1203) in 31995 ms on localhost (187/200)
15/08/21 11:27:00 INFO Executor: Finished task 187.0 in stage 8.0 (TID 1204). 1602 bytes result sent to driver
15/08/21 11:27:00 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 1204) in 32290 ms on localhost (188/200)
15/08/21 11:27:00 INFO Executor: Finished task 188.0 in stage 8.0 (TID 1205). 1601 bytes result sent to driver
15/08/21 11:27:00 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 1205) in 32335 ms on localhost (189/200)
15/08/21 11:27:01 INFO Executor: Finished task 189.0 in stage 8.0 (TID 1206). 1668 bytes result sent to driver
15/08/21 11:27:01 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 1206) in 32506 ms on localhost (190/200)
15/08/21 11:27:01 INFO Executor: Finished task 190.0 in stage 8.0 (TID 1207). 1941 bytes result sent to driver
15/08/21 11:27:01 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 1207) in 26641 ms on localhost (191/200)
15/08/21 11:27:01 INFO Executor: Finished task 193.0 in stage 8.0 (TID 1210). 1669 bytes result sent to driver
15/08/21 11:27:01 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 1210) in 25623 ms on localhost (192/200)
15/08/21 11:27:01 INFO Executor: Finished task 191.0 in stage 8.0 (TID 1208). 1669 bytes result sent to driver
15/08/21 11:27:01 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 1208) in 26525 ms on localhost (193/200)
15/08/21 11:27:01 INFO Executor: Finished task 192.0 in stage 8.0 (TID 1209). 1669 bytes result sent to driver
15/08/21 11:27:01 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 1209) in 26399 ms on localhost (194/200)
15/08/21 11:27:01 INFO Executor: Finished task 194.0 in stage 8.0 (TID 1211). 1426 bytes result sent to driver
15/08/21 11:27:01 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 1211) in 25739 ms on localhost (195/200)
15/08/21 11:27:01 INFO Executor: Finished task 195.0 in stage 8.0 (TID 1212). 1602 bytes result sent to driver
15/08/21 11:27:01 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 1212) in 25407 ms on localhost (196/200)
15/08/21 11:27:02 INFO Executor: Finished task 196.0 in stage 8.0 (TID 1213). 1670 bytes result sent to driver
15/08/21 11:27:02 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 1213) in 24942 ms on localhost (197/200)
15/08/21 11:27:03 INFO Executor: Finished task 197.0 in stage 8.0 (TID 1214). 1667 bytes result sent to driver
15/08/21 11:27:03 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 1214) in 8847 ms on localhost (198/200)
15/08/21 11:27:03 INFO Executor: Finished task 198.0 in stage 8.0 (TID 1215). 1601 bytes result sent to driver
15/08/21 11:27:03 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 1215) in 8882 ms on localhost (199/200)
15/08/21 11:27:03 INFO Executor: Finished task 199.0 in stage 8.0 (TID 1216). 1874 bytes result sent to driver
15/08/21 11:27:03 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 1216) in 9097 ms on localhost (200/200)
15/08/21 11:27:03 INFO DAGScheduler: ResultStage 8 (processCmd at CliDriver.java:423) finished in 347.231 s
15/08/21 11:27:03 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/21 11:27:03 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2429a105
15/08/21 11:27:03 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 563.959350 s
15/08/21 11:27:03 INFO StatsReportListener: task runtime:(count: 200, mean: 27600.175000, stdev: 6823.737408, max: 43548.000000, min: 8847.000000)
15/08/21 11:27:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:03 INFO StatsReportListener: 	8.8 s	18.1 s	19.4 s	24.1 s	26.2 s	31.4 s	39.0 s	40.1 s	43.5 s
15/08/21 11:27:03 INFO StatsReportListener: fetch wait time:(count: 200, mean: 1.830000, stdev: 2.262985, max: 15.000000, min: 0.000000)
15/08/21 11:27:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:03 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	4.0 ms	5.0 ms	15.0 ms
15/08/21 11:27:03 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 11:27:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:03 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 11:27:03 INFO StatsReportListener: task result size:(count: 200, mean: 1735.070000, stdev: 135.216179, max: 2150.000000, min: 1426.000000)
15/08/21 11:27:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:03 INFO StatsReportListener: 	1426.0 B	1426.0 B	1602.0 B	1669.0 B	1737.0 B	1805.0 B	1940.0 B	1943.0 B	2.1 KB
15/08/21 11:27:03 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 99.680861, stdev: 1.163693, max: 99.941338, min: 87.882493)
15/08/21 11:27:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:03 INFO StatsReportListener: 	88 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 11:27:03 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.006836, stdev: 0.008296, max: 0.057890, min: 0.000000)
15/08/21 11:27:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:03 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 11:27:03 INFO StatsReportListener: other time pct: (count: 200, mean: 0.312304, stdev: 1.164250, max: 12.114524, min: 0.058662)
15/08/21 11:27:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:03 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	12 %
15/08/21 11:27:03 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 11:27:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:27:03 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 11:27:03 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 11:27:03 INFO DAGScheduler: Final stage: ResultStage 9(processCmd at CliDriver.java:423)
15/08/21 11:27:03 INFO DAGScheduler: Parents of final stage: List()
15/08/21 11:27:03 INFO DAGScheduler: Missing parents: List()
15/08/21 11:27:03 INFO DAGScheduler: Submitting ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:27:03 INFO MemoryStore: ensureFreeSpace(71408) called with curMem=1422078, maxMem=22226833244
15/08/21 11:27:03 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 69.7 KB, free 20.7 GB)
15/08/21 11:27:03 INFO MemoryStore: ensureFreeSpace(26160) called with curMem=1493486, maxMem=22226833244
15/08/21 11:27:03 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.5 KB, free 20.7 GB)
15/08/21 11:27:03 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:53620 (size: 25.5 KB, free: 20.7 GB)
15/08/21 11:27:03 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/21 11:27:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423)
15/08/21 11:27:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/21 11:27:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 1217, localhost, PROCESS_LOCAL, 8222 bytes)
15/08/21 11:27:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 1217)
15/08/21 11:27:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 11:27:03 INFO CodecConfig: Compression: GZIP
15/08/21 11:27:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 11:27:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 11:27:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 11:27:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 11:27:03 INFO ParquetOutputFormat: Validation is off
15/08/21 11:27:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 11:27:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 11:27:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 40,692,456
15/08/21 11:27:03 INFO ColumnChunkPageWriteStore: written 615B for [c_name] BINARY: 100 values, 2,207B raw, 551B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:27:03 INFO ColumnChunkPageWriteStore: written 455B for [c_custkey] INT32: 100 values, 407B raw, 419B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:27:03 INFO ColumnChunkPageWriteStore: written 466B for [o_orderkey] INT32: 100 values, 407B raw, 430B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:27:03 INFO ColumnChunkPageWriteStore: written 441B for [o_orderdate] BINARY: 100 values, 1,407B raw, 393B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:27:03 INFO ColumnChunkPageWriteStore: written 622B for [o_totalprice] DOUBLE: 100 values, 807B raw, 578B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
15/08/21 11:27:03 INFO ColumnChunkPageWriteStore: written 141B for [sum_quantity] DOUBLE: 100 values, 74B raw, 97B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 18 entries, 144B raw, 18B comp}
15/08/21 11:27:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508211127_0009_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_temporary/0/task_201508211127_0009_m_000000
15/08/21 11:27:04 INFO SparkHadoopMapRedUtil: attempt_201508211127_0009_m_000000_0: Committed
15/08/21 11:27:04 INFO Executor: Finished task 0.0 in stage 9.0 (TID 1217). 577 bytes result sent to driver
15/08/21 11:27:04 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 1217) in 255 ms on localhost (1/1)
15/08/21 11:27:04 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/21 11:27:04 INFO DAGScheduler: ResultStage 9 (processCmd at CliDriver.java:423) finished in 0.255 s
15/08/21 11:27:04 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5420cddd
15/08/21 11:27:04 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 0.325411 s
15/08/21 11:27:04 INFO StatsReportListener: task runtime:(count: 1, mean: 255.000000, stdev: 0.000000, max: 255.000000, min: 255.000000)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	255.0 ms	255.0 ms	255.0 ms	255.0 ms	255.0 ms	255.0 ms	255.0 ms	255.0 ms	255.0 ms
15/08/21 11:27:04 INFO StatsReportListener: task result size:(count: 1, mean: 577.000000, stdev: 0.000000, max: 577.000000, min: 577.000000)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B
15/08/21 11:27:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 69.803922, stdev: 0.000000, max: 69.803922, min: 69.803922)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	70 %	70 %	70 %	70 %	70 %	70 %	70 %	70 %	70 %
15/08/21 11:27:04 INFO StatsReportListener: other time pct: (count: 1, mean: 30.196078, stdev: 0.000000, max: 30.196078, min: 30.196078)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	30 %	30 %	30 %	30 %	30 %	30 %	30 %	30 %	30 %
15/08/21 11:27:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:27:04 INFO DefaultWriterContainer: Job job_201508211127_0000 committed.
15/08/21 11:27:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 11:27:04 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_common_metadata
15/08/21 11:27:04 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 11:27:04 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 11:27:04 INFO DAGScheduler: Final stage: ResultStage 10(processCmd at CliDriver.java:423)
15/08/21 11:27:04 INFO DAGScheduler: Parents of final stage: List()
15/08/21 11:27:04 INFO DAGScheduler: Missing parents: List()
15/08/21 11:27:04 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 11:27:04 INFO MemoryStore: ensureFreeSpace(3104) called with curMem=1519646, maxMem=22226833244
15/08/21 11:27:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.0 KB, free 20.7 GB)
15/08/21 11:27:04 INFO MemoryStore: ensureFreeSpace(1802) called with curMem=1522750, maxMem=22226833244
15/08/21 11:27:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1802.0 B, free 20.7 GB)
15/08/21 11:27:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:53620 (size: 1802.0 B, free: 20.7 GB)
15/08/21 11:27:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874
15/08/21 11:27:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/21 11:27:04 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
15/08/21 11:27:04 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1218, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 11:27:04 INFO Executor: Running task 0.0 in stage 10.0 (TID 1218)
15/08/21 11:27:04 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1218). 606 bytes result sent to driver
15/08/21 11:27:04 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1218) in 12 ms on localhost (1/1)
15/08/21 11:27:04 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/21 11:27:04 INFO DAGScheduler: ResultStage 10 (processCmd at CliDriver.java:423) finished in 0.013 s
15/08/21 11:27:04 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@6483370a
15/08/21 11:27:04 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 0.028641 s
15/08/21 11:27:04 INFO StatsReportListener: task runtime:(count: 1, mean: 12.000000, stdev: 0.000000, max: 12.000000, min: 12.000000)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	12.0 ms	12.0 ms	12.0 ms	12.0 ms	12.0 ms	12.0 ms	12.0 ms	12.0 ms	12.0 ms
Time taken: 566.011 seconds
15/08/21 11:27:04 INFO CliDriver: Time taken: 566.011 seconds
15/08/21 11:27:04 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 11:27:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 8.333333, stdev: 0.000000, max: 8.333333, min: 8.333333)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %
15/08/21 11:27:04 INFO StatsReportListener: other time pct: (count: 1, mean: 91.666667, stdev: 0.000000, max: 91.666667, min: 91.666667)
15/08/21 11:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 11:27:04 INFO StatsReportListener: 	92 %	92 %	92 %	92 %	92 %	92 %	92 %	92 %	92 %
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/21 11:27:04 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/21 11:27:04 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/21 11:27:04 INFO DAGScheduler: Stopping DAGScheduler
15/08/21 11:27:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/21 11:27:07 INFO Utils: path = /tmp/spark-8c304fc0-a8bf-4fd0-afbc-c3818c7aa7bf/blockmgr-344df924-f7f3-4e14-b551-a5542d88a00d, already present as root for deletion.
15/08/21 11:27:07 INFO MemoryStore: MemoryStore cleared
15/08/21 11:27:07 INFO BlockManager: BlockManager stopped
15/08/21 11:27:07 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/21 11:27:07 INFO SparkContext: Successfully stopped SparkContext
15/08/21 11:27:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/21 11:27:07 INFO Utils: Shutdown hook called
15/08/21 11:27:07 INFO Utils: Deleting directory /tmp/spark-8c304fc0-a8bf-4fd0-afbc-c3818c7aa7bf
15/08/21 11:27:07 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/21 11:27:07 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/21 11:27:08 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/08/21 11:27:11 INFO Utils: Deleting directory /tmp/spark-eb9aedf1-8fab-4028-9769-23f52a2b0217
15/08/21 11:27:11 INFO Utils: Deleting directory /tmp/spark-de06bc57-0f6b-4f9e-adf6-32a2a52d30ca
